{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P2C1_CurveFitting/instructor/P2C1_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_CurveFitting/instructor/P2C1_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **2.1.2: Curve Fitting Through the Delta Rule**\n",
    "In the previous seqeunce we saw how a simple binary discrimination task could be solved by training a simple 'neural circuit' model using scalar reward feedback. Scalar reward feedback was sufficient for this simple problem, but as we will see in this sequence, as the discrimination task becomes more challenging, learning becomes problematically slow when driven solely by reward. To effectively train our neural circuit model we will need to update our parameters more efficiently. With perturbation, we were using a measure and see approach. In this sequence we will instead leverage our knowledge of the structure of the circuit to make more effient updates, specifically targeting those parameters of the model that were causally implicated in the output for adaption. Note that these updates will still be driven purely by a reward signal, i.e. the organism will still have no clear indication about what the correct action was when it fails.\n",
    "\n",
    "### Objective: See how reward driven perturbation is problematically slow on a more complicated problem. Solve this slightly more complicated problem using the \"Delta Rule\". See how how using knowledge of the structure of the model let's us make far more effcient parameter updates. Connect these methods with physiological neural plasticity mechanisms.\n",
    "\n",
    "In this sequence we will:\n",
    "\n",
    "* Introduce a 10-category classification tasks, i.e. a task where stimuli indicate which specific action, from a set of 10 possible actions, should be executed to obtain a reward.\n",
    "\n",
    "* See how the simple 'neural circuit' model which 'learned' to solve a binary discrimination problems based on reward feedback, is incapable of solving this more complext problem in a reasonable amount of time.\n",
    "\n",
    "* See how a slightly adapted 'neural circuit' model which uses knowledge of the model structure for efficeint parameter updates can solve this more complex problem in a reasonable amount of time.\n",
    "\n",
    "* Relate these structure based updates to various neural plasticity mechanisms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz ucimlrepo > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "data_set = fetch_ucirepo(id=80)\n",
    "X = data_set.data.features.values\n",
    "# Translate the data to have a minimum of 0\n",
    "X_translated = X - X.min()\n",
    "# Scale the data to have a range from 0 to 12 (which is 6 - (-6))\n",
    "scaling_factor = 12 / (X.max() - X.min())\n",
    "X_scaled = X_translated * scaling_factor\n",
    "# Finally, shift the data to be centered between -6 and 6\n",
    "X_final = X_scaled - 6\n",
    "\n",
    "y = data_set.data.targets.values\n",
    "rng = np.random.default_rng(seed=2021)\n",
    "scramble_permutation = rng.permutation(X.shape[1])\n",
    "Xs = X_final[:, scramble_permutation]\n",
    "y1 = y % 2\n",
    "y2 = np.array(y >= 5, dtype=y.dtype)\n",
    "simple_index = ((y.flatten()==1) | (y.flatten()==0))\n",
    "X_simple = Xs[simple_index]\n",
    "y1_simple = y1[simple_index]\n",
    "# if you only had one feature which would likely be best for discrimination\n",
    "epsilon = 10\n",
    "class_a_sep = np.mean(X_simple[y1_simple.flatten() == 1, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 1, :], axis=0) + epsilon)\n",
    "class_b_sep = np.mean(X_simple[y1_simple.flatten() == 0, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 0, :], axis=0) + epsilon)\n",
    "best_feature = np.argmax(class_a_sep - class_b_sep)\n",
    "print(f'Best feature is {best_feature}')\n",
    "X_simple_1_feature = X_simple[:, [best_feature]]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P2C1_S1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.2.1: Learning to Do 10 Different Right Things\n",
    "\n",
    "In our the last sequence we motivated a binary classification problem with the image of a lurking predator that must choose between striking and not striking based on sensory input. We focused on the case where the behavioural map from sensory input to action was to be entirely 'learned' by the organism, using feedback signals from the environment. In this sequence we will work with an almost identical problem, except that now instead of two possible actions, strike and no-strike, we deal with 10 possible distinct actions. Different sensory input patterns will indicate which of the 10 possible distinct actions yeild a reward. We can imagine these highly abstracted actions corresponding to different prey capture and/or predator avoidance behaviours. The organism must match the correct behaviour to the correct stimuli to eat and avoid being eaten. Before we build a highly abstracted model of a neural circuit to solve this problem, let's see if your neural network is up to the task. Given a sensory input pattern, determine which of ten actions to chose. Try to maximize your average score and see how well you can perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to try out the 10-action discrimination task.\n",
    "\n",
    "class InteractiveMNISTCritter():\n",
    "  def __init__(self,\n",
    "               features=Xs,\n",
    "               labels=y,\n",
    "               num_actions=2,\n",
    "               feedback_type='reward_only', seed=123):\n",
    "    # Initialize dataset, settings for image scrambling and feedback\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    "    self.num_actions = num_actions\n",
    "    # Ensure num_actions matches the number of unique labels\n",
    "    unique_labels = len(np.unique(self.labels))\n",
    "    if self.num_actions != unique_labels:\n",
    "        raise ValueError(f\"The number of actions ({self.num_actions}) must equal the number of unique labels ({unique_labels}).\")\n",
    "    # features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)\n",
    "    # labels is num_data_points x 1 (values 0-9 or 0/1 depending)\n",
    "    self.feedback_type = feedback_type\n",
    "    self.rng = np.random.default_rng(seed)\n",
    "    sample_order = self.rng.permutation(self.features.shape[0])\n",
    "    self.features = self.features[sample_order]\n",
    "    self.labels = self.labels[sample_order]\n",
    "    # initialize game state\n",
    "    self.current_index = 0\n",
    "    self.score = 0\n",
    "    self.best_possible_score = 0\n",
    "    self.successful_actions = np.zeros(self.num_actions)\n",
    "    self.failed_actions = np.zeros(self.num_actions)\n",
    "    # Initialize widgets\n",
    "    self.action_buttons = [] # List to store buttons\n",
    "    for action_idx in range(self.num_actions):\n",
    "        # Create a button for each action\n",
    "        button = widgets.Button(description=f'Action {action_idx}')\n",
    "        button.on_click(self.on_button_click)\n",
    "        self.action_buttons.append(button)\n",
    "    self.score_display = widgets.Output()\n",
    "    self.feedback_display = widgets.Output()\n",
    "\n",
    "    # Initialize the figure for image display\n",
    "    self.fig, self.ax = plt.subplots(figsize=(4, 4))\n",
    "    remove_ip_clutter(self.fig)\n",
    "    self.show_next_image()\n",
    "\n",
    "    # Arrange widgets in a layout\n",
    "\n",
    "    button_split_index = (num_actions + 1) // 2  # This ensures the longer row is on top if odd number\n",
    "    row1_buttons = widgets.HBox(self.action_buttons[:button_split_index])\n",
    "    row2_buttons = widgets.HBox(self.action_buttons[button_split_index:])\n",
    "    buttons_layout = widgets.VBox([row1_buttons, row2_buttons])\n",
    "    board_buttons = widgets.VBox([self.fig.canvas, buttons_layout])\n",
    "    self.ui = widgets.HBox([board_buttons, widgets.VBox([self.score_display,\n",
    "                                                         self.feedback_display])])\n",
    "\n",
    "  def show_next_image(self):\n",
    "    # Display the next image\n",
    "    image = self.features[self.current_index]\n",
    "    if len(image) == 64:\n",
    "        image = image.reshape(8, 8)\n",
    "    elif len(image) == 1:\n",
    "      scalar_value = image.flatten()[0]\n",
    "      # Initialize the 8x8 array with -6 (black)\n",
    "      image = np.full((8, 8), -6.0)\n",
    "      # Set the second ring to 6 (white)\n",
    "      image[1:-1, 1:-1] = 6\n",
    "      # Set the third (inner ring) back to -6 (black)\n",
    "      image[2:-2, 2:-2] = -6\n",
    "      # Assuming scalar_value is already in the range -6 to 6\n",
    "      #print(scalar_value)\n",
    "      image[3:-3, 3:-3] = scalar_value\n",
    "    else:\n",
    "      raise ValueError(f'Unexpected image shape: {image.shape}')\n",
    "    # Display the image\n",
    "    #print(image)\n",
    "    self.fig.clf()\n",
    "    self.ax = self.fig.add_subplot(111)\n",
    "    self.ax.set_xlim(-.5, 7.5)\n",
    "    self.ax.set_ylim(-0.5, 7.5)\n",
    "    self.ax.set_aspect('equal')\n",
    "    self.ax.axis('off')\n",
    "    self.ax.imshow(image, cmap='gray', vmin=-6, vmax=6)\n",
    "    self.fig.canvas.draw_idle()  # Force redraw\n",
    "\n",
    "  def on_button_click(self, button):\n",
    "    # Disable buttons while processing\n",
    "    for btn in self.action_buttons:\n",
    "      btn.disabled = True\n",
    "    # Access the button's description to know which button was clicked\n",
    "    chosen_action = int(button.description.split(' ')[-1])\n",
    "    correct_action = self.labels[self.current_index]\n",
    "    if chosen_action == correct_action:\n",
    "      self.score += 1\n",
    "      self.successful_actions[chosen_action] += 1\n",
    "    else:\n",
    "      self.score -= 1\n",
    "      self.failed_actions[chosen_action] += 1\n",
    "    # Show text feedback\n",
    "    was_correct = 'Correct' if chosen_action == correct_action else 'Incorrect'\n",
    "    feedback = f'Your last choice: {chosen_action}\\n{was_correct}'\n",
    "    with self.feedback_display:\n",
    "      clear_output(wait=True)\n",
    "      print(feedback)\n",
    "    # Show score\n",
    "    with self.score_display:\n",
    "      clear_output(wait=True)\n",
    "      average_score = self.score / (self.current_index+1)\n",
    "      print(f'Total Score: {self.score}')\n",
    "      print(f'Number of Trials: {self.current_index + 1}')\n",
    "      print(f'Successful Actions: {self.successful_actions}')\n",
    "      print(f'Failed Actions: {self.failed_actions}')\n",
    "      print(f'Average Score Per Trial: {average_score:.2f}')\n",
    "    # Prepare the next image\n",
    "    self.current_index += 1\n",
    "    self.show_next_image()\n",
    "\n",
    "    # Re-enable buttons\n",
    "    for btn in self.action_buttons:\n",
    "      btn.disabled = False\n",
    "\n",
    "\n",
    "scramble_10 = InteractiveMNISTCritter(features=Xs, labels=y, num_actions=10)\n",
    "display(scramble_10.fig.canvas)\n",
    "clear_output()\n",
    "display(scramble_10.ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For us at least, this was basically impossible. Maybe though, we will be able to come up an algorithm that can learn these associations. As before the dataset that underlies this strike-no-strike decision problem is sourced from the UCI Machine Learning Repository, (Alpaydin,E. and Kaynak,C. 1998. https://doi.org/10.24432/C50P49). Let's take a quick look at the data before we start coming up with a simple machine learning algorithm to correctly distinguish between the strike and no-strike cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# What is the shape and type of the data we have to work with\n",
    "print(f'features data type: {Xs.dtype}')\n",
    "print(f'features shape: {Xs.shape}')\n",
    "print(f'labels data type: {y.dtype}')\n",
    "print(f'labels shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# what do the labels look like, what is their range\n",
    "print(y[:10])\n",
    "print(f'max label: {np.max(y)}')\n",
    "print(f'min label: {np.min(y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "***Recall:*** In a Supervised Learning context what we have been thinking of as the inputs to a sensory-behaviour circuit are called **features**. In a statistical/regression/experimental context these features or inputs are thought of as the independent variables or regressors. The 'correct' behavioural output is called the **label** or **target** in a supervised learning ML context, or the dependent variable in a statistical/regression/experimental context. Supervised Learning revolves around the concept of feature-label or $(\\mathbf{x},\\mathbf{y})$ pairs. The basic goal of Supervised Learning is to discover the rule or process that generates these pairs, or a good approximation of it, solely by examining examples. The set of examples that we have to learn the rule from is called the **training set**. A reminder that we will use the terms features and label when we want to emphasize the abstract algorithm and learning problem, and the terms sensory-input and target behaviour when we want to emphasize the embodied neural context of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the game we setup above we used the labels to correspond to the numbered actions. Note that we only recieved feedback about whether or not we had chosen the correct action, and not about what the correct action should have been. So even though the labels in this data set determine which action is rewarding, learning is still only based on reward, the teaching signal does not indicate which action would have been correct when an incorrect action is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# what do the features look like, what is there range\n",
    "print(Xs[:10])\n",
    "print(f'max features: {np.max(Xs)}')\n",
    "print(f'min features: {np.min(Xs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the visualization above a high 'feature value' corresponded to lighter colors in the center pixels and a lower feature value corresponded to darker feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# and for scalar data always good to look at a histogram\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.hist(Xs[y.flatten() == 0][3])\n",
    "ax1.set_title('Feature Distribution of 33rd Feature When Correct Action is 0')\n",
    "ax2.hist(Xs[y.flatten() == 9][33])\n",
    "ax2.set_title('Feature Distribution of 33rd Feature When Correct Action is 9')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Looking at the distribution of this one particular feature across these two particular classes (Action 0 is correct vs. action 9 is correct), we see that from this feature alone we are unlikely to be be able to correctly discriminate between these two classes. Other features, and possibly combinations of features will be required for successful classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So this problem is too hard for our eyes and brains, but let's see if our perturbation learning model is able to learn this classification. Now that we have 10 possible output actions we are going to have to change the structure of our model slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We still use a simple organism as the inspiration for this model. One that has an array of sensory input neurons corresponding to the features, which are in turn connected to a layer of (highly abstracted) motor output neurons, with each neuron in the outpur layer corresponding to one specific action. We imagine that the activation level of these output neurons roughly corresponds to a firing rate, and that the action actually taken is determined by which of these actions neurons fires first... It's a race! (Box on poisson process model of firing rates and poisson races).\n",
    "\n",
    "We model this creature's sensory-behaviour system as follows. Let $\\mathbf{x}$ be the raw sensory input (vector) in a given episode. We imagine that $\\mathbf{x}$ corresponds to the activation level and firing rate of each photosensitive sensory neuron. These input neurons are then connected by a fully connected network of synapse to each of the output neurons. The activation level of the output neurons is computed as\n",
    "$$\\mathbf{y} = \\mathbf{Wx} + \\mathbf{b}$$\n",
    "Here, $\\mathbf{b}$ is a vector giving the bias, or baseline activation level of each of the output neurons and $\\mathbf{W}$ is the strength of the synaptic weights between the input neurons and the output neurons, represented as a $\\text{(number of actions)} \\times \\text{(number of features)}$ matrix. To streamline notation, exposition, and algorithm implementation we will typically \"hide\" the bias term $\\mathbf{b}$ by augmenting the base feature set with a new constant valued feature. Then, the baises become the corresponding column of $\\mathbf{W}$. (A quick notation reminder: bold lowercase letters typically represent column vectors, bold uppercase letters typically denote matrices or higher-order tensors.)\n",
    "\n",
    "We then use softmax normalization to transform these activations into a probability distribution over the possible actions. Recall that softmax normalization of a vector is defined as\n",
    "$$\\text{softmax}(y_i) := \\frac{e^{y_i}}{\\sum_{j} e^{y_j}}$$\n",
    "\n",
    "So the probability that action $i$ is taken given the softmax normalized value of the $i^{th}$ component of $\\mathbf{y}$, that is $\\text{softmax}(y_i)$. Thus the weights $\\mathbf{W}$ define the organism's policy under this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "--Picture of this very simple neural circuit here--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Having established how behaviour is determined by sensory input, $\\mathbf{x}$, and parameters $\\mathbf{W}$ i.e. our policy, we a rule for modifying this policy based on environmental feedback.\n",
    "\n",
    "As in the previous sequence our goal is still to come up with a rule that prescribes changes to the synaptic weights $\\mathbf{W}$ in such a way that the reward obtained over time increases (and ideally approaches the theoretical maximum over many learning trials). Let's see how our mini-batched measure and update rule works on this problem. The measure and update or weight perturbation update rule is as follows.\n",
    "$$ \\text{Parameter Update} = \\alpha \\cdot \\frac{\\text{Measured Perturbation in Performance}}{\\text{Perturbation in Parameters}}$$\n",
    "\n",
    "where $\\alpha$ is some constant of proportionality, in this case usually called the learning rate or step-size meta-parameter of the learning algorithm.\n",
    "\n",
    "Let's try it and see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Before augmentation this is the shape of the feature set\n",
    "print(f'Shape of features before augmentation: {Xs.shape}')\n",
    "Xs.shape\n",
    "# After augmentation there is one extra column of features\n",
    "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
    "print(f'Shape of features after augmentation: {Xs_aug.shape}')\n",
    "\n",
    "# We also want to encode the labels as 1-hots and also the reward implications\n",
    "# of each action for each stimuli to streamline the evaluations\n",
    "y_one_hot = np.eye(10)[y.flatten()]\n",
    "y_rewards = (2 * y_one_hot) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "y_one_hot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "y_rewards[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def np_sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def np_softmax(x):\n",
    "  x_shift = x - np.max(x, axis=0)\n",
    "  return np.exp(x_shift) / np.sum(np.exp(x_shift), axis=0)\n",
    "\n",
    "def eval_params(W, x, y):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(10) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "       (can be single input, mini-batch of inputs or the whole batch of inputs)\n",
    "    y: (outputs(10) x batch np.array) target behavioural output as a vector\n",
    "       of the rewards recieved for each action taken given the x value\n",
    "       (can be a single target, mini-batch of targets, or whole batch),\n",
    "       needs to correspond to input\n",
    "\n",
    "  Returns:\n",
    "    R_bar: the average/expected reward obtained given the parameters, over the\n",
    "           (mini-)batch of inputs and targets. (mini-batch could be size 1)\n",
    "  \"\"\"\n",
    "  h = np.dot(W,x) # output x batch\n",
    "  y_hat = np_softmax(h) # outnput x batch\n",
    "  batch_expected_reward = np.sum(y * y_hat, axis=0)\n",
    "  R_bar = np.mean(batch_expected_reward)\n",
    "  return R_bar\n",
    "\n",
    "\n",
    "def eval_params_accuracy_percision_recall(W, x, y):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(10) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "       (can be single input, mini-batch of inputs or the whole batch of inputs)\n",
    "    y: (outputs(10) x batch np.array) target behavioural output as a 1-hot\n",
    "       encoding the correct behaviour\n",
    "       (can be a single target, mini-batch of targets, or whole batch),\n",
    "       needs to correspond to input\n",
    "\n",
    "  Returns:\n",
    "    accuracy: percentage of correct classifications in the batch\n",
    "    percision: # correct deployments of action / # total deployments of action\n",
    "    recall: # correct deployments of action / # total cases when that action was correct\n",
    "  \"\"\"\n",
    "  h = np.dot(W,x) # output x batch\n",
    "  selected_indices = np.argmax(h, axis=0) # output x batch\n",
    "  selected = np.zeros_like(h)\n",
    "  selected[selected_indices, range(h.shape[1])] = 1\n",
    "  confusion = np.dot(y, selected.T) # outputs x outputs\n",
    "  # row sums are true classifications, col sums are selected classifications\n",
    "  accuracy = np.sum(np.diag(confusion)) / np.sum(confusion) if np.sum(confusion) > 0 else 0\n",
    "  class_precision = np.divide(np.diag(confusion), np.sum(confusion, axis=1),\n",
    "                              out=np.zeros_like(np.diag(confusion), dtype=float),\n",
    "                              where=np.sum(confusion, axis=1)!=0)\n",
    "  class_recall = np.divide(np.diag(confusion), np.sum(confusion, axis=0),\n",
    "                           out=np.zeros_like(np.diag(confusion), dtype=float),\n",
    "                           where=np.sum(confusion, axis=0)!=0)\n",
    "  return accuracy, class_precision, class_recall, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "Xs_aug.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "5620 / 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "W = np.zeros((10,65))\n",
    "%timeit eval_params(W, Xs_aug.T, y_rewards.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "%timeit eval_params(W, Xs_aug[:562,:].T, y_rewards[:562].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Don't know if this isn't working because the algo is too slow or the model is not rich enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Mini-Batched Measure and Update Training Loop\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 75\n",
    "alpha = 0.1 #learning rate or step size\n",
    "num_batches = 20  # Number of mini-batches\n",
    "batch_size = 281  # Size of each mini-batch\n",
    "a = np.sqrt(6. / (10+65))\n",
    "perturbation_scale = 2*a # std of gaussian perturbations\n",
    "W_init = learn_rng.uniform(low=-a, high=a, size=(10,65))\n",
    "W = W_init.copy()\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)  # Shuffle the indices for each epoch\n",
    "  for batch in range(num_batches):\n",
    "    # Select a mini-batch for this iteration\n",
    "    batch_indices = indices[batch * batch_size : (batch + 1) * batch_size]\n",
    "    X_batch = Xs_aug[batch_indices, :].T\n",
    "    y_batch = y_rewards[batch_indices].T\n",
    "    R_bar_old = eval_params(W, X_batch, y_batch)\n",
    "    W_perturbations = learn_rng.normal(0, perturbation_scale, size=(10,65))\n",
    "    #perturb and evaluate each W_j separately\n",
    "    finite_differences_W = np.zeros(W.shape)\n",
    "    for ii in range(W_perturbations.shape[0]):\n",
    "      for jj in range(W_perturbations.shape[1]):\n",
    "        original_value = W[ii,jj]\n",
    "        W[ii, jj] += W_perturbations[ii, jj]\n",
    "        R_bar_perturbed_Wij = eval_params(W, X_batch, y_batch)\n",
    "        finite_differences_W[ii,jj] = (R_bar_perturbed_Wij - R_bar_old) / W_perturbations[ii,jj]\n",
    "        W[ii,jj] = original_value  # Revert the perturbation\n",
    "\n",
    "    delta_W = alpha * finite_differences_W\n",
    "    W += delta_W\n",
    "\n",
    "  if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "    accuracy, _, _, _ = eval_params_accuracy_percision_recall(W, Xs_aug.T, y_one_hot.T)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Full Batch Accuracy: {accuracy:.6f} | Time elapsed: {elapsed_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Alot depends on random initialization here so important not to draw conclusions from a single training run, this is the average learning trajectory of the above loop over twenty runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "accuracy, prec, recall, confusion = eval_params_accuracy_percision_recall(W, Xs_aug.T, y_one_hot.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "By looking at the confusion matrix we can see that on this first run at least, this model learned to do a pretty good job correctly paring each of the 10 possible actions with the correct class of stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So it's able to learn to discriminate between all the different classes, in about 3 minutes. That's pretty good, but could it be fasters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.2.2 Learning to do the right thing using the Delta Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Looking at the code block above this is because we must loop through every single parameter to make updates and there are 10 x 65 = 650 of these, so there are 650 evaluations just to do one full update of the parameters, so great that this works but it is so slow.\n",
    "\n",
    "Now we had a similar number of units when we introduced a 10 unit hidden layer (10 x 65 + 10 x 1 = 660) to our binary discrimination task, and that performed relatively well. But that was also a much simpler problem.\n",
    "\n",
    "Let's see if we can leverage what we know about the structure of the model to make more efficient parameter updates. We focus on how to update params based on the experience of a single training example at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "A reward is recieved, if it is positive, this was good and in general we should reinforce the parameters by an amount that is proportional to their causal impact on bringing about the action choice that led to this reward. Similarly, if that reward is negative we should un-reinforce the parameters by an amount that is proportional to their causal impact on bringing about the action choice that led to this reward. Here is how we will achieve this.\n",
    "\n",
    "We are going to treat the $\\mathbf{y}$ as estimates of the reward that will be recieved when taking the corresponding action. (Then, the softmax normalization is how the critter translates these estimates of expected reward into a probability over actions). Taking this view, the organism only recieves information about the reward deriving from the particular action it took. If the reward is higher than what it expected the organism should update its parameters in a way that increases the expected reward when taking this action given the particular stimuli $\\mathbf{x}$ conversely if the reward is lower than expected the organism should update its parameters in a way that decreases the expected reward when taking this action given this particular stimuli. One of the simplest possible update rules that achieves this is the 'Delta Rule' or sometimes called the Widrow-Hoff rule or Least Mean Square (LMS) method\n",
    "\n",
    "$$\\delta \\mathbf{W}_{ij} =\n",
    "\\begin{cases}\n",
    "\\alpha \\cdot (r - \\mathbf{y}_i) \\cdot x_j & \\text{if } i = a, \\\\\n",
    "0 & \\text{if } i \\neq a.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{y}_i$ is the $i^{th}$ element of $\\mathbf{y}$ and $\\mathbf{x}_j$ is the $j^{th}$ element of $\\mathbf{x}$. Notice that only the weights directly responsible for estimating the reward from the chosen action are updated.\n",
    "\n",
    "Many of the learning rules we encounter will have this basic form: The update of a parameter will be proportional to some error signal or divergence between an expected/predicted/anticipated value and an observed value, in this case $y_i$ gives the expected reward when taking action $a=i$ conditional on stimuli/input pattern $\\mathbf{x}$ and $r$ is the actual reward recieved so $(r-\\mathbf{y}_i)$ is that error term. The parameter update will also be proportional to the causal relevance of that term for the error, in this case this is captured by the $\\mathbf{x}_j$ term. If $\\mathbf{x}_j$ was zero in this case $\\mathbf{W}_{ij}$ would have had no bearing on $\\mathbf{y}_i$ and so there would be no update. When $x_j$ is non-zero the sign (+/-) of $\\mathbf{x}_j$ aligns the parameter change correctly with the type of error. So that $\\mathbf{W}_ij$ increases both when the predicted reward is less than the actual with $\\mathbf{x}_j$ positive and when the predicted reward is greater than the actual with $\\mathbf{x}_j$ negative; and conversely $\\mathbf{W}_ij$ decreases both when the predicted reward is less than the actual with $\\mathbf{x}_j$ negative and when the predicted reward is greater than the actual with $\\mathbf{x}_j$ positve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now let's program a training loop that uses this new update rule. First we need to code up an explicit action selection function. Previously, in our eval params function we were simply taking expected values, given the organism's probabilities of taking different actions. Now, however our update rule really only makes sense when a particular actions is taken and a particular reward is recieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def select_action(W, x, rng=None, temp=1, type='deterministic', ):\n",
    "  \"\"\"\n",
    "  Chooses actions based on a simple behaviour circuit, with parameters W\n",
    "  given inputs x. Choice is deterministic or stochastic as determined by type.\n",
    "  If stochastic we use softmax selection with temp and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(10) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "       (can be single input, mini-batch of inputs or the whole batch of inputs)\n",
    "    temp: scalar temperature parameter for softmax selection, low values make it\n",
    "          closer to deterministic, high values make it closer to uniform random\n",
    "          selection\n",
    "    type: 'deterministic' or 'stochastic'\n",
    "  Returns:\n",
    "    selected_action_index, selected_action_probabilities, selected_aciton_one_hot\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(0)\n",
    "  v = np.dot(W, x) # (output x batch)\n",
    "  if type == 'deterministic':\n",
    "    selected_indices = np.argmax(v, axis=0) # output x batch\n",
    "  elif type == 'stochastic':\n",
    "    v_shift = v - np.max(v, axis=0)\n",
    "    selection_probs = np.exp(v_shift / temp) / np.sum(np.exp(v_shift / temp), axis=0)\n",
    "    cs_selection_probs = np.cumsum(selection_probs, axis=0)\n",
    "    rands = rng.random(size=(1, v.shape[1]))\n",
    "    is_greater = cs_selection_probs > rands\n",
    "    selected_indices = np.argmax(is_greater, axis=0)\n",
    "  else:\n",
    "    raise ValueError(f'Unknown type {type}')\n",
    "  selected = np.zeros_like(v)\n",
    "  selected[selected_indices, range(v.shape[1])] = 1\n",
    "  return selected_indices, selected, v\n",
    "\n",
    "def eval_params(W, x, y, use_expectation=True):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(10) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "       (can be single input, mini-batch of inputs or the whole batch of inputs)\n",
    "    y: (outputs(10) x batch np.array) target behavioural output as a vector\n",
    "       of the rewards recieved for each action taken given the x value\n",
    "       (can be a single target, mini-batch of targets, or whole batch),\n",
    "       needs to correspond to input\n",
    "\n",
    "  Returns:\n",
    "    R_bar: the average/expected reward obtained given the parameters, over the\n",
    "           (mini-)batch of inputs and targets. (mini-batch could be size 1)\n",
    "  \"\"\"\n",
    "  h = np.dot(W,x) # output x batch\n",
    "  y_hat = np_softmax(h) # outnput x batch\n",
    "  if use_expectation:\n",
    "    batch_expected_reward = np.sum(y * y_hat, axis=0)\n",
    "    R_bar = np.mean(batch_expected_reward)\n",
    "  else:\n",
    "    selected_indices, selected, v = select_action(W, x, temp=1, type='stochastic')\n",
    "    actual_rewards = y[selected_indices, range(y.shape[1])]\n",
    "    R_bar = np.mean(actual_rewards)\n",
    "  return R_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Delta Rule on Reward Prediction for Selected Action Only\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 5000\n",
    "alpha = 0.0000006 #learning rate or step size\n",
    "a = np.sqrt(6. / (10+65))\n",
    "W_init = learn_rng.uniform(low=-a, high=a, size=(10,65))\n",
    "W = W_init.copy()\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "for epoch in range(num_epochs):\n",
    "  selected_actions, selected_one_hot, v = select_action(W, Xs_aug.T, rng=learn_rng, type='stochastic')\n",
    "  range_index = np.arange(len(selected_actions))\n",
    "  y_selected = y_rewards[range_index, selected_actions].T\n",
    "  v_selected = v[selected_actions, range_index]\n",
    "  delta_W = np.zeros_like(W)\n",
    "  for action in range(W.shape[0]):\n",
    "    action_mask = selected_actions == action\n",
    "    if np.any(action_mask):\n",
    "      # stimuli that caused actions of this type\n",
    "      X_relevant = Xs_aug.T[:, action_mask]  # input x num actions of this type\n",
    "      # reward errors for actions of this type\n",
    "      reward_error = y_selected[action_mask] - v_selected[action_mask] # num_actions of this type\n",
    "      reward_error = reward_error[np.newaxis, :]  # (1, num_actions of this type)\n",
    "      delta_W[action, :] = np.sum(alpha * X_relevant * reward_error, axis=1)\n",
    "  W += delta_W\n",
    "\n",
    "  if epoch == 0 or (epoch + 1) % 500 == 0:\n",
    "    accuracy, _, _, _ = eval_params_accuracy_percision_recall(W, Xs_aug.T, y_one_hot.T)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Full Batch Accuracy: {accuracy:.6f} | Time elapsed: {elapsed_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "It seems a bit faster, but not that much faster. This is because the comparison isn't really fair. In our measure and update loop we're using the proper expected reward, whereas in this reward-expectation error driven update we're using a specific sample of the reward based on actual sampled actions. Here's a version of our measure and update rule using actual sampled actions instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "It's difficult to compare these directly, since the perturbation method with actual action sampling requires getting the purturbation scale dialed in just right so that perturbations are likely to change actual behavioural outcomes, in contrast if we just look at expectations there is a lot of information coming in there, basically every single action is able to evaluated at once. In contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Mini-Batched Measure and Update Training Loop Using Selected Actions\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 10\n",
    "alpha = 0.1 #learning rate or step size\n",
    "num_batches = 20  # Number of mini-batches\n",
    "batch_size = 281  # Size of each mini-batch\n",
    "a = np.sqrt(6. / (10+65))\n",
    "perturbation_scale = 2*a # std of gaussian perturbations\n",
    "W_init = learn_rng.uniform(low=-a, high=a, size=(10,65))\n",
    "W = W_init.copy()\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)  # Shuffle the indices for each epoch\n",
    "  for batch in range(num_batches):\n",
    "    # Select a mini-batch for this iteration\n",
    "    batch_indices = indices[batch * batch_size : (batch + 1) * batch_size]\n",
    "    X_batch = Xs_aug[batch_indices, :].T\n",
    "    y_batch = y_rewards[batch_indices].T\n",
    "    R_bar_old = eval_params(W, X_batch, y_batch)\n",
    "    W_perturbations = learn_rng.normal(0, perturbation_scale, size=(10,65))\n",
    "    #perturb and evaluate each W_j separately\n",
    "    finite_differences_W = np.zeros(W.shape)\n",
    "    for ii in range(W_perturbations.shape[0]):\n",
    "      for jj in range(W_perturbations.shape[1]):\n",
    "        original_value = W[ii,jj]\n",
    "        W[ii, jj] += W_perturbations[ii, jj]\n",
    "        R_bar_perturbed_Wij = eval_params(W, X_batch, y_batch, use_expectation=False)\n",
    "        finite_differences_W[ii,jj] = (R_bar_perturbed_Wij - R_bar_old) / W_perturbations[ii,jj]\n",
    "        W[ii,jj] = original_value  # Revert the perturbation\n",
    "\n",
    "    delta_W = alpha * finite_differences_W\n",
    "    W += delta_W\n",
    "\n",
    "  if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "    accuracy, _, _, _ = eval_params_accuracy_percision_recall(W, Xs_aug.T, y_one_hot.T)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Full Batch Accuracy: {accuracy:.6f} | Time elapsed: {elapsed_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We're going to really want to walk through this step by step and see how the probability of the action taken increases or decrease, and then how by definition the probability of all the other actions goes up or down to balance this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "accuracy, prec, recall, confusion = eval_params_accuracy_percision_recall(W, Xs_aug.T, y_one_hot.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This more complex circuit get's us even closer to the theoretical maximum performance of 0.5033..., but it's taking longer to get there. This is because there are more parameters to figure out good values for. Consequently, function evaluations take a bit longer, and more significantly, we need many more function evaluations â€” an additional one for each parameter â€” in each iteration. With more hidden units and more time we can likely learn perfect discrimination, but it will take even longer (more than 5 minutes!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Although the toy neural circuit models in this sequence are a far cry from actual neural circuits, they still provide insight into potential learning processes in the brain. We can imagine a scenario where synaptic strengths between neurons in a circuit undergo small, transient perturbations. The brain might integrate and compare the performance of these perturbations over a learning episode (for example, a day) to previous performance levels. (Though, we leave aside the specifics of how this integration and comparison occur.)\n",
    "\n",
    "If performance improves with a perturbation, synaptic changes could be consolidated in the direction of the perturbation, proportionate to the degree of improvement. Conversely, if performance worsens, changes might be consolidated in the opposite direction, also proportional to the performance decrease. This concept, while still vague, suggests a mechanism of synaptic adjustment based on performance feedback.\n",
    "\n",
    "One critical point to consider is the scalability of such a learning process. The number of learning episodes required for effective optimization grows with the number of parameters in a neural circuit. This implies that 'measure and update' perturbation-based learning cannot be the primary mechanism driving neural plasticity in large, complex neural circuits. This limitation is critical, as life simply isn't long enough to accommodate the learning episodes needed for such extensive optimization.\n",
    "\n",
    "However, as demonstrated in our example, a more complex circuit achieved significantly better performance in the discrimination task, so large complex circuits can be useful. This suggests that even if empirical evidence of perturbation-based learning in the brain exists and its physiological implementation is understood, such processes are unlikely to be the primary drivers of neural plasticity for complex and challenging behaviors.\n",
    "\n",
    "(One counterargument in favor of simple learning rules is that extensive learning might not be necessary if genetic predisposition starts the circuit off close to an optimal parameter configuration. Then subsequently, relatively slow learning processes could 'fine-tune' the neural circuit's configuration. However, as noted in our earlier discussions on evolution, changing environments necessitate that a significant portion of behavior must emerge from learning, thereby limiting the extent to which genetic predispositions can facilitate efficient and adaptive learning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This concludes our sequence on fitting data through perturbations. In the sequence to come, we will see how different tricks and insights can make our learning algorithms faster and more efficient. We close with a thought on how all of this learning is based simply on changes in average reward. These learning algorithms make no use of information about what the right output should have been (compared to what it was) in any specific situation. Could we make more effective parameter updates if we incorporated information about which situations our circuit already yields 'correct' behavior and which ones it does not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In Summary\n",
    "\n",
    "Rewards only, boo. Too slow. No information about what was good or bad about your chosen action. Not such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to take the quiz\n",
    "comprehension_quiz = [\n",
    "  {\n",
    "    \"question\": \"How does the complexity of a neural circuit (number of parameters) impact the number of learning iterations and hence time to learn using a 'measure and update' learning process?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"It does not affect the number of iterations required; the process scales well regardless of circuit complexity.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Actually, the the number of iterations required grows with the complexity of the circuit due to more parameters requiring optimization.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"The process requires few learning iterations, by leveraging algorithmic economies of scale\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Contrary to this, an increase in parameters leads the more learning iterations being required.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"The process becomes requires more learning episodes for larger, more complex circuits.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct! More parameters mean more complexity and thus more learning episodes are needed.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"The number of learning iterations needed is solely dependent on the type of learning task, not the circuit complexity.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Circuit complexity, particularly the number of parameters, plays a significant role in the number of learning iterations needed.\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Why was the introduction of a 'hidden layer' beneficial in our complex discrimination tasks?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"It allowed the model to perform tasks more quickly but with reduced accuracy.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"The hidden layer's primary benefit is not speed at the cost of accuracy, but rather an enhancement in handling complex patterns.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It introduces feature interactions and non-linearities, enabling the model to capture complex patterns.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Exactly! Hidden layers allow for complex interactions and non-linear processing of features, which can be crucial for generating behaviour contingent on rich sensory input.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It reduces the number of parameters needed, simplifying the model.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Adding a hidden layer typically increases the number of parameters, adding complexity to the model.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It primarily improves the model's visualization, making it easier to interpret.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"While interpretability is important, this is not an advantage of adding a hidden layer.\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"In the context of machine learning, what is the impact of using a 'mini-batch' approach?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"It guarantees a 10x speedup in learning algorithms.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"While mini-batches can speed up learning, the magnitude of this speed-up will depend on choice of mini-batch size and the interaction of this size with underlying algorithmic efficiencies of scale at the hardware implementation level.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It reduces the time for evaluations by using a smaller, representative data sample.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct! A mini-batch approach uses a smaller subset of data for quicker evaluations, though it introduces some noise to the estimates.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It decreases the accuracy of parameter evaluation\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct! Mini-batches do introduce some noise to parameter evaluation, but is used thoughtfully this usually isn't an issue.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It eliminates the need for parameter updates in the learning process.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Mini-batches still require parameter updates; they just alter the way data is processed during learning.\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What challenge arises when comparing different machine learning algorithms?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"Algorithms cannot be compared due to their differing objectives.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"While objectives can vary, this doesn't make comparison impossible; it's more about how different parameters and conditions affect performance.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"All algorithms perform similarly when given the same data and task.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Performance can vary significantly between algorithms depending on their design and the specificities of the task and data.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"Performance is highly dependent on meta-parameter choices, making direct comparisons challenging.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Variations in learning rate, perturbation scale, mini-batch size, and other meta-parameters can significantly impact algorithm performance, complicating direct comparisons.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"Algorithms' performance cannot be measured or quantified.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Performance can indeed be measured and quantified, but the challenge lies in accounting for differences in meta-parameters and conditions.\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "display_quiz(comprehension_quiz)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P2C1_Sequence3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
