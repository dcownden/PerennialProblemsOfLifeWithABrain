{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHK+YkF1Gfw+teh68uhZ0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/super-opt-perturb/sequences/P2C1_CurveFitting/P2C1_Title.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book starts with evolution because ultimately, all aspects of the brain are shaped by evolution. The second part of the book focuses on how neural plasticity relates to supervised learning, viewed through this evolutionary lens. We are sharing it now to get feedback on what works and what does not and the developments we should do.\n",
        "___________________________________________________________________\n"
      ],
      "metadata": {
        "id": "ZmyVvcm4u4bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 2.1 Curve Fitting\n",
        "Objective: This chapter aims to\n",
        "1. Introduce the basic supervised learning task, a.k.a. 'curve fitting'\n",
        "2. Show how this relates to the fundamental evolutionary challenge of learning adaptive behaviours quickly.\n",
        "3. Highlight how specific phisiological mechanisms of neural plasticity relate, and possibly/partially implement supervised learning algorithms in the brain to achieve quick learning of adaptive behaviours.\n",
        "\n",
        "You will learn:\n",
        "1. How to optimize a function through perturbation, and how this relates to physiologically plausible plasticity mechanisms (local Hebbian learning modulated by glowbal reward signal, link this to node/weight pertutrbation, reinforce, probabalistic release (any others?).\n",
        "2. How to optimize a function through the 'delta rule' and how this relates to physiologically plausible plasticity methods, e.g. hebbian learning when local targets are provided (not just global reinforcement signals).\n",
        "3. How data scale and model and target complexity affect the time it takes to learn a good solution.\n",
        "4. How gradient descent is an effective scalable method, and how it serves as definative theoretical foundation, a kind of pole star, from which other learning algorithms can be understood (typically as noisey and biased approximations gradient desecent).\n",
        "\n",
        "  \n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "akARV6ndvBPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chapter is the first of three in the second part of the book. The second part of the book is about\n",
        "## Supervised Learning: Improving Behaviour When the Target is Known.\n",
        "\n",
        "### Objective: Part 1 of the book aims to introduce the fundamental concepts of\n",
        "1. Curve Fitting: How to tweak parameters to improve a loss function\n",
        "2. Generalization Error: (Train, Test, Validate) How to tweak parameters to improve performance on out of sample data.\n",
        "3. Importances of details: data scale, data set mixture (and order), model architecture, parameter fitting algorithm, all interact with eachother and make a huge difference both in terms of final model quality and time (number of learning experiences) to achieve that quality.\n",
        "\n",
        "In Part 1 of the book learned to view the brain as an evolved tool for rapidly adapting behaviour to be well suited to a given environment or situation. Now we start to use this perspective to show how the various supervised learning algorithms we explore might serve this adaptive purpose, and how these difference adaptive algorithms might correspond with physiological neural plasticity processes.  Supervised learning presupposes the existence of targets/goals, or at the very least reward signals to guide learning. How such targets (and corresponding errors, i.e. differences between output and target) and or reward signals are computed and communicated within the . Aditionally, most (all?!?) of machine learning can be framed as extenstions or complications of basic supervised learning tasks. These techniques and ideas learned in this part of the book are foundational more general (and biologically relevant) problems investigated later in the book."
      ],
      "metadata": {
        "id": "mQDFa3Ao0r9B"
      }
    }
  ]
}