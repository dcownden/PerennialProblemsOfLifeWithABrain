{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P2C1_Optimization/P2C1_Sequence4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_Optimization/P2C1_Sequence4.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **2.1.4: Learning Behaviour as a Form of High Dimensional Optimization**\n",
    "\n",
    "### Objective: Explicitly connect the kinds of simple optimization process we saw in the previous sequences to learning adaptive behaviours.\n",
    "\n",
    "In this sequence we will:\n",
    "\n",
    "* Introduce a slightly more complex version of the strike-no-strike problem where the decision depends on 64 sensory inputs (features) instead of 1, and develop a simple artificial neural network that can learn to discriminate between when to strike and not strike based on these inputs.\n",
    "\n",
    "* Use perturb-measure-step to train this network\n",
    "\n",
    "* Adapt perturb-measure-step to\n",
    "  - No longer require seperate perturbation and base evaluation modes\n",
    "  - Learn from a single experiential episode (senory experience, action selection, resultant reward)\n",
    "\n",
    "* See how performance is improved on the strike-no-strike discrimination task when using a more complex network to determine behaviour based on sensory input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz ucimlrepo > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import asyncio\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "  return\n",
    "\n",
    "data_set = fetch_ucirepo(id=80)\n",
    "X = data_set.data.features.values\n",
    "# Translate the data to have a minimum of 0\n",
    "X_translated = X - X.min()\n",
    "# Scale the data to have a range from 0 to 12 (which is 6 - (-6))\n",
    "scaling_factor = 12 / (X.max() - X.min())\n",
    "X_scaled = X_translated * scaling_factor\n",
    "# Finally, shift the data to be centered between -6 and 6\n",
    "X_final = X_scaled - 6\n",
    "\n",
    "y = data_set.data.targets.values\n",
    "rng = np.random.default_rng(seed=2021)\n",
    "scramble_permutation = rng.permutation(X.shape[1])\n",
    "Xs = X_final[:, scramble_permutation]\n",
    "y1 = y % 2\n",
    "y2 = np.array(y >= 5, dtype=y.dtype)\n",
    "simple_index = ((y.flatten()==1) | (y.flatten()==0))\n",
    "X_simple = Xs[simple_index]\n",
    "y1_simple = y1[simple_index]\n",
    "# if you only had one feature which would likely be best for discrimination\n",
    "epsilon = 10\n",
    "class_a_sep = np.mean(X_simple[y1_simple.flatten() == 1, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 1, :], axis=0) + epsilon)\n",
    "class_b_sep = np.mean(X_simple[y1_simple.flatten() == 0, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 0, :], axis=0) + epsilon)\n",
    "best_feature = np.argmax(class_a_sep - class_b_sep)\n",
    "# print(f'Best feature is {best_feature}')\n",
    "X_simple_1_feature = X_simple[:, [best_feature]]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P2C1_S4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.1 Learning Strike-No-Strike with Perturb-Measure-Step.\n",
    "\n",
    "Now that we know a bit about optimization in higher-dimensions we are going to introduce a harder, higher-dimensional version of our strike-no-strike problem. Whereas previously we had only a single input, now we are going to allow for 64 different inputs. So our cartoon organism that inspires this problem can now be thought of as having 64 photo-sensitive receptors, and based on this combination of inputs it must decide whether to strike or not. As before, the organism pays a cost of one if it strikes when it shouldn't and recieves a reward of one if it strikes when it should. It receives no cost or reward when it does not strike. To get a sense of this more complex discrimination problem, try it yourself by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to try out the more complex 'strike-no-strike' discrimination task.\n",
    "\n",
    "class InteractiveMNISTPredator():\n",
    "  def __init__(self,\n",
    "               features=Xs,\n",
    "               labels=y,\n",
    "               feedback_type='on_strike_only', seed=123):\n",
    "    # Initialize dataset, settings for image scrambling and feedback\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    "    # features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)\n",
    "    # labels is num_data_points x 1 (values 0-9 or 0/1 depending)\n",
    "    self.feedback_type = feedback_type\n",
    "    self.rng = np.random.default_rng(seed)\n",
    "    sample_order = self.rng.permutation(self.features.shape[0])\n",
    "    self.features = self.features[sample_order]\n",
    "    self.labels = self.labels[sample_order]\n",
    "    # initialize game state\n",
    "    self.current_index = 0\n",
    "    self.current_image = None\n",
    "    self.previous_image = None\n",
    "    self.score = 0\n",
    "    self.best_possible_score = 0\n",
    "    self.successful_strikes = 0\n",
    "    self.failed_strikes = 0\n",
    "    self.non_strikes = 0\n",
    "    # Initialize widgets\n",
    "    self.strike_button = widgets.Button(description='Strike')\n",
    "    self.no_strike_button = widgets.Button(description='No Strike')\n",
    "    self.score_display = widgets.Output()\n",
    "    self.feedback_display = widgets.Output()\n",
    "\n",
    "    # Initialize the figure for image display\n",
    "    self.fig, self.ax = plt.subplots(figsize=(4, 4))\n",
    "    remove_ip_clutter(self.fig)\n",
    "    self.prev_fig, self.prev_ax = plt.subplots(figsize=(4, 4))\n",
    "    remove_ip_clutter(self.prev_fig)\n",
    "    self.show_next_image()\n",
    "    # Bind event handlers\n",
    "    self.strike_button.on_click(self.on_strike_clicked)\n",
    "    self.no_strike_button.on_click(self.on_no_strike_clicked)\n",
    "\n",
    "    # Arrange widgets in a layout\n",
    "    buttons_layout = widgets.HBox([self.strike_button, self.no_strike_button])\n",
    "    current_buttons = widgets.VBox([self.fig.canvas, buttons_layout])\n",
    "    previous_feedback = widgets.VBox([self.prev_fig.canvas, self.feedback_display])\n",
    "    self.ui = widgets.HBox([previous_feedback, current_buttons, self.score_display])\n",
    "\n",
    "  def show_next_image(self):\n",
    "    # Display the next image\n",
    "    image = self.features[self.current_index]\n",
    "\n",
    "    if len(image) == 64:\n",
    "        image = image.reshape(8, 8)\n",
    "    elif len(image) == 1:\n",
    "      scalar_value = image.flatten()[0]\n",
    "      # Initialize the 8x8 array with -6 (black)\n",
    "      image = np.full((8, 8), -6.0)\n",
    "      # Set the first ring to 6 (white)\n",
    "      image[0, 0] = 6\n",
    "      # Set the second ring to 6 (white)\n",
    "      image[1:-1, 1:-1] = 6\n",
    "      # Set the third (inner ring) back to -6 (black)\n",
    "      image[2:-2, 2:-2] = -6\n",
    "      # Assuming scalar_value is already in the range -6 to 6\n",
    "      #print(scalar_value)\n",
    "      image[3:-3, 3:-3] = scalar_value\n",
    "    else:\n",
    "      raise ValueError(f'Unexpected image shape: {image.shape}')\n",
    "    if self.current_image is not None:\n",
    "      self.previous_image = self.current_image\n",
    "    self.current_image = image\n",
    "    # Display the image\n",
    "    #print(image)\n",
    "    self.fig.clf()\n",
    "    self.prev_fig.clf()\n",
    "    self.ax = self.fig.add_subplot(111)\n",
    "    self.prev_ax = self.prev_fig.add_subplot(111)\n",
    "    self.ax.set_xlim(-.5, 7.5)\n",
    "    self.ax.set_ylim(-0.5, 7.5)\n",
    "    self.prev_ax.set_xlim(-.5, 7.5)\n",
    "    self.prev_ax.set_ylim(-0.5, 7.5)\n",
    "    self.ax.set_aspect('equal')\n",
    "    self.prev_ax.set_aspect('equal')\n",
    "    self.ax.axis('off')\n",
    "    self.prev_ax.axis('off')\n",
    "    self.ax.imshow(self.current_image, cmap='gray', vmin=-6, vmax=6)\n",
    "    if self.previous_image is not None:\n",
    "      self.prev_ax.imshow(self.previous_image, cmap='gray', vmin=-6, vmax=6)\n",
    "    self.ax.set_title('Current Sensory Input')\n",
    "    self.prev_ax.set_title('Previous Sensory Input')\n",
    "    self.fig.canvas.draw()\n",
    "    self.prev_fig.canvas.draw()\n",
    "\n",
    "  def on_strike_clicked(self, button):\n",
    "    self.process_decision('Strike')\n",
    "\n",
    "  def on_no_strike_clicked(self, button):\n",
    "    self.process_decision('No Strike')\n",
    "\n",
    "  def process_decision(self, decision):\n",
    "    # freeze buttons while we process\n",
    "    self.strike_button.disabled = True\n",
    "    self.no_strike_button.disabled = True\n",
    "\n",
    "    # Process the user's decision, update score, and provide feedback\n",
    "    correct_action = 'Strike' if self.labels[self.current_index] == 1 else 'No Strike'\n",
    "    if decision == 'Strike':\n",
    "      if decision == correct_action:\n",
    "        self.score += 1\n",
    "        self.successful_strikes += 1\n",
    "      else:\n",
    "        self.score -= 1\n",
    "        self.failed_strikes += 1\n",
    "    elif decision == 'No Strike':\n",
    "      self.non_strikes += 1\n",
    "      # no strike means no gain or loss\n",
    "    else:\n",
    "      raise ValueError(f'Unknown decision: {decision}')\n",
    "\n",
    "    # Show feedback and score\n",
    "    if (self.feedback_type == 'both' or\n",
    "      (self.feedback_type == 'on_strike_only' and decision == 'Strike')):\n",
    "      # Show informative feedback\n",
    "      feedback = f'Your last choice: {decision}\\nCorrect last choice: {correct_action}'\n",
    "    else:\n",
    "      # Show uninformative feedback\n",
    "      feedback = 'Feedback only available after striking.'\n",
    "    with self.feedback_display:\n",
    "      clear_output(wait=True)\n",
    "      print(feedback)\n",
    "\n",
    "    # Show score\n",
    "    with self.score_display:\n",
    "      clear_output(wait=True)\n",
    "      average_score = self.score / (self.current_index+1)\n",
    "      print(f'Total Score: {self.score}')\n",
    "      print(f'Number of Trials: {self.current_index + 1}')\n",
    "      print(f'Successful Strikes: {self.successful_strikes}')\n",
    "      print(f'Failed Strikes: {self.failed_strikes}')\n",
    "      print(f'Non-Strikes: {self.non_strikes}')\n",
    "      print(f'Average Score Per Trial: {average_score:.2f}')\n",
    "\n",
    "    # Prepare the next image\n",
    "    self.current_index += 1\n",
    "    #print(self.current_index)\n",
    "    self.show_next_image()\n",
    "    # Re-enable buttons\n",
    "    self.strike_button.disabled = False\n",
    "    self.no_strike_button.disabled = False\n",
    "\n",
    "\n",
    "scramble_bin_hard = InteractiveMNISTPredator(features=Xs,\n",
    "                                             labels=y1,\n",
    "                                             feedback_type='both')\n",
    "display(scramble_bin_hard.fig.canvas)\n",
    "display(scramble_bin_hard.prev_fig.canvas)\n",
    "clear_output()\n",
    "display(scramble_bin_hard.ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Maybe you were able to solve this with some savant like abstract pattern recognition powers. We were not able to learn to discriminate between the strike and no-strike situations over a dozen or so trials. The point here is that this is a non-trivial discrimination to learn. However, it is, as we will see in the next bit of code, a discrimination that a simple artificial neural network can learn to solve. (As before The dataset that underlies this strike-no-strike decision problem is sourced from the UCI Machine Learning Repository, Alpaydin,E. and Kaynak,C. 1998. https://doi.org/10.24432/C50P49.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We model this creature's sensory-behaviour system much as before. Now though, $\\mathbf{x}$ is the raw sensory input (vector) of length 64 in a given episode. Each element $x_i$ of $\\mathbf{x}$ corresponds to the activation level of a single photosensitive neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# the data set we're working with has 5620 example sensory inputs,\n",
    "# each consisting of 64 values\n",
    "print(Xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# this is the first example\n",
    "print(Xs[0].reshape(8,8,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# visualizing the example we see that lower values correspond to darker pixels\n",
    "# and higher values correspond to lighter values\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "remove_ip_clutter(fig)\n",
    "ax.imshow(Xs[0].reshape(8,8), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These input neurons are then connected by synapses to a single output neuron. The activation level of this output neuron is computed as\n",
    "$$z = \\mathbf{Wx} + b$$\n",
    "Here, $b$ is the (scalar) bias, or baseline activation level of the output neuron, and $\\mathbf{W}$ is a matrix of synaptic weights between the input neurons and the single output neuron. (In this case where there is only one output neuron so $\\mathbf{W}$ has shape 1x64 so could also be thought of as a row vector.)  \n",
    "\n",
    "Often to simplify exposition and coding the input $\\mathbf{x}$ is augmented to have a feature which is always 1, and then the bias terms can be treated as the weight connecting to this constant valued feature. That is\n",
    "\n",
    "$$z = \\mathbf{Wx}$$\n",
    "\n",
    "Though now $\\mathbf{W}$ has shape 1x65. As before, the probabilistic spiking of this output neuron determines the strike-no-strike behaviour of the organism, specifically:\n",
    "$$ \\Pr \\{\\text{strike}\\} = \\sigma(z) $$\n",
    "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma(z)$$\n",
    "\n",
    "Recall that $\\sigma(z): \\frac{1}{1+e^{-z}} = \\frac{e^z}{1+e^z}$ is the standard logistic (sigmoid) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The organism recieves a reward of 1 for striking at the right time and a penalty of -1 for striking at the wrong time. It also recieves a reward of zero when not striking, regardless of whether striking would have returned a reward or a penalty. Given this, complete the coding exercise below to write a function that determines the reward recieved for a given sensory input $\\mathbf{x}$, the organism's probablistic response to the stimulus, $\\Pr \\{\\text{strike}\\} = \\sigma(\\mathbf{Wx})$, and the resultant outcome of the behaviour given the presence ($y=1$) or absence ($y=0$) of prey. Note that reward depends on three inputs, two $\\mathbf{x}$ and $y$ have to do with the state of the environment, and are fully outside of the control of the organism, the other $\\mathbf{W}$ determines the organism's response to the environment, and it is this $\\mathbf{W}$, that the organism has some control over in that sense that $\\mathbf{W}$ is what changes as a result of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Complete the lines with ... set the appropriate rewards for\n",
    "# for the evaluations function\n",
    "raise NotImplementedError(\"Exercise: Set the reward for different outcomes\")\n",
    "################################################################################\n",
    "\n",
    "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
    "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
    "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
    "# After augmentation there is one extra column of features\n",
    "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
    "\n",
    "def np_sigmoid(x):\n",
    "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def eval_params_stochastic_single(W, x, y, verbose=False, rng=None):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(1) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) np.array) sensory input\n",
    "    y: (outputs(1) np.array) target behavioural output\n",
    "\n",
    "  Returns:\n",
    "    R: the reward obtained given the parameters, inputs and targets\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # activaation\n",
    "  z = np.dot(W,x)\n",
    "  # strike probability\n",
    "  strike_prob = np_sigmoid(z)\n",
    "  # what the organism actually does\n",
    "  # rng.random is a sample from the uniform distribution on [0,1)\n",
    "  did_strike = rng.random() < strike_prob\n",
    "  if did_strike == True: #organism strikes\n",
    "    if y == 1: #prey is present\n",
    "      R = ...\n",
    "    else: # prey is not present\n",
    "      R = ...\n",
    "  else: # organism does not strike\n",
    "    R = ...\n",
    "  if verbose:\n",
    "    print(f'Probability of striking: {strike_prob}')\n",
    "    action_string = 'Strike' if did_strike == True else 'No Strike'\n",
    "    print(f'Action taken: {action_string}')\n",
    "    target_string = 'Strike' if y == 1 else 'No Strike'\n",
    "    print(f'Correct Action: {target_string}')\n",
    "    print(f'Reward recieved: {R}')\n",
    "  else:\n",
    "    return R\n",
    "\n",
    "eval_rng = np.random.default_rng(0)\n",
    "W_test = np.zeros((1,65))\n",
    "eval_params_stochastic_single(W_test, Xs_aug[0], y1[0], verbose=True, rng=eval_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
    "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
    "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
    "# After augmentation there is one extra column of features\n",
    "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
    "\n",
    "def np_sigmoid(x):\n",
    "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def eval_params_stochastic_single(W, x, y, verbose=False, rng=None):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(1) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) np.array) sensory input\n",
    "    y: (outputs(1) np.array) target behavioural output\n",
    "\n",
    "  Returns:\n",
    "    R: the reward obtained given the parameters, inputs and targets\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # activaation\n",
    "  z = np.dot(W,x)\n",
    "  # strike probability\n",
    "  strike_prob = np_sigmoid(z)\n",
    "  # what the organism actually does\n",
    "  # rng.random is a sample from the uniform distribution on [0,1)\n",
    "  did_strike = rng.random() < strike_prob\n",
    "  if did_strike == True: #organism strikes\n",
    "    if y == 1: #prey is present\n",
    "      R = 1\n",
    "    else: # prey is not present\n",
    "      R = -1\n",
    "  else: # organism does not strike\n",
    "    R = 0\n",
    "  if verbose:\n",
    "    print(f'Probability of striking: {strike_prob}')\n",
    "    action_string = 'Strike' if did_strike == True else 'No Strike'\n",
    "    print(f'Action taken: {action_string}')\n",
    "    target_string = 'Strike' if y == 1 else 'No Strike'\n",
    "    print(f'Correct Action: {target_string}')\n",
    "    print(f'Reward recieved: {R}')\n",
    "  else:\n",
    "    return R\n",
    "\n",
    "eval_rng = np.random.default_rng(0)\n",
    "W_test = np.zeros((1,65))\n",
    "eval_params_stochastic_single(W_test, Xs_aug[0], y1[0], verbose=True, rng=eval_rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So that evaluates the reward over a single experience. We can use numpy broadcasting to apply this same reward calculation efficiently to many, even all, the input-out pairs in our data set. We call this **batch** evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Complete the lines with ... to compute the number of\n",
    "# True Positives, False Positives, True Negative and False Negatives in the batch\n",
    "raise NotImplementedError(\"Exercise: Compute the number of different Outcomes\")\n",
    "################################################################################\n",
    "\n",
    "# to_remove solution\n",
    "\n",
    "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
    "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
    "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
    "# After augmentation there is one extra column of features\n",
    "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
    "\n",
    "def np_sigmoid(x):\n",
    "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(1) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "    y: (outputs(1) x batch np.array) target behavioural output\n",
    "\n",
    "  Returns:\n",
    "    R: the reward obtained given the parameters, inputs and targets\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # activation\n",
    "  z = np.dot(W,x) # 1 x batch\n",
    "  # strike probability\n",
    "  strike_probs = np_sigmoid(z) # 1 x batch\n",
    "  # what the organism actually does\n",
    "  # rng.random is a sample from the uniform distribution on [0,1)\n",
    "  did_strike = rng.random(size=strike_probs.shape) < ...  # 1 x batch\n",
    "  R = np.zeros(did_strike.shape)\n",
    "  did_not_strike = ...\n",
    "  should_strike = ...\n",
    "  should_not_strike = ...\n",
    "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
    "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
    "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
    "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
    "  R[TP] = 1\n",
    "  R[FP] = -1\n",
    "  R[FN] = 0\n",
    "  R[TN] = 0\n",
    "  TPs = np.sum(TP)\n",
    "  FPs = np.sum(FP)\n",
    "  FNs = np.sum(FN)\n",
    "  TNs = np.sum(TN)\n",
    "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
    "  if verbose:\n",
    "    table = [[\"Should Strike\", TPs, FNs],\n",
    "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
    "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
    "    print(\"Confusion_matrix: \")\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "    print(f'Total Reward: {np.sum(R)}')\n",
    "    return None\n",
    "  else:\n",
    "    return np.sum(R), confusion_matrix\n",
    "\n",
    "eval_rng = np.random.default_rng(0)\n",
    "W_test = np.zeros((1,65))\n",
    "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
    "# for broadcasting to work\n",
    "print('Evaluation 1')\n",
    "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)\n",
    "print('\\nEvaluation 2')\n",
    "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
    "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
    "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
    "# After augmentation there is one extra column of features\n",
    "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
    "\n",
    "def np_sigmoid(x):\n",
    "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(1) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "    y: (outputs(1) x batch np.array) target behavioural output\n",
    "\n",
    "  Returns:\n",
    "    R: the reward obtained given the parameters, inputs and targets\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # activation\n",
    "  z = np.dot(W,x) # 1 x batch\n",
    "  # strike probability\n",
    "  strike_probs = np_sigmoid(z) # 1 x batch\n",
    "  # what the organism actually does\n",
    "  # rng.random is a sample from the uniform distribution on [0,1)\n",
    "  did_strike = rng.random(size=strike_probs.shape) < strike_probs  # 1 x batch\n",
    "  R = np.zeros(did_strike.shape)\n",
    "  did_not_strike = np.logical_not(did_strike)\n",
    "  should_strike = y == 1\n",
    "  should_not_strike = y == 0\n",
    "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
    "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
    "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
    "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
    "  R[TP] = 1\n",
    "  R[FP] = -1\n",
    "  R[FN] = 0\n",
    "  R[TN] = 0\n",
    "  TPs = np.sum(TP)\n",
    "  FPs = np.sum(FP)\n",
    "  FNs = np.sum(FN)\n",
    "  TNs = np.sum(TN)\n",
    "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
    "  if verbose:\n",
    "    table = [[\"Should Strike\", TPs, FNs],\n",
    "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
    "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
    "    print(\"Confusion_matrix: \")\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "    print(f'Total Reward: {np.sum(R)}')\n",
    "    return None\n",
    "  else:\n",
    "    return np.sum(R), confusion_matrix\n",
    "\n",
    "eval_rng = np.random.default_rng(0)\n",
    "W_test = np.zeros((1,65))\n",
    "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
    "# for broadcasting to work\n",
    "print('Evaluation 1')\n",
    "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)\n",
    "print('\\nEvaluation 2')\n",
    "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note that the two evaluations give different total rewards, even though the exact same synaptic weights $\\mathbf{W}$ are being used, on the exact same batch of inputs $\\mathbf{x}$ and prey presence indicators $y$. This is expected given the inherent stochasticity in the organism's behaviour. This stochastic evaluation of the synaptic weights will make things difficult for the perturb-measure-step alogorithm though, because it relies upon precise function evaluations to get good estimates of the rate of improvement in a given direction in parameter space. We can overcome this stochastic evaluation issue though by using our knowledge of how the different probabilities of striking or not determine the expected, or average reward. By directly evaluating expected reward we can recover a precise, deterministic evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def eval_params_expectation_batch(W, x, y, verbose=False):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(1) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "    y: (outputs(1) x batch np.array) target behavioural output\n",
    "\n",
    "  Returns:\n",
    "    R_exp: the expected reward obtained over the batch given the parameters, inputs and targets\n",
    "  \"\"\"\n",
    "  # activaation\n",
    "  a = np.dot(W,x) # 1 x batch\n",
    "  # strike probability\n",
    "  y_hat = np_sigmoid(a) # 1 x batch\n",
    "  # Expected true positives (TPs) and false positives (FPs)\n",
    "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
    "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
    "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
    "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
    "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
    "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
    "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
    "  if verbose:\n",
    "    table = [[\"Should Strike\", TPs, FNs],\n",
    "             [\"Shouldn't Strike\", FPs, TNs]]\n",
    "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
    "    print(\"Confusion_matrix: \")\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "    print(f'Total Reward: {R_exp}')\n",
    "    return None\n",
    "  else:\n",
    "    return R_exp, confusion_matrix\n",
    "\n",
    "W_test = np.zeros((1,65))\n",
    "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
    "# for broadcasting to work\n",
    "print('Evaluation 1')\n",
    "eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=True)\n",
    "print('\\nEvaluation 2')\n",
    "eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note that as hoped, the evaluation of parameters using expected reward, is consistent, as it should be. As a sanity check we see that the distribution of stochastic evaluations is roughly symmetric, and centered around this expectation, with the average of many such stochastic evaluations becoming close to our calculated expected value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the distribution of stochastic parameter evaluation, relative to the expectation.\n",
    "def eval_params_expectation_batch(W, x, y, verbose=False):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(1) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "    y: (outputs(1) x batch np.array) target behavioural output\n",
    "\n",
    "  Returns:\n",
    "    R_exp: the expected reward obtained over the batch given the parameters, inputs and targets\n",
    "  \"\"\"\n",
    "  # activaation\n",
    "  a = np.dot(W,x) # 1 x batch\n",
    "  # strike probability\n",
    "  y_hat = np_sigmoid(a) # 1 x batch\n",
    "  # Expected true positives (TPs) and false positives (FPs)\n",
    "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
    "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
    "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
    "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
    "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
    "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
    "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
    "  if verbose:\n",
    "    table = [[\"Should Strike\", TPs, FNs],\n",
    "             [\"Shouldn't Strike\", FPs, TNs]]\n",
    "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
    "    print(\"Confusion_matrix: \")\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "    print(f'Total Reward: {R_exp}')\n",
    "    return None\n",
    "  else:\n",
    "    return R_exp, confusion_matrix\n",
    "\n",
    "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W: (outputs(1) x inputs(65) np.array)\n",
    "       weights between sensory neurons and output neuron\n",
    "    x: (input(65) x batch np.array) sensory input\n",
    "    y: (outputs(1) x batch np.array) target behavioural output\n",
    "\n",
    "  Returns:\n",
    "    R: the reward obtained given the parameters, inputs and targets\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # activaation\n",
    "  a = np.dot(W,x) # 1 x batch\n",
    "  # strike probability\n",
    "  y_hat = np_sigmoid(a) # 1 x batch\n",
    "  # what the organism actually does\n",
    "  # rng.random is a sample from the uniform distribution on [0,1)\n",
    "  y_sample = rng.random(size=y_hat.shape) < y_hat  # 1 x batch\n",
    "  R = np.zeros(y_sample.shape)\n",
    "  did_strike = y_sample == 1\n",
    "  did_not_strike = y_sample == 0\n",
    "  should_strike = y == 1\n",
    "  should_not_strike = y == 0\n",
    "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
    "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
    "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
    "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
    "  R[TP] = 1\n",
    "  R[FP] = -1\n",
    "  R[FN] = 0\n",
    "  R[TN] = 0\n",
    "  TPs = np.sum(TP)\n",
    "  FPs = np.sum(FP)\n",
    "  FNs = np.sum(FN)\n",
    "  TNs = np.sum(TN)\n",
    "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
    "  if verbose:\n",
    "    table = [[\"Should Strike\", TPs, FNs],\n",
    "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
    "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
    "    print(\"Confusion_matrix: \")\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "    print(f'Total Reward: {np.sum(R)}')\n",
    "    return None\n",
    "  else:\n",
    "    return np.sum(R), confusion_matrix\n",
    "\n",
    "W_test = np.zeros((1,65))\n",
    "exp_reward, _ = eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=False)\n",
    "\n",
    "# Generate stochastic rewards\n",
    "stochastic_rewards = []\n",
    "for _ in range(500):  # Simulate 100 times to create a distribution\n",
    "  r, _ = eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=False)\n",
    "  stochastic_rewards.append(r)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "remove_ip_clutter(fig)\n",
    "ax.hist(stochastic_rewards, bins=20, alpha=0.75, label='Stochastic Evaluations')\n",
    "ax.axvline(x=exp_reward, color='r', linestyle='dashed', linewidth=2, label=f'Expected Reward: {exp_reward}')\n",
    "ax.set_title('Comparison of Stochastic Evaluations and Expected Reward')\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As yet another sanity check we calculate the expected reward when striking and not striking with equal probability, in all circumstances, which is what we expect from a $\\mathbf{W}$ of all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# There are this many cases where striking is good\n",
    "np.sum(y1 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# And this many cases where striking is bad\n",
    "np.sum(y1 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# W = 0 should strike half the time no matter what,\n",
    "# in which case would expect a reward of\n",
    "(2829 - 2791) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "That all checks out, so now that we have some confidence in our evaluation function let's see if perturb-measure-step is able to find a good set of values for $\\mathbf{W}$ using the batch expected value version of parameter evaluation. Execture the code cell below to run this the training loop. The process will take a minute or two to complete, while its running read through the code and see if you can make sense of what it is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Perturb-Measure-Step Training Loop\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_steps = 60000\n",
    "step_scale = 0.002\n",
    "dimensional_scale_factor = np.sqrt(65)\n",
    "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
    "W_init = np.zeros((1,65))\n",
    "W = W_init\n",
    "start_time = time.time()\n",
    "for step in range(num_steps):\n",
    "  R_current, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "  raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
    "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "  test_perturbation = unit_test_perturb * perturbation_scale\n",
    "  R_test, _ = eval_params_expectation_batch(W + test_perturbation, Xs_aug.T, y1.T)\n",
    "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
    "  W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
    "\n",
    "  if step == 0 or (step + 1) % 3000 == 0:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Step {step + 1}/{num_steps} | Expected Full Batch Reward: {R_current:.6f} | Time: {elapsed_time:.2f}s | Avg. Time Per Step: {1000*elapsed_time/(step+1):.2f}ms')\n",
    "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The best possible score is 2829, and perturb measure step is able to discover network parameters that achieve a score of roughly 2500, in 60,000 steps. This is pretty good, better than we the humans were able to do in terms of figuring out the pattern, before we got bored of the problem in a few minutes. This is all well and good as an optimization exercise, but if we want to connect this form of optimization back to our inspirational cartoon of neural behaviour as a kind of learning, there are a few issues. Three major issues stand out as ways in which the perturb-measure-step training loop above deviates from a process that is simple and local enough to serve as plausible (even if very abstract) model of a physiological syanptic plasticity processes. These key issues are:\n",
    "1. The organism's striking behaviour is stochastic but expected reward outcomes, not actual obtained reward outcomes are used to drive updates to the synaptic parameters $\\mathbf{W}$.\n",
    "2. The evaluation of a given synaptic configuration is based on performance over all of the 5620 of distcint input-outpur pairs (sensory-pattern, prey-presence) in the data set that defines the \"environment\" of this learning problem. Physiologically viable would be evaluations over a single, or at least relatively few, stimulus-response-reward episodes.\n",
    "3. Evaluations are performed in seperate perturbation and non-perturbation modes in the training loop above. A single mode of evaluation that operated fully \"online\" and in congunction with the ongoing generation of behaviour is more simple and easy to imagine physiological implementations of.\n",
    "\n",
    "There are of course many other ways in which the this cartoon learning system deviates from what might plausibly be implemented in an actual simple neural system, but these have more to do with the abstractness of the model, and can concievibly be remedied with careful choices about how to make the model more concrete so as to map nicely onto measurable phyiological features of neural plasticity. In contrast, for the critical points oulined above, it is difficult to imagine how any of these key issues can be overcome physiologically, without invoking additional complex neural circuits and processes, the orgin of which also need to be explained. So for this sequence and the next we focus on addressing these core issues: tracking base versus perturbation evaluation, using batched instead of single experience based reward to drive learning, and using expected versus actual recieved (stochastic) reward to drive learning.\n",
    "\n",
    "In the rest of this sequence we will adapt the base perturb-measure-step update rule to address each of these three issues. But first let's just get a bit of a sense of how these issues impinge upon the fantasy of using perturb-measure-step as an algorithm that might feasibly be used by a living organism to update the connection strengths of this simple network determining behaviour in response to stimulus. In each interation in the vanilla perturb-measure-step training loop implemented above the organism first evaluates its current parameters based on the expected reward over all 5620 possible experiences. It then perturbs its synaptic parameters and evaluates its performance again on all 5620 experiences, these two evaluations are compared to determine $\\Delta R$ and this together with the pertrubations $\\Delta W_i$ determine the synaptic connections update according to\n",
    "\n",
    "$$ W_i' = W_i + s \\ \\Delta W_i \\ \\Delta R $$\n",
    "where\n",
    "$$\\Delta R = R(\\mathbf{W} + \\Delta \\mathbf{W}) - R(\\mathbf{W})$$\n",
    "and $R(\\mathbf{W})$ is our reward/evaluation function.\n",
    "Note that $s$ needs to be carefully choosen to account both for the average size of the perturbation $\\| \\mathbf{W} \\|$, the appropriately level of scaling given the expected alignment of a random perturbation with the gradient given the dimensionality of $\\mathbf{W}$, and the relative scale of the gradient. While this is can be a challenge in practical applications, the \"dialing-in\" of meta-parameters of learning algorithms is something that we expect evolution to be quite good at.\n",
    "\n",
    "This process would require the organism somehow integrate all of the reward outcomes of all 5620 experiences, remember this aggregated outcome, then integrate up the reward outcomes of another 5620 experiences accumulated while in 'perturbation' mode, and then update its parameters based on a comparison of the remembered and the recently accumlated aggregate reward outcome. This all seems a bit complicated, and difficult to implement with simple, primarily local, synaptic plasticity mechanisms. What we would like intead is a version of perturb-measure-step that updates its synaptic weights as a result of every reward experience, doesn't rely on an expected reward calculation, and that does not have a seperate perturbation mode.\n",
    "\n",
    "Removing the seperate perturbation mode is perhaps the easiest issue to address so we take this on first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.2 Step-Measure-Step (ALOPEX)\n",
    "\n",
    "One simple way to avoid a seperate perturbation evaluation is to simply compress the perturbation and the update into a single step. We call such an update method step-measure-step as contrasted with perturb measure step. This new update rule looks like this.\n",
    "\n",
    "\n",
    "\n",
    "$$\\ W_{i}(t+1)= W_i(t) + s \\ (W_i(t) - W_i(t-1)) \\ \\ R(\\mathbf{W}(t)) - R(\\mathbf{W}(t-1)) + \\xi_{i}(t)$$\n",
    "\n",
    "Previously we used $\\Delta$ to denote the perturbation and did not directly reference the parameter update. Because now we are combining the perturbation and the parameter update we use $\\Delta$ to denote this combined change, that is\n",
    "$$ \\ W_{i}(t+1) - W_i(t) = \\Delta W_i(t) $$\n",
    "\n",
    "Then the above simplifies\n",
    "\n",
    "$$ \\Delta W_i(t) = s \\ \\Delta W_i(t-1) \\ \\Delta R(t) + \\xi_i(t) $$\n",
    "Here $\\Delta R(t) = R(\\mathbf{W}(t)) - R(\\mathbf{W}(t-1))$\n",
    "\n",
    "Run the code cell below to see if this works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Step-Measure-Step Training Loop\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_steps = 100000\n",
    "init_step_scale = 0.003 * np.sqrt(65)\n",
    "init_noise_scale = 0.0005\n",
    "total_scale = 1.0\n",
    "#later_step_scale = 0.002 * np.sqrt(65)\n",
    "#later_noise_scale = 0.0001\n",
    "W_init = np.zeros((1,65))\n",
    "W = W_init\n",
    "delta_W = np.zeros((1,65))\n",
    "R, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "start_time = time.time()\n",
    "step_scale = init_step_scale\n",
    "noise_scale = init_noise_scale\n",
    "for step in range(num_steps):\n",
    "  R_old = R\n",
    "  R, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "  delta_R = R - R_old\n",
    "  delta_W_noise = learn_rng.normal(0, noise_scale, size=(1,65))\n",
    "  delta_W = total_scale * (step_scale * delta_W * delta_R + delta_W_noise)\n",
    "  W += delta_W\n",
    "\n",
    "  if step == 0 or (step + 1) % 10000 == 0:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print(f'Delta R: {delta_R}')\n",
    "    #print(f'Delta W: {delta_W}')\n",
    "    print(f'Step {step + 1}/{num_steps} | Expected Full Batch Reward: {R:.6f} | Time: {elapsed_time:.2f}s | Avg. Time Per Step: {1000*elapsed_time/(step+1):.2f}ms')\n",
    "    #if step > 40000:\n",
    "    #  step_scale = later_step_scale\n",
    "    #  noise_scale = later_noise_scale\n",
    "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So this trick of smushing the perturbation and the update into a single step seems to work okay, though not as well as doing the very careful perturbation, but it shows that much of the correct pattern response to stimulus can be discovered through this mode. This particular idea for smushing together the perturbation and the update  is known as ALOPEX (an acronym from \"ALgorithms Of Pattern EXtraction\", first proposed by Tzanakou and Harth in 1974.) This is one way of not having a seperate perturbation and non-perturbation mode. There are other ways, such as leveraging the stochasticity of relative spike timings (ref seung), and using reward signals directly to drive learning. We will discuss these later. For now, it is enough to know that this issue can be mitigated, though with some apparrent cost to overall performance (relative to perturb-measure-step).\n",
    "\n",
    "Another point of comparison here is that the average time per step is almost half that of perturb-measure-step. This is to be expected since perturb-measure-step requires two evaluations per step, whereas this step-measure-step only requires a single evaluation per step. However, this results in each step leading to less improvement, so that even though 100 thousand steps have been taken by step-measure-step, overall performance is not as good as that of perturb-measure-step after just 60 thousand steps.\n",
    "\n",
    "Moving on to the next issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "_______________________________________________________________________\n",
    "# Algo Box: Comparing Algorithm Speedâ€”Clock Time versus Iterations\n",
    "(This Box Needs Help)\n",
    "\n",
    "When comparing the \"speed\" of algoriths, the quantity of greatest practical interest is the total time taken to reach a \"good\" solution according to some criterion for some specific problem. Total time taken can be decomposed as follows\n",
    "\n",
    "$$\n",
    "\\text{Total Time Taken} = \\text{Iterations Required} \\times \\text{Average Time per Iteration}\n",
    "$$\n",
    "\n",
    "Breaking down the total clock time into these components provides insight in the following ways:\n",
    "\n",
    "   - **Iterations Required:** This metric reflects the abstract properties of the algorithmâ€”its inherent efficiency and suitability for the problem at hand. Itâ€™s independent of the computing environment, which makes it valuable for theoretical comparisons across different settings.\n",
    "\n",
    "   - **Time Per Iteration:** This measures how long each cycle of the algorithm's process takes, influenced by two sub-factors:\n",
    "     - **Computational Complexity:** The inherent difficulty and amount of computation involved in each iteration of the algorithm. This is another abstraction, again useful for general comparisons\n",
    "     - **Implementation Efficiency:** How well the algorithm's computational steps are optimized for the hardware it runs on, including factors like code optimization and hardware utilization. While these factors are very specific to the compuational context of the algorithm, in practical engineering setting, especially when dealing with large models and lots of data, these factors can be of paramount importance.\n",
    "\n",
    "While theorists are of course drawn to things like iterations required and computational complexity, implementation efficiency is also of profound importance. As an example the current \"AI Revolution\" was catalyzed in part by co-opting GPU's, mass-market high-performance hardware designed to make video games look good, for the more niche academic application of multiplying matrices quickly. (This is not so odd as it might seem as transforming abstract video-game states into 3d models and then into 2d projections for a screen at 60 frames per second involves a lot multiplying matrices quickly.)\n",
    "\n",
    "(In much the same way the theoretical underpinnings of Bayesian MCMC approaches were well understood for many decades prior to the this approach to inference becoming of any practical use with the advent of widely available fast, parrellel computation, and efficient implementations)\n",
    "\n",
    "Implementation efficiency is something that we have to be especially aware of when jumping between computational platforms. An algorithm that is inefficient in some ways, but is easily parallelizable may be overall more effective when implemented on hardware that supports parllelization. Different algorithms allow for diffent kinds of implementation. An algorithm that requires many steps, each of great complexity might seem slow, but if those steps and complexity can be parellized out at the synapse level, i.e. very \"efficiently\" implemented in the brain, then perhaps overall it is a fast algorithm. Everything matters.\n",
    "\n",
    "_______________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.3 Full-Batch versus Mini-Batch Perturb-Measure-Step\n",
    "\n",
    "Another issue in terms of physiological viability was parameter evaluation based on performance across all possible experiences. In a simple model of learning and synaptic plasticity, we would like learning to be driven by the outcomes of a single stimulus-response-reward experience.\n",
    "\n",
    "This relates to an important idea in machine learning. The idea of the mini-batch. In a simple ML context we are (for now at least) interested in the performance of the parameters in aggregate over every input-target pair in the data set. For a large data set evaluation over the entire data set can become computationally costly. However, it is typically possible to *estimate* performance over the entire data using a small random sample of the input-target pairs in the data set. Such a sample of data points is called a mini-batch of the data. (As contrasted with the entire data-set which is called a full-batch or just batch). In the extreme case a mini-batch can consist of a single input-output pair. In this extreme case we will have moved to a situation where the organism is learning based on a single episode of sensory expereience, action selection and reward. Let's see what having a mini-batch of different sizes does to how quickly it takes for perturb-measure-step to find a good solution, both in terms of number of steps taken and total time taken. For now we test out this mini-batch idea on perturb-measure-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Mini-Batch(20) Perturb-Measure-Step Training Loop\n",
    "learn_rng = np.random.default_rng(2)\n",
    "step_scale = 0.002\n",
    "mini_batch_size = 20\n",
    "num_epochs = 1500\n",
    "dimensional_scale_factor = np.sqrt(65)\n",
    "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
    "W_init = np.zeros((1,65))\n",
    "W = W_init\n",
    "iterations = 0\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "for epoch in range(num_epochs):\n",
    "  np.random.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    iterations += 1\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_Xs = Xs_aug[batch_indices].T\n",
    "    batch_y1 = y1[batch_indices].T\n",
    "    R_current, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
    "    raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
    "    unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "    test_perturbation = unit_test_perturb * perturbation_scale\n",
    "    R_test, _ = eval_params_expectation_batch(W + test_perturbation, batch_Xs, batch_y1)\n",
    "    directional_grad_est = (R_test - R_current) / perturbation_scale\n",
    "    W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
    "\n",
    "  if epoch == 0 or (epoch + 1) % 100 == 0:\n",
    "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} | Steps: {iterations} | Expected Full Batch Reward: {total_expected_reward:.6f} | Time: {elapsed_time:.2f}s | Avg. Time per Step: {1000*elapsed_time/iterations:.2f}ms')\n",
    "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So with a mini-batch of size 20, this works pretty well. The average time per step for mini-batch (20) learning is much shorter, roughly one sixth, of the average time per step for full batch learning (the absolute time taken will vary drastically depending on the speed of the computer running this notebook, but the ratio of these times will be relatively stable accross different hardware). So in the same amount of wall clock time, say a minute, mini-batch learning can make ~300 thousand optimization iterations, whereas full batch learning makes only ~48 thousand optimization iterations in that same amount of time. Now the iterations that full-batch learning makes lead to greater improvement overall in this particular example, so for this particular problem mini-batch learning was not particularly advantageous, but on the other hand it was not problematic either.\n",
    "\n",
    "Now let's see what happens when we go to the extreme case of evaluating a single data point at a time. A case that aligns more cleanly with a model of learning where plasticity is driven by episodes consisting of stimuli-response-reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Singleton Perturb-Measure-Step Training Loop\n",
    "learn_rng = np.random.default_rng(0)\n",
    "step_scale = 0.002\n",
    "mini_batch_size = 1\n",
    "num_epochs = 80\n",
    "dimensional_scale_factor = np.sqrt(65)\n",
    "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
    "W_init = np.zeros((1,65))\n",
    "W = W_init\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "iterations = 0\n",
    "for epoch in range(num_epochs):\n",
    "  np.random.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    iterations += 1\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_Xs = Xs_aug[batch_indices].T\n",
    "    batch_y1 = y1[batch_indices].T\n",
    "    R_current, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
    "    raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
    "    unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "    test_perturbation = unit_test_perturb * perturbation_scale\n",
    "    R_test, _ = eval_params_expectation_batch(W + test_perturbation, batch_Xs, batch_y1)\n",
    "    directional_grad_est = (R_test - R_current) / perturbation_scale\n",
    "    W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
    "\n",
    "  if epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} | Steps: {iterations} | Expected Full-Batch Reward: {total_expected_reward:.6f} | Time: {elapsed_time:.2f}s | Avg. Time per Step: {1000*elapsed_time/iterations:.2f}ms')\n",
    "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So learning from a single experience using perturb-measure-step doesn't appear to be a problem. The final result is not as good as full-batch or mini-batch learning (after a similar amount of training time), but it is not that much worse. So from a physiolgical implementation perspective this kind of singleton learning is very appealing.\n",
    "\n",
    "One important thing to note is that the per-step time of singleton learning very close to the per-step time of mini-batch learning. This has to do with the way that numpy is able to efficiently utilize the available hardware resources when doing vector operations. This holds generally in typicall computation environments: evaluation time of mini-batches generally increases sub-linearly with mini-batch size up to some small size is basically constant. However, the accuracy of evaluations can be greatly improved by using slightly larger mini-batch size that have very low marginal computational cost. Run the cell below to see how accuracy and evaluation-time trade-off against eachother as a function of mini-batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to see how accuracy and evaluation time trade-off against eachother as a function of mini-batch size when evaluating a particular set of parameters, $\\mathbf{W}$.\n",
    "learn_rng = np.random.default_rng(0)\n",
    "step_scale = 0.002\n",
    "mini_batch_size = 1\n",
    "num_epochs = 5\n",
    "dimensional_scale_factor = np.sqrt(65)\n",
    "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
    "W_init = np.zeros((1,65))\n",
    "W = W_init\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "iterations = 0\n",
    "for epoch in range(num_epochs):\n",
    "  np.random.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    iterations += 1\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_Xs = Xs_aug[batch_indices].T\n",
    "    batch_y1 = y1[batch_indices].T\n",
    "    R_current, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
    "    raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
    "    unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "    test_perturbation = unit_test_perturb * perturbation_scale\n",
    "    R_test, _ = eval_params_expectation_batch(W + test_perturbation, batch_Xs, batch_y1)\n",
    "    directional_grad_est = (R_test - R_current) / perturbation_scale\n",
    "    W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
    "\n",
    "reward_full_batch, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "avg_full_batch_reward = reward_full_batch / Xs.shape[0]\n",
    "\n",
    "mini_batch_sizes = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "#mini_batch_sizes = [5, 20, 500]\n",
    "mape_list = []\n",
    "times_list = []\n",
    "time_std_devs = []\n",
    "sample_rewards = {}\n",
    "\n",
    "for size in mini_batch_sizes:\n",
    "  avg_rewards = []\n",
    "  for _ in range(10000):\n",
    "    rng.shuffle(indices)\n",
    "    batch_indices = indices[:size]\n",
    "    batch_Xs = Xs_aug[batch_indices].T\n",
    "    batch_y1 = y1[batch_indices].T\n",
    "    reward_mini_batch, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
    "    avg_mini_batch_reward = reward_mini_batch / size\n",
    "    avg_rewards.append(avg_mini_batch_reward)\n",
    "  mape = np.mean(np.abs((avg_rewards - avg_full_batch_reward) / avg_full_batch_reward)) * 100\n",
    "  mape_list.append(mape)\n",
    "  sample_rewards[size] = avg_rewards  # Storing rewards for histogram\n",
    "  batch_Xs = Xs_aug[:size].T\n",
    "  batch_y1 = y1[:size].T\n",
    "  print(f'timeit result for mini-batch of size {size}:')\n",
    "  timeit_result = %timeit -o eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
    "  times_list.append(timeit_result.average * 1000)\n",
    "  time_std_devs.append(timeit_result.stdev * 1000)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "#lower_bounds = [mean - std for mean, std in zip(times_list, time_std_devs)]\n",
    "#upper_bounds = [mean + std for mean, std in zip(times_list, time_std_devs)]\n",
    "\n",
    "plt.errorbar(times_list, mape_list, xerr=time_std_devs, fmt='o', color='blue', ecolor='lightgray', elinewidth=3, capsize=0)\n",
    "plt.scatter(times_list, mape_list, color='blue')\n",
    "for i, txt in enumerate(mini_batch_sizes):\n",
    "    plt.annotate(txt, (times_list[i], mape_list[i]))\n",
    "plt.xlabel('Average Evaluation Time (ms)')\n",
    "plt.ylabel('Mean Absolute Percentage Error (%)')\n",
    "plt.yscale('log')  # Set the y-axis to a logarithmic scale\n",
    "plt.title('Accuracy (MAPE) vs. Evaluation Time \\n by Mini-Batch size')\n",
    "plt.grid(True, which=\"both\", ls=\"--\")  # Adjust grid to appear properly on log scale\n",
    "plt.show()\n",
    "\n",
    "# Concatenate all rewards to determine optimal bin edges\n",
    "all_data = np.concatenate([sample_rewards[size] for size in [5, 20, 500]])\n",
    "# Determine optimal bin edges using the entire dataset\n",
    "bin_edges = np.histogram_bin_edges(all_data, bins='auto')  # 'auto' lets numpy choose the optimal number of bins\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "for size in [5, 20, 500]:\n",
    "  plt.hist(sample_rewards[size], bins=bin_edges, alpha=0.5, label=f'Mini-Batch Size {size}')\n",
    "\n",
    "plt.xlabel('Mini-Batch Avg. Reward per Episode Estimate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Mini-Batch Evaluation Estimates')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note the high variability in evaluation time. Despite this a general trend is detectable. Below a certain mini-batch size, evaluation times are all very close, but eventuallly mini-batch size starts to have a large enough impact on evaluation time to be consistently detectable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Bonus Thinking Question**\n",
    "Why are there funny little peaks in the mini-batch avg. reward per episode estimates for mini-batches of size 5?\n",
    "\n",
    "**Answer**: These peak correspond to the number of viable striking opportunities in the mini-batch. With perfect performance an average reward per episode of 0, 0.2, 0.4, 0.6, 0.8 and 1 will be obtained (these correspond to the peaks) depending solely on the number of striking opportunities available in the mini-batch. From this we can see that the average reward recieved on a given mini-batch is strongly influenced both by the parameters governing behaviour and by the particular data points in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The above simulation helps to illustrate how the trade-off between accuracy of evaluation and time per evaluation can change with batch size. It is important to note though the particular shape of this trade-off curve depends on **everything** about the problem, the size and and variability of the data, the complexity of the evaluation function, the implemention of the evaluation function and in particular how efficiently the evaluation function is implemented relative to the available hardware resources. Also how important accurate evalulations are for the learning algorithm can be hugely variable, so the implications of this trade-off need to be considered within the broader context of the optimization/learning problem. All that to say, mini-batches can be a very powerful tool to accelerate learning in an ML context, but some care is required to utilize them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Algo Box: Mini-Batch\n",
    "\n",
    "In many ML problems the goal is to find parameters $\\theta$ such that some objective $J(\\theta)$ is maximized or minimized. Further, in such problems this objective function is often defined in relation to a discrete and finite data set $\\mathcal{D}$, as the sum of evaluations of some loss function, $l(\\mathbf{d},\\theta)$, with parameters $\\theta$ and arguments $\\mathbf{d} \\in \\mathcal{D}$, i.e.\n",
    "\n",
    "$$ J(\\theta) = \\sum_{\\mathbf{d} \\in \\mathcal{D}} l(\\mathbf{d,\\theta)} $$\n",
    "\n",
    "Any iterative optimization algorithm will be evaluating either the objective function directly, or gradients of first (and possibly also second) order, as part of its optimization proccess. As the data set grows larger, i.e. contains more data points, evaluations of the objective function and its gradients, will require more compute. However, if many of the data points are similar (or even identical) all this extra computation may not be provide much additional information about how best to update the parameters $\\theta$ to improve the objective $J(\\theta)$.\n",
    "\n",
    "An effective method to more efficiently utilize computational resources is mini-batch learning. In mini-batch learning, instead of using the entire data set to compute the parameter update, mini-batch learning uses a small, randomly selected subset of the data. This not only speeds up the computation but also introduces randomness into the optimization process, which can help in escaping local extrema, and hence find better global solutions. In psuedo code a full-batch training loop looks like:\n",
    "\n",
    "___\n",
    "* Set initial parameter guess for $\\theta$\n",
    "* While convergence condition not met:\n",
    "    *  Update parameters according to $\\theta \\leftarrow \\text{opt_step}(\\theta,\\mathcal{D})$\n",
    "___\n",
    "\\\n",
    "In constrast mini-batch training looks like:\n",
    "___\n",
    "* Set initial parameter guess for $\\theta$\n",
    "* While convergence condition not met:\n",
    "    * Select a random subset $\\mathcal{D}_{\\text{mini}} \\subset \\mathcal{D}$ of fixed size $n$\n",
    "    * Update parameters according to $\\theta \\leftarrow \\text{opt_step}(\\theta, \\mathcal{D}_{\\text{mini}})$\n",
    "___\n",
    "\n",
    "\\\n",
    "In practice mini-batches are typically not just sampled at random, but rather the entire data set is partioned randomly, and then learning proceeds by iterating through each distinct, non-overlapping mini-batch. After each mini-batch has been used the data set is shuffled and partioned again, to produce a new set of random mini-batches. This ensures that no one data point is given more importance during learning over another, and also that learning isn't overly influenced by any particular partioning. Each iteration through all the mini-batches is refered to as an *epoch*. Most mini-batch training loops use epochs and have this form:\n",
    "___\n",
    "* Set initial parameter guess for $\\theta$\n",
    "* While convergence condition not met:\n",
    "    * New epoch, shuffle and partition dataset $\\mathcal{D}$ into random mini-batches $\\mathcal{D}_1, \\mathcal{D}_2, \\dots, \\mathcal{D}_k$ of fixed size $n$\n",
    "    * For each mini-batch $\\mathcal{D}_i$:\n",
    "        * Update parameters according to $\\theta \\leftarrow \\text{opt_step}(\\theta, \\mathcal{D}_i)$\n",
    "___\n",
    "\n",
    "In the extreme case mini-batches can even consist of a single data point. The mini-batch idea developed in the context of Gradient Descent optimization so, for historical reasons learning with mini-batches of size 1 is called \"Stochastic Gradient Descent\". This historically grounded terminology is slightly confusing though as the mini-batch idea can and is usefully applied to many different iterative optimization algorithms. We will use the terms full-batch, mini-batch, and singleton based learning and evaluation to apply generally to any optimization process.\n",
    "\n",
    "In practical ML settings, whenever the data-set becomes sufficiently large, mini-batches are almost always used, as they allow for the most efficient use of computational resources. However, determining the best mini-batch size is a tricky problem. Neither extreme is ideal. Full-batch learning provides the most accurate evaluation possible but is computationally expensive. Singleton learning is the least computationally expenisive but provides the least accurate evaluation. The mini-batch size that perfectly trades-off between these two factors, evaluation accuracy and computational time, will depend on many factors including:\n",
    " * Hardware that the computation is being run on\n",
    " * Implementation efficiency of the evaluation function and optimization process relative to the hardware\n",
    " * Variability of the underlying data-set\n",
    " * The optimization alogirithm being used\n",
    " * The type and structure of model being trained\n",
    "All off these factors are important, and to make the problem of selecting mini-batch size harder, all of these factors interact to determine optimal mini-batch size.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Unfortunately, from a biological plausibility perpsective, smushing the update step and the perturbation step together, i.e. step-measure-step, does not mix well with the mini-batches. The key issue in combining these two ideas is that in the mini-batch training loops above **the same** mini-batch of data points was used to evaluate both the base parameters and the perturbed parameters alowing for a precise estimate of the rate of improvement in the direction of the test perturbation. If two **different** mini-batches are used to estimate the reward at two different points in the parameter space (as happens when we try to use step-measure-step with mini-batches). Using different mini-batches of data to estimate of the degree of improvement between the base and perturbed parameters adds so much noise to the process that learning becomes intractable. In some cases learning may still be possible but even if it is possible it will be too slow to be practical both in an ML context and in the context of rapdily aquiring adaptive behaviours.\n",
    "\n",
    "Finally we turn our attention to using actual sampled rewards, instead of a computed expectation of reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.4 Recieved Reward Perturb-Measure-Step\n",
    "\n",
    "Later on in this book we will see how an organism might develop an internal model of how their actions interact with the dynamics of the environment to change the state of the environment and produce rewards, but for the moment we are focused on arriving at the simplest viable learning circuit that can rapidly learn good behaviour on the strike-no-stike problem, while invoking only simple and easy to imagine physiological mechanisms of plasticity. So we wish to exclude the computation of expected reward, and instead focus on the actual rewards recieved by the organism and how they might drive synaptic weight changes. In the training loop below we implement perturb-measure-step but using stochastic evaluation. Let's see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Sampled-Reward Perturb-Measure-Step Training Loop\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_steps = 30000\n",
    "step_scale = 0.000005\n",
    "dimensional_scale_factor = np.sqrt(65)\n",
    "perturbation_scale = 0.01 # std of gaussian test perturbations\n",
    "W_init = np.zeros((1,65))\n",
    "W = W_init\n",
    "start_time = time.time()\n",
    "for step in range(num_steps):\n",
    "  R_current, _ = eval_params_stochastic_batch(W, Xs_aug.T, y1.T)\n",
    "  raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
    "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "  test_perturbation = unit_test_perturb * perturbation_scale\n",
    "  R_test, _ = eval_params_stochastic_batch(W + test_perturbation, Xs_aug.T, y1.T)\n",
    "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
    "  W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
    "\n",
    "  if step == 0 or (step + 1) % 3000 == 0:\n",
    "    R_exp, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Step {step + 1}/{num_steps} | Expected Full-Batch Reward: {R_exp:.6f} | Time: {elapsed_time:.2f}s | Avg. Time per Step: {1000*elapsed_time/iterations:.2f}ms')\n",
    "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So this works okay. The extra noise injected into the learning process by using stochastic evaluation of the reward function makes it harder for perturb-measure-step to \"measure\" the rate of improvement in a test direction, and as a result some learning occurs but it is slower and less effective than when we use the exact expected reward to measure/estimate the rate of improvement in the direction of the random test perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.5 Putting them all together\n",
    "\n",
    "So there are simple ideas that overcome our key issues with perturb-measure-step as implemented initially. We can effectively learn from single experiences, we can effectively learn from actual rewards resulting from actual rewards made (not expectated reward given hypothetical probabilities of taking given actions), and we can effectively use the difference in reward between two episodes for learning instead of requiring a seperate perturbation evaluations mode of operation for the network. All of these are lovely ideas, however, they do not combine particularly well, at least not in a straightforward way. Learning is still possible when all of these ideas are combined, but improvement is so slow as to be totally impractical. Below is an example of learning not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Sampled-Reward Singleton Step-Measure-Step Training Loop\n",
    "# Delete or comment out the %%skip to run this cell\n",
    "%%skip\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 100\n",
    "mini_batch_size = 1\n",
    "step_scale =  0.000001\n",
    "noise_scale = 0.0000005\n",
    "W_init = np.zeros((1,65))\n",
    "W = W_init\n",
    "delta_W = np.zeros((1,65))\n",
    "iteration=0\n",
    "R, _ = eval_params_stochastic_batch(W, Xs_aug.T, y1.T)\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "  np.random.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    iteration += 1\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_Xs = Xs_aug[batch_indices].T\n",
    "    batch_y1 = y1[batch_indices].T\n",
    "    R_old = R\n",
    "    R, _ = eval_params_stochastic_batch(W, batch_Xs, batch_y1)\n",
    "    delta_R = R - R_old\n",
    "    delta_W_noise = learn_rng.normal(0, noise_scale, size=(1,65))\n",
    "    delta_W = total_scale * (step_scale * delta_W * delta_R + delta_W_noise)\n",
    "    W += delta_W\n",
    "  if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} | Steps: {iteration} | Expected Full-Batch Reward: {total_expected_reward:.6f} | Time: {elapsed_time:.2f}s | Avg. Time per Step: {1000*elapsed_time/iterations:.2f}ms')\n",
    "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.6 Is Solving High Dimensional Optimization Problem Necissary for Adaptive Behaviour? Hard Yes.\n",
    "\n",
    "From a machine learning perspective, when thinking about the performance of a model, there are several things that might be limiting performance. Maybe the data is fundamentally difficult, e.g. there are identical inputs with different correct outputs, i.e. $\\mathbf{x} = \\mathbf{x}'$, but $y = 0 $ and $y'=1$. Another possibility is that the learning algorithm being used is not effective given the model archetecture and the available data. Yet a third possibility is that archetecture of the model itstelf is not complex or flexible enough to capture the relevent patterns in the data. This last reason happens to be the case here.\n",
    "\n",
    "One reason for having a large, flexible brain is to produce complex behaviour that is contingent on the state of the world in complex ways. The simple strike-no-strike network we've been using consists of a single layer of synaptic connections between the sensory inputs and the motor output. In a shallow network structure like this, a sensory input either directly inhibits or promotes the striking behaviour. There is no possibility for a sensory input to promote striking in some sensory contexts, but inhibit it in others. This is a deficincy. Context is important, loud thumping and roaring noises at a concert means the band is good, similar noises when camping in the wilderness means a large animal is nearby. A shallow network does not allow for such context dependence.\n",
    "\n",
    "How do we allow for context dependence, or put another way, interactions between the inputs (beyond simply summing them together). As a starting point our current sensory-behavioural circuit is effectively equivalent to logistic regression, i.e. each sensory input element (feature in ML parlance) can either inhibit or potentiate striking behaviour to varying degrees, but there is no possibility for conditional interaction between features. By 'conditional interaction,' we mean a scenario where, for instance, feature 1 typically inhibits the behavior, except when feature 2 is positive, under which condition feature 1 becomes potentiating. These kinds of feature interactions are impossible in the current model. One way to allow for such interactions is to augment the base set of features with composite features, e.g. incorporate all the pairwise products of the existing feature set, so that instead of 65 features (bias included) we have $(\\frac{64 \\cdot 63}{2} + 64 + 64 + 1) = 2145$ features to work with. This could work, but what if we want something that depends on the interaction of more than 2 features, adding higher order polynomial terms will quickly make the problem intractable (Reference appendix section on why hidden layers not polynomials if we do that). If we had some mechanistically grounded understanding or hypothesis about the relationship between the features and the target we might be able to cherry pick some small subset of higher order interaction terms, but the ML mindset is in large part about automating the feature selection processes based on the data alone.\n",
    "\n",
    "It turns out that instead of resorting the regression on polynomial terms to capture feature interactions, there is a much more compact and expressive way of allowing for feature interactions. The idea is to allow for feature interactions to emerge as needed in a 'hidden' computational layer of our highly abstracted neurons.\n",
    "\n",
    "Simply adding an additional layer to the network, consisting of some intermediate (hidden) neurons between the inputs and the outputs, allows for context dependent inhibition and promotion of striking. There are theorems showing that a much as polynomials of arbitrarily high degree are a kind of universal function approximator (e.g. taylor series), simiarly networks with a singal (but potentially very large hidden layers) are a kind of universal function approximator (cites). With this in mind, let's see if adding a hidden layer to our network improves performance.\n",
    "\n",
    "Our new strike-no-strike network with 1 hidden layer is structured as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As before $\\mathbf{x}$ is the raw sensory input (vector) in a given episode and each element of $\\mathbf{x}$ corresponds to the activation level and firing rate of a single photosensitive neuron.\n",
    "These input neurons are then connected by synapses to a 'hidden layer' of intermediate computational neurons, say 10 of them. The activation level of these hidden layer neurons is computed as\n",
    "$$\\mathbf{h} = \\sigma(\\mathbf{W}_{in} \\mathbf{x})$$\n",
    "Now $\\mathbf{W}_{in}$ is a matrix of synaptic weights between the input neurons and the hidden layer neurons. (In this case $\\mathbf{W}$ has shape $10 \\times 65$. Each the values in the $i^{th}$ row of $\\mathbf{W}_{in}$ given the sign and strength of the connections coming into the $i^{th}$ element of $h$ and similarly each value in the $j^{th}$ column of $\\mathbf{W}_{in}$ corresponds to connection strengths coming out of the $j^{th}$ sensory input neuron.)  We still us $\\sigma$ to represent the standard logistic sigmoid function, but in this case applied elementwise the vector output of the standard matrix product $\\mathbf{W}_{in} \\mathbf{x}$. Then, much as before our striking probability is computed as\n",
    "$$z = \\mathbf{W}_{out} \\cdot \\mathbf{h}$$\n",
    "and\n",
    "$$ \\Pr \\{\\text{strike}\\} = \\sigma(z) $$\n",
    "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma(z)$$\n",
    "Here $\\mathbf{W}_{out}$ has shape $1  \\times 10$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We will need to re-write a new eval params function for this new model, let's do it. To keep things quick and simple we will just use expecation based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def eval_params_batch_expectation_hidden(W_in, W_out, x, y, verbose=False):\n",
    "  \"\"\"\n",
    "  evaluates parameters of simple behaviour circuit given inputs and target\n",
    "  outputs, use numpy broadcasting to be fast and concise\n",
    "  Args:\n",
    "    W_in: (hidden_neurons x inputs(65) np.array)\n",
    "           weights between sensory neurons and hidden layer neurons\n",
    "    W_out: (output(1) x hidden_neurons np.array)\n",
    "           weights between hidden layer neurons and output\n",
    "    x: (input(64) x batch np.array) sensory input\n",
    "       (can be single input, mini-batch of inputs or the whole batch of inputs)\n",
    "    y: (outputs(1) x batch np.array) target behavioural output\n",
    "       (can be a single target, mini-batch of targets, or whole batch),\n",
    "       needs to correspond to input\n",
    "\n",
    "  Returns:\n",
    "    R_exp: the average/expected total reward obtained given the parameters, over the\n",
    "           (mini-)batch of inputs and targets. (mini-batch could be size 1)\n",
    "  \"\"\"\n",
    "  # activation\n",
    "  h = np_sigmoid(np.dot(W_in,x)) # hidden_neurons x batch\n",
    "  strike_prob = np_sigmoid(np.dot(W_out,h)) # 1 x batch\n",
    "  # Expected true positives (TPs) and false positives (FPs)\n",
    "  TPs = np.sum(strike_prob * y)  # Sum of strike probabilities where true label is 1\n",
    "  FPs = np.sum(strike_prob * (1 - y))  # Sum of strike probabilities where true label is 0\n",
    "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
    "  FNs = np.sum((1 - strike_prob) * y)  # Sum of no strike probabilities where true label is 1\n",
    "  TNs = np.sum((1 - strike_prob) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
    "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
    "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
    "  if verbose:\n",
    "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
    "    table = [[\"Should Strike\", TPs, FNs],\n",
    "             [\"Shouldn't Strike\", FPs, TNs]]\n",
    "    print(\"Confusion_matrix: \")\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
    "    print(f'Total Reward: {R_exp}')\n",
    "    return None\n",
    "  else:\n",
    "    return R_exp, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We've got a more complicated circuit with more parameters, how much longer does it take us to evaluate this circuit compared to our previous one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "W = np.zeros((1,65))\n",
    "%timeit eval_params_expectation_batch(W, Xs_aug.T, y1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "W_in = np.zeros((10,65))\n",
    "W_out = np.zeros((1,10))\n",
    "%timeit eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Roughly 5x longer per function evaluation, which means not only will we likely need more iterations of our algorithm because it is harder to find good parameters in high dimensions (we have 660 parameters now, which is a lot more than 65), but also each of those steps will take longer to process because function evaluations are also more costly. It will all be worth it if we can get better final performance though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title 10 Hidden Units - Perturb-Measure-Step Training Loop\n",
    "llearn_rng = np.random.default_rng(0)\n",
    "num_steps = 20000\n",
    "step_scale = 0.025\n",
    "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
    "num_hidden_units = 10\n",
    "# initializing both layers as zero leads to some issues, so we\n",
    "# use a Xavier/Glorot random initialization scheme\n",
    "in_init = np.sqrt(6 / (65 + num_hidden_units))\n",
    "W_in_init = learn_rng.uniform(-in_init, in_init, size=(num_hidden_units, 65))\n",
    "out_init = np.sqrt(6 / (10 + 1))\n",
    "W_out_init = learn_rng.uniform(-out_init, out_init, size=(1, num_hidden_units))\n",
    "flat_params = np.concatenate((W_in_init.flatten(), W_out_init.flatten()))\n",
    "dimensional_scale_factor = np.sqrt(len(flat_params))\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "for step in range(num_steps):\n",
    "  W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
    "  W_in = W_in.reshape((num_hidden_units,65))\n",
    "  W_out = W_out.reshape((1,num_hidden_units))\n",
    "  R_current, _ = eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)\n",
    "  raw_param_perturb = learn_rng.standard_normal(size=len(flat_params))\n",
    "  unit_param_perturb = raw_param_perturb / np.linalg.norm(raw_param_perturb.flatten())\n",
    "  test_perturbation = unit_param_perturb * perturbation_scale\n",
    "  perturbed_flat_params = flat_params + test_perturbation\n",
    "  W_in_test, W_out_test = np.split(perturbed_flat_params, [65 * num_hidden_units])\n",
    "  W_in_test = W_in_test.reshape((num_hidden_units,65))\n",
    "  W_out_test = W_out_test.reshape((1,num_hidden_units))\n",
    "  R_test, _ = eval_params_batch_expectation_hidden(W_in_test, W_out_test, Xs_aug.T, y1.T)\n",
    "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
    "  flat_params += step_scale * dimensional_scale_factor * directional_grad_est * unit_param_perturb\n",
    "\n",
    "  if step == 0 or (step + 1) % 1000 == 0:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Step {step + 1}/{num_steps} completed | Current Total Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
    "W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
    "W_in = W_in.reshape((num_hidden_units,65))\n",
    "W_out = W_out.reshape((1,num_hidden_units))\n",
    "eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So this new, more complex circuit is great. We're much closer to the theoretical maximum performance of 2829, maybe with a few more hidden units, and a little longer training time we could have perfect discrimination. Let's see what happens when we go up to 20 hidden units. As a heads up this is going to take awhile (about 3 minutes) so you should read ahead while waiting for this training loop to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title 20 Hidden Units - Perturb-Measure-Step Training Loop\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_steps = 20000\n",
    "step_scale = 0.025\n",
    "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
    "num_hidden_units = 20\n",
    "# initializing both layers as zero leads to some issues, so we\n",
    "# use a Xavier/Glorot random initialization scheme\n",
    "in_init = np.sqrt(6 / (65 + num_hidden_units))\n",
    "W_in_init = learn_rng.uniform(-in_init, in_init, size=(num_hidden_units, 65))\n",
    "out_init = np.sqrt(6 / (10 + 1))\n",
    "W_out_init = learn_rng.uniform(-out_init, out_init, size=(1, num_hidden_units))\n",
    "flat_params = np.concatenate((W_in_init.flatten(), W_out_init.flatten()))\n",
    "dimensional_scale_factor = np.sqrt(len(flat_params))\n",
    "start_time = time.time()\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "for step in range(num_steps):\n",
    "  W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
    "  W_in = W_in.reshape((num_hidden_units,65))\n",
    "  W_out = W_out.reshape((1,num_hidden_units))\n",
    "  R_current, _ = eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)\n",
    "  raw_param_perturb = learn_rng.standard_normal(size=len(flat_params))\n",
    "  unit_param_perturb = raw_param_perturb / np.linalg.norm(raw_param_perturb.flatten())\n",
    "  test_perturbation = unit_param_perturb * perturbation_scale\n",
    "  perturbed_flat_params = flat_params + test_perturbation\n",
    "  W_in_test, W_out_test = np.split(perturbed_flat_params, [65 * num_hidden_units])\n",
    "  W_in_test = W_in_test.reshape((num_hidden_units,65))\n",
    "  W_out_test = W_out_test.reshape((1,num_hidden_units))\n",
    "  R_test, _ = eval_params_batch_expectation_hidden(W_in_test, W_out_test, Xs_aug.T, y1.T)\n",
    "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
    "  flat_params += step_scale * dimensional_scale_factor * directional_grad_est * unit_param_perturb\n",
    "\n",
    "  if step == 0 or (step + 1) % 1000 == 0:\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f'Step {step + 1}/{num_steps} completed | Current Total Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
    "W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
    "W_in = W_in.reshape((num_hidden_units,65))\n",
    "W_out = W_out.reshape((1,num_hidden_units))\n",
    "eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This more complex circuit gets us even closer to the theoretical maximum performance of 2829, but it's taking longer to get there. This is a consequence of both the function evaluations required at each step being slower (for the more complex function) and because more iterations are needed to effectively search the higher dimensional space for a good configuration of $\\mathbf{W}_{in}$ and $\\mathbf{W}_{out}$. With even more hidden units and more time we can likely learn perfect discrimination, but it will take even longer (more than 3 minutes!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Although the toy neural circuit models in this sequence are a far cry from actual neural circuits, they still provide insight into possible mechanisms of synaptic plasticity the brain. We can imagine a scenario where synaptic strengths between neurons in a circuit undergo small, transient perturbations. The brain might integrate and compare the performance of these perturbations over a learning episode (for example, a day) to previous performance levels. (We leave aside the specifics of how this integration and comparison occur for now.)\n",
    "\n",
    "If performance improves with a perturbation, synaptic changes could be consolidated in the direction of the perturbation, proportionate to the degree of improvement. Conversely, if performance worsens, changes might be consolidated in the opposite direction, also proportional to the performance decrease. This concept, while still vague, suggests a mechanism of synaptic adjustment based on performance feedback.\n",
    "\n",
    "One critical point to consider is the scalability of such a learning process. The number of learning episodes required for effective optimization grows with the number of parameters in a neural circuit. This implies that 'perturb-measure-step' plasticity cannot be the primary mechanism driving neural plasticity in large, complex neural circuits that learn rapidly. This limitation is critical, the lifetimes of most animals simply aren't long enough to accommodate the number of learning iterations needed for extensive optimization.\n",
    "\n",
    "However, as demonstrated above, a more complex circuit achieved significantly better performance in the discrimination task, so large complex circuits can be useful. This suggests that even if empirical evidence of perturbation-based learning in the brain exists and its physiological implementation is understood, such processes are unlikely to be the primary drivers of neural plasticity for complex and challenging behaviors.\n",
    "\n",
    "(One counterargument in favor of simple learning rules is that extensive learning might not be necessary if genetic predisposition starts the circuit off close to an optimal parameter configuration. Then subsequently, relatively slow learning processes could 'fine-tune' the neural circuit's configuration. However, as noted in our earlier discussions on evolution, changing environments necessitate that a significant portion of behavior must emerge from learning, thereby limiting the extent to which genetic predispositions can facilitate efficient and adaptive learning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this sequence we looked at how a simple neural circuit, and then a slightly more complex one were able to solve the strike-no-strike discrimination problem, when trained using perturb-measure-step. We found that several of the \"unrealistic\" aspects of perturb-measure-step (full batch learning, a seperate perturbation mode, used expected instead of actual reward to inform learning) could be mitigated, but only at the cost of slowing down learning (though using mini-batches in isolation, when evaluating the perturbation and current parameters on the same mini-batch was actually a speed up). Additionally we saw that using a more complex network (with hidden layers of increasing size) allowed for better perfomance on the discrimination task, but this extra performance came at a cost, function evaluation was more complex and there were more parameters to be optimized, and searching higher-dimensional spaces for good parameter configurations is always harder (more places to look, more directions to try, in addition to more compute required to for each test point evaluation).\n",
    "\n",
    "In the next sequence we will introduce a different perspective on neural plasticity, one in which synaptic plasticity is driven between a mismatch between an expectation or prediction about reward, and the actual recieved reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# E-Phys/Neuro Box\n",
    "Neuroscientists have long conceptualized the learning process of neurons in terms of perturb, measure, step (cites). Synapses do not always produce an influence on the post-synaptic neuron if a spike comes in. In fact, they only produce a respose in about 50%% of all cases (cites, have figure element). We also know that there are mechanisms that can broadcast reward to all neurons, e.g. through Dopamine (cites). A neuron can then use a rule where learning is both proportional to rewards and proportional to the transmission at that point of time. The effect is that synapses whose existence helps make rewards higher gets consistently strengthened and those whose existence make rewards lower get consistently weakened. This way, single synapses can biophysically implement a global optimization algorithm.\n",
    "\n",
    "[insert pictuere from fiete paper around here].\n",
    "\n",
    "In our simple model of behaviour the pre-synaptic activity for synapse $w_i$ is $x_i$ and the post synaptic activity is $z = \\sum_i {w_i} \\cdot x_i$. We do not explicitly model spiking behaviour here though how stochastic spiking behaviour creates the perturbations necissary for perturb, measure, step has been extensively studied in (cites).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to display the box\n",
    "%%html\n",
    "<div style=\"background-color: #f2f2f2; border: 1px solid #d3d3d3; padding: 10px;\">\n",
    "    <p style=\"color: #333;\"> Neuroscientists have long conceptualized *the* the learning process of neurons in terms of perturb, measure, step (cites). Synapses do not always produce an influence on the post-synaptic neuron if a spike comes in. In fact, they only produce a respose in about 50%% of all cases (cites, have figure element). We also know that there are mechanisms that can broadcast reward to all neurons, e.g. through Dopamine (cites). A neuron can then use a synaptic plasticity rule where strengthening (weakening if reward is negative) of the synaptic connection is both proportional to rewards and proportional to the transmission at that point in time. The effect is that synapses whose existence helps make rewards higher gets consistently strengthened and those whose existence make rewards lower get consistently weakened. This way, single synapses can biophysically implement a global optimization algorithm.\n",
    "\n",
    "[insert pictuere from fiete paper around here].</p>\n",
    "</div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P2C1_Sequence4",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
