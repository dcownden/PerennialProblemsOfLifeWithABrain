{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/split-and-simple-perturb/sequences/P2C1_Optimization/P2C1_Sequence5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "K10DbaXu1WBL"
      },
      "source": [
        "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dsQ6T5OZ1WBM"
      },
      "source": [
        "___\n",
        "# **2.1.5: Learning Behaviour Quickly Using Delta Rule**\n",
        "\n",
        "### Objective: Investigate a new, simple optimization procedure with clear physiological interpretation.\n",
        "\n",
        "In this sequence we will:\n",
        "\n",
        "* Continue to work with the strike-no-strike problem where the decision depends on 64 sensory inputs (features) to investigate a more rapid plasticty rule known as the \"Delta Rule\"\n",
        "\n",
        "* Compare the \"Delta Rule\" to perturb-measure-step both in terms of learning rate and physilogical plausibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rK0I5ANa1WBM"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Run the following cell to setup and install the various dependencies and helper functions for this ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "g9MNYxfk1WBN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89197849-a4da-4227-82b6-27af18a9619e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best feature is 21\n",
            "Random seed 2021 has been set.\n",
            "This notebook isn't using and doesn't need a GPU. Good.\n",
            "Running in colab\n"
          ]
        }
      ],
      "source": [
        "# @title Dependencies, Imports and Setup\n",
        "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
        "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
        "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz ucimlrepo > /dev/null 2> /dev/null #google.colab\n",
        "\n",
        "import asyncio\n",
        "import requests\n",
        "from requests.exceptions import RequestException\n",
        "import numpy as np\n",
        "import itertools\n",
        "import collections\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.colors import LogNorm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import gridspec\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pygraphviz as pgv\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import warnings\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from io import BytesIO\n",
        "from enum import Enum\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
        "from jupyterquiz import display_quiz\n",
        "from vibecheck import DatatopsContentReviewContainer\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from tqdm.notebook import tqdm\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "data_set = fetch_ucirepo(id=80)\n",
        "X = data_set.data.features.values\n",
        "# Translate the data to have a minimum of 0\n",
        "X_translated = X - X.min()\n",
        "# Scale the data to have a range from 0 to 12 (which is 6 - (-6))\n",
        "scaling_factor = 12 / (X.max() - X.min())\n",
        "X_scaled = X_translated * scaling_factor\n",
        "# Finally, shift the data to be centered between -6 and 6\n",
        "X_final = X_scaled - 6\n",
        "\n",
        "y = data_set.data.targets.values\n",
        "rng = np.random.default_rng(seed=2021)\n",
        "scramble_permutation = rng.permutation(X.shape[1])\n",
        "Xs = X_final[:, scramble_permutation]\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "y1 = y % 2\n",
        "y2 = np.array(y >= 5, dtype=y.dtype)\n",
        "simple_index = ((y.flatten()==1) | (y.flatten()==0))\n",
        "X_simple = Xs[simple_index]\n",
        "y1_simple = y1[simple_index]\n",
        "# if you only had one feature which would likely be best for discrimination\n",
        "epsilon = 10\n",
        "class_a_sep = np.mean(X_simple[y1_simple.flatten() == 1, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 1, :], axis=0) + epsilon)\n",
        "class_b_sep = np.mean(X_simple[y1_simple.flatten() == 0, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 0, :], axis=0) + epsilon)\n",
        "best_feature = np.argmax(class_a_sep - class_b_sep)\n",
        "print(f'Best feature is {best_feature}')\n",
        "X_simple_1_feature = X_simple[:, [best_feature]]\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
        "# random seed settings and\n",
        "# getting torch to use gpu if it's there\n",
        "\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook but not needed.\")\n",
        "    print(\"If possible, in the menu under `Runtime` -> \")\n",
        "    print(\"`Change runtime type.`  select `CPU`\")\n",
        "\n",
        "  return device\n",
        "\n",
        "\n",
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "  display(Markdown(string))\n",
        "\n",
        "\n",
        "# the different utility .py files used in this notebook\n",
        "filenames = []\n",
        "# just run the code straight out of the response, no local copies needed!\n",
        "for filename in filenames:\n",
        "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
        "  response = requests.get(url)\n",
        "  # Check that we got a valid response\n",
        "  if response.status_code == 200:\n",
        "    code = response.content.decode()\n",
        "    exec(code)\n",
        "  else:\n",
        "    print(f'Failed to download {url}')\n",
        "\n",
        "# environment contingent imports\n",
        "try:\n",
        "  print('Running in colab')\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "  from google.colab import data_table\n",
        "  data_table.disable_dataframe_formatter()\n",
        "  #from google.colab import output as colab_output\n",
        "  #colab_output.enable_custom_widget_manager()\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  print('Not running in colab')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "%matplotlib widget\n",
        "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
        "plt.ioff() #need to use plt.show() or display explicitly\n",
        "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "def remove_ip_clutter(fig):\n",
        "  fig.canvas.header_visible = False\n",
        "  fig.canvas.toolbar_visible = False\n",
        "  fig.canvas.resizable = False\n",
        "  fig.canvas.footer_visible = False\n",
        "  fig.canvas.draw()\n",
        "\n",
        "\n",
        "def content_review(notebook_section: str):\n",
        "  return DatatopsContentReviewContainer(\n",
        "    \"\",  # No text prompt\n",
        "    notebook_section,\n",
        "    {\n",
        "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
        "      \"name\": \"neuro_book\",\n",
        "      \"user_key\": \"xuk960xj\",\n",
        "    },\n",
        "  ).render()\n",
        "feedback_prefix = \"P2C1_S5\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.1 Learning Strike-No-Strike with the Delta Rule.\n",
        "\n",
        "Recall from the previous sequence that cartoon organism that inspires this problem can be thought of as having 64 photo-sensitive receptors, and based on this combination of inputs from these receptors it must decide whether to strike or not. The organism pays a cost of one if it strikes when it shouldn't and recieves a reward of one if it strikes when it should. It receives no cost or reward when it does not strike. To get a sense of this discrimination problem, try it yourself by running the code cell below."
      ],
      "metadata": {
        "id": "OR3gHzvHXExf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown **Run this cell** to try out the more complex 'strike-no-strike' discrimination task.\n",
        "\n",
        "class InteractiveMNISTPredator():\n",
        "  def __init__(self,\n",
        "               features=Xs,\n",
        "               labels=y1,\n",
        "               extra_labels=y,\n",
        "               feedback_type='on_strike_only', seed=123):\n",
        "    # Initialize dataset, settings for image scrambling and feedback\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "    # features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)\n",
        "    # labels is num_data_points x 1 (values 0-9 or 0/1 depending)\n",
        "    self.feedback_type = feedback_type\n",
        "    self.rng = np.random.default_rng(seed)\n",
        "    #sample_order = np.arange(self.features.shape[0])\n",
        "    sample_order = self.rng.permutation(self.features.shape[0])\n",
        "    self.features = self.features[sample_order]\n",
        "    self.labels = self.labels[sample_order]\n",
        "    self.extra_labels = extra_labels[sample_order]\n",
        "    # initialize game state\n",
        "    self.current_index = 0\n",
        "    self.current_image = None\n",
        "    self.previous_image = None\n",
        "    self.score = 0\n",
        "    self.best_possible_score = 0\n",
        "    self.successful_strikes = 0\n",
        "    self.failed_strikes = 0\n",
        "    self.non_strikes = 0\n",
        "    # Initialize widgets\n",
        "    self.strike_button = widgets.Button(description='Strike')\n",
        "    self.no_strike_button = widgets.Button(description='No Strike')\n",
        "    self.score_display = widgets.Output()\n",
        "    self.feedback_display = widgets.Output()\n",
        "\n",
        "    # Initialize the figure for image display\n",
        "    self.fig, self.ax = plt.subplots(figsize=(4, 4))\n",
        "    remove_ip_clutter(self.fig)\n",
        "    self.prev_fig, self.prev_ax = plt.subplots(figsize=(4, 4))\n",
        "    remove_ip_clutter(self.prev_fig)\n",
        "    self.show_next_image()\n",
        "    # Bind event handlers\n",
        "    self.strike_button.on_click(self.on_strike_clicked)\n",
        "    self.no_strike_button.on_click(self.on_no_strike_clicked)\n",
        "\n",
        "    # Arrange widgets in a layout\n",
        "    buttons_layout = widgets.HBox([self.strike_button, self.no_strike_button])\n",
        "    current_buttons = widgets.VBox([self.fig.canvas, buttons_layout])\n",
        "    previous_feedback = widgets.VBox([self.prev_fig.canvas, self.feedback_display])\n",
        "    self.ui = widgets.HBox([previous_feedback, current_buttons, self.score_display])\n",
        "\n",
        "  def show_next_image(self):\n",
        "    # Display the next image\n",
        "    image = self.features[self.current_index]\n",
        "\n",
        "    if len(image) == 64:\n",
        "        image = image.reshape(8, 8)\n",
        "    elif len(image) == 1:\n",
        "      scalar_value = image.flatten()[0]\n",
        "      # Initialize the 8x8 array with -6 (black)\n",
        "      image = np.full((8, 8), -6.0)\n",
        "      # Set the first ring to 6 (white)\n",
        "      image[0, 0] = 6\n",
        "      # Set the second ring to 6 (white)\n",
        "      image[1:-1, 1:-1] = 6\n",
        "      # Set the third (inner ring) back to -6 (black)\n",
        "      image[2:-2, 2:-2] = -6\n",
        "      # Assuming scalar_value is already in the range -6 to 6\n",
        "      #print(scalar_value)\n",
        "      image[3:-3, 3:-3] = scalar_value\n",
        "    else:\n",
        "      raise ValueError(f'Unexpected image shape: {image.shape}')\n",
        "    if self.current_image is not None:\n",
        "      self.previous_image = self.current_image\n",
        "    image = np.flipud(image)\n",
        "    self.current_image = image\n",
        "    # Display the image\n",
        "    #print(image)\n",
        "    self.fig.clf()\n",
        "    self.prev_fig.clf()\n",
        "    self.ax = self.fig.add_subplot(111)\n",
        "    self.prev_ax = self.prev_fig.add_subplot(111)\n",
        "    self.ax.set_xlim(-.5, 7.5)\n",
        "    self.ax.set_ylim(-0.5, 7.5)\n",
        "    self.prev_ax.set_xlim(-.5, 7.5)\n",
        "    self.prev_ax.set_ylim(-0.5, 7.5)\n",
        "    self.ax.set_aspect('equal')\n",
        "    self.prev_ax.set_aspect('equal')\n",
        "    self.ax.axis('off')\n",
        "    self.prev_ax.axis('off')\n",
        "    self.ax.imshow(self.current_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
        "    if self.previous_image is not None:\n",
        "      self.prev_ax.imshow(self.previous_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
        "    self.ax.set_title('Current Sensory Input')\n",
        "    self.prev_ax.set_title('Previous Sensory Input')\n",
        "    self.fig.canvas.draw()\n",
        "    self.prev_fig.canvas.draw()\n",
        "\n",
        "  def on_strike_clicked(self, button):\n",
        "    self.process_decision('Strike')\n",
        "\n",
        "  def on_no_strike_clicked(self, button):\n",
        "    self.process_decision('No Strike')\n",
        "\n",
        "  def process_decision(self, decision):\n",
        "    # freeze buttons while we process\n",
        "    self.strike_button.disabled = True\n",
        "    self.no_strike_button.disabled = True\n",
        "\n",
        "    # Process the user's decision, update score, and provide feedback\n",
        "    correct_action = 'Strike' if self.labels[self.current_index] == 1 else 'No Strike'\n",
        "    if decision == 'Strike':\n",
        "      if decision == correct_action:\n",
        "        self.score += 1\n",
        "        self.successful_strikes += 1\n",
        "      else:\n",
        "        self.score -= 1\n",
        "        self.failed_strikes += 1\n",
        "    elif decision == 'No Strike':\n",
        "      self.non_strikes += 1\n",
        "      # no strike means no gain or loss\n",
        "    else:\n",
        "      raise ValueError(f'Unknown decision: {decision}')\n",
        "\n",
        "    # Show feedback and score\n",
        "    if (self.feedback_type == 'both' or\n",
        "      (self.feedback_type == 'on_strike_only' and decision == 'Strike')):\n",
        "      # Show informative feedback\n",
        "      feedback = f'Your last choice: {decision}\\nCorrect last choice: {correct_action}'\n",
        "    else:\n",
        "      # Show uninformative feedback\n",
        "      feedback = 'Feedback only available after striking.'\n",
        "    with self.feedback_display:\n",
        "      clear_output(wait=True)\n",
        "      #print(self.labels[self.current_index])\n",
        "      #print(self.extra_labels[self.current_index])\n",
        "      print(feedback)\n",
        "\n",
        "    # Show score\n",
        "    with self.score_display:\n",
        "      clear_output(wait=True)\n",
        "      average_score = self.score / (self.current_index+1)\n",
        "      print(f'Total Score: {self.score}')\n",
        "      print(f'Number of Trials: {self.current_index + 1}')\n",
        "      print(f'Successful Strikes: {self.successful_strikes}')\n",
        "      print(f'Failed Strikes: {self.failed_strikes}')\n",
        "      print(f'Non-Strikes: {self.non_strikes}')\n",
        "      print(f'Average Score Per Trial: {average_score:.2f}')\n",
        "\n",
        "    # Prepare the next image\n",
        "    self.current_index += 1\n",
        "    #print(self.current_index)\n",
        "    self.show_next_image()\n",
        "    # Re-enable buttons\n",
        "    self.strike_button.disabled = False\n",
        "    self.no_strike_button.disabled = False\n",
        "\n",
        "\n",
        "scramble_bin_hard = InteractiveMNISTPredator(features=Xs,\n",
        "                                             labels=y1,\n",
        "                                             feedback_type='both')\n",
        "display(scramble_bin_hard.fig.canvas)\n",
        "display(scramble_bin_hard.prev_fig.canvas)\n",
        "clear_output()\n",
        "display(scramble_bin_hard.ui)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y_YjCpnZCNyn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "e2591325e6444cdc835343a898ead83d",
            "4b7a8f10cc924a20909f81656a0b19c2",
            "579d13f7de4341cb8cc0429425e8d871",
            "69d49a3ec576423e88ce61ab9d085348",
            "163a4b520edd4f38ab62e6aa77dd21f0",
            "dff20ca2f2fc443e91aea0dd8c6cbbeb",
            "73546dd43bb342f894f0272c169c43de",
            "48cebd353b07475cacf28a0eaf36d048",
            "d22b5add981343c9bf5c3c57f5b402ee",
            "25a517bbb2f84f3aa4fdfde67bcc708f",
            "a5f2bd72343a4c62831f98d048d3a81f",
            "ca8b60b5a9004a65836a3d68fc8f130d",
            "ccf4b6fd6058476b8fd3e2b8f252bf2c",
            "d2072b2877094b1eb81a9933c8a37072",
            "7b84fbe04d7b40d9ad870513f249b901",
            "c51866ea9f8140fcaf846b24ec7c81cd",
            "5daeda447b494cd0a33398393c6ec96c",
            "c806c47bbd774b9e9e9031180b6e64b5",
            "efca66dca651439da66b09f6828aecd4",
            "0c894be28c2d4e79b1ff1dcad5005cf2",
            "d10dfab7102847fd9b09e01546256ca9",
            "8b9e7923b262462bbb934218e7dbd332",
            "67e6069dbf364ec390b95d0a202af5be",
            "c20cab87ca8c49d9bede6e7a9a421f3c",
            "70836d662f3c4eae9e47d6810ecd6efc",
            "d4fcefb5aabc4723aae148dc91439fe2"
          ]
        },
        "outputId": "ef40c066-e5cb-4d5b-a420-589218196c7d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(VBox(children=(Canvas(footer_visible=False, header_visible=False, resizable=False, toolbar=Tool…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2591325e6444cdc835343a898ead83d"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We model this creature's sensory-behaviour system as follows, $\\mathbf{x}$ is the raw sensory input (column vector) of length 64 in a given episode. Each element $x_i$ of $\\mathbf{x}$ corresponds to the activation level of a single photosensitive neuron. We visualize and example sensory input in the code cell below."
      ],
      "metadata": {
        "id": "1CgwmAlsJG_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {},
        "id": "qJgsx8nSI5K_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "2e928a1a62634a87a5163adc6f9ebd28",
            "ed17c0a4ea284704b5cc281de7632090",
            "1606981914144f078d5d2b862c21dd24",
            "6ad63969345e48a2bd12660a95082763"
          ]
        },
        "outputId": "abae8df0-48ae-48be-aa54-a8eef577a4dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(footer_visible=False, header_visible=False, resizable=False, toolbar=Toolbar(toolitems=[('Home', 'Reset…"
            ],
            "image/png": "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",
            "text/html": [
              "\n",
              "            <div style=\"display: inline-block;\">\n",
              "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
              "                    Figure\n",
              "                </div>\n",
              "                <img src='data:image/png;base64,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' width=400.0/>\n",
              "            </div>\n",
              "        "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e928a1a62634a87a5163adc6f9ebd28"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ],
      "source": [
        "# visualizing the example we see that lower values correspond to darker pixels\n",
        "# and higher values correspond to lighter values\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "remove_ip_clutter(fig)\n",
        "ax.imshow(Xs[0].reshape(8,8), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These input neurons are then connected by synapses to a single output neuron. The activation level of this output neuron is computed as\n",
        "$$a = \\mathbf{Wx} + b$$\n",
        "Here, $b$ is the (scalar) bias, or baseline activation level of the output neuron, and $\\mathbf{W}$ is a matrix of synaptic weights between the input neurons and the single output neuron. (In this case where there is only one output neuron so $\\mathbf{W}$ has shape 1x64 so could also be thought of as a row vector.)  \n",
        "\n",
        "To simplify exposition and coding the input $\\mathbf{x}$ is augmented to have a feature which is always 1, and then the bias terms can be treated as the weight connecting to this constant valued feature. That is\n",
        "\n",
        "$$a = \\mathbf{Wx}$$\n",
        "\n",
        "Though now $\\mathbf{W}$ has shape 1x65. The probability of striking is determined by the activation level of the output neuron, together with a temperature parameter $\\tau$ which determines how exploratory the behaviour of the organism is, specifically:\n",
        "$$ \\Pr \\{\\text{strike}\\} = \\sigma_{\\tau}(a) $$\n",
        "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma_{\\tau}(a)$$\n",
        "\n",
        "Here $$\\sigma(a): \\frac{1}{1+e^{\\frac{-a}{\\tau}}} = \\frac{e^{\\frac{a}{\\tau}}}{1+e^\\frac{a}{\\tau}}$$ is the logistic (sigmoid) function, with temperature parameter $\\tau$. High values of $\\tau$ make striking and not striking have nearly equal probability regardless of the value of $a$, whereas very low values of $\\tau$ mean that even a very slightly positive $a$ value will correspond to near certainty of striking, and a very slight negative vale will result in an almost certain chance of not striking. In other words $\\tau$ determines how responsive the striking probabilities are to changes in $a$.\n",
        "\n",
        "Instead of thinkg of $a$ as simply a way of determining the striking probability, we are going to treat $a$ as a kind of prediction or expectation of the reward that will occur if the striking action is taken given the current sensory experience $\\mathbf{x}$. If there is a higher expectation of reward, the probability of taking the striking action increases, if there is a lower expectation of reward the probability of taking the striking action decreases, according the the temperature scaled softmax function above.\n",
        "\n",
        "So far we've just made things more complicated, adding a $\\tau$ parameter to our softmax function not much help.\n",
        "\n",
        "The simplicity comes in the form of the plasticity rule that this allows for. If we treat $a$ as the organisms internal representation of the expected reward when taking this action, then the organism can compare its expectation of reward with the reward actually received, and use this contrast between expectation and reality, to update the synaptic weights that had a causal impact on this expectation, in such a way that the expected reward more closely aligns with the received reward. Given our simple network the rule that does this is"
      ],
      "metadata": {
        "id": "pOUN7F2-eL2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Delta Rule (Reward Prediction)\n",
        "\n",
        "$$\\Delta W_i = s (r-a) x_i $$\n",
        "\n",
        "Or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s (r-a) \\mathbf{x}$$\n"
      ],
      "metadata": {
        "id": "cyjeHHWJJ_vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is saying that if the received reward is greater than the predicted reward, then it would be better if $a$ had been larger, which means that in the cases where $x_i$ was positive, the weight from $x_i$ should be increased, in proportion to the strength of activation of $x_i$ and in the case that $x_i$ was negative, the weight from $x_i$ should be decreased, again in proportion to the strength of the activation of $x_i$. Conversely if, the reward received is less than predicted then it would have been better if $a$ was smaller and so, in the case where $x_i$ is positive the weight from $x_i$ should be decreased, in proportion to the strength of the activation of $x_i$ and in the case that $x_i$ is negative, the weight from $x_i$ should be decreased.\n",
        "\n",
        "This rule can be understood as shifting the weights $\\mathbf{W}$ in the direction that minimizes the squared error of the reward prediction.\n",
        "\n",
        "$$ a = \\mathbf{W}\\mathbf{x}$$\n",
        "$$\\begin{align} \\nabla_\\mathbf{W} (r-a)^2 & = 2 \\ (r-a) \\ \\nabla_{\\mathbf{W}}a \\\\\n",
        "& = 2 (r-a) \\mathbf{x}\n",
        "\\end{align}$$\n",
        "The factor of 2 is subsumed in the step scaling factor $s$.\n",
        "\n",
        "Under one version of this updating scheme, let's call it the realish version, no update is made to the network if the organism refrains from striking. In this realishtic scenario this prediction is conceived of as the predicted reward when striking, so this difference between expected and received reward simply does not apply to the case where no striking occurs. In this scenario it is important to have the exploration level of the organism controlled by the $\\tau$ parameter in the sigmoid, striking probability function. During the earlier episodes of the organism's life it should be more exploratory, but once it has learned good predictions of reward across it's range of experiences it can settle into simply exploiting the knowledge it has. Deciding when and how to transition from exploratory behaviours to more directly reward maximizing behaviours is know as the exploration-exploitation problem, and is a central challenge in Reinforcement Learning. We just mention this in passing now, but will dive more deeply into the exploration-exploitation tradeoff in sequence (blah).\n",
        "\n",
        "Under another version of this scheme, let's call it the \"cheating\" version, an update is always made based on the $r_{\\text{strike}}$, the reward the organism would have recieved if it strikes, regardless of whether it actually strikes or not. This could be implemented by the organism having some additional, post-hoc prey detection system that augments the organisms basic behavioural inputs with additional teaching signals that allows the organism to learn from situations where it's like \"Oh I wish I had struck at the food that is now swimming past me\" and also situations where it's like \"Oh there really was no food there, good thing I didn't strike\". Such mechanism likely do exist, but are definitely more complex than simply reinforcing actions taken based on actual intrinsic reward recieved (We could approximate such a system by \"rewarding\" not striking when no prey is present with 1 and giving a penalty of 1 when an organism fails to strike when prey is present, but this leaves open the question of if and how such rewards (to scaffold learning) need to be kept seperate from 'actual' rewards that count towards fitness in the evolutionary sense. This is a complex topic that we will revist throught the book, in particular in sequences (ref)).\n",
        "\n",
        "From an evolutionary adaptive behaviour perspective we are interested in the organism learning to take rewarding actions. This suggest an alternative updating scheme where the reward recieved directly reinforces the probability of the taking the action that produced it. To do this we need know how a change in $a$ translates into an increase (or decrease) in the probability of striking, so that we can increase (decrease) action probabilites in proprtion to the rewards they recieve. We call this the Action Probability Delta Rule"
      ],
      "metadata": {
        "id": "B79wUBdBeQNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action Probability Reinforcement\n",
        "\n",
        "$$ \\Delta W_i = s r \\sigma'(a) x_i$$\n",
        "\n",
        "or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s r \\sigma'(a) \\mathbf{x}$$\n",
        "\n",
        "Here $\\sigma'$ is the derivative of the sigmoid function. We no longer need to explicitly control exploration versus exploitation with $\\tau$, so we simply set $\\tau =1$ here for simplicity. The above rule uses the raw rewards to reinforce actions. The probability of striking is given by $\\sigma(a)$ so the way to change $\\mathbf{W}$ to increase the probability of striking is to shift the weights in the direction of the gradient of the probability of taking the action given by $\\sigma'(a)$. Using the defination of $\\sigma$ above and the chain rule from calculus we can derive that $\\sigma'(a) = \\sigma(a) (1-\\sigma(a))$. This is telling us what we already know intuitively about the sigmoid function, which is that it is relatively flat when probalities are close to zero or one, and steepest when probabilities are close to even, that is 0.5.\n",
        "\n",
        "The above rule can be understood as shifting weights $\\mathbf{W}$ so that the probability of striking in a particular instance ($\\mathbf{x}$) is increased (or decreased in the case of negative reward) in proportion to the rewards that result from taking the striking action.\n",
        "$$ \\Pr\\{\\text{strike}|\\mathbf{x}\\} = \\sigma(\\mathbf{W}\\mathbf{x})$$\n",
        "$$\\begin{align} \\Delta \\mathbf{W} &= s \\cdot \\nabla_\\mathbf{W} \\Pr\\{\\text{strike}| \\mathbf{x}\\} \\cdot r \\\\\n",
        "& = s \\cdot \\sigma'(a) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx} \\cdot r \\\\\n",
        "& = s \\cdot \\sigma'(a) \\cdot \\mathbf{x} \\cdot r\n",
        "\\end{align}$$\n",
        "\n",
        "This rule also has a realishtic and a cheating variant. The realishtic variant uses the actual reward $r$ obtained from sampling an action, and experiencing the subsequent reward. However, in ML contexts, learning can be greatly accelerated by instead using the expected reward $\\mathbb{E}[r]$, given the probability of taking the action.\n",
        "\n",
        "Instead of using the raw reward as above, using a deviation of reward from some baseline value $\\hat{r}$ typically leads to more efficient learning. The value $\\hat{r}$ might be a fixed constant (meta-parameter) of the learning algorithm, or computed as a simple moving average of previous rewards recieved, or computed in some more complex way e.g. as $a$ was in the reward-prediction version of the Delta Rule above.\n",
        "\n",
        "Incorporating this idea of a deviation from baseline gives the most general form of the delta rule\n",
        "\n"
      ],
      "metadata": {
        "id": "wxyLSWkRJRMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General (Single Output) Delta Rule\n",
        "\n",
        "$$ \\Delta W_i = s \\ (r - \\hat{r}) \\ \\sigma'(a) \\ x_i$$\n",
        "\n",
        "or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s \\ (r - \\hat{r}) \\ \\sigma'(a) \\ \\mathbf{x}$$\n",
        "\n",
        "If we set $\\hat{r}=a$ and ignore the sigmoid derivative term, we have our linear delta rule. If we fix $\\hat{r}=0$ then we have our action probability reinforcement rule. Note that we are only considering the case where a single array of inputs $\\mathbf{x}$ are connected by weights $\\mathbf{W}$ to a single output activation $a$ with activation function $\\sigma(a)$."
      ],
      "metadata": {
        "id": "T4kitKtALMY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the coding exercise below to see how these two different udpated rules compare with our perturb-measure-step rule, both in our realishtic and cheating variants."
      ],
      "metadata": {
        "id": "IxyE8pOgMPOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO for students: Replace ... in the lines below with one of the following\n",
        "# options, each option get's used exactly once\n",
        "# a) (r_exp-r_hat) * (out_spike_prob) * (1 - out_spike_prob) * x\n",
        "# b) (r_all - a) * x\n",
        "# c) (r-a) * out_spike * x\n",
        "# d) (r-r_hat) * out_spike * (out_spike_prob) * (1 - out_spike_prob) * x\n",
        "# e) (r_exp_perturb - r_exp) / perturbation_scale\n",
        "# f) (r_perturb - r) / perturbation_scale\n",
        "# This will implement different update rules driven by: Reward Prediction Error,\n",
        "# Reinforcement of Action ProbabilitiesTrue Positives, or Peturb-Measure\n",
        "# estimate of the gradient of expected reward. Both in a strict, experience\n",
        "# driven way, and in \"cheating\" a bit way.\n",
        "raise NotImplementedError(\"Exercise: Implement different update rules to compare\")\n",
        "################################################################################\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  strike_prob = np_sigmoid(a, tau)\n",
        "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
        "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  r_exp = out_spike_prob * r_all # + (1-out_spike_prob) * 0 # expected reward\n",
        "  return a, strike_prob, strike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           cheating=False,\n",
        "                           tau=1,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.001):\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheating:\n",
        "    # cheat a bit and learn regardless of whether organism actually strikes or not\n",
        "    update = np.mean(..., axis=1, keepdims=True).T\n",
        "  else:\n",
        "    # more realishtic, learn only when organism actually strikes\n",
        "    update = np.mean(..., axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y, r_hat=0,\n",
        "                     reinforce_exp=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    # cheat a bit and learn using expected reward\n",
        "    update = np.mean(..., axis=1, keepdims=True).T\n",
        "  else:\n",
        "    # more realishtic, learn only from actual recieved rewards\n",
        "    update = np.mean(..., axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.0001,\n",
        "                         reinforce_exp=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    # cheat a bit, evaluate perturbation using expected reward\n",
        "    directional_grad_est = ...\n",
        "  else:\n",
        "    # more realishtic, evaluate perturbation using sampled rewards\n",
        "    directional_grad_est = ...\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "################################################################################\n",
        "# Exercise Complete, simulations and plotting logic follow\n",
        "################################################################################\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "cheat_alg_lrs = {'Reward Prediction': 0.0001,\n",
        "                 'Action Probability': 0.008,\n",
        "                 'Perturb Measure': 0.016}\n",
        "cheat_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "cheat_cum_avg_r = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy()}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = cheat_alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, reinforce_exp=True, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      cheat_reward_results[alg_name].append(np.mean(r_exp_full))\n",
        "      cheat_cum_avg_r[alg_name].append(np.mean(reward_results[alg_name]))\n",
        "    num_steps += 1\n",
        "n_algorithms = len(alg_names)\n",
        "cmap = plt.cm.viridis\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "for alg_name in alg_names:\n",
        "  smoothed_values = cheat_cum_avg_r[alg_name]\n",
        "  ax.plot(smoothed_values, label=f'{alg_name}', color=cmap(norm(lr)))\n",
        "  ax.set_title(f'Cumulative Reward')\n",
        "  ax.set_xlabel('Steps')\n",
        "  ax.set_ylabel('Cumulative Avg. Reward per Episode')\n",
        "  ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "vCPV8sEPcklj",
        "outputId": "521e2fc6-9e9f-4c5f-8d9d-23204c46d41b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exercise: Implement different update rules to compare",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7e68ca1a5607>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# estimate of the gradient of expected reward. Both in a strict, experience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# driven way, and in \"cheating\" a bit way.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exercise: Implement different update rules to compare\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exercise: Implement different update rules to compare"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  strike_prob = np_sigmoid(a, tau)\n",
        "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
        "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  r_exp = strike_prob * r_all # + (1-out_spike_prob) * 0 # expected reward\n",
        "  return a, strike_prob, strike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           cheat=False,\n",
        "                           tau=1,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.001):\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn regardless of whether organism actually strikes or not\n",
        "    update = (r_all - a) * x\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    update = (r-a) * out_spike * x\n",
        "  # average the update over all elements in the batch\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y, r_hat=0,\n",
        "                     cheat=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn using expected reward\n",
        "    update = (r_exp - r_hat) * (strike_prob) * (1 -  strike_prob) * x\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    # reward of zero recieved when not striking\n",
        "    update = (r-r_hat) * (srike_prob) * (1 - strike_prob) * x\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.0001,\n",
        "                         cheat=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # evaluate perturbation using expected reward, avg over the mini-batch\n",
        "    directional_grad_est = (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
        "  else: #realishtic\n",
        "    # evaluate perturbation using sampled rewards, avg over the mini-batch\n",
        "    directional_grad_est = (np.mean(r_perturb - r)) / perturbation_scale\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "################################################################################\n",
        "# Exercise Complete, simulations and plotting logic follow\n",
        "################################################################################\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "cheat_alg_lrs = {'Reward Prediction': 0.0001,\n",
        "                 'Action Probability': 0.008,\n",
        "                 'Perturb Measure': 0.016}\n",
        "cheat_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "cheat_cum_avg_r = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = cheat_alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, cheat=True, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      cheat_reward_results[alg_name].append(np.mean(r_exp_full))\n",
        "      cheat_cum_avg_r[alg_name].append(np.mean(cheat_reward_results[alg_name]))\n",
        "    num_steps += 1\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "for alg_name in alg_names:\n",
        "  smoothed_values = cheat_cum_avg_r[alg_name]\n",
        "  ax.plot(smoothed_values, label=f'{alg_name}')\n",
        "  ax.set_title(f'Cumulative Reward')\n",
        "  ax.set_xlabel('Steps')\n",
        "  ax.set_ylabel('Cumulative Avg. Reward per Episode')\n",
        "  ax.legend()\n",
        "\n",
        "# Ensure the layout is clean and no subplot titles or axes are overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jlB_fG4yCbwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "for alg_name in alg_names:\n",
        "  smoothed_values = cheat_cum_avg_r[alg_name]\n",
        "  ax.plot(smoothed_values, label=f'{alg_name}')\n",
        "  ax.set_title(f'Cumulative Reward')\n",
        "  ax.set_xlabel('Steps')\n",
        "  ax.set_ylabel('Cumulative Avg. Reward per Episode')\n",
        "  ax.legend()\n",
        "\n",
        "# Ensure the layout is clean and no subplot titles or axes are overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "75xBUppoS0Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spike = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  r = np.where(y == 1, out_spike, -out_spike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  #expected reward\n",
        "  r_exp = out_spike_prob * r_all # + (1-out_spike_prob) * 0\n",
        "  return a, out_spike_prob, out_spike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           reinforce_exp=False,\n",
        "                           tau=1/1000,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_all-a) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-a) * out_spike * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y, r_hat=0,\n",
        "                     reinforce_exp=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_exp-r_hat) * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-r_hat) * out_spike * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.0001,\n",
        "                         reinforce_exp=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    directional_grad_est = (r_exp_perturb - r_exp) / perturbation_scale\n",
        "  else:\n",
        "    directional_grad_est = (r_perturb - r) / perturbation_scale\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "lr_ranges = {'Reward Prediction': [0.0002, 0.0001, 0.000005],\n",
        "             'Action Probability': [0.016, 0.008, 0.004],\n",
        "             'Perturb Measure': [0.017, 0.016, 0.015]}\n",
        "reward_results = {alg_name: {lr: [] for lr in lr_ranges[alg_name]} for alg_name in alg_names}\n",
        "smoothed_results = {alg_name: {lr: [] for lr in lr_ranges[alg_name]} for alg_name in alg_names}\n",
        "W_s = {alg_name: {lr: W_init for lr in lr_ranges[alg_name]} for alg_name in alg_names}\n",
        "#alpha = 0.05\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr_range = lr_ranges[alg_name]\n",
        "      for lr_idx, lr in enumerate(lr_range):\n",
        "        W = W_s[alg_name][lr]\n",
        "        new_W, r, r_exp = alg_func(W, batch_x, batch_y, reinforce_exp=True, rng=learn_rng, learning_rate=lr)\n",
        "        _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "        W_s[alg_name][lr] = new_W\n",
        "        reward_results[alg_name][lr].append(np.mean(r_exp_full))\n",
        "        smoothed_results[alg_name][lr].append(np.mean(reward_results[alg_name][lr]))\n",
        "    num_steps += 1\n",
        "n_algorithms = len(alg_names)\n",
        "cmap = plt.cm.viridis\n",
        "fig, axes = plt.subplots(n_algorithms, 1, figsize=(10, 6 * n_algorithms), sharex=True)\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "for i, alg_name in enumerate(alg_names):\n",
        "  ax = axes[i] if n_algorithms > 1 else axes\n",
        "  learning_rates = np.array(lr_ranges[alg_name])\n",
        "  norm = plt.Normalize(learning_rates.min(), learning_rates.max())\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for lr in learning_rates:\n",
        "    smoothed_values = smoothed_results[alg_name][lr]\n",
        "    ax.plot(smoothed_values, label=f'LR={lr}', color=cmap(norm(lr)))\n",
        "    # Set the title, labels and legend\n",
        "  ax.set_title(f'{alg_name}')\n",
        "  ax.set_xlabel('Steps')\n",
        "  ax.set_ylabel('Cumulative Avg. Reward per Episode')\n",
        "  ax.legend()\n",
        "\n",
        "# Ensure the layout is clean and no subplot titles or axes are overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ETlggGbatIZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMIda-42WXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as a caveat, direct comparison of learning algorithms is difficult. Maybe different meta-parmeters (learning rates, tua values) would make one algorithm come out better than the other. Here where we are focused on aquiring pretty good behaviour as quickly as possible (as contrasted with obtaining as good as possible final performance, with-in a feasible"
      ],
      "metadata": {
        "id": "wu4hhkg-vG0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y1 == 0)"
      ],
      "metadata": {
        "id": "MuYJH9guU5ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y1 == 1)"
      ],
      "metadata": {
        "id": "X82CBfzatGW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y1 == 1) / len(y1)"
      ],
      "metadata": {
        "id": "s4o2SqIvU9Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_ap"
      ],
      "metadata": {
        "id": "a9vheRl2tJAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spike = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  r = np.where(y == 1, out_spike, -out_spike)\n",
        "  r_all = 2*y-1\n",
        "  r_exp = out_spike_prob * r_all\n",
        "  return a, out_spike_prob, out_spike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           reinforce_exp=False,\n",
        "                           tau=1,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_all-a) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-a) * out_spike * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y, r_hat=0,\n",
        "                     reinforce_exp=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_exp-r_hat) * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-r_hat) * out_spike * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.001,\n",
        "                         reinforce_exp=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    directional_grad_est = (r_exp_perturb - r_exp) / perturbation_scale\n",
        "  else:\n",
        "    directional_grad_est = (r_perturb - r) / perturbation_scale\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 2\n",
        "lr_rp = 0.001\n",
        "lr_ap = 0.005\n",
        "lr_pm = 0.005\n",
        "perturbation_scale = 0.0001\n",
        "mini_batch_size = 1\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W_rp = W_init\n",
        "W_ap = W_init\n",
        "W_pm = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "num_steps = 0\n",
        "rewards_rp = []\n",
        "rewards_ap = []\n",
        "rewards_pm = []\n",
        "smoothed_rp = []\n",
        "smoothed_ap = []\n",
        "smoothed_pm = []\n",
        "alpha = 0.001\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W_rp, r_rp, r_rp_exp = reward_prediction_step(W_rp, batch_x, batch_y, reinforce_exp=True, tau=tau, rng=learn_rng, learning_rate=lr_rp)\n",
        "    W_ap, r_ap, r_ap_exp = action_prob_step(W_ap, batch_x, batch_y, reinforce_exp=True, tau=tau, rng=learn_rng, learning_rate=lr_ap)\n",
        "    W_pm, r_pm, r_pm_exp = perturb_measure_step(W_pm, batch_x, batch_y, reinforce_exp=True, tau=1, rng=learn_rng, learning_rate=lr_pm)\n",
        "    num_steps += 1\n",
        "    rewards_rp.append(np.mean(r_rp))\n",
        "    rewards_ap.append(np.mean(r_ap))\n",
        "    rewards_pm.append(np.mean(r_pm))\n",
        "\n",
        "    if len(smoothed_rp) == 0:\n",
        "      smoothed_rp.append(rewards_rp[-1])\n",
        "      smoothed_ap.append(rewards_ap[-1])\n",
        "      smoothed_pm.append(rewards_pm[-1])\n",
        "    else:\n",
        "      smoothed_rp.append(alpha * rewards_rp[-1] + (1 - alpha) * smoothed_rp[-1])\n",
        "      smoothed_ap.append(alpha * rewards_ap[-1] + (1 - alpha) * smoothed_ap[-1])\n",
        "      smoothed_pm.append(alpha * rewards_pm[-1] + (1 - alpha) * smoothed_pm[-1])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
        "ax.plot(smoothed_rp, label='Reward Prediction', color='blue', alpha=0.5)\n",
        "ax.plot(smoothed_ap, label='Action Probability', color='red', alpha=0.5)\n",
        "ax.plot(smoothed_pm, label='Perturb Measure', color='green', alpha=0.5)\n",
        "\n",
        "ax.set_title('(Smoothed) Average Rewards')  # Title of the plot\n",
        "ax.set_xlabel('Steps')  # Label for the x-axis\n",
        "ax.set_ylabel('Average Reward')  # Label for the y-axis\n",
        "ax.legend()  # Add a legend\n",
        "\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "854bVhbjOuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)"
      ],
      "metadata": {
        "id": "EY7zU206eGh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "print(Xs_aug.shape)\n",
        "print(Xs_aug[:1])"
      ],
      "metadata": {
        "id": "S05lN-I9cmgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = 0.2 * (learn_rng.random(size=(1,65)) - 0.5)\n",
        "print(W)\n",
        "x = Xs_aug[:5].T\n",
        "print(x)"
      ],
      "metadata": {
        "id": "T9zsErHTiveg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.dot(W, x)\n",
        "print(a)\n",
        "out_spike_prob = np_sigmoid(a)\n",
        "print(out_spike_prob)"
      ],
      "metadata": {
        "id": "JH63UEmYdFKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_spikes = np.array(learn_rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "print(out_spikes)"
      ],
      "metadata": {
        "id": "eOmGv02Ae_vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y1[:5].T\n",
        "print(y)"
      ],
      "metadata": {
        "id": "ovnnZwjzf_Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " R = np.where(y == 1, out_spikes, -out_spikes)\n",
        " print(R)"
      ],
      "metadata": {
        "id": "CIWRpmzmf1A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_R = np.where(y == 1, 1, -1)\n",
        "print(all_R)"
      ],
      "metadata": {
        "id": "8Vyb4wuDhzL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = np.mean(np.abs(all_R - a))\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "JpuLMy4fiBSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "id": "8nfWOFHbgGL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y-a"
      ],
      "metadata": {
        "id": "VX66Mh_Ubul9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_R - a"
      ],
      "metadata": {
        "id": "u5Ie8RIobxDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(R)"
      ],
      "metadata": {
        "id": "hOXHtaKahQxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_pass(W, x, y, tau=1, rng=None, learning_rate=0.001, verbose=False):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spikes = np.array(learn_rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  all_R = np.where(y == 1, 1, -1)\n",
        "  mae = np.mean(np.abs(all_R - a))\n",
        "  if verbose:\n",
        "    R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "    did_strike = out_spikes == 1\n",
        "    did_not_strike = out_spikes == 0\n",
        "    should_strike = y == 1\n",
        "    should_not_strike = y == 0\n",
        "    TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "    FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "    FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "    TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "    R[TP] = 1\n",
        "    R[FP] = -1\n",
        "    R[FN] = 0\n",
        "    R[TN] = 0\n",
        "    TPs = np.sum(TP)\n",
        "    FPs = np.sum(FP)\n",
        "    FNs = np.sum(FN)\n",
        "    TNs = np.sum(TN)\n",
        "    confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    print(f'Mean Absolute Error: {mae}')\n",
        "    return None\n",
        "  else:\n",
        "    R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "    # only learn from strikes taken\n",
        "    update = np.mean((R-a) * out_spikes * x, axis=1, keepdims=True).T\n",
        "    # always learn\n",
        "    #update = np.mean((all_R-a) * x, axis=1, keepdims=True).T\n",
        "    W_new = W + update * learning_rate\n",
        "    return W_new, a, out_spikes, R, mae\n",
        "\n",
        "forward_backward_pass(W, x, y, tau=1, rng=learn_rng, learning_rate=0.00001)"
      ],
      "metadata": {
        "id": "_Yfp2tPvi3Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_backward_pass(W, x, y, tau=1, rng=learn_rng, learning_rate=0.00001)"
      ],
      "metadata": {
        "id": "voz_8TAZamtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 200\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "W_init = 0.2 * (learn_rng.random(size=(1,65)) - 0.5)\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W, _, _, _, _ = forward_backward_pass(W, batch_x, batch_y, tau=tau, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "  if epoch % 10 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_y = y1.T\n",
        "    _, _, _, batch_R, batch_mae = forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "    print(f'Epoch {epoch} | Reward Sample: {np.sum(batch_R)} | Strike Reward MAE: {batch_mae} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "forward_backward_pass(W, Xs_aug.T, y1.T, tau=0.0001, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "yz6fy__PgN3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems to get in the neighbourhood of 2300, so not as good as perturb measure step. But why? Because here we are minimizing a prediction error, which is only closely correleated with correctly choosing the right action as often as possible. So while this prediction mis-match could be a nice way for learning to occur, if we really want to maximize reward, we need to make sure that reward is what we are most directly maximizing. To do this we need to change our update rule so that we directly increase the probability of striking when a strike happens and it is rewarded, and decrease the probability of striking when it happens and a negative reward results."
      ],
      "metadata": {
        "id": "Znyp5dxHdbXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action Probability Reinforcement\n",
        "\n",
        "$$ \\Delta W_i = s r \\sigma'(a) x_i$$\n",
        "\n",
        "or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s r \\sigma'(a) \\mathbf{x}$$"
      ],
      "metadata": {
        "id": "G7wZDUgWewB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal here is to more directly associate action with reward, as contrasted with doing this inderectly by tying action probabilities to a reward prediction."
      ],
      "metadata": {
        "id": "FwgSmE63f2xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_pass(W, x, y, tau=1, rng=None, learning_rate=0.001, verbose=False):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spikes = np.array(learn_rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  all_R = np.where(y == 1, 1, -1)\n",
        "  if verbose:\n",
        "    R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "    did_strike = out_spikes == 1\n",
        "    did_not_strike = out_spikes == 0\n",
        "    should_strike = y == 1\n",
        "    should_not_strike = y == 0\n",
        "    TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "    FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "    FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "    TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "    R[TP] = 1\n",
        "    R[FP] = -1\n",
        "    R[FN] = 0\n",
        "    R[TN] = 0\n",
        "    TPs = np.sum(TP)\n",
        "    FPs = np.sum(FP)\n",
        "    FNs = np.sum(FN)\n",
        "    TNs = np.sum(TN)\n",
        "    confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "    # only learn from strikes taken\n",
        "    # reward probability based updated\n",
        "    update = np.mean(R * out_spikes * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "    # always learn\n",
        "    #update = np.mean(all_R * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "    W_new = W + update * learning_rate\n",
        "    return W_new, a, out_spikes, R\n",
        "\n",
        "forward_backward_pass(W, x, y, tau=1, rng=learn_rng, learning_rate=0.00001)"
      ],
      "metadata": {
        "id": "eUvs2c-Y9S4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 200\n",
        "learning_rate = 0.0002\n",
        "mini_batch_size = 1\n",
        "W_init = 0.2 * (learn_rng.random(size=(1,65)) - 0.5)\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W, _, _, _ = forward_backward_pass(W, batch_x, batch_y, tau=tau, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "  if epoch % 20 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_y = y1.T\n",
        "    _, _, _, batch_R = forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "    print(f'Epoch {epoch} | Reward Sample: {np.sum(batch_R)} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "forward_backward_pass(W, Xs_aug.T, y1.T, tau=0.0001, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "hboWkIa-g73C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is basically the same as perturb measure step, except that it doesn't require a seperate purturbation mode"
      ],
      "metadata": {
        "id": "7hUcs6nfi_81"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KbxBKtlPi_RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to see how a simple stochastic spiking network can learn to solve this problem\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def delta_rule_batch(W, x, y, tau, learning_rate, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise, used delta rule\n",
        "  based on reward perdiction to compute updates\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activation level given input\n",
        "  a = np.dot(W, x) # 1 x batch\n",
        "  # strike probability\n",
        "  out_spike_prob = np_sigmoid(a, tau) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  out_spikes = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)  # 1 x batch\n",
        "  R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "  did_strike = out_spikes == 1\n",
        "  did_not_strike = out_spikes == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "  updates = learning_rate * (R-a) * out_spikes * x #inuput dim x batch\n",
        "  mean_squared_reward_prediction_errors = np.mean((y - a)**2)\n",
        "  if verbose:\n",
        "    TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "    FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "    FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "    TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "    R[TP] = 1\n",
        "    R[FP] = -1\n",
        "    R[FN] = 0\n",
        "    R[TN] = 0\n",
        "    TPs = np.sum(TP)\n",
        "    FPs = np.sum(FP)\n",
        "    FNs = np.sum(FN)\n",
        "    TNs = np.sum(TN)\n",
        "    confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), updates, mean_squared_reward_prediction_errors\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "W_init = np.zeros((1,65))\n",
        "tau_init = 5\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = tau_init/(10*epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    _, W_update, _ = delta_rule_batch(W, batch_x, batch_y, tau, learning_rate, rng=learn_rng)\n",
        "    W = W + W_update.T\n",
        "  if epoch % 2 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_y = y1.T\n",
        "    batch_reward, _, mean_squared_reward_prediction_errors = delta_rule_batch(W, batch_x, batch_y, tau, learning_rate, rng=learn_rng)\n",
        "    print(f'Epoch {epoch} | Batch Reward: {batch_reward} | Avg. Prediction Error {mean_squared_reward_prediction_errors}|Time elapsed: {elapsed_time:.2f} seconds')\n",
        "delta_rule_batch(W, Xs_aug.T, y1.T, tau, learning_rate, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "b2iezpbdL0oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to see how a simple stochastic spiking network can learn to solve this problem\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -200, 200) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def update_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise, input and output\n",
        "  neurons are spiking\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # input spike probs\n",
        "  squash_x = np_sigmoid(x) # inputs x batch\n",
        "  # activation level given input\n",
        "  a = np.dot(W, squash_x) # 1 x batch\n",
        "  # strike probability\n",
        "  out_spike_prob = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  out_spikes = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)  # 1 x batch\n",
        "  R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "  did_strike = out_spikes == 1\n",
        "  did_not_strike = out_spikes == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  updates = learning_rate * R * squash_x #inuput dim x batch\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix, updates\n",
        "\n",
        "\n",
        "  return replicated_x\n",
        "\n",
        "def compact_update_stochastic_spiking_batch(W, x, y, learning_rate, rng=None):\n",
        "  \"\"\"\n",
        "  Evaluates parameters of a simple behaviour circuit given inputs and target outputs.\n",
        "  Optimized for performance by focusing solely on update computations.\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "        weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "    learning_rate: scalar, the learning rate for updates\n",
        "\n",
        "  Returns:\n",
        "    updates: gradient updates for weight adjustments\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  squash_x = np_sigmoid(x)\n",
        "  a = np.dot(W, x)  # 1 x batch\n",
        "  out_spike_prob = np_sigmoid(a)  # Logistic sigmoid function\n",
        "  out_spikes = (rng.random(out_spike_prob.shape) < out_spike_prob).astype(int)\n",
        "  # Compute rewards\n",
        "  R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "  # Compute updates\n",
        "  updates = learning_rate * R * squash_x\n",
        "  return updates\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "input_replications = 100\n",
        "W_init = np.zeros((1,65*input_replications))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "batch_y = y1.T\n",
        "update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W_update = compact_update_stochastic_spiking_batch(W, batch_x, batch_y, learning_rate, rng=learn_rng)\n",
        "    W = W + W_update.T\n",
        "  if epoch % 2 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1.T\n",
        "    batch_reward, confusion_matrix, W_update = update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng)\n",
        "    print(f'Epoch {epoch} completed | Batch Reward Sample: {batch_reward}|Time elapsed: {elapsed_time:.2f} seconds')\n",
        "update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "xcotlVflIrdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to see how a simple stochastic spiking network can learn to solve this problem\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -200, 200) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def update_stochastic_spiking_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise, input and output\n",
        "  neurons are spiking\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # input spike probs\n",
        "  in_spike_probs = np_sigmoid(x) # inputs x batch\n",
        "  # sampled spikes\n",
        "  in_spikes = np.array(rng.random(size=x.shape) < in_spike_probs, int)\n",
        "  # activation level given input spikes\n",
        "  a = np.dot(W, in_spikes) # 1 x batch\n",
        "  # strike probability\n",
        "  out_spike_prob = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  out_spikes = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)  # 1 x batch\n",
        "  R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "  did_strike = out_spikes == 1\n",
        "  did_not_strike = out_spikes == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  updates = learning_rate * R * out_spikes * in_spikes #inuput dim x batch\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix, updates\n",
        "\n",
        "\n",
        "  return replicated_x\n",
        "\n",
        "def compact_update_stochastic_spiking_batch(W, x, y, learning_rate, rng=None):\n",
        "  \"\"\"\n",
        "  Evaluates parameters of a simple behaviour circuit given inputs and target outputs.\n",
        "  Optimized for performance by focusing solely on update computations.\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "        weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "    learning_rate: scalar, the learning rate for updates\n",
        "\n",
        "  Returns:\n",
        "    updates: gradient updates for weight adjustments\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # Compute input spikes\n",
        "  in_spike_probs = np_sigmoid(x)  # Logistic sigmoid function\n",
        "  in_spikes = (rng.random(x.shape) < in_spike_probs).astype(int)\n",
        "  # Compute output spikes\n",
        "  a = np.dot(W, in_spikes)  # 1 x batch\n",
        "  out_spike_prob = np_sigmoid(a)  # Logistic sigmoid function\n",
        "  out_spikes = (rng.random(out_spike_prob.shape) < out_spike_prob).astype(int)\n",
        "  # Compute rewards\n",
        "  R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "  # Compute updates\n",
        "  updates = learning_rate * (R * out_spikes) * in_spikes\n",
        "  return updates\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "input_replications = 100\n",
        "W_init = np.zeros((1,65*input_replications))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "batch_y = y1.T\n",
        "update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W_update = compact_update_stochastic_spiking_batch(W, batch_x, batch_y, learning_rate, rng=learn_rng)\n",
        "    W = W + W_update.T\n",
        "  if epoch % 2 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1.T\n",
        "    batch_reward, confusion_matrix, W_update = update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng)\n",
        "    print(f'Epoch {epoch} completed | Batch Reward Sample: {batch_reward}|Time elapsed: {elapsed_time:.2f} seconds')\n",
        "update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "KC-J3m1AhRZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "id": "FPta8uE7A0xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "for _ in range(100):\n",
        "  batch_reward, _, _ = update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng)\n",
        "  rewards.append(batch_reward)\n",
        "\n",
        "# Convert list to numpy array for better handling in matplotlib\n",
        "rewards_array = np.array(rewards)\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(rewards_array, bins=30, color='blue', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eTT3vID_62vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.zeros((1,65))"
      ],
      "metadata": {
        "id": "OeSPA7D98lRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "Ws4BK0Qd8XwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "id": "aMujOt-k8esw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_single_action(W, x, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  pre_spike_probs = np_sigmoid(x)\n",
        "  input_spikes = np.array(rng.random(size=x.shape) < pre_spike_probs, int)\n",
        "  # activaation\n",
        "  output_activation = np.dot(W,pre_spikes)\n",
        "  # strike probability, does the output spike\n",
        "  output_spike_prob = np_sigmoid(a)\n",
        "  sampled_action = rng.random() < y_hat\n",
        "  return sampled_action, input_spikes\n",
        "\n",
        "def stochastic_single_reward(a, y):\n",
        "  if a == 1:\n",
        "    if y == 1\n",
        "      R = 1\n",
        "    elif y == 0:\n",
        "      R = -1\n",
        "    else:\n",
        "      raise ValueError(f'Unknown target: {y}')\n",
        "  elif a == 0:\n",
        "    R = 0\n",
        "  else:\n",
        "    raise ValueError(f'Unknown action: {a}')\n",
        "  return R\n",
        "\n",
        "def stochastic_single_update(W, x, y, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  sampled_action, input_spikes = stochastic_single_action(W, x, rng)\n",
        "  reward = stochastic_single_reward(sampled_action, y)\n",
        "  W += learning_rate * sampled_action * input_spikes * reward\n",
        "  return W\n"
      ],
      "metadata": {
        "id": "QA745wjh4lR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The organism recieves a reward of 1 for striking at the right time and a penalty of -1 for striking at the wrong time."
      ],
      "metadata": {
        "id": "9fnPqbxRKPhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO for students: Complete the lines with ... set the appropriate rewards for\n",
        "# for the evaluations function\n",
        "raise NotImplementedError(\"Exercise: Set the reward for different outcomes\")\n",
        "################################################################################\n",
        "\n",
        "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
        "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
        "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
        "# After augmentation there is one extra column of features\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def eval_params_stochastic_single(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) np.array) sensory input\n",
        "    y: (outputs(1) np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x)\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a)\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random() < y_hat\n",
        "  if y_sample == 1: #organism strikes\n",
        "    if y == 1: #prey is present\n",
        "      R = ...\n",
        "    else: # prey is not present\n",
        "      R = ...\n",
        "  else: # organism does not strike\n",
        "    R = ...\n",
        "  if verbose:\n",
        "    print(f'Probability of striking: {y_hat}')\n",
        "    action_string = 'Strike' if y_sample == 1 else 'No Strike'\n",
        "    print(f'Action taken: {action_string}')\n",
        "    target_string = 'Strike' if y == 1 else 'No Strike'\n",
        "    print(f'Correct Action: {target_string}')\n",
        "    print(f'Reward recieved: {R}')\n",
        "  else:\n",
        "    return R\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "eval_params_stochastic_single(W_test, Xs_aug[0], y1[0], verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "dAB-xsyQNzFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
        "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
        "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
        "# After augmentation there is one extra column of features\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def eval_params_stochastic_single(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) np.array) sensory input\n",
        "    y: (outputs(1) np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x)\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a)\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random() < y_hat\n",
        "  if y_sample == 1: #organism strikes\n",
        "    if y == 1: #prey is present\n",
        "      R = 1\n",
        "    else: # prey is not present\n",
        "      R = -1\n",
        "  else: # organism does not strike\n",
        "    R = 0\n",
        "  if verbose:\n",
        "    print(f'Probability of striking: {y_hat}')\n",
        "    action_string = 'Strike' if y_sample == 1 else 'No Strike'\n",
        "    print(f'Action taken: {action_string}')\n",
        "    target_string = 'Strike' if y == 1 else 'No Strike'\n",
        "    print(f'Correct Action: {target_string}')\n",
        "    print(f'Reward recieved: {R}')\n",
        "  else:\n",
        "    return R\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "eval_params_stochastic_single(W_test, Xs_aug[0], y1[0], verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "mz00q3D8h1kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that evaluates the reward over a single experience. We can use numpy broadcasting to apply this same reward calculation efficiently to many, even all, the input-out pairs in our data set. We call this **batch** evaluation."
      ],
      "metadata": {
        "id": "gPh6sBUTnZnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO for students: Complete the lines with ... to compute the number of\n",
        "# True Positives, False Positives, True Negative and False Negatives in the batch\n",
        "raise NotImplementedError(\"Exercise: Compute the number of different Outcomes\")\n",
        "################################################################################\n",
        "\n",
        "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random(size=y_hat.shape) < y_hat  # 1 x batch\n",
        "  R = np.zeros(y_sample.shape)\n",
        "  did_strike = y_sample == ...\n",
        "  did_not_strike = y_sample == ...\n",
        "  should_strike = y == ...\n",
        "  should_not_strike = y == ...\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
        "# for broadcasting to work\n",
        "print('Evaluation 1')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)\n",
        "print('\\nEvaluation 2')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "CI-p0If_8ucJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
        "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
        "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
        "# After augmentation there is one extra column of features\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random(size=y_hat.shape) < y_hat  # 1 x batch\n",
        "  R = np.zeros(y_sample.shape)\n",
        "  did_strike = y_sample == 1\n",
        "  did_not_strike = y_sample == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
        "# for broadcasting to work\n",
        "print('Evaluation 1')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)\n",
        "print('\\nEvaluation 2')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "pMYaMDiYp9TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the two evaluations give different total rewards, even though the exact same synaptic weights $\\mathbf{W}$ are being used, on the exact same batch of inputs $\\mathbf{x}$ and prey presence indicators $y$. This is expected given the inherent stochasticity in the organisms behaviour. This stochastic evaluation of the synaptic weights will make things difficult for the perturb-measure-step alogorithm though, because it relies upon precise function evaluations to get good estimates of the rate of improvement in a given direction in parameter space. We can overcome this stochastic evaluation issue though by using our knowledge of how the different probabilities of striking or not determine the expected, or average reward. By directly evaluating expected reward we can recover a precise, deterministic evaluation function."
      ],
      "metadata": {
        "id": "SNMQj2pq5c9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_params_expectation_batch(W, x, y, verbose=False):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R_exp: the expected reward obtained over the batch given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # Expected true positives (TPs) and false positives (FPs)\n",
        "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
        "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
        "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
        "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
        "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
        "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "             [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {R_exp}')\n",
        "    return None\n",
        "  else:\n",
        "    return R_exp, confusion_matrix\n",
        "\n",
        "W_test = np.zeros((1,65))\n",
        "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
        "# for broadcasting to work\n",
        "print('Evaluation 1')\n",
        "eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=True)\n",
        "print('\\nEvaluation 2')\n",
        "eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "ZDlv-zwC6hkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that as hoped, the evaluation of parameters using expected reward, is consistent, as it should be. As a sanity check we see that the distribution of stochastic evaluations is roughly symmetric, and centered around this expectation, with the average of many such stochastic evaluations becoming close to our calculated expected value."
      ],
      "metadata": {
        "id": "JACJHFAi90df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown **Run this cell** to visualize the distribution of stochastic parameter evaluation, relative to the expectation.\n",
        "def eval_params_expectation_batch(W, x, y, verbose=False):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R_exp: the expected reward obtained over the batch given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # Expected true positives (TPs) and false positives (FPs)\n",
        "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
        "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
        "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
        "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
        "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
        "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "             [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {R_exp}')\n",
        "    return None\n",
        "  else:\n",
        "    return R_exp, confusion_matrix\n",
        "\n",
        "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random(size=y_hat.shape) < y_hat  # 1 x batch\n",
        "  R = np.zeros(y_sample.shape)\n",
        "  did_strike = y_sample == 1\n",
        "  did_not_strike = y_sample == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix\n",
        "\n",
        "W_test = np.zeros((1,65))\n",
        "exp_reward, _ = eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=False)\n",
        "\n",
        "# Generate stochastic rewards\n",
        "stochastic_rewards = []\n",
        "for _ in range(500):  # Simulate 100 times to create a distribution\n",
        "  r, _ = eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=False)\n",
        "  stochastic_rewards.append(r)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "remove_ip_clutter(fig)\n",
        "ax.hist(stochastic_rewards, bins=20, alpha=0.75, label='Stochastic Evaluations')\n",
        "ax.axvline(x=exp_reward, color='r', linestyle='dashed', linewidth=2, label=f'Expected Reward: {exp_reward}')\n",
        "ax.set_title('Comparison of Stochastic Evaluations and Expected Reward')\n",
        "ax.set_xlabel('Reward')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fGcMioTH9zxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As yet another sanity check we calculate the expected reward when striking and not striking with equal probability, in all circumstances, which is what we expect from a $\\mathbf{W}$ of all zeros."
      ],
      "metadata": {
        "id": "q_M7oYlFC9y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are this many cases where striking is good\n",
        "np.sum(y1 == 1)"
      ],
      "metadata": {
        "id": "k1lP9VvCPLNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And this many cases where striking is bad\n",
        "np.sum(y1 == 0)"
      ],
      "metadata": {
        "id": "FjR08i1kPSoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W = 0 should strike half the time no matter what,\n",
        "# in which case would expect a reward of\n",
        "(2829 - 2791) / 2"
      ],
      "metadata": {
        "id": "fKcCR599Piys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That all checks out, so now that we have some confidence in our evaluation function let's see if perturb-measure-step is able to find a good set of values for $\\mathbf{W}$ using the batch expected value version of parameter evaluation. Run the training loop. The process will take a minute or two to complete, while its running inspect the code and see if it makes sense to you."
      ],
      "metadata": {
        "id": "EK8zI-aSDTJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 60000\n",
        "step_scale = 0.002\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "for step in range(num_steps):\n",
        "  R_current, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  R_test, _ = eval_params_expectation_batch(W + test_perturbation, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 3000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Expected Total Batch Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "DPCBERK3Q0kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best possible score is 2829, and perturb measure step is able to discover network parameters that achieve a score of roughly 2500, in 60,000 steps. This is pretty good, better than we the humans were able to do in terms of figuring out the pattern, before we got bored of the problem in a few minutes. This is all well and good as an optimization exercise, but if we want to connect this form of optimization back to our inspirational cartoon of neural behaviour as a kind of learning, there are a few issues. Three major issues stand out as ways in which the perturb-measure-step training loop above deviates from a process that is simple and local enough to serve as plausible (even if very abstract) model of a physiological syanptic plasticity processes. These key issues are:\n",
        "1. The organism's striking behaviour is stochastic but expected reward outcomes, not actual obtained reward outcomes are used to drive updates to the synaptic parameters $\\mathbf{W}$.\n",
        "2. The evaluation of a given synaptic configuration is based on performance over all of the 5620 of distcint input-outpur pairs (sensory-pattern, prey-presence) in the data set that defines the \"environment\" of this learning problem. Physiologically viable would be evaluations over a single, or at least relatively few, stimulus-response-reward episodes.\n",
        "3. Evaluations are performed in seperate perturbation and non-perturbation modes in the training loop above. A single mode of evaluation that operated fully \"online\" and in congunction with the ongoing generation of behaviour is more simple and easy to imagine physiological implementations of.\n",
        "\n",
        "There are of course many other ways in which the this cartoon learning system deviates from what might plausibly be implemented in an actual simple neural system, but these have more to do with the abstractness of the model, and can concievibly be remedied with careful choices about how to make the model more concrete so as to map nicely onto measurable phyiological features of neural plasticity. In contrast, for the critical points oulined above, it is difficult to imagine how any of these key issues can be overcome physiologically, without invoking additional complex neural circuits and processes, the orgin of which also need to be explained. So for this sequence and the next we focus on addressing these core issues, different-modes, batch versus single experience based reward, and expected versus actual recieved (stochastic) reward.\n",
        "\n",
        "In the rest of this sequence we will adapt the base perturb-measure-step update rule to address each of these three issues. But first let's just get a bit of a sense of how these issues impinge upon the fantasy of using perturb-measure-step as an algorithm that might feasibly be used by a living organism to update the connection strengths of this simple network determining behaviour in response to stimulus. In each interation in the vanilla perturb-measure-step training loop implemented above the organism first evaluates its current parameters based on the expected reward over all 5620 possible experiences. It then perturbs its synaptic parameters and evaluates its performance again on all 5620 experiences, these two evaluations are compared to determine $\\Delta R$ and this together with the pertrubations $\\Delta W_i$ determine the synaptic connections update according to\n",
        "\n",
        "$$ W_i' = W_i + s \\ \\Delta W_i \\ \\Delta R $$\n",
        "where\n",
        "$$\\Delta R = R(\\mathbf{W} + \\Delta \\mathbf{W}) - R(\\mathbf{W})$$\n",
        "and $R(\\mathbf{W})$ is our reward/evaluation function.\n",
        "Note that $s$ needs to be carefully choosen to account both for the average size of the perturbation $\\| \\mathbf{W} \\|$, the appropriately level of scaling given the expected alignment of a random perturbation with the gradient given the dimensionality of $\\mathbf{W}$, and the relative scale of the gradient. While this is can be a challenge in practical applications, the \"dialing-in\" of meta-parameters of learning algorithms is something that we expect evolution to be quite good at.\n",
        "\n",
        "This process would require the organism somehow integrate all of the reward outcomes of all 5620 experiences, remember this aggregated outcome, then integrate up the reward outcomes of another 5620 experiences accumulated while in 'perturbation' mode, and then update its parameters based on a comparison of the remembered and the recently accumlated aggregate reward outcome. This all seems a bit complicated, and difficult to implement with simple, primarily local, synaptic plasticity mechanisms. What we would like intead is a version of perturb-measure-step that updates its synaptic weights as a result of every reward experience, doesn't rely on an expected reward calculation, and that does not have a seperate perturbation mode.\n",
        "\n",
        "Removing the seperate perturbation mode is perhaps the easiest issue to address so we take this on first."
      ],
      "metadata": {
        "id": "T-Qw7kZ7T8Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.2 Stochastic-Step-Measure-Step\n",
        "\n",
        "One simple way to avoid a seperate perturbation evaluation is to simply compress the perturbation and the update into a single step. We call such an update method step-measure-step as contrasted with perturb measure step. This new update rule looks like this.\n",
        "\n",
        "\n",
        "\n",
        "$$\\ W_{i}(t+1)= W_i(t) + s \\ (W_i(t) - W_i(t-1)) \\ \\ R(\\mathbf{W}(t)) - R(\\mathbf{W}(t-1)) + \\xi_{i}(t)$$\n",
        "\n",
        "Previously we used $\\Delta$ to denote the perturbation and did not directly reference the parameter update. Because now we are combining the perturbation and the parameter update we use $\\Delta$ to denote this combined change, that is\n",
        "$$ \\ W_{i}(t+1) - W_i(t) = \\Delta W_i(t) $$\n",
        "\n",
        "Then the above simplifies\n",
        "\n",
        "$$ \\Delta W_i(t) = s \\ \\Delta W_i(t-1) \\ \\Delta R(t) + \\xi_i(t) $$\n",
        "Here $\\Delta R(t) = R(\\mathbf{W}(t)) - R(\\mathbf{W}(t-1))$\n",
        "\n",
        "Let's see if\n"
      ],
      "metadata": {
        "id": "Rdp1m0JJV2ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Stochastic-Step-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 100000\n",
        "init_step_scale = 0.003 * np.sqrt(65)\n",
        "init_noise_scale = 0.0005\n",
        "total_scale = 1.0\n",
        "#later_step_scale = 0.002 * np.sqrt(65)\n",
        "#later_noise_scale = 0.0001\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "delta_W = np.zeros((1,65))\n",
        "R, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "start_time = time.time()\n",
        "step_scale = init_step_scale\n",
        "noise_scale = init_noise_scale\n",
        "for step in range(num_steps):\n",
        "  R_old = R\n",
        "  R, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "  delta_R = R - R_old\n",
        "  delta_W_noise = learn_rng.normal(0, noise_scale, size=(1,65))\n",
        "  delta_W = total_scale * (step_scale * delta_W * delta_R + delta_W_noise)\n",
        "  W += delta_W\n",
        "\n",
        "  if step == 0 or (step + 1) % 10000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    #print(f'Delta R: {delta_R}')\n",
        "    #print(f'Delta W: {delta_W}')\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Total Expected Reward: {R:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "    #if step > 40000:\n",
        "    #  step_scale = later_step_scale\n",
        "    #  noise_scale = later_noise_scale\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "5Q6yqqAJWCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this trick of smushing the perturbation and the update into a single step seems to work okay, though not as well as doing the very careful perturbation, but it shows that much of the correct pattern response to stimulus can be discovered through this mode. This particular idea for smushing together the perturbation and the update was is known as ALOPEX (an acronym from \"ALgorithms Of Pattern EXtraction\",  first proposed by Tzanakou and Harth in 1974.) This is one way of not having a seperate perturbation and non-perturbation mode. There are other ways, such as leveraging the stochasticity of relative spike timings (ref seung), and using reward signals directly (or above some anticipated baseline) and we will discuss these later. For now, it is enough to know that this issue can be mitigated, though with some apparrent cost to overall performance (relative to perturb-measure-step). Let's move on to the next issue."
      ],
      "metadata": {
        "id": "-73AjlBda7kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.3 Full-Batch versus Mini-Batch Perturb-Measure-Step\n",
        "\n",
        "Another issue in terms of physiological viability was parameter evaluation based on performance across all possible experiences. In a simple model of learning and synaptic plasticity, we would like learning to be driven by the outcomes of a single stimulus-response-reward experience.\n",
        "\n",
        "This relates to an important idea in machine learning. The idea of the mini-batch. Even though we are ulitmately interested is the performance of the parameters over all of the input-target pairs in the data set, given how computers operate, it is typically possible to generate a sample evaluation based on a small random sample of the input-target pairs in the data set. Such a samlpe is called a mini-batch of the data. (As contrasted with the entire data-set which is called a full-batch or just batch). Evaluations on these mini-batches provides an estimate of the desired full-batch evaluation. In the extreme case a mini-batch can consist of a single input-output pair. In ML the idea of using mini-batches developed in the context of Gradient Descent optimization (which we haven't covered yet). So even though the mini-batch idea applies to all kinds of optimzation processes, for historical reasons,  learning with mini-batches of size 1 is called \"Stochastic Gradient Descent\", and learning with mini-batches of size greater than 1 but less than the full batch is called \"Mini-Batch Stochastic Gradient Descent\", and learning with the full batch is just called Gradient Descent. In practical ML settings, whenever the data-set becomes sufficiently large, mini-batches are almost always used, as they allow for the most efficient use of computational resources. Here we explicitly seperate the mini-batch idea from Gradient Descent, and so use the terms full-batch, mini-batch, and singleton based learning and evaluations. Let's see what having a mini-batch of different sizes does to our learning rates. For now we test out this mini-batch idea on perturb-measure-step."
      ],
      "metadata": {
        "id": "HIOigwyZeYwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mini-Batch(10) Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "step_scale = 0.002\n",
        "mini_batch_size = 10\n",
        "num_epochs = 400\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for epoch in range(num_epochs):\n",
        "  np.random.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_Xs = Xs_aug[batch_indices].T\n",
        "    batch_y1 = y1[batch_indices].T\n",
        "    R_current, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
        "    raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "    unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "    test_perturbation = unit_test_perturb * perturbation_scale\n",
        "    R_test, _ = eval_params_expectation_batch(W + test_perturbation, batch_Xs, batch_y1)\n",
        "    directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "    W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if epoch == 0 or (epoch + 1) % 20 == 0:\n",
        "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Expected Total Batch Reward: {total_expected_reward:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "b_Q3ceuPUUkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So with a mini-batch of size 10, this works really quite well, basically just as well as evaluating the parameters on the entire experience, what about when we scale down to the extreme case of a mini-batch of size 1?"
      ],
      "metadata": {
        "id": "pjayFNS7ilBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Singleton Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "step_scale = 0.002\n",
        "mini_batch_size = 1\n",
        "num_epochs = 80\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for epoch in range(num_epochs):\n",
        "  np.random.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_Xs = Xs_aug[batch_indices].T\n",
        "    batch_y1 = y1[batch_indices].T\n",
        "    R_current, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
        "    raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "    unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "    test_perturbation = unit_test_perturb * perturbation_scale\n",
        "    R_test, _ = eval_params_expectation_batch(W + test_perturbation, batch_Xs, batch_y1)\n",
        "    directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "    W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Expected Total Batch Reward: {total_expected_reward:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "zChE6Ud1i0dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(65) * 0.002"
      ],
      "metadata": {
        "id": "xj0WOGHEXSWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So learning from a single experience using perturb-measure-step doesn't appear to be a problem at all. If anything it leads to faster (in terms of computational time) learning, since evaluations on a single (or small mini-batch of) input-output pair(s) are much quicker to compute than evaluations of the parameters based on the entirety of the avialble data-set.\n",
        "\n",
        "Now, unfortunately, the smushing together of the update step and the perturbation step, does not mix well with the mini-batch idea. The key issue here, is that in the mini-batch training loops above **the same** mini-batch was used to evaluate both the base parameters and the perturbed parameters alowing for a precise estimate of the rate of improvement in the direction of the test perturbation. If two **different** mini-batches are used to estimate the reward at two different points in the parameter space (as happens in when we try to use step-measure-step with mini-batches), the noise introduced by using different mini-batches makes the estimate the degree of improvement between the two points much noisier. While learning is still possible with this extra noise, it will make learning too slow to be practical both in an ML context and in the context of rapdily aquiring adaptive behaviours.\n",
        "\n",
        "Finally we turn our attention to using actual sampled rewards, instead of computed expectation of reward."
      ],
      "metadata": {
        "id": "0BZyRrdxj35_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.4 Recieved Reward Perturb-Measure-Step\n",
        "\n",
        "Later on in this book we will see how an organism might develop an internal model of how their actions interact with the dynamics of the environment to change the state of the environment and produce rewards, but for the moment we are focused on arriving at the simplest viable learning circuit that can rapidly learn good behaviour on the strike-no-stike problem, while invoking only simplest imaginable physiological mechanisms of plasticity. So we wish to exclude the computation of expected reward, and instead focus on the actual rewards recieved by the organism and how they might drive synaptic weight changes. In the training loop below we implement perturb-measure-step but using stochastic evaluation. Let's see how it goes"
      ],
      "metadata": {
        "id": "ye0f3r02J2he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampled-Reward Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 30000\n",
        "step_scale = 0.000005\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.01 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "for step in range(num_steps):\n",
        "  R_current, _ = eval_params_stochastic_batch(W, Xs_aug.T, y1.T)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  R_test, _ = eval_params_stochastic_batch(W + test_perturbation, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 3000 == 0:\n",
        "    R_exp, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Expected Total Batch Reward: {R_exp:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "SveIOTt2LWB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.5 Putting them all together\n",
        "\n",
        "So there are simple ideas that overcome our key issues with perturb-measure-step as implemented initially. We can effectively learn from single experiences, we can effectively learn from actual rewards resulting from actual rewards made (not expectated reward given hypothetical probabilities of taking given actions), and we can effectively use the difference in reward between two episodes for learning instead of requiring a seperate perturbation evaluations mode of operation for the network. All of these are lovely ideas, however, they do not combine particularly well, at least not in a straightforward way. Learning is still possible when all of these ideas are combined, but improvement is so slow as to be totally impractical. As can be seen in the figure below, (or if you want to burn a bunch of cpu cycles you can run the cell below"
      ],
      "metadata": {
        "id": "Jb1aF07qNrlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampled-Reward Singleton Stochastic-Step-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 10000\n",
        "mini_batch_size = 1\n",
        "step_scale =  0.000001\n",
        "noise_scale = 0.0000005\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "delta_W = np.zeros((1,65))\n",
        "R, _ = eval_params_stochastic_batch(W, Xs_aug.T, y1.T)\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "  np.random.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_Xs = Xs_aug[batch_indices].T\n",
        "    batch_y1 = y1[batch_indices].T\n",
        "    R_old = R\n",
        "    R, _ = eval_params_stochastic_batch(W, batch_Xs, batch_y1)\n",
        "    delta_R = R - R_old\n",
        "    delta_W_noise = learn_rng.normal(0, noise_scale, size=(1,65))\n",
        "    delta_W = total_scale * (step_scale * delta_W * delta_R + delta_W_noise)\n",
        "    W += delta_W\n",
        "  if epoch == 0 or (epoch + 1) % 10 == 0:\n",
        "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Expected Total Batch Reward: {total_expected_reward:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "k-k30JNENuXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.3.6 Is Solving High Dimensional Optimization Problem Necissary for Adaptive Behaviour? Hard Yes.\n",
        "\n",
        "From a machine learning perspective, when thinking about the performance of a model, there are several things that might be limiting performance. Maybe the data is fundamentally difficult, e.g. there are identical inputs with different correct outputs, i.e. $\\mathbf{x} = \\mathbf{x}$, but $y = 0 $ and $y'=1$. Another possibility is that learning algorithm being used is not effective given the model archetecture and the available data. Yet a third possibility is that archetecture of the model itstelf is not complex or flexible enough to capture the relevent patterns in the data. This last reason happens to be the case here.\n",
        "\n",
        "One reason for having a large, flexible brain is to produce complex behaviour that is contingent on the state of the world in complex ways. The simple strike-no-strike network we've been using, consists of a single layer of synaptic connections between the sensory inputs and the motor output. In a shallow network structure like this a sensory input either directly inhibits or promotes the striking behaviour, there is no possibility for a sensory input to promote striking in some sensory contexts, but inhibit it in others. This is a deficincy. Context is important, loud thumping and roaring noises at a concert means the band is good, similar noises when camping in the wilderness means a large animal is nearby. A shallow network does not allow for such context dependence.\n",
        "\n",
        "How do we allow for context dependence, or put another way, interactions between the inputs (beyond simply summing them together). As a starting point our current sensory-behavioural circuit is effectively equivalent to logistic regression, i.e. each sensory input element (feature in ML parlance) can either inhibit or potentiate striking behaviour to varying degrees, but there is no possibility for conditional interaction between features. By 'conditional interaction,' we mean a scenario where, for instance, feature 1 typically inhibits the behavior, except when feature 2 is positive, under which condition feature 1 becomes potentiating. These kinds of feature interactions are impossible in the current model. One way to allow for such interactions is to augment the base set of features with composite features, e.g. incorporate all the pairwise products of the existing feature set, so that instead of 65 features (bias included) we have $(\\frac{64 \\cdot 63}{2} + 64 + 64 + 1) = 2145$ features to work with. This could work, but what if we want something that depends on the interaction of more than 2 features, adding higher order polynomial terms will quickly make the problem intractable (Reference appendix section on why hidden layers not polynomials if we do that). If we had some mechanistically grounded understanding or hypothesis about the relationship between the features and label could might be able to cherry pick some small subset of higher order interaction terms, but the ML mindset is in large part about automating the feature selection processes based on the data alone. In turns out that instead of resorting the regression on polynomial terms to capture feature interactions, there is a much more compact and expressive way of allowing for feature interactions. The idea is to allow for feature interactions to emerge as needed in a 'hidden' computational layer of our highly abstracted neurons.\n",
        "\n",
        "Simply adding an additional layer to the network, consisting of some intermediate (hidden) neurons between the inputs and the outputs, allows for context dependent inhibition and promotion of striking are possible. There are theorems showing that a much as polynomials of arbitrarily high degree are a kind of universal function approximator (e.g. taylor series), simiarly networks with a singal (but potentially very large hidden layers) are a kind of universal function approximator. With this in mind, let's see if adding a hidden layer to our network improves performance.\n",
        "\n",
        "Our new strike-no-strike network with 1 hidden layer is structured as follows"
      ],
      "metadata": {
        "id": "u1lGLbkNVUj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before $\\mathbf{x}$ is the raw sensory input (vector) in a given episode and each element of $\\mathbf{x}$ corresponds to the activation level and firing rate of a single photosensitive neuron.\n",
        "These input neurons are then connected by synapses to a 'hidden layer' of intermediate computational neurons, say 10 of them. The activation level of these hidden layer neurons is computed as\n",
        "$$\\mathbf{h} = \\sigma(\\mathbf{W}_{in} \\cdot \\mathbf{x})$$\n",
        "Now $\\mathbf{W}_{in}$ is a matrix of synaptic weights between the input neurons and the hidden layer neurons, and $\\cdot$ denotes standard matrix vector multiplication. (In this case $\\mathbf{W}$ has shape $10 \\times 65$. Each the values in the $i^{th}$ row of $\\mathbf{W}_{in}$ given the sign and strength of the connections coming into the $i^{th}$ element of $h$ and similarly each value in the $j^{th}$ column of $\\mathbf{W}_{in}$ corresponds to connection strengths coming out of the $j^{th}$ sensory input neuron.)  We still us $\\sigma$ to represent the standard logistic sigmoid function, but in this case applied elementwise the vector output of the product $\\mathbf{W}_{in} \\cdot \\mathbf{x}$. Then, much as before our striking probability is computed as\n",
        "$$a = \\mathbf{W}_{out} \\cdot \\mathbf{h}$$\n",
        "and\n",
        "$$ \\Pr \\{\\text{strike}\\} = \\sigma(a) $$\n",
        "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma(a)$$\n",
        "Here $\\mathbf{W}_{out}$ has shape $1  \\times 10$.\n"
      ],
      "metadata": {
        "id": "CvsVIoAvVNh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to re-write a new eval params function for this new model, let's do it. To keep things quick and simple we will just use our expecation based evaluation"
      ],
      "metadata": {
        "id": "j9tt8sJofITF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_params_batch_expectation_hidden(W_in, W_out, x, y, verbose=False):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W_in: (hidden_neurons x inputs(65) np.array)\n",
        "           weights between sensory neurons and hidden layer neurons\n",
        "    W_out: (output(1) x hidden_neurons np.array)\n",
        "           weights between hidden layer neurons and output\n",
        "    x: (input(64) x batch np.array) sensory input\n",
        "       (can be single input, mini-batch of inputs or the whole batch of inputs)\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "       (can be a single target, mini-batch of targets, or whole batch),\n",
        "       needs to correspond to input\n",
        "\n",
        "  Returns:\n",
        "    R_exp: the average/expected total reward obtained given the parameters, over the\n",
        "           (mini-)batch of inputs and targets. (mini-batch could be size 1)\n",
        "  \"\"\"\n",
        "  # activation\n",
        "  h = np_sigmoid(np.dot(W_in,x)) # hidden_neurons x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(np.dot(W_out,h)) # 1 x batch\n",
        "  # Expected true positives (TPs) and false positives (FPs)\n",
        "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
        "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
        "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
        "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
        "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
        "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "             [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {R_exp}')\n",
        "    return None\n",
        "  else:\n",
        "    return R_exp, confusion_matrix\n"
      ],
      "metadata": {
        "id": "wequmEMQfJMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've got a more complicated circuit with more parameters, how much longer does it take us to evaluate this circuit compared to our previous one?"
      ],
      "metadata": {
        "id": "Eiem7eE-fw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.zeros((1,65))\n",
        "%timeit eval_params_expectation_batch(W, Xs_aug.T, y1.T)"
      ],
      "metadata": {
        "id": "vyebIZ8Vf3bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_in = np.zeros((10,65))\n",
        "W_out = np.zeros((1,10))\n",
        "%timeit eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)"
      ],
      "metadata": {
        "id": "htd8pC1uf55M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roughly 5x longer per function evaluation, which means not only will we likely need more iterations of our algorithm because it is harder to find good parameters in high dimensions (we have 660 parameters now, which is a lot more than 65), but also each of those steps will take longer to process because function evaluations are also more costly. It will all be worth it if we can get better final performance though."
      ],
      "metadata": {
        "id": "Px0KZWgFgK1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10 Hidden Units - Perturb-Measure-Step Training Loop\n",
        "llearn_rng = np.random.default_rng(0)\n",
        "num_steps = 20000\n",
        "step_scale = 0.025\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "num_hidden_units = 10\n",
        "# initializing both layers as zero leads to some issues, so we\n",
        "# use a Xavier/Glorot random initialization scheme\n",
        "in_init = np.sqrt(6 / (65 + num_hidden_units))\n",
        "W_in_init = learn_rng.uniform(-in_init, in_init, size=(num_hidden_units, 65))\n",
        "out_init = np.sqrt(6 / (10 + 1))\n",
        "W_out_init = learn_rng.uniform(-out_init, out_init, size=(1, num_hidden_units))\n",
        "flat_params = np.concatenate((W_in_init.flatten(), W_out_init.flatten()))\n",
        "dimensional_scale_factor = np.sqrt(len(flat_params))\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for step in range(num_steps):\n",
        "  W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "  W_in = W_in.reshape((num_hidden_units,65))\n",
        "  W_out = W_out.reshape((1,num_hidden_units))\n",
        "  R_current, _ = eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)\n",
        "  raw_param_perturb = learn_rng.standard_normal(size=len(flat_params))\n",
        "  unit_param_perturb = raw_param_perturb / np.linalg.norm(raw_param_perturb.flatten())\n",
        "  test_perturbation = unit_param_perturb * perturbation_scale\n",
        "  perturbed_flat_params = flat_params + test_perturbation\n",
        "  W_in_test, W_out_test = np.split(perturbed_flat_params, [65 * num_hidden_units])\n",
        "  W_in_test = W_in_test.reshape((num_hidden_units,65))\n",
        "  W_out_test = W_out_test.reshape((1,num_hidden_units))\n",
        "  R_test, _ = eval_params_batch_expectation_hidden(W_in_test, W_out_test, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  flat_params += step_scale * dimensional_scale_factor * directional_grad_est * unit_param_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 1000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Current Total Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "W_in = W_in.reshape((num_hidden_units,65))\n",
        "W_out = W_out.reshape((1,num_hidden_units))\n",
        "eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T, verbose=True)\n"
      ],
      "metadata": {
        "id": "MDoAqfWghM--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this new, more complex circuit is great. We're much closer to the theoretical maximum performance of 2829, maybe with a few more hidden units, and a little longer training time we could have perfect discrimination. Let's see what happens when we go up to 20 hidden units. As a heads up this is going to take awhile (about 3 minutes) so you should read ahead while waiting for this training loop to complete"
      ],
      "metadata": {
        "id": "m4hKKsmKnjrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 20 Hidden Units - Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 20000\n",
        "step_scale = 0.025\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "num_hidden_units = 20\n",
        "# initializing both layers as zero leads to some issues, so we\n",
        "# use a Xavier/Glorot random initialization scheme\n",
        "in_init = np.sqrt(6 / (65 + num_hidden_units))\n",
        "W_in_init = learn_rng.uniform(-in_init, in_init, size=(num_hidden_units, 65))\n",
        "out_init = np.sqrt(6 / (10 + 1))\n",
        "W_out_init = learn_rng.uniform(-out_init, out_init, size=(1, num_hidden_units))\n",
        "flat_params = np.concatenate((W_in_init.flatten(), W_out_init.flatten()))\n",
        "dimensional_scale_factor = np.sqrt(len(flat_params))\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for step in range(num_steps):\n",
        "  W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "  W_in = W_in.reshape((num_hidden_units,65))\n",
        "  W_out = W_out.reshape((1,num_hidden_units))\n",
        "  R_current, _ = eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)\n",
        "  raw_param_perturb = learn_rng.standard_normal(size=len(flat_params))\n",
        "  unit_param_perturb = raw_param_perturb / np.linalg.norm(raw_param_perturb.flatten())\n",
        "  test_perturbation = unit_param_perturb * perturbation_scale\n",
        "  perturbed_flat_params = flat_params + test_perturbation\n",
        "  W_in_test, W_out_test = np.split(perturbed_flat_params, [65 * num_hidden_units])\n",
        "  W_in_test = W_in_test.reshape((num_hidden_units,65))\n",
        "  W_out_test = W_out_test.reshape((1,num_hidden_units))\n",
        "  R_test, _ = eval_params_batch_expectation_hidden(W_in_test, W_out_test, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  flat_params += step_scale * dimensional_scale_factor * directional_grad_est * unit_param_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 1000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Current Total Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "W_in = W_in.reshape((num_hidden_units,65))\n",
        "W_out = W_out.reshape((1,num_hidden_units))\n",
        "eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "LV67Oyy1nxSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This more complex circuit gets us even closer to the theoretical maximum performance of 2829, but it's taking longer to get there. This is a consequence of both, the function evaluations required at each step being slower (for the more complex function) and because more iterations are needed to effectively search the higher dimensional space for a good configuration of $\\mathbf{W}_{in}$ and $\\mathbf{W}_{out}$. With even more hidden units and more time we can likely learn perfect discrimination, but it will take even longer (more than 3 minutes!)."
      ],
      "metadata": {
        "id": "DoYe61pfdTzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the toy neural circuit models in this sequence are a far cry from actual neural circuits, they still provide insight into possible mechanisms of synaptic plasticity the brain. We can imagine a scenario where synaptic strengths between neurons in a circuit undergo small, transient perturbations. The brain might integrate and compare the performance of these perturbations over a learning episode (for example, a day) to previous performance levels. (We leave aside the specifics of how this integration and comparison occur for now.)\n",
        "\n",
        "If performance improves with a perturbation, synaptic changes could be consolidated in the direction of the perturbation, proportionate to the degree of improvement. Conversely, if performance worsens, changes might be consolidated in the opposite direction, also proportional to the performance decrease. This concept, while still vague, suggests a mechanism of synaptic adjustment based on performance feedback.\n",
        "\n",
        "One critical point to consider is the scalability of such a learning process. The number of learning episodes required for effective optimization grows with the number of parameters in a neural circuit. This implies that 'perturb-measure-step' plasticity cannot be the primary mechanism driving neural plasticity in large, complex neural circuits that learn rapidly. This limitation is critical, the lifetimes of most animals simply aren't long enough to accommodate the number of learning iterations needed for extensive optimization.\n",
        "\n",
        "However, as demonstrated above, a more complex circuit achieved significantly better performance in the discrimination task, so large complex circuits can be useful. This suggests that even if empirical evidence of perturbation-based learning in the brain exists and its physiological implementation is understood, such processes are unlikely to be the primary drivers of neural plasticity for complex and challenging behaviors.\n",
        "\n",
        "(One counterargument in favor of simple learning rules is that extensive learning might not be necessary if genetic predisposition starts the circuit off close to an optimal parameter configuration. Then subsequently, relatively slow learning processes could 'fine-tune' the neural circuit's configuration. However, as noted in our earlier discussions on evolution, changing environments necessitate that a significant portion of behavior must emerge from learning, thereby limiting the extent to which genetic predispositions can facilitate efficient and adaptive learning.)"
      ],
      "metadata": {
        "id": "4s0X34-jgHZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sequence we looked at how a simple neural circuit, and then a slightly more complex one were able to solve the strike-no-strike discrimination problem, when trained using perturb-measure-step. We found that several of the \"unrealistic\" aspects of perturb-measure-step (full batch learning, a seperate perturbation mode, used expected instead of actual reward to inform learning) could be mitigated, but only at the cost of slowing down learning (though using mini-batches in isolation, when evaluating the perturbation and current parameters on the same mini-batch was actually a speed up). Additionally we saw that using a more complex network (with hidden layers of increasing size) allowed for better perfomance on the discrimination task, but this extra performance came at a cost, function evaluation was more complex and there were more parameters to be optimized, and searching higher-dimensional spaces for good parameter configurations is always harder (more places to look, more directions to try, in addition to more compute required to for each test point evaluation).\n",
        "\n",
        "In the next sequence we will introduce a different perspective on neural plasticity, one in which synaptic plasticity is driven between a mismatch between an expectation or prediction about reward, and the actual recieved reward."
      ],
      "metadata": {
        "id": "B-UDcsvvhF9V"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "P2C1_Sequence5",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2591325e6444cdc835343a898ead83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b7a8f10cc924a20909f81656a0b19c2",
              "IPY_MODEL_579d13f7de4341cb8cc0429425e8d871",
              "IPY_MODEL_69d49a3ec576423e88ce61ab9d085348"
            ],
            "layout": "IPY_MODEL_163a4b520edd4f38ab62e6aa77dd21f0"
          }
        },
        "4b7a8f10cc924a20909f81656a0b19c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dff20ca2f2fc443e91aea0dd8c6cbbeb",
              "IPY_MODEL_73546dd43bb342f894f0272c169c43de"
            ],
            "layout": "IPY_MODEL_48cebd353b07475cacf28a0eaf36d048"
          }
        },
        "579d13f7de4341cb8cc0429425e8d871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d22b5add981343c9bf5c3c57f5b402ee",
              "IPY_MODEL_25a517bbb2f84f3aa4fdfde67bcc708f"
            ],
            "layout": "IPY_MODEL_a5f2bd72343a4c62831f98d048d3a81f"
          }
        },
        "69d49a3ec576423e88ce61ab9d085348": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ca8b60b5a9004a65836a3d68fc8f130d",
            "msg_id": "",
            "outputs": []
          }
        },
        "163a4b520edd4f38ab62e6aa77dd21f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff20ca2f2fc443e91aea0dd8c6cbbeb": {
          "model_module": "jupyter-matplotlib",
          "model_name": "MPLCanvasModel",
          "model_module_version": "^0.11",
          "state": {
            "_cursor": "default",
            "_data_url": "data:image/png;base64,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",
            "_dom_classes": [],
            "_figure_label": "Figure 2",
            "_image_mode": "full",
            "_message": "x=6.67 y=6.45",
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "MPLCanvasModel",
            "_rubberband_height": 0,
            "_rubberband_width": 0,
            "_rubberband_x": 0,
            "_rubberband_y": 0,
            "_size": [
              400,
              400
            ],
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "MPLCanvasView",
            "capture_scroll": false,
            "footer_visible": false,
            "header_visible": false,
            "layout": "IPY_MODEL_d10dfab7102847fd9b09e01546256ca9",
            "pan_zoom_throttle": 33,
            "resizable": false,
            "toolbar": "IPY_MODEL_8b9e7923b262462bbb934218e7dbd332",
            "toolbar_position": "left",
            "toolbar_visible": false
          }
        },
        "73546dd43bb342f894f0272c169c43de": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ccf4b6fd6058476b8fd3e2b8f252bf2c",
            "msg_id": "",
            "outputs": []
          }
        },
        "48cebd353b07475cacf28a0eaf36d048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22b5add981343c9bf5c3c57f5b402ee": {
          "model_module": "jupyter-matplotlib",
          "model_name": "MPLCanvasModel",
          "model_module_version": "^0.11",
          "state": {
            "_cursor": "default",
            "_data_url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAYAAACAvzbMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa50lEQVR4nO3deXCUhRnH8d+GzUmQBFRQZIIlgIiIWNSZEk2wWCpaVBAdWgQs8eyoFcWrKmCpDsUqtY6lUkcqIIcH9RwUGc8eKgQ8KHJMA4iiKMpRhECSp3/Qfbub7GbDQ/AF8/3MZCZk99198ubNfvO+7+4SMTMTAAD7KCPsAQAAhyYCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCEqempkbz5s3TiBEj1LVrVxUUFCgrK0tHHnmkSkpKdOutt+rDDz8Me8yD2muvvabx48dr+vTp+31bZqYnnnhCF1xwgYqKipSbm6v8/Hx17txZJSUlGjNmjObPn69t27bt/+DYb506dVIkEtGoUaPCHqXJjR8/XuPHj9fatWvDHuXgYjAzs3/84x/WtWtXkxR8ZGZmWps2bSwjIyPh64MHD7aqqqqwRz4ojRs3ziRZaWnpft3O119/baWlpQnrPRqNWps2bSwajSZ8/dFHH22S2bF/ioqKTJKNHDky7FGaXGxbe/XVV8Me5aDCHoik5557TmVlZVq1apXatm2re+65R6tWrdLu3bu1efNm7d69W++++65uueUWHXbYYXr66af1zTffhD32d9qIESP0+uuvq0WLFrrhhhu0atUqVVVVafPmzdq5c6fee+89TZo0Sb169Qp7VKDZioY9QNhWr16t4cOHq6qqSscff7xeeuklHXPMMQnXadGihfr06aM+ffpo7Nix+vnPfx7StM3D6tWr9dxzz0mSJk6cqFtuuSXh8mg0qhNPPFEnnniibrrpJu3cuTOMMQGEvQsUtosuusgkWU5Ojq1cubLRy9XW1gafN+awzauvvhrsBtdVd/knn3zSzjrrLDviiCMsEonYuHHjzMxs5MiRwSGC2tpamzZtmvXt29fatGmT9FBOZWWlXXfddXb88cdby5YtLTc317p162bXXnutrVu3Lumcjz76qEmyoqIiMzNbvHixDR061Nq3b29ZWVl27LHH2vXXX29fffVVvftS3GGlZB+NPdQ0b968YJl//etfjVomlQ8++MAuu+wyKy4uttzcXGvZsqX17NnTbrvtNvviiy+SLlP35/HKK6/YwIED7fDDD7fs7Gw77rjjbPz48bZz586U97tgwQK74IILrEOHDpaZmWmtWrWyY4891s466yybPHmybd68Oelya9assSuvvNKKi4stJyfHWrVqZb1797YJEybY1q1bky5Td9uqqKiwn/70p9ahQweLRqNWWlpqK1asCK7z9ttvN7jOhg8f7joM2dAhrPht18zsiSeesNLSUissLLTc3Fzr1auXTZkyxWpqapLeduxw5rhx46yqqsruuece69mzp+Xl5VlBQYH179/fXnzxxZSzqRGHoOLvo+7cqT5ivyfNVbMOyGeffRac3xg9erT7dpoyIGPGjDFJFolErLCw0Fq0aFEvICNGjLAhQ4aYJMvIyLDCwkLLyMhIeICeOXOmZWdnB/eZnZ1tubm5wb9btWplL730Ur1Z4gMya9Ysy8zMNEnWunXrhHNBPXr0sO3btwfLrV+/3tq1a2ctW7YMzh+1a9cu4WPOnDmNWp/xAXn55ZcbtUwykyZNSpg5Ly/PsrKygn8fddRRVlFRUW+5+J/Hb3/7W4tEIhaJRKygoMAikUiwfL9+/ay6urre8hMmTEh4kMnLy7P8/PyEryV7IJs7d27Cz6xVq1YJ/+7YsWPSoMZvW08++WTwMzvssMMsJycn2C5jD5ANbetfffWV5eTkmCSbNWtW41e2NT4gv/jFL4Jtt6CgIGG9jBgxIultx2a/9dZb7fTTTw/OidVdPv7BP543INdee621a9cuWL6wsDBhm+7Tp88+rKHvnmYdkNmzZwcbxvPPP+++naYKSOxB5uabb7ZNmzaZmdmuXbts7dq1Zvb/X8L8/HyLRqN27733Bn+Vbt++3T799FMzM3v55ZctIyPDotGo3XTTTVZZWWm1tbVWW1trH330kQ0dOjR4gKm7JxILSF5enmVnZ1t5ebmtX7/ezMx27NhhDz74YPAAdccdd7jWRTqVlZXBA3XPnj33ac8w5s9//nOwrn7zm9/Yxo0bzcysurraFi9ebGeeeaZJsmOOOSYhhPHfQ0FBgWVkZNitt94a7K1s3brV7rzzzuBn+cgjjyQsu3bt2iBaY8aMsU8++SS4bMuWLfbmm2/a1VdfbYsXL05YbsmSJcF67du3r73//vtmZlZTU2PPPvusHXXUUSbJOnfuXG/e+G0rPz/fBg4caCtWrAguX7VqlZmZzZkzxyRZy5Ytbdu2bUnX2wMPPGCSrG3btrZr165Gr2+zxgWksLDQsrKy7L777gu23S+//NLKy8uD72HRokX1lo89uLdu3dqys7Nt6tSpwR7g+vXr7cILLwyWf+aZZ+ot7w3IvizfHDXrgNx+++3BhhH/i76vmiogsQedVOJ3px944IGk16mpqbEuXbqYJPvTn/6U8rYGDRpkkuy6665L+HosIKkeCMws2EsqLi5O+b3s77OwLrvssmCOSCRivXv3tquvvtoeeeQR++CDDxIOIda1bdu24C/TBQsWJL3Onj177Pvf/75Jsvvvvz/p99DQX7SDBw82Sda/f/+Er8+dO9ckWdeuXffp+/3xj38crNMdO3bUu7yioiJ49tnkyZMTLovftk499dSke0VmZrt377YjjzzSJNnUqVOTXqdnz55pt8NUGhMQKfWhzNjPo7y8vN5l8c/Iqxtts73b/RlnnBHsHddFQA6MZv0srM2bNweft2nTJsRJ9srIyNDNN9+c9nqFhYW64oorkl72xhtvaPXq1Tr88MNVXl6e8jZGjBghSXrppZdSXuf2229P+vXzzjtPkrRmzZoD9my0hx56SHfccYdatmwpM9PSpUv10EMPafTo0erZs6fat2+vMWPG6PPPP6+37FNPPaUtW7aod+/eGjBgQNLbj0ajGjZsmKTU6yA7O1s33nhj0sti6+D9999P+HpBQYEkafv27dqxY0ejvtctW7YEM4wdO1Z5eXn1rtO7d28NHjxYkjR79uyUtzV27Fi1aNEi6WWZmZkaPXq0JOnhhx+ud/k///lPffDBB5Kkyy+/vFGz76uOHTtq5MiRSS8bNGiQpPrrtO7yl156ab2vZ2RkBNvr8uXLg+8DB1azDsjBpri4WEceeWTa651yyinKyspKetnf/vY3SdLWrVt19NFHq3379kk/LrvsMknSunXrkt5OmzZtVFxcnPSyo48+Ovj866+/TjuvRzQa1V133aVPPvlEM2bMUHl5uXr16hV835s2bdL999+vE044Qe+8807CsrF1sGLFipTff/v27XXXXXdJSr0OevToofz8/KSXxdbBV199lfD1U089VYcffrg2btyo0047TQ8++KA++ugjmVnK77WioiK4vH///imvd9ZZZ0na+wC7Z8+epNfp27dvyuWlvWHIyMhQRUWFKioqEi6bNm2aJKm0tFTdunVr8Ha8TjnlFEUikaSXpVqn8crKylIuf/rppysa3fvE0sWLF+/npGiMZh2Qtm3bBp83tNF+WxoTj3TX+/TTTyVJe/bs0eeff57yI/bAn+opsK1atUp5H7Ff0tj9HEitW7fW8OHDNW3aNC1btkxbt27VwoUL9ZOf/ESS9OWXX2rIkCHatWtXsExsHezatavBdRB7BXuqvajGrIPq6uqErxcUFGj27Nk64ogjtHz5cl1zzTXq3r27CgsLNWjQIM2cObPeOtu0aVPweYcOHVLeZ+zp5dXV1Sm313TbUKdOnYK9svi9kG3btmnu3LmSlHLvtik0Zp02tE01tH5ycnKC3+n4dYoDp1kHpEePHsHnS5cuDXGSvVIdetiX69XU1EiSTjvtNNnec1xpPw4lOTk56t+/v5599tngUMiGDRu0YMGC4DqxdXDxxRc36vtv6ren6N+/vyorK/XYY49p5MiR6tKli7Zu3arnnntOl1xyiXr37q1PPvmkSe8zpjHb0FVXXSVJevzxx4PDbLHP27ZtGxwqA9Jp1gHp16+fMjL2roL58+e7byf2l1P8X8F1bd261X37+6J9+/aSUh+W+S6JP06/cuXK4PODYR20bNlSl1xyiaZPn65Vq1Zpw4YNmjRpknJycoI9k5j4vYYNGzakvM3YZdFodL/O2Q0cOFAdO3bU9u3bNWfOHEn/P3w1atQoZWdnu2/7QGsovLF3KpDq74nFwnow/I5+lzTrgLRr105DhgyRtPcvsFWrVjV62fi/3AsLCyVJH3/8ccrrv/32284p903sGPhnn30WynHgWJC/jT2b+PMT8Q96sXWwZMkSbdy48YDP0RgdOnTQTTfdpBtuuEGStHDhwuCyk08+OVhvixYtSnkbr7zyiiSpV69eyszMdM/SokWLIL4PP/xwwvmQA3XyvKm8/vrrKbetN998Mzik2KdPn4TL0v2Obt++XStWrEh5v7HzLofaHvuB1qwDIu19q4z8/Hzt3LlTgwcPTnto4euvv9aQIUMS/lqJvR/Tp59+mjQUmzZtCv7CO9D69esXnPy+/vrrtXv37gav39Tnfg477DBJe59Z5FVZWdmomP/lL38JPj/55JODz4cOHaqCggLt2bNHY8aMafCXvra2dr9mrauqqqrBy3NzcyX9P7TS3vMmsfMSkydPTnpO5r333tNTTz0lScGzx/bH6NGjFY1G9c477+j666+XtPfkedeuXff7tg+k9evXJ/zcY2pra3X33XdLko4//nj17Nkz4fLY72hsHdZ17733Nviza4rt+ruo2Qeka9eumjFjhrKysrR8+XKddNJJmjRpktasWRNcp6amRkuXLtWdd96p733ve3r66acTbuMHP/iBioqKJEkjR47U4sWLZWaqra3Va6+9prKyMtXW1n4r3080GtXUqVMVjUb11ltv6YwzztCiRYsSTkz++9//1tSpU3XKKafooYceatL7P+GEEyTtfSrl3//+d9dtLF++XN27d9c555yjxx57LOEcxZ49e7R06VJdeumluu+++yTtfeZTSUlJcJ2CggJNmTJFkjRnzhydc845evvtt4OfQW1trVasWKHf/e536tGjh55//nnXnMlMmjRJZ599tmbMmJFwOKqqqkrz5s3T5MmTJUnnnHNOwnITJ05UZmam1qxZowEDBgRPQ62trdWLL76ogQMHqrq6Wp07d26Sk9xHHXVU8FTkN954Q9KBPXneVFq3bq2rrrpK06ZNCw5Hffzxxxo2bJheffVVSXvXZV3xT9keN25c8ASKL7/8UrfddpsmTpwYPAU7mdh2PWvWLN5INd6383KTg99bb71lxcXFCW+LkJWVVe/t3CORiA0bNsx2796dsPyCBQuCVxLrf6/kjr0lRJcuXRJe9V5XY198V/f9hBoyf/58a9WqVXCfmZmZ1rZt24S3xpBkEydOTFiu7nthJRP/vleVlZUJl+3Zs8e6deuW8NYPRUVFVlRUZE888UTauc32rsv4GeN/FvFvJSLJTj755JQvAv3jH/+Y8NYl2dnZ1rZt24SfkySbOXNmwnL788LQ+BchSrLc3Nx6c3fv3j14ZXy8OXPmJMwbeyuS2L8b81Ym++KVV14JlvO88ryufXkvrGQa2vbi38qkpKQk2KYLCwsT1vftt9+e9Larq6utX79+Cb/HhYWFwdvUTJ48ucEXEs6YMSPhd6lDhw5WVFRkffv2beTa+W5q9nsgMX379tVHH32k2bNn62c/+5mKi4uVk5Oj7du3q02bNiopKdGvfvUrrVixQo8//ni9Y9ADBgzQm2++qXPPPVeFhYWqqalRx44ddcstt2jJkiXBid1vy/nnn681a9Zo3LhxOvXUU5Wfn68tW7YoOztbvXr1Unl5uebPn6+xY8c26f1Go1EtWrRI5eXlOvbYY7Vjxw6tW7dO69at03/+859G3caAAQO0evVq/f73v9fQoUPVvXt3ZWdna8uWLcrLy1OXLl100UUXac6cOXr33XcTXpcS78orr9TKlSt14403qlevXsFt5Ofnq0+fPrrmmmu0cOHCJjkkFHP55Zfr4Ycf1rBhw3TCCScoLy9P27ZtU2FhoU4//XRNmTJFFRUVSbeHiy++WMuXL9cVV1yhzp07q6qqStFoVCeddJImTJigDz/8UN27d2+yWc8888zgZPzBfvI8JisrS4sWLdLdd9+tbt26qaqqSq1bt9YPf/hDvfDCC/r1r3+ddLkWLVrohRde0IQJE3TccccpKytLkUhEP/rRj7Rw4cKULxiNGT58uGbMmKGSkhLl5eVp48aNWrduXYNPemgOImacFQKaoyVLlgQnm1euXHlQn/8oKyvT66+/rnHjxmn8+PFhj4P/YQ8EaKb+8Ic/SNq7J3IwxwMHLwICNEMvvviiZs6cKUlpD98AqTT7/5EQaC42bNigkpISffPNN/riiy8kSeeee67OPvvskCfDoYqAAM1EdXW11q1bp0gkomOOOUYXXnhhypPOQGNwEh0A4MI5EACACwEBALgcdOdADoV3xOzUqVPYI6T117/+NewR0iotLQ17hLRib8J3MDsUtsdly5aFPUJaDb2VycHiQP0Hbl7sgQAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXCJmZmEPES8SiYQ9Qlq//OUvwx4hrYKCgrBHSKusrCzsEdJau3Zt2COkNWrUqLBHwLfkIHu4Zg8EAOBDQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALgQEAOBCQAAALhEzs7CHiBeJRMIeAUATGzVqVNgjpPXaa6+FPUJalZWVYY+QgD0QAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuBAQAIALAQEAuETDHqCuTp06hT1CWpWVlWGPkNall14a9ghpTZ8+PewR0lq2bFnYI6Q1ZcqUsEdI61D4WWPfsQcCAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAFwICAHAhIAAAl2jYA9RVVlYW9ghpRSKRsEdIa9myZWGPkNb5558f9ghpHQozrl27NuwR0tqyZUvYI6RVUFAQ9giHHPZAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4EJAAAAuBAQA4BIxMwt7iHiRSCTsEYDAeeedF/YIaT3zzDNhj5DW+PHjwx4hrbKysrBHSKu0tDTsERKwBwIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAAAXAgIAcCEgAACXiJlZ2EPEi0QiYY/wndCpU6ewR0hr7dq1YY+Ab8mhsD1Onz497BHSKi0tDXuEBOyBAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwIWAAABcCAgAwCUa9gB1mVnYI3wnRCKRsEf4TigoKAh7hLSmT58e9ghplZWVhT1CWlOmTAl7hLRKS0vDHiEBeyAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwISAAABcCAgBwiZiZhT0EAODQwx4IAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXAgIAMCFgAAAXP4LvFyrDSKR2MkAAAAASUVORK5CYII=",
            "_dom_classes": [],
            "_figure_label": "Figure 1",
            "_image_mode": "full",
            "_message": "",
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "MPLCanvasModel",
            "_rubberband_height": 0,
            "_rubberband_width": 0,
            "_rubberband_x": 0,
            "_rubberband_y": 0,
            "_size": [
              400,
              400
            ],
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "MPLCanvasView",
            "capture_scroll": false,
            "footer_visible": false,
            "header_visible": false,
            "layout": "IPY_MODEL_67e6069dbf364ec390b95d0a202af5be",
            "pan_zoom_throttle": 33,
            "resizable": false,
            "toolbar": "IPY_MODEL_c20cab87ca8c49d9bede6e7a9a421f3c",
            "toolbar_position": "left",
            "toolbar_visible": false
          }
        },
        "25a517bbb2f84f3aa4fdfde67bcc708f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2072b2877094b1eb81a9933c8a37072",
              "IPY_MODEL_7b84fbe04d7b40d9ad870513f249b901"
            ],
            "layout": "IPY_MODEL_c51866ea9f8140fcaf846b24ec7c81cd"
          }
        },
        "a5f2bd72343a4c62831f98d048d3a81f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca8b60b5a9004a65836a3d68fc8f130d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccf4b6fd6058476b8fd3e2b8f252bf2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2072b2877094b1eb81a9933c8a37072": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Strike",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5daeda447b494cd0a33398393c6ec96c",
            "style": "IPY_MODEL_c806c47bbd774b9e9e9031180b6e64b5",
            "tooltip": ""
          }
        },
        "7b84fbe04d7b40d9ad870513f249b901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "No Strike",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_efca66dca651439da66b09f6828aecd4",
            "style": "IPY_MODEL_0c894be28c2d4e79b1ff1dcad5005cf2",
            "tooltip": ""
          }
        },
        "c51866ea9f8140fcaf846b24ec7c81cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5daeda447b494cd0a33398393c6ec96c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c806c47bbd774b9e9e9031180b6e64b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "efca66dca651439da66b09f6828aecd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c894be28c2d4e79b1ff1dcad5005cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d10dfab7102847fd9b09e01546256ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b9e7923b262462bbb934218e7dbd332": {
          "model_module": "jupyter-matplotlib",
          "model_name": "ToolbarModel",
          "model_module_version": "^0.11",
          "state": {
            "_current_action": "",
            "_dom_classes": [],
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "ToolbarModel",
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "ToolbarView",
            "button_style": "",
            "collapsed": true,
            "layout": "IPY_MODEL_70836d662f3c4eae9e47d6810ecd6efc",
            "orientation": "vertical",
            "toolitems": [
              [
                "Home",
                "Reset original view",
                "home",
                "home"
              ],
              [
                "Back",
                "Back to previous view",
                "arrow-left",
                "back"
              ],
              [
                "Forward",
                "Forward to next view",
                "arrow-right",
                "forward"
              ],
              [
                "Pan",
                "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect",
                "arrows",
                "pan"
              ],
              [
                "Zoom",
                "Zoom to rectangle\nx/y fixes axis",
                "square-o",
                "zoom"
              ],
              [
                "Download",
                "Download plot",
                "floppy-o",
                "save_figure"
              ]
            ]
          }
        },
        "67e6069dbf364ec390b95d0a202af5be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c20cab87ca8c49d9bede6e7a9a421f3c": {
          "model_module": "jupyter-matplotlib",
          "model_name": "ToolbarModel",
          "model_module_version": "^0.11",
          "state": {
            "_current_action": "",
            "_dom_classes": [],
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "ToolbarModel",
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "ToolbarView",
            "button_style": "",
            "collapsed": true,
            "layout": "IPY_MODEL_d4fcefb5aabc4723aae148dc91439fe2",
            "orientation": "vertical",
            "toolitems": [
              [
                "Home",
                "Reset original view",
                "home",
                "home"
              ],
              [
                "Back",
                "Back to previous view",
                "arrow-left",
                "back"
              ],
              [
                "Forward",
                "Forward to next view",
                "arrow-right",
                "forward"
              ],
              [
                "Pan",
                "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect",
                "arrows",
                "pan"
              ],
              [
                "Zoom",
                "Zoom to rectangle\nx/y fixes axis",
                "square-o",
                "zoom"
              ],
              [
                "Download",
                "Download plot",
                "floppy-o",
                "save_figure"
              ]
            ]
          }
        },
        "70836d662f3c4eae9e47d6810ecd6efc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4fcefb5aabc4723aae148dc91439fe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e928a1a62634a87a5163adc6f9ebd28": {
          "model_module": "jupyter-matplotlib",
          "model_name": "MPLCanvasModel",
          "model_module_version": "^0.11",
          "state": {
            "_cursor": "default",
            "_data_url": "data:image/png;base64,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",
            "_dom_classes": [],
            "_figure_label": "Figure 3",
            "_image_mode": "full",
            "_message": "x=7.19 y=6.70\n[-6.00]",
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "MPLCanvasModel",
            "_rubberband_height": 0,
            "_rubberband_width": 0,
            "_rubberband_x": 0,
            "_rubberband_y": 0,
            "_size": [
              400,
              400
            ],
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "MPLCanvasView",
            "capture_scroll": false,
            "footer_visible": false,
            "header_visible": false,
            "layout": "IPY_MODEL_ed17c0a4ea284704b5cc281de7632090",
            "pan_zoom_throttle": 33,
            "resizable": false,
            "toolbar": "IPY_MODEL_1606981914144f078d5d2b862c21dd24",
            "toolbar_position": "left",
            "toolbar_visible": false
          }
        },
        "ed17c0a4ea284704b5cc281de7632090": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1606981914144f078d5d2b862c21dd24": {
          "model_module": "jupyter-matplotlib",
          "model_name": "ToolbarModel",
          "model_module_version": "^0.11",
          "state": {
            "_current_action": "",
            "_dom_classes": [],
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "ToolbarModel",
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "ToolbarView",
            "button_style": "",
            "collapsed": true,
            "layout": "IPY_MODEL_6ad63969345e48a2bd12660a95082763",
            "orientation": "vertical",
            "toolitems": [
              [
                "Home",
                "Reset original view",
                "home",
                "home"
              ],
              [
                "Back",
                "Back to previous view",
                "arrow-left",
                "back"
              ],
              [
                "Forward",
                "Forward to next view",
                "arrow-right",
                "forward"
              ],
              [
                "Pan",
                "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect",
                "arrows",
                "pan"
              ],
              [
                "Zoom",
                "Zoom to rectangle\nx/y fixes axis",
                "square-o",
                "zoom"
              ],
              [
                "Download",
                "Download plot",
                "floppy-o",
                "save_figure"
              ]
            ]
          }
        },
        "6ad63969345e48a2bd12660a95082763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}