{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/split-and-simple-perturb/sequences/P2C1_Optimization/P2C1_Sequence5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "K10DbaXu1WBL"
      },
      "source": [
        "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "dsQ6T5OZ1WBM"
      },
      "source": [
        "___\n",
        "# **2.1.5: Learning Behaviour Quickly**\n",
        "\n",
        "### Objective: Investigate a new, simple optimization procedure with clear physiological interpretation.\n",
        "\n",
        "In this sequence we will:\n",
        "\n",
        "* Continue to work with the strike-no-strike problem where the decision depends on 64 sensory inputs (features) to investigate more rapid plasticty rules.\n",
        "\n",
        "* Compare these new plasticity rules to perturb-measure-step both in terms of learning rate and physilogical plausibility.\n",
        "\n",
        "* Understand the efficacy of these new learning rules using an optimization perspective.\n",
        "\n",
        "* Test these new learning rules on a harder learning problem, still with 64 sensory features, but now with 10 possible behavioural responses. Note how scale affects the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "rK0I5ANa1WBM"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Run the following cell to setup and install the various dependencies and helper functions for this ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "g9MNYxfk1WBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf86cb8-2655-46d1-86ae-25cbe418d99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best feature is 21\n",
            "Random seed 2021 has been set.\n",
            "This notebook isn't using and doesn't need a GPU. Good.\n",
            "Running in colab\n"
          ]
        }
      ],
      "source": [
        "# @title Dependencies, Imports and Setup\n",
        "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
        "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
        "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz ucimlrepo > /dev/null 2> /dev/null #google.colab\n",
        "\n",
        "import asyncio\n",
        "import requests\n",
        "from requests.exceptions import RequestException\n",
        "import numpy as np\n",
        "import itertools\n",
        "import collections\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.colors import LogNorm\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib import gridspec\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pygraphviz as pgv\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import warnings\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from io import BytesIO\n",
        "from enum import Enum\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
        "from jupyterquiz import display_quiz\n",
        "from vibecheck import DatatopsContentReviewContainer\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from tqdm.notebook import tqdm\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "data_set = fetch_ucirepo(id=80)\n",
        "X = data_set.data.features.values\n",
        "# Translate the data to have a minimum of 0\n",
        "X_translated = X - X.min()\n",
        "# Scale the data to have a range from 0 to 12 (which is 6 - (-6))\n",
        "scaling_factor = 12 / (X.max() - X.min())\n",
        "X_scaled = X_translated * scaling_factor\n",
        "# Finally, shift the data to be centered between -6 and 6\n",
        "X_final = X_scaled - 6\n",
        "\n",
        "y = data_set.data.targets.values\n",
        "rng = np.random.default_rng(seed=2021)\n",
        "scramble_permutation = rng.permutation(X.shape[1])\n",
        "Xs = X_final[:, scramble_permutation]\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "y1 = y % 2\n",
        "y2 = np.array(y >= 5, dtype=y.dtype)\n",
        "simple_index = ((y.flatten()==1) | (y.flatten()==0))\n",
        "X_simple = Xs[simple_index]\n",
        "y1_simple = y1[simple_index]\n",
        "# if you only had one feature which would likely be best for discrimination\n",
        "epsilon = 10\n",
        "class_a_sep = np.mean(X_simple[y1_simple.flatten() == 1, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 1, :], axis=0) + epsilon)\n",
        "class_b_sep = np.mean(X_simple[y1_simple.flatten() == 0, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 0, :], axis=0) + epsilon)\n",
        "best_feature = np.argmax(class_a_sep - class_b_sep)\n",
        "print(f'Best feature is {best_feature}')\n",
        "X_simple_1_feature = X_simple[:, [best_feature]]\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
        "# random seed settings and\n",
        "# getting torch to use gpu if it's there\n",
        "\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook but not needed.\")\n",
        "    print(\"If possible, in the menu under `Runtime` -> \")\n",
        "    print(\"`Change runtime type.`  select `CPU`\")\n",
        "\n",
        "  return device\n",
        "\n",
        "\n",
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "  display(Markdown(string))\n",
        "\n",
        "\n",
        "# the different utility .py files used in this notebook\n",
        "filenames = []\n",
        "# just run the code straight out of the response, no local copies needed!\n",
        "for filename in filenames:\n",
        "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
        "  response = requests.get(url)\n",
        "  # Check that we got a valid response\n",
        "  if response.status_code == 200:\n",
        "    code = response.content.decode()\n",
        "    exec(code)\n",
        "  else:\n",
        "    print(f'Failed to download {url}')\n",
        "\n",
        "# environment contingent imports\n",
        "try:\n",
        "  print('Running in colab')\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "  from google.colab import data_table\n",
        "  data_table.disable_dataframe_formatter()\n",
        "  #from google.colab import output as colab_output\n",
        "  #colab_output.enable_custom_widget_manager()\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  print('Not running in colab')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "%matplotlib widget\n",
        "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
        "plt.ioff() #need to use plt.show() or display explicitly\n",
        "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "def remove_ip_clutter(fig):\n",
        "  fig.canvas.header_visible = False\n",
        "  fig.canvas.toolbar_visible = False\n",
        "  fig.canvas.resizable = False\n",
        "  fig.canvas.footer_visible = False\n",
        "  fig.canvas.draw()\n",
        "\n",
        "\n",
        "def content_review(notebook_section: str):\n",
        "  return DatatopsContentReviewContainer(\n",
        "    \"\",  # No text prompt\n",
        "    notebook_section,\n",
        "    {\n",
        "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
        "      \"name\": \"neuro_book\",\n",
        "      \"user_key\": \"xuk960xj\",\n",
        "    },\n",
        "  ).render()\n",
        "feedback_prefix = \"P2C1_S5\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.1 Learning Strike-No-Strike Quickly\n",
        "\n",
        "Recall from the previous sequence that the cartoon organism that inspires this problem can be thought of as having 64 photo-sensitive receptors, and based on the combination of inputs from these receptors it must decide whether to strike or not. The organism pays a cost of one if it strikes when it shouldn't (prey is absent) and recieves a reward of one if it strikes when it should (prey is present). It receives no cost or reward when it does not strike. To get a sense of this discrimination problem try it yourself by running the code cell below."
      ],
      "metadata": {
        "id": "OR3gHzvHXExf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown **Run this cell** to try out the more complex 'strike-no-strike' discrimination task.\n",
        "\n",
        "class InteractiveMNISTPredator():\n",
        "  def __init__(self,\n",
        "               features=Xs,\n",
        "               labels=y1,\n",
        "               extra_labels=y,\n",
        "               feedback_type='on_strike_only', seed=123):\n",
        "    # Initialize dataset, settings for image scrambling and feedback\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "    # features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)\n",
        "    # labels is num_data_points x 1 (values 0-9 or 0/1 depending)\n",
        "    self.feedback_type = feedback_type\n",
        "    self.rng = np.random.default_rng(seed)\n",
        "    #sample_order = np.arange(self.features.shape[0])\n",
        "    sample_order = self.rng.permutation(self.features.shape[0])\n",
        "    self.features = self.features[sample_order]\n",
        "    self.labels = self.labels[sample_order]\n",
        "    self.extra_labels = extra_labels[sample_order]\n",
        "    # initialize game state\n",
        "    self.current_index = 0\n",
        "    self.current_image = None\n",
        "    self.previous_image = None\n",
        "    self.score = 0\n",
        "    self.best_possible_score = 0\n",
        "    self.successful_strikes = 0\n",
        "    self.failed_strikes = 0\n",
        "    self.non_strikes = 0\n",
        "    # Initialize widgets\n",
        "    self.strike_button = widgets.Button(description='Strike')\n",
        "    self.no_strike_button = widgets.Button(description='No Strike')\n",
        "    self.score_display = widgets.Output()\n",
        "    self.feedback_display = widgets.Output()\n",
        "\n",
        "    # Initialize the figure for image display\n",
        "    self.fig, self.ax = plt.subplots(figsize=(4, 4))\n",
        "    remove_ip_clutter(self.fig)\n",
        "    self.prev_fig, self.prev_ax = plt.subplots(figsize=(4, 4))\n",
        "    remove_ip_clutter(self.prev_fig)\n",
        "    self.show_next_image()\n",
        "    # Bind event handlers\n",
        "    self.strike_button.on_click(self.on_strike_clicked)\n",
        "    self.no_strike_button.on_click(self.on_no_strike_clicked)\n",
        "\n",
        "    # Arrange widgets in a layout\n",
        "    buttons_layout = widgets.HBox([self.strike_button, self.no_strike_button])\n",
        "    current_buttons = widgets.VBox([self.fig.canvas, buttons_layout])\n",
        "    previous_feedback = widgets.VBox([self.prev_fig.canvas, self.feedback_display])\n",
        "    self.ui = widgets.HBox([previous_feedback, current_buttons, self.score_display])\n",
        "\n",
        "  def show_next_image(self):\n",
        "    # Display the next image\n",
        "    image = self.features[self.current_index]\n",
        "\n",
        "    if len(image) == 64:\n",
        "        image = image.reshape(8, 8)\n",
        "    elif len(image) == 1:\n",
        "      scalar_value = image.flatten()[0]\n",
        "      # Initialize the 8x8 array with -6 (black)\n",
        "      image = np.full((8, 8), -6.0)\n",
        "      # Set the first ring to 6 (white)\n",
        "      image[0, 0] = 6\n",
        "      # Set the second ring to 6 (white)\n",
        "      image[1:-1, 1:-1] = 6\n",
        "      # Set the third (inner ring) back to -6 (black)\n",
        "      image[2:-2, 2:-2] = -6\n",
        "      # Assuming scalar_value is already in the range -6 to 6\n",
        "      #print(scalar_value)\n",
        "      image[3:-3, 3:-3] = scalar_value\n",
        "    else:\n",
        "      raise ValueError(f'Unexpected image shape: {image.shape}')\n",
        "    if self.current_image is not None:\n",
        "      self.previous_image = self.current_image\n",
        "    image = np.flipud(image)\n",
        "    self.current_image = image\n",
        "    # Display the image\n",
        "    #print(image)\n",
        "    self.fig.clf()\n",
        "    self.prev_fig.clf()\n",
        "    self.ax = self.fig.add_subplot(111)\n",
        "    self.prev_ax = self.prev_fig.add_subplot(111)\n",
        "    self.ax.set_xlim(-.5, 7.5)\n",
        "    self.ax.set_ylim(-0.5, 7.5)\n",
        "    self.prev_ax.set_xlim(-.5, 7.5)\n",
        "    self.prev_ax.set_ylim(-0.5, 7.5)\n",
        "    self.ax.set_aspect('equal')\n",
        "    self.prev_ax.set_aspect('equal')\n",
        "    self.ax.axis('off')\n",
        "    self.prev_ax.axis('off')\n",
        "    self.ax.imshow(self.current_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
        "    if self.previous_image is not None:\n",
        "      self.prev_ax.imshow(self.previous_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
        "    self.ax.set_title('Current Sensory Input')\n",
        "    self.prev_ax.set_title('Previous Sensory Input')\n",
        "    self.fig.canvas.draw()\n",
        "    self.prev_fig.canvas.draw()\n",
        "\n",
        "  def on_strike_clicked(self, button):\n",
        "    self.process_decision('Strike')\n",
        "\n",
        "  def on_no_strike_clicked(self, button):\n",
        "    self.process_decision('No Strike')\n",
        "\n",
        "  def process_decision(self, decision):\n",
        "    # freeze buttons while we process\n",
        "    self.strike_button.disabled = True\n",
        "    self.no_strike_button.disabled = True\n",
        "\n",
        "    # Process the user's decision, update score, and provide feedback\n",
        "    correct_action = 'Strike' if self.labels[self.current_index] == 1 else 'No Strike'\n",
        "    if decision == 'Strike':\n",
        "      if decision == correct_action:\n",
        "        self.score += 1\n",
        "        self.successful_strikes += 1\n",
        "      else:\n",
        "        self.score -= 1\n",
        "        self.failed_strikes += 1\n",
        "    elif decision == 'No Strike':\n",
        "      self.non_strikes += 1\n",
        "      # no strike means no gain or loss\n",
        "    else:\n",
        "      raise ValueError(f'Unknown decision: {decision}')\n",
        "\n",
        "    # Show feedback and score\n",
        "    if (self.feedback_type == 'both' or\n",
        "      (self.feedback_type == 'on_strike_only' and decision == 'Strike')):\n",
        "      # Show informative feedback\n",
        "      feedback = f'Your last choice: {decision}\\nCorrect last choice: {correct_action}'\n",
        "    else:\n",
        "      # Show uninformative feedback\n",
        "      feedback = 'Feedback only available after striking.'\n",
        "    with self.feedback_display:\n",
        "      clear_output(wait=True)\n",
        "      #print(self.labels[self.current_index])\n",
        "      #print(self.extra_labels[self.current_index])\n",
        "      print(feedback)\n",
        "\n",
        "    # Show score\n",
        "    with self.score_display:\n",
        "      clear_output(wait=True)\n",
        "      average_score = self.score / (self.current_index+1)\n",
        "      print(f'Total Score: {self.score}')\n",
        "      print(f'Number of Trials: {self.current_index + 1}')\n",
        "      print(f'Successful Strikes: {self.successful_strikes}')\n",
        "      print(f'Failed Strikes: {self.failed_strikes}')\n",
        "      print(f'Non-Strikes: {self.non_strikes}')\n",
        "      print(f'Average Score Per Trial: {average_score:.2f}')\n",
        "\n",
        "    # Prepare the next image\n",
        "    self.current_index += 1\n",
        "    #print(self.current_index)\n",
        "    self.show_next_image()\n",
        "    # Re-enable buttons\n",
        "    self.strike_button.disabled = False\n",
        "    self.no_strike_button.disabled = False\n",
        "\n",
        "\n",
        "scramble_bin_hard = InteractiveMNISTPredator(features=Xs,\n",
        "                                             labels=y1,\n",
        "                                             feedback_type='both')\n",
        "display(scramble_bin_hard.fig.canvas)\n",
        "display(scramble_bin_hard.prev_fig.canvas)\n",
        "clear_output()\n",
        "display(scramble_bin_hard.ui)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "y_YjCpnZCNyn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "34a32a0c41f949a3aec009f0102dff77",
            "4a5ccb4a5b0f44469ef1b2f8647ce042",
            "c089bf3c2c2644b79bb7398d8f9f0654",
            "837beff7259248f296be658bfc819c19",
            "1812b5f4bf0a46e98666fd4ffae62718",
            "f490ae96c11e4c53adc0495291134e30",
            "50a932b6ca194e878aa2439acc61e8bd",
            "53225a74db974128962d5a9f0a9c027e",
            "5009d9c3bd12452ab7d9331a15bbe0b8",
            "d6c2ee1a77ec47ecad7ef230a81a8afe",
            "52ce7f7ead1548c596a5e95c13682f1c",
            "f812f2cf6abb4acf993e9d11dae9df1a",
            "4c56a51b69ee4a12b25d0850e4c17f10",
            "e220128611f94fae80285b7bcc61bc9f",
            "3f346cd8d42f4d10a51a7c3304e9883a",
            "9999c189fa824bcea60a11a929fe6294",
            "cef706152f2d4e27910731dbeedd98ba",
            "ef1f14514b9644a4906c56c225e8a524",
            "d1f0d445b89e4a898008b881a107c676",
            "6666e485a1a34bdd8057edf14c2fd120",
            "cc682e57c98a46e28d8611773ee55e3c",
            "607e23142ce845a1a355ecf39663d1d1",
            "cd492da82f97434393682df764cc8458",
            "418b7e4a07e94b10859402be50595bd3",
            "3bb7b68c6628478c9bd7b36b20f4477d",
            "002359b4762b462f9a017c7415c8d1ba"
          ]
        },
        "outputId": "bfe7178f-5c34-45a4-d8f2-324b597faef3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(VBox(children=(Canvas(footer_visible=False, header_visible=False, resizable=False, toolbar=Tool…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34a32a0c41f949a3aec009f0102dff77"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We model this creature's sensory-behaviour system as follows, $\\mathbf{x}$ is the raw sensory input (column vector) of length 64 in a given episode. Each element $x_i$ of $\\mathbf{x}$ corresponds to the activation level of a single photosensitive neuron. We visualize an example sensory input as an $8 \\times 8$ grid in the code cell below."
      ],
      "metadata": {
        "id": "1CgwmAlsJG_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {},
        "id": "qJgsx8nSI5K_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "ff5b32f47ed2437996e88f6a02cbf49b",
            "4cdcf1aaf236412eb6c82218b8662b79",
            "e3e6749470b74012b2494374781757f2",
            "b01c1968fdcf449292325d5eb61b128d"
          ]
        },
        "outputId": "9d4ef541-fd0f-4cd5-d4c4-f03df5a0253f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Canvas(footer_visible=False, header_visible=False, resizable=False, toolbar=Toolbar(toolitems=[('Home', 'Reset…"
            ],
            "image/png": "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",
            "text/html": [
              "\n",
              "            <div style=\"display: inline-block;\">\n",
              "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
              "                    Figure\n",
              "                </div>\n",
              "                <img src='data:image/png;base64,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' width=400.0/>\n",
              "            </div>\n",
              "        "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff5b32f47ed2437996e88f6a02cbf49b"
            }
          },
          "metadata": {
            "application/vnd.jupyter.widget-view+json": {
              "colab": {
                "custom_widget_manager": {
                  "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
                }
              }
            }
          }
        }
      ],
      "source": [
        "# visualizing the example we see that lower values correspond to darker pixels\n",
        "# and higher values correspond to lighter values\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "remove_ip_clutter(fig)\n",
        "ax.imshow(Xs[0].reshape(8,8), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These input neurons are then connected by synapses to a single output neuron. The activation level, $z$, of this output neuron is computed as\n",
        "$$z = \\mathbf{Wx} + b$$\n",
        "Here, $b$ is the (scalar) bias, or baseline activation level of the output neuron, and $\\mathbf{W}$ is a matrix of synaptic weights between the input neurons and the single output neuron. (In this case where there is only one output neuron so $\\mathbf{W}$ has shape 1x64 so could also be thought of as a row vector.)  \n",
        "\n",
        "To simplify exposition and coding the input $\\mathbf{x}$ is augmented to have a feature which is always 1, and then the bias terms can be treated as the weight connecting to this constant valued feature. That is\n",
        "\n",
        "$$z = \\mathbf{Wx}$$\n",
        "\n",
        "And now $\\mathbf{W}$ has shape 1x65. The probability of striking is determined by the activation level of the output neuron, together with a temperature parameter $\\tau$ which determines how exploratory the behaviour of the organism is, specifically:\n",
        "$$ \\Pr \\{\\text{strike}\\} = \\sigma_{\\tau}(z) $$\n",
        "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma_{\\tau}(z)$$\n",
        "\n",
        "Here $$\\sigma(a): \\frac{1}{1+e^{\\frac{-z}{\\tau}}} = \\frac{e^{\\frac{z}{\\tau}}}{1+e^\\frac{z}{\\tau}}$$ is the logistic (sigmoid) function, with temperature parameter $\\tau$. High values of $\\tau$ make striking and not striking have nearly equal probability regardless of the value of $z$, whereas very low values of $\\tau$ mean that even a very slightly positive $z$ value will correspond to near certainty of striking, and a very slight negative value will result in an almost certain chance of not striking. In other words $\\tau$ determines how responsive the striking probabilities are to changes in $z$.\n",
        "\n",
        "Instead of thinking of $z$ as simply just a way of determining the striking probability, we could also treat $z$ as a kind of prediction or expectation of the reward that will occur if the striking action is taken given the current sensory experience $\\mathbf{x}$. If there is a higher expectation of reward, the probability of taking the striking action increases, if there is a lower expectation of reward the probability of taking the striking action decreases, according the the temperature scaled softmax function above.\n",
        "\n",
        "Taking this perspective, where $z$ as the organism's internal representation of the expected reward when taking the striking action, then the organism could compare its expectation of reward with the reward actually receives, and use the difference between expectation and reality to update the synaptic weights that had a causal impact on this expectation in such a way that the expected reward more closely aligns with the received reward. Given our simple network a rule that does this is:"
      ],
      "metadata": {
        "id": "pOUN7F2-eL2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward-Prediction-Error Episodic Update Rule\n",
        "\n",
        "$$\\Delta W_i = s \\cdot (r-z) \\cdot x_i $$\n",
        "\n",
        "Or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s \\cdot (r-z) \\cdot \\mathbf{x}$$\n"
      ],
      "metadata": {
        "id": "cyjeHHWJJ_vX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is saying that if the received reward is greater than the predicted reward, then it would be better if $a$ had been larger, which means that in the cases where $x_i$ was positive, the weight from $x_i$ should be increased, in proportion to the strength of activation of $x_i$ and in the case that $x_i$ was negative, the weight from $x_i$ should be decreased, again in proportion to the strength of the activation of $x_i$. Conversely if, the reward received is less than predicted then it would have been better if $a$ was smaller and so, in the case where $x_i$ is positive the weight from $x_i$ should be decreased, in proportion to the strength of the activation of $x_i$ and in the case that $x_i$ is negative, the weight from $x_i$ should be decreased. (Note that in the vector formulation above that should technically be $\\mathbf{x}^T$ since $\\mathbf{x}$ is column vector (64, 1) whereas $\\mathbf{W}$ is a row vector (1, 64), and the shape and orientation of $\\Delta \\mathbf{W}$ should match the shape of $\\mathbf{W}$.)\n",
        "\n",
        "This rule can be understood as shifting the weights $\\mathbf{W}$ in the direction that minimizes the squared error of the reward prediction.\n",
        "\n",
        "$$ z = \\mathbf{W}\\mathbf{x}$$\n",
        "$$\\begin{align} \\nabla_\\mathbf{W} (r-z)^2 & = 2 \\ (r-z) \\ \\nabla_{\\mathbf{W}}z \\\\\n",
        "& = 2 (r-z) \\mathbf{x}\n",
        "\\end{align}$$\n",
        "The factor of 2 is subsumed in the step scaling factor $s$.\n",
        "\n",
        "We call this an episodic update rule because it can concievably be implemented after a single experience of sensory input, action selection, and resultant reward.\n",
        "\n",
        "Under one version of this updating scheme, let's call it the \"realishtic\" version, no update is made to the network if the organism refrains from striking. In this scenario this prediction pertains to the reward when striking, so this difference between expected and received reward simply does not apply to the case where no striking occurs. Note that the $\\tau$ parameter in the sigmoid, striking probability function determines how exploratory the behaviour is independently of the prediction. $\\tau$ is very important. During the earlier episodes of the organism's life it should be more exploratory (higher $\\tau$), but once it has learned good predictions of reward across a representative range of experiences it can settle into simply exploiting the knowledge it has (lower $\\tau$). Deciding when and how to transition from exploratory behaviours to more directly reward maximizing behaviours is known as an exploration-exploitation trade-off, and is a central challenge in Reinforcement Learning. We just mention this in passing now, but will dive more deeply into the exploration-exploitation tradeoff in sequence (blah).\n",
        "\n",
        "Under another version of this scheme, let's call it the \"cheating\" version, an update is always made based on the $r_{\\text{strike}}$, the reward the organism would have recieved if it strikes, regardless of whether it actually strikes or not, i.e. it gets to know what the right answer was regardless of what action it takes. This could be implemented by the organism having some additional, post-hoc prey detection that augments the organism's basic behavioural inputs with additional teaching signals. These additional signals allow the organism to learn from situations where it's like \"Oh I wish I had struck at the food that is now swimming past me\" and also situations where it's like \"Oh there really was no food there, good thing I didn't strike\". Such mechanism likely exist, but are more complex than the simple case of reinforcing actions taken based on intrinsic reward recieved. Our focus for now is this simple case. (We could approximate such a system by \"rewarding\" not striking when no prey is present with 1 and giving a penalty of 1 when an organism fails to strike when prey is present, but this leaves open the question of if and how such \"learning scaffolding\" rewards need to be and can be kept seperate from 'actual' rewards that count towards fitness in the evolutionary sense. This is a complex topic that we will revist throught the book, in particular in sequences (ref)).\n",
        "\n",
        "From an evolutionary and adaptive behaviour perspective we are interested in how the organism can quickly learn to take rewarding actions. This suggest an alternative updating scheme where the reward recieved directly reinforces the probability of taking the action that produced it. To do this we need know how a change in $z$ translates into an increase (or decrease) in the probability of striking, so that we can increase (decrease) action probabilites in proportion to the rewards recieved as a result of particular action choices. We call this the Action-Probability-Reinforcement update rule"
      ],
      "metadata": {
        "id": "B79wUBdBeQNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action Probability Reinforcement Episodic Update Rule\n",
        "\n",
        "$$ \\Delta W_i = s \\cdot r \\cdot \\sigma'(z) \\cdot x_i$$\n",
        "\n",
        "or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s \\cdot r \\cdot \\sigma'(z) \\cdot \\mathbf{x}$$\n",
        "\n",
        "Here $\\sigma'$ is the derivative of the sigmoid function. We no longer need to explicitly control exploration versus exploitation with $\\tau$, so we simply set $\\tau =1$ here for simplicity. The above rule uses the raw rewards to reinforce actions. The probability of striking is given by $\\sigma(a)$ so the way to change $\\mathbf{W}$ to increase the probability of striking is to shift the weights in the direction of the gradient of the probability of taking the action given by $\\sigma'(a)$. Using the defination of $\\sigma$ above and the chain rule from calculus we can derive that $\\sigma'(a) = \\sigma(a) (1-\\sigma(a))$. This is telling us what we already know intuitively about the sigmoid function, which is that it is relatively flat when probalities are close to zero or one, and steepest when probabilities are close to even, that is 0.5.\n",
        "\n",
        "The above rule can be understood as shifting weights $\\mathbf{W}$ so that the probability of striking in a particular instance ($\\mathbf{x}$) is increased (or decreased in the case of negative reward) in proportion to the rewards that result from taking the striking action.\n",
        "$$ \\Pr\\{\\text{strike}|\\mathbf{x}\\} = \\sigma(\\mathbf{W}\\mathbf{x})$$\n",
        "$$\\begin{align} \\Delta \\mathbf{W} &= s \\cdot \\nabla_\\mathbf{W} \\Pr\\{\\text{strike}| \\mathbf{x}\\} \\cdot r \\\\\n",
        "& = s \\cdot \\sigma'(z) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx} \\cdot r \\\\\n",
        "& = s \\cdot \\sigma'(z) \\cdot \\mathbf{x} \\cdot r \\\\\n",
        "& = s \\cdot \\sigma(z) (1 - \\sigma(z)) \\cdot \\mathbf{x} \\cdot r\n",
        "\\end{align}$$\n",
        "\n",
        "Again we call this an episodic update rule because it can concievably be applied after a single episode or experience consisting ot a sensory experience, a selection of action, and a resultant reward.\n",
        "\n",
        "Again, this rule also has a realishtic and a cheating variant. The realishtic variant uses the actual reward $r$ obtained from sampling an action, and experiencing the subsequent reward. However, in the cheating variant (often used in ML contexts) learning is greatly accelerated by using the expected reward $\\mathbb{E}[r]$ given the probability distribution over possible actions instead of utilizing a particular sample from the distribution of possible actions and the particular resultant reward. This cheating variant is only episodic if it is able to somehow consider all possible actions and all possible outcomes within that single episode, certainly possible, but adds the complexity of trying to learn from hypothetical outcomes, much as in the cheating version of Reward-Prediction-Error update rule.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wxyLSWkRJRMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the coding exercise below to see how these two different udpated rules compare with our perturb-measure-step rule, both in our realishtic and cheating variants. Note the that the `eval_params` function we use below can be thought of as containing both an action selection component that depends on the sensory inputs $\\mathbf{x}$ and the parameters $\\mathbf{W}$ and then a reward component that calculates the appropriate reward given some (as yet unobserved by organisms) aspect of the environment encoded in $y$ and the action selected."
      ],
      "metadata": {
        "id": "IxyE8pOgMPOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO for students: Replace ... in the lines below with one of the following\n",
        "# options, each option get's used exactly once\n",
        "# a) r * (strike_prob) * (1 - strike_prob) * x\n",
        "# b) (r_all - z) * x\n",
        "# c) (np.mean(r_perturb - r)) / perturbation_scale\n",
        "# d) r_exp * (strike_prob) * (1 -  strike_prob) * x\n",
        "# e) (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
        "# f) (r-z) * strike * x\n",
        "# This will implement different update rules driven by: Reward Prediction Error,\n",
        "# Reinforcement of Action ProbabilitiesTrue Positives, or Peturb-Measure\n",
        "# estimate of the gradient of expected reward. Both in a strict, experience\n",
        "# driven way, and in \"cheating\" a bit way.\n",
        "raise NotImplementedError(\"Exercise: Implement different update rules to compare\")\n",
        "################################################################################\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z = np.dot(W, x)\n",
        "  strike_prob = np_sigmoid(z, tau)\n",
        "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
        "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  r_exp = strike_prob * r_all # + (1-strike_prob) * 0 # expected reward\n",
        "  return z, strike_prob, strike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           cheat=False,\n",
        "                           tau=1.0,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.0001):\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn regardless of whether organism actually strikes or not\n",
        "    update = ...\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    update = ...\n",
        "  # average the update over all elements in the batch\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y,\n",
        "                     cheat=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn using expected reward\n",
        "    update = ...\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    # reward of zero recieved when not striking\n",
        "    update = ...\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.01,\n",
        "                         cheat=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # evaluate perturbation using expected reward, avg over the mini-batch\n",
        "    directional_grad_est = ...\n",
        "  else: #realishtic\n",
        "    # evaluate perturbation using sampled rewards, avg over the mini-batch\n",
        "    directional_grad_est = ...\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "################################################################################\n",
        "# Exercise Complete, simulations and plotting logic follow\n",
        "################################################################################\n",
        "# simulation\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "cooling_rate = 0.04\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "cheat_alg_lrs = {'Reward Prediction': 0.0001,\n",
        "                 'Action Probability': 0.008,\n",
        "                 'Perturb Measure': 0.0016}\n",
        "cheat_actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "cheat_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = cheat_alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      if alg_name == 'Reward Prediction':\n",
        "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
        "      else:\n",
        "        tau = 1.0\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      cheat_actual_reward_results[alg_name].append(np.mean(r))\n",
        "      cheat_exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
        "    num_steps += 1\n",
        "    if num_steps > 1000:\n",
        "      break\n",
        "# plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "for alg_name in alg_names:\n",
        "  eval = np.array(cheat_exp_reward_results[alg_name])\n",
        "  eval = np.cumsum(eval)\n",
        "  eval = eval / (np.arange(len(eval)) + 1)\n",
        "  ax.plot(eval, label=f'{alg_name}-(Cheatiing A Bit)')\n",
        "  ax.set_title(f'Cumulative per Episode Average of Expected (Full Batch) Reward')\n",
        "  ax.set_xlabel('Learning Episodes')\n",
        "  ax.set_ylabel('Cumulative Avg. Expected (Full Batch) Reward')\n",
        "  ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "for alg_name in alg_names:\n",
        "  eval = np.array(cheat_actual_reward_results[alg_name])\n",
        "  eval = np.cumsum(eval)\n",
        "  eval = eval / (np.arange(len(eval)) + 1)\n",
        "  ax.plot(eval, label=f'{alg_name}-(Cheating A Bit)')\n",
        "  ax.set_title(f'Cumulative Average of Actual Reward Per Learning Episode')\n",
        "  ax.set_xlabel('Learning Episodes')\n",
        "  ax.set_ylabel('Cumulative Avg. Actual Reward')\n",
        "  ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "vCPV8sEPcklj",
        "outputId": "9da20b99-c905-4c2f-f077-40df23be9c52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Exercise: Implement different update rules to compare",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fbc290276818>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# estimate of the gradient of expected reward. Both in a strict, experience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# driven way, and in \"cheating\" a bit way.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exercise: Implement different update rules to compare\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Exercise: Implement different update rules to compare"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "  - W (ndarray, shape: (1, n_inputs)): Connective strength weights between inputs and the output.\n",
        "  - x (ndarray, shape: (n_inputs, batch)): Input features, col is a sample, each row an input feature type.\n",
        "  - y (ndarray, shape: (1, batch)): Binary indication of prey presence, used to determine the reward.\n",
        "  - tau (float, optional): Temperature parameter for the sigmoid function, controlling its steepness.\n",
        "    A higher tau value leads to a steeper function.\n",
        "  - rng (np.random.Generator, optional): NumPy random number generator instance for reproducibility.\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z = np.dot(W, x) # 1 x batch = 1 x n_inputs @ n_inputs x batch\n",
        "  strike_prob = np_sigmoid(z, tau) # 1 x batch\n",
        "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
        "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  r_exp = strike_prob * r_all # + (1-strike_prob) * 0 # expected reward\n",
        "  return z, strike_prob, strike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           cheat=False,\n",
        "                           tau=1.0,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.0001):\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn regardless of whether organism actually strikes or not\n",
        "    update = (r_all - z) * x\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    update = (r-z) * strike * x\n",
        "  # average the update over all elements in the batch\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y,\n",
        "                     cheat=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn using expected reward\n",
        "    update = r_exp * (strike_prob) * (1 -  strike_prob) * x\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    # reward of zero recieved when not striking\n",
        "    update = r * (strike_prob) * (1 - strike_prob) * x\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.01,\n",
        "                         cheat=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # evaluate perturbation using expected reward, avg over the mini-batch\n",
        "    directional_grad_est = (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
        "  else: #realishtic\n",
        "    # evaluate perturbation using sampled rewards, avg over the mini-batch\n",
        "    directional_grad_est = (np.mean(r_perturb - r)) / perturbation_scale\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "################################################################################\n",
        "# Exercise Complete, simulations and plotting logic follow\n",
        "################################################################################\n",
        "# simulation\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "cooling_rate = 0.04\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "# cheating simulation\n",
        "cheat_alg_lrs = {'Reward Prediction': 0.0001,\n",
        "                 'Action Probability': 0.008,\n",
        "                 'Perturb Measure': 0.0016}\n",
        "cheat_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "cheat_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = cheat_alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      if alg_name == 'Reward Prediction':\n",
        "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
        "      else:\n",
        "        tau = 1.0\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=True, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      cheat_reward_results[alg_name].append(np.mean(r))\n",
        "      cheat_exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
        "    num_steps += 1\n",
        "    if num_steps > 1000:\n",
        "      break\n",
        "\n",
        "num_steps = 0\n",
        "real_alg_lrs = {'Reward Prediction': 0.0001,\n",
        "                'Action Probability': 0.003,\n",
        "                'Perturb Measure': 0.0000002}\n",
        "# realishtic simulation\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "actual_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = real_alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      if alg_name == 'Reward Prediction':\n",
        "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
        "      else:\n",
        "        tau = 1.0\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      actual_reward_results[alg_name].append(np.mean(r))\n",
        "      actual_exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
        "    num_steps += 1\n",
        "    if num_steps > 1000:\n",
        "      break\n",
        "# plotting\n",
        "with plt.xkcd():\n",
        "  # Create subplots with a shared x-axis\n",
        "  #fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8), sharex=True)\n",
        "  fig, ax1 = plt.subplots(figsize=(8,6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "\n",
        "  # Colors for algorithms\n",
        "  colors = {'Reward Prediction': 'b', 'Action Probability': 'g', 'Perturb Measure': 'r'}\n",
        "\n",
        "  # First subplot for expected rewards\n",
        "  ax1.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    eval_cheat = np.array(cheat_exp_reward_results[alg_name])\n",
        "    eval_cheat = np.cumsum(eval_cheat)\n",
        "    eval_cheat = eval_cheat / (np.arange(len(eval_cheat)) + 1)\n",
        "    ax1.plot(eval_cheat, linestyle='--', color=colors[alg_name], label=f'{alg_name}-Cheating')\n",
        "\n",
        "    eval_real = np.array(actual_exp_reward_results[alg_name])\n",
        "    eval_real = np.cumsum(eval_real)\n",
        "    eval_real = eval_real / (np.arange(len(eval_real)) + 1)\n",
        "    ax1.plot(eval_real, linestyle='-', color=colors[alg_name], label=f'{alg_name}-Realishtic')\n",
        "\n",
        "  ax1.set_title('Cumulative per Episode Average of\\nExpected (Full Batch) Reward')\n",
        "  ax1.set_ylabel('Cumulative Avg. Expected Reward')\n",
        "  ax1.legend()\n",
        "  # Second subplot for actual rewards\n",
        "  # ax2.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  # for alg_name in alg_names:\n",
        "  #  eval_cheat = np.array(cheat_reward_results[alg_name])\n",
        "  #  eval_cheat = np.cumsum(eval_cheat)\n",
        "  #  eval_cheat = eval_cheat / (np.arange(len(eval_cheat)) + 1)\n",
        "  #  ax2.plot(eval_cheat, linestyle='--', color=colors[alg_name], label=f'{alg_name}-Cheating')\n",
        "\n",
        "  #  eval_real = np.array(actual_reward_results[alg_name])\n",
        "  #  eval_real = np.cumsum(eval_real)\n",
        "  #  eval_real = eval_real / (np.arange(len(eval_real)) + 1)\n",
        "  #  ax2.plot(eval_real, linestyle='-', color=colors[alg_name], label=f'{alg_name}-Realishtic')\n",
        "\n",
        "  #ax2.set_title('Cumulative Average of Actual Reward Per Learning Episode')\n",
        "  #ax2.set_xlabel('Learning Episodes')\n",
        "  #ax2.set_ylabel('Cumulative Avg. Actual Reward')\n",
        "  #ax2.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jlB_fG4yCbwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the plots produced by the coding exercise we can see that Reward-Prediction-Error and Action-Probability-Reinforcement both outperform Perturb-Measure-Step, and that this difference is much greater for the realishtic variants than for the cheating variants\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l6hakJQYyWok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although all the learning algorithms learn less quickly when not allowed to cheat, Perturb-Measure-Step suffers the most. (Realishtic vs. Cheating vs. is: Reward-Prediction-Error can only learn when it strikes and there is an actual reward recieved (plus or minus one, to contrast with the prediction); Action-Probability-Reinforcement only reinforces the actual action taken by the reward recieved, not all actions by the expected reward; and perturb measure step compares an actual reward recieved under the base evaluation mode, with the actual reward recieved under the perturbation mode, not the expected reward.\n",
        "). This big slow down for Perturb-Measure-Step is because for this algorithms it is the difference in reward between the perturbation, and the base mode evaluations that drives learning. However, when learning from a single experience, and using actual sampled rewards, very few outcomes actually result in effective learning, as can be seen in the table below:\n",
        "\n",
        "| Case | Correct<br>Action | Perturb<br>Action | Base<br>Action | Perturb - Base<br>Reward<br>Difference | Learning<br>Happens | P(Strike, Perturb)<br> greater than<br>P(Strike, Base) | Learning is<br>Helpful |\n",
        "|:-------------:|:-------------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n",
        "|1 | Strike    | Strike    | Strike    | 0  | No  | - | | -   |\n",
        "|2 | Strike    | Strike    | No-Strike | 1  | Yes | Yes | Yes |\n",
        "|3 | Strike    | Strike    | No-Strike | 1  | Yes | No  | No  |\n",
        "|4 | Strike    | No-Strike | Strike    | -1 | Yes | Yes | No  |\n",
        "|5 | Strike    | No-Strike | Strike    | -1 | Yes | No  | Yes |\n",
        "|6 | Strike    | No-Strike | No-Strike | 0  | No  | -   | -   |\n",
        "|7 | No-Strike | Strike    | Strike    | 0  | No  | -   | -   |\n",
        "|8 | No-Strike | Strike    | No-Strike | -1 | Yes | Yes | Yes |\n",
        "|9 | No-Strike | Strike    | No-Strike | -1 | Yes | No  | No  |\n",
        "|10| No-Strike | No-Strike | Strike    | 1  | Yes | Yes | No  |\n",
        "|11| No-Strike | No-Strike | Strike    | 1  | Yes | No  | Yes |\n",
        "|12| No-Strike | No-Strike | No-Strike | 0  | No  | -   | -   |\n",
        "\n",
        "This is a bit dense, but what it is saying is that first of all, there are many cases where no learning occurs because both the pertubation and the base mode sample the same action and recieve the same rewards, so there is no reward difference at all to drive learning. Note that for small perturbations the action probabilities of the base and perturbed networks will be very similar, so these no learning cases will be the most commen case by far as perturbation sizes become increasinly small. Then, even in the cases where the perturbation mode and the base mode sample different actions, it is possible that the sampled actions will run counter to the way the perturbation shifts the probability of striking. Case 3 in the table above is an example of this. Under the perturbation the probability of striking is not greater than (but rather less than) the probability of striking with the base parameters, however, despite this, the less likely outcome where the perturb network samples striking, and the base network samples not-striking occurs. Because the sample is not aligned with the underlying shift in probabilities, learning from this difference is counterproductive. Now on average, learning will help shift probabilities towards striking in the right situations. However, it will do so only slowly, as many learning episodes will result in no learning, or counterproductive learning.\n"
      ],
      "metadata": {
        "id": "Dlz4O6r3zzh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.2 Why Delta Rule is Faster Than Perturb-Measure-Step\n",
        "Although Perturb-Measure-Step suffered the most from being realishtic, even when learning on the full batch instead of just one example at a time, and using expected rewards, we still see that Perturb-Measure-Step is slow compared to Reward-Prediction-Error and Action-Probability-Reinforcement."
      ],
      "metadata": {
        "id": "3Ve7dnS8OCVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 10000\n",
        "num_steps = 0\n",
        "mini_batch_size = Xs_aug.T.shape[0]\n",
        "cooling_rate = 0.04\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "alg_lrs = {'Reward Prediction': 0.0015,\n",
        "                 'Action Probability': 0.08,\n",
        "                 'Perturb Measure': 0.001}\n",
        "actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      if alg_name == 'Reward Prediction':\n",
        "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
        "      else:\n",
        "        tau = 1.0\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      r = np.mean(r)\n",
        "      r_exp_full = np.mean(r_exp_full)\n",
        "      actual_reward_results[alg_name].append(r)\n",
        "      exp_reward_results[alg_name].append(r_exp_full)\n",
        "    num_steps += 1\n",
        "  if num_steps > 10000:\n",
        "    break\n",
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    eval = np.array(exp_reward_results[alg_name])\n",
        "    eval = np.cumsum(eval)\n",
        "    eval = eval / (np.arange(len(eval)) + 1)\n",
        "    ax.plot(eval, label=f'{alg_name}-(Cheating Full, Batch)')\n",
        "    ax.set_title(f'Cumulative per Episode Average of Expected (Full Batch) Reward')\n",
        "    ax.set_xlabel('Learning Episodes')\n",
        "    ax.set_ylabel('Cumulative Avg. Expected (Full Batch) Reward')\n",
        "    ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  #fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  #theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  #ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  #for alg_name in alg_names:\n",
        "  #  eval = actual_reward_results[alg_name]\n",
        "  #  eval = np.cumsum(eval)\n",
        "  #  eval = eval / (np.arange(len(eval)) + 1)\n",
        "  #  ax.plot(eval, label=f'{alg_name} (Cheating Full Batch)')\n",
        "  #  ax.set_title(f'Cumulative Average of Actual Reward Per Learning Episode')\n",
        "  #  ax.set_xlabel('Learning Episodes')\n",
        "  #  ax.set_ylabel('Cumulative Avg. Actual Reward')\n",
        "  #  ax.legend()\n",
        "  #plt.tight_layout()\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "k5X_KDNaOcck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous sequence looking optimization in higher dimensions, analysis showed that the expected improvement from our perturb-measure-step was proportional to the square of the magnitude of the gradient, diveded by $N$ the number of dimensions in the optimization problem, so in our the number of parameters ($65$) to be optimized:\n",
        "\n",
        "$$\n",
        "\\text{Perturb-Measure-Step Expected Improvement per Step} \\propto \\frac{\\| \\mathbf{g} \\|^2}{n}$$\n",
        "\n",
        "And a bit of further analysis found that by thoughtfully scaling the step size to compensate for the low level of alignment of a random test direction with the gradient this could be improved to\n",
        "\n",
        "$$\n",
        "\\text{Scaled-Perturb-Measure-Step Expected Improvement per Step} \\propto \\frac{\\| \\mathbf{g} \\|^2}{\\sqrt{n}}$$\n",
        "\n",
        "What this tells us is that with Perturb-Measure-Step learning will always be kind of a zigg-zagging through parameter space, with paramter update steps being almost perpendicular to the direction of maximal improvement (the gradient $\\mathbf{g}$)\n",
        "\n",
        "With this in mind, let's look compute this gradient, $\\mathbf{g}$, for our evaluation function, applied to the entire batch of data. Then we can see how the our new update rules (Reward-Prediction-Error and Action-Probability-Reinforcement) relate to the gradient. To do that, we need to translate our evaluation function from python code into math symbols. In code our evaluation function was"
      ],
      "metadata": {
        "id": "Qohat9GYgiF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "  - W (ndarray, shape: (1, n_inputs)): Connective strength weights between inputs and the output.\n",
        "  - x (ndarray, shape: (n_inputs, batch)): Input features, col is a sample, each row an input feature type.\n",
        "  - y (ndarray, shape: (1, batch)): Binary indication of prey presence, used to determine the reward.\n",
        "  - tau (float, optional): Temperature parameter for the sigmoid function, controlling its steepness.\n",
        "    A higher tau value leads to a steeper function.\n",
        "  - rng (np.random.Generator, optional): NumPy random number generator instance for reproducibility.\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z = np.dot(W, x)\n",
        "  strike_prob = np_sigmoid(z, tau)\n",
        "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
        "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  r_exp = strike_prob * r_all # + (1-strike_prob) * 0 # expected reward\n",
        "  return z, strike_prob, strike, r, r_all, r_exp"
      ],
      "metadata": {
        "id": "2KwHdFze-d2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will leave aside the issues of having a stochastic evaluation function for the moment. Randomness in the evaluation could come from which particular samples from the full batch we use in our mini-batch, and randomness in the evaluation could come from randomness in behaviour, i.e. whether or not the organism strikes. To simplify things here we eliminate both these sources of randomness by 1) evaluating performance over the full batch of data, and 2) looking at expected reward given the striking probabilities. Then we can write:\n",
        "\n",
        "$$ \\mathbb{E}[R(\\mathbf{W}, \\mathbf{x}, y)] = \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\frac{1}{n} \\sum_{a\\in\\mathcal{A}} \\Pr(a | \\mathbf{x}) \\cdot r(a|y)$$\n",
        "\n",
        "This defines the criteria by which we evaluate the parameters and the resulting behavioural performance of the organism. Here $\\mathcal{D}$ is the set of all $n$ data points in the full-batch and $\\mathcal{A}$ is the set of actions $\\{ \\text{strike}, \\text{no-strike} \\}$. Note that $R(\\mathbf{W}, \\mathbf{x}, y)$ is random variable that depends on which actions the organism (probablistically) selects in which situations, whereas $r(a|y)$ is deterministic (in our example, in the more general case this might also be a random variable, but we are not considering this more general case just yet). The above expression corresponds to the mean of `r_exp` in the code block above, when eval params is applided to the whole batch of data. When writing about the probability of selecting actions we will typically us the notation\n",
        "\n",
        "$$\\pi_{\\mathbf{W}}(a|\\mathbf{x})$$\n",
        "\n",
        "to emphasize this is the probability of an action being selected by an organism with policy $\\pi$ parameterized by $\\mathbf{W}$ given experience $\\mathbf{x}$. (In the case where the parameters consist of more than a single weight matrix/vector we typically use $\\theta$ to denote all the parameters of the policy function and write $\\pi_{\\theta}$).\n",
        "\n",
        "The gradient with respect to $\\mathbf{W}$ of this expected reward expression is the *direction* of parameter change that will cause the greatest increase in expected reward for a small change in $\\mathbf{W}$, scaled by the *rate* of improvement in expected reward given a small change in $\\mathbf{W}$ in that direction. (The gradient is a vector, it has direction and magnitude!) Let's compute this gradient.\n",
        "\n",
        "$$\\begin{align} \\nabla_{\\mathbf{W}} \\mathbb{E}[R(\\mathbf{W}, \\mathbf{x}, y)] &= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\frac{1}{n} \\sum_{a\\in\\mathcal{A}} \\nabla_{\\mathbf{W}} \\pi_{\\mathbf{W}}(a | \\mathbf{x}) \\cdot r(a|y) \\\\\n",
        "&= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\sum_{a\\in\\mathcal{A}} \\frac{1}{n} \\cdot \\pi_{\\mathbf{W}}(a | \\mathbf{x}) \\cdot r(a|y) \\cdot \\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(a|\\mathbf{x})) \\end{align}$$\n",
        "\n",
        "Math trick alert! this rearangement of the terms comes from the application of the chain rule to:\n",
        "$$\\frac{\\mathrm{d}}{\\mathrm{d}x}\\log(f(x)) = \\frac{f'(x)}{f(x) } \\iff \\frac{\\mathrm{d}}{\\mathrm{d}x} f(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\log(f(x)) \\cdot f(x)$$\n",
        "\n",
        "There is a really good reason for organizing the gradient of the expected reward like this, since this now correspondes with how the organisms experiences the environment given it's current policy as determined by parameters $\\mathbf{W}$. Specifically, the organism encounters a data point $(\\mathbf{x}, y)$ from the data set, each point being equally likely to be encountered, so it encounters this particular point with probability $\\frac{1}{n}$. Then, in response to the experience of $\\mathbf{x}$ the *policy* of the organism as parameterized by $\\mathbf{W}$ selects or samples an action according to\n",
        "\n",
        "$$\\begin{align}\n",
        "\\pi_{\\mathbf{W}}(\\text{strike} | \\mathbf{x}) &= \\sigma(\\mathbf{Wx}) \\\\\n",
        "\\pi_{\\mathbf{W}}(\\text{no-strike} | \\mathbf{x}) &= 1 - \\sigma(\\mathbf{Wx})\n",
        "\\end{align}$$\n",
        "\n",
        "Once the organism has selected an action, the response/state of the environment determines the reward that the organism experiences according $r(a|y)$. Thus, the terms in the double sum over can be understood as accounting for probability of that a particular environmental state occurs, $\\frac{1}{n}$, the probability that a particular action is selected in response to the organisms sensory experience of that particular state $\\pi_{\\mathbf{W}}(a | \\mathbf{x})$ given the current policy, the reward that results from taking that particular action in that particular situation, $r(a|y)$, and lastly a gradient term $\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(a|\\mathbf{x}))$\n",
        "\n",
        "What this tells us, is that if the organism were to just go about its life experiencing $\\mathbf{x}$'s, selecting actions according to its policy, experiencing rewards, and then in response to these experiences of sensation-action-reward shift the parameters of its policy in the direction of $\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(a|\\mathbf{x}))$ scaled by the reward experienced, it would actually be implementing (an episodic approximation of) gradient ascent on the expected reward. This is perfect, because expected reward is ***exactly*** the quantity the organism should be trying to optimize (presuming that evolultion has linked aligned the intrinsic rewards experienced by the organism with reproductive success)."
      ],
      "metadata": {
        "id": "bfFMx4b--gGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay so let's dig into this gradient term for our particular policy. Let's look at the case when $a = \\text{strike}$ first, expanding using our definitions and the chain rule for derivatives\n",
        "\n",
        "$$ \\begin{align}\n",
        "\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(\\text{strike}|\\mathbf{x})) &= \\nabla_{\\mathbf{W}} \\log(\\sigma(\\mathbf{Wx})) \\\\\n",
        "&= \\frac{1}{\\sigma(\\mathbf{Wx})} \\cdot  \\nabla_{\\mathbf{W}} \\sigma(\\mathbf{Wx}) \\\\\n",
        "&= \\frac{1}{\\sigma(\\mathbf{Wx})} \\cdot \\sigma(\\mathbf{Wx}) \\cdot (1-\\sigma(\\mathbf{Wx})) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx} \\\\\n",
        "&= (1-\\sigma(\\mathbf{Wx})) \\cdot \\mathbf{x}\n",
        "\\end{align}$$\n",
        "\n",
        "$$ \\begin{align}\n",
        "\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(\\text{no-strike}|\\mathbf{x})) &= \\nabla_{\\mathbf{W}} \\log(1 - \\sigma(\\mathbf{Wx})) \\\\\n",
        "&= \\frac{1}{1 - \\sigma(\\mathbf{Wx})} \\cdot  \\nabla_{\\mathbf{W}} (1 - \\sigma(\\mathbf{Wx})) \\\\\n",
        "&= \\frac{1}{1 - \\sigma(\\mathbf{Wx})} \\cdot -\\sigma(\\mathbf{Wx}) \\cdot (1-\\sigma(\\mathbf{Wx})) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx} \\\\\n",
        "&= -\\sigma(\\mathbf{Wx}) \\cdot \\mathbf{x}\n",
        "\\end{align}$$\n"
      ],
      "metadata": {
        "id": "e3C_c35oXz8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So if we wanted have an update rule based on following the gradient of the expected reward it would look like this\n",
        "\n",
        "## Expected Reward Gradient Update Rule\n",
        "\n",
        "$$\\Delta W_i = s \\cdot (1-\\sigma(z)) \\cdot r \\cdot x_i $$\n",
        "\n",
        "Or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s \\cdot (1-\\sigma(z)) \\cdot r \\cdot \\mathbf{x}$$"
      ],
      "metadata": {
        "id": "oXc-y7AnwHdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So in the cases where the organism strikes, if the reward is positive it increase weights $w_i$ that were connected to positive inputs, and decrease weights that were connected to negative inputs, so as to increase the probability of striking in the case of experiencing this particular $\\mathbf{x}$. If there is regular structure to the environment in terms of a consistent correlation between $\\mathbf{x}$ and $y$, this adaptation will not only help the organism to select the correct action the next time it experiences this exact same situations $(\\mathbf{x}, y)$, but it will also help the organism to select the correct action with higher probability on subsequent experiences which are sufficinetly similar. If the reward is negative the organism decreases weights that are connected to postive inputs, and decreases weights that are connected to negative inputs so as to decrease the probability of striking in such circumstances.\n",
        "\n",
        "Now, let's compare this ideal update to our two outher update rules\n",
        "\n",
        "$$\\begin{align}\n",
        "\\Delta \\mathbf{W} (\\text{Reward Prediction}) &= s \\cdot (r-z) \\cdot \\mathbf{x} \\\\\n",
        "\\Delta \\mathbf{W} (\\text{Action Probability}) &= s \\cdot r \\cdot \\sigma(z) \\cdot (1 - \\sigma(z)) \\mathbf{x} \\\\\n",
        "\\Delta \\mathbf{W} (\\text{Expected Reward Gradient}) &= s \\cdot r \\cdot (1 - \\sigma(z)) \\mathbf{x}\n",
        "\\end{align}$$\n",
        "\n",
        "Or in terms of the policy (which gives the probability with which the sampled action $a$ was chosen):\n",
        "\n",
        "$$\\begin{align}\n",
        "\\Delta \\mathbf{W} (\\text{Reward Prediction}) &= s \\cdot (r-z) \\cdot \\mathbf{x} \\\\\n",
        "\\Delta \\mathbf{W} (\\text{Action Probability}) &= s \\cdot r \\cdot \\pi(a) \\cdot (1 - \\pi(a)) \\mathbf{x} \\\\\n",
        "\\Delta \\mathbf{W} (\\text{Expected Reward Gradient}) &= s \\cdot r \\cdot (1 - \\pi(a)) \\cdot \\mathbf{x}\n",
        "\\end{align}$$\n",
        "\n",
        "What each of these have in common is that they all shift the parameters in the direction defined by $\\mathbf{x}$ scaled by the reward $r$, modulo some scaling and/or shifting. Thus they all have the effect of increasing the probability of striking when striking is rewarded, and decreasing the probability when striking is punished (negative reward). Reward-Prediction-Error scales the magnitude of the learning step by using ($r-z$) instead of raw $r$ to drive learning (and disregards anything to do with probability as that is under control of the $\\tau$ parameter). Action-Probability-Reinforcement scales by probabilities of striking and not-striking, so that updates are smaller when the probability of taking the action is either very high (close to one) or very low (close to zero) and is largest when the probability of taking versus not taking the action is smallest. In contrast, the Expected-Reward-Gradient episodic update is only scaled by the probability of not taking the action, so the more likely the action is the more strongly it is reinforced. In all cases though, each rule prescribes a parameter change in the ***same direction***. These rules only differ in the way the parameter update is scaled.\n",
        "\n",
        "This different scaling or weighting of the episodic update rules means that on average when applied over many iterations they will not shift the parameters in the same direction, and as a result will lead to slightly different long term learning outcomes.\n",
        "\n",
        "To get a sense of this we look at the divergence angle between these updates, when applied over mini-batches of size 1 (perfect alignment) size 10 and size 20."
      ],
      "metadata": {
        "id": "yReSbpeGwHto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def action_log_prob_step(W, x, y,\n",
        "                         cheat=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn using expected reward\n",
        "    update = r_exp * (1 -  strike_prob) * x\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    # reward of zero recieved when not striking\n",
        "    update = r * (1 - strike_prob) * x\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp"
      ],
      "metadata": {
        "id": "gFG3SR5KX4Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown **Run this cell** to see a comparison of the gradient of the different algorithms - Reward Prediction, Action Probability and Action Log Probability.\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "cooling_rate = 0.04\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Action Log Probability']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, action_log_prob_step]\n",
        "alg_lrs = {'Reward Prediction': 0.0001,\n",
        "           'Action Probability': 0.003,\n",
        "           'Action Log Probability': 0.001}\n",
        "actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      if alg_name == 'Reward Prediction':\n",
        "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
        "      else:\n",
        "        tau = 1.0\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      r = np.mean(r)\n",
        "      r_exp_full = np.mean(r_exp_full)\n",
        "      actual_reward_results[alg_name].append(r)\n",
        "      exp_reward_results[alg_name].append(r_exp_full)\n",
        "    num_steps += 1\n",
        "    if num_steps > 1000:\n",
        "      break\n",
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    eval = np.array(exp_reward_results[alg_name])\n",
        "    eval = np.cumsum(eval)\n",
        "    eval = eval / (np.arange(len(eval)) + 1)\n",
        "    ax.plot(eval, label=f'{alg_name}-(Cheating Less)')\n",
        "    ax.set_title(f'Cumulative per Episode Average of Expected (Full Batch) Reward')\n",
        "    ax.set_xlabel('Learning Episodes')\n",
        "    ax.set_ylabel('Cumulative Avg. Expected (Full Batch) Reward')\n",
        "    ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    eval = actual_reward_results[alg_name]\n",
        "    eval = np.cumsum(eval)\n",
        "    eval = eval / (np.arange(len(eval)) + 1)\n",
        "    ax.plot(eval, label=f'{alg_name} (Cheating Less)')\n",
        "    ax.set_title(f'Cumulative Average of Actual Reward Per Learning Episode')\n",
        "    ax.set_xlabel('Learning Episodes')\n",
        "    ax.set_ylabel('Cumulative Avg. Actual Reward')\n",
        "    ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Y63aQL1zNckk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the comparative performance of these three different update rules, we see that they all have relatively similar performance in terms of speed of learning. This stems from the fact that they all are leveraging knowledge of how the behaviour is produced by the network to inform the weight updates. This knowledge manifests as the term $\\mathbf{x}$ in each of these update rules, since this determines the causal impact of changes in the weight parameters $\\mathbf{W}$ on downstream probabilities of the selected action being taken or not, given the current input. This should be contrasted with perturb-measure-step which makes no use of such knowledge. Perturb-measure-step assumes total ignorance of the structure of the evaluation function, both the behaviour generating aspect of parameter evaluation, and the reward response aspect of the evaluation function. A useful analogy (which we formalize later in sequence ref) is of observations as kind of currency to be spent on making better inferences about the world. Different learning algorithms can all recieve the same data-points, but how they \"spend\" that data on inferences is what differentiates them. There is information in the data, but the model archetecture and the learning algorithm determine how efficiently the information in that data is used (or not) to improve some objective. Loosely speaking, within this spending data analogy, perturb-measure-step is spending a lot of its data on inference about the paramaters generate reward, through behaviour selection. In contrast these other update rules, take the structure of the behaviour generating function as a given, which allows them to spend all of their data on learning the association between sensory inputs and rewarding behaviour, i.e. directly improving behaviour, without \"wasting\" any data on implicit inference about how behaviour is generated, (as this knowledge is already embedded in the update rule). This is very abstract. To put it in more concrete terms suppose that on a particular episode $x_0$ has a value of zero. Because this will have had no causal impact on the action selected in that episode none of the gradient aligned update rules will make an update to $w_0$. In contrast, perturb-measure-step may well update to $w_0$, so long as the perturbed evaluation is different from the baseline evaluation. This is because, based on a single learning episode, perturb-measure-step can only make a rough guess based on correlations as to the causal impact of any one parameter on the outcome. On the one hand an algorithm like perturb-measure-step is very appealing because it implements a kind of optimization with no knowledge of the structure of the network that generates the behaviour or the environmental dynamics that then provide a reward. However, this comes at the high cost taking many learning trials to discover good parameters. Algorithms like perturb-measure-step are likely implemented and useful for configuring many small and simple neural circuits. However, as networks become large, perturb-measure-step becomes biologically implausible, not because of the mechansims of its physiological implementation, but because it is not effective enought to explain rapidly aquired adaptive behaviours driven by synaptic plasticity in animals. More complicated synaptic adaptivity process, the somehow leverage structural knowledge of how behaviour is generated to accelerate learning, are more difficult to imagine physiological mechansims for, but never-the-less are among the simplest update rules that might account for the speed and efficacy of adaptive neural plasticity observed in animals. We need to think of biological plausibility as constrained on two ends, from below, by the plausibility of the electro-physiological mechanisms of neural plasticity, but also, and just as importantly, above by the plausibility of the efficacy of the learning rule. A physiologically simple mechanism that predicts learning on the time course of millions of learning episodes, when behaviourally we see learning over a time course of dozens of episodes, is still biologically implausible. Practical efficacy is just as important a constraint on the dynamics or synaptic plasticity as eletro-physiological mechansims!"
      ],
      "metadata": {
        "id": "AVQFw902X4xS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________________\n",
        "## Loss Box: Expected-Reward-Gradient **is** Action-Log-Probability-Reinforcement\n",
        "\n",
        "$$\\begin{align} \\nabla_{\\mathbf{\\theta}} \\mathbb{E}[R(\\mathbf{\\theta}, \\mathbf{x}, y)] &= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\frac{1}{n} \\sum_{a\\in\\mathcal{A}} \\nabla_{\\mathbf{\\theta}} \\pi_{\\mathbf{\\theta}}(a | \\mathbf{x}) \\cdot r(a|y) \\\\\n",
        "&= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\sum_{a\\in\\mathcal{A}} \\frac{1}{n} \\cdot \\pi_{\\mathbf{\\theta}}(a | \\mathbf{x}) \\cdot r(a|y) \\cdot \\nabla_{\\mathbf{\\theta}} \\log(\\pi_{\\mathbf{\\theta}}(a|\\mathbf{x})) \\end{align}$$\n",
        "\n",
        "Here $\\mathcal{D}$ is the set of all $n$ data points in the full-batch and $\\mathcal{A}$ is the set of actions available to the policy. Note that $R(\\mathbf{\\theta}, \\mathbf{x}, y)$ is random variable that depends on which actions the organism (probablistically) selects in which situations, whereas $r(a|y)$ is deterministic in this case. This formulation is a beautiful thing. It tells us that an organism can maximize their expected reward, simply by doing what they do, experiencing their rewards and then shifting the parameters of their policy in the direction of the gradient of the log probability of the action just taken, scaled by the resultant reward. How this derivative is computed and applied physiologically is a rich topic which we address in part later.\n",
        "\n",
        "Putting this in terms of an episodic or single experience update rule for the parameters we have\n",
        "\n",
        "$$\\Delta \\theta = s \\cdot r(a|y) \\cdot \\nabla_{\\theta} \\log(\\pi_\\theta(a|\\mathbf{x})$$\n",
        "________________"
      ],
      "metadata": {
        "id": "jBr3n0eLZDKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.3 Learning To Do More Than One Thing\n",
        "\n",
        "So far we have just focused on the case where the organism has to choose between doing something, in our cartoon example striking at prey, or refraining from doing that. There is only one thing to do, and it is either done or not done. Now we want to look at a more general kind of problem where there are many possible actions to choose from. We envision a cartoon scenario much like the first one, but now instead of either striking or not striking, we imagine that there are 10 disctinct prey types, each requiring a different capture technique. If the predator selects the correct capture technique for the particular prey type encountered, they are successful and get a reward of one, but if they select the wrong capture technique (How these specific techniques are learned, and what it would even mean to learn specific stereotyped action patterns from a sensory motor perspective, we leave aside for now, and focus solely on the problem of the predator learning to associate the correct capture technique with the correct prey type.)\n",
        "\n",
        "To get a sense of this discrimination problem run the cell belwo and try it our yourself"
      ],
      "metadata": {
        "id": "oILhjaGM9oD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown **Run this cell** to try out this new 10-fold discrimination task.\n",
        "\n",
        "class InteractiveMNISTPredator_Multi():\n",
        "  def __init__(self,\n",
        "               features=Xs,\n",
        "               labels=y,\n",
        "               extra_labels=y,\n",
        "               feedback_type='on_strike_only', seed=123):\n",
        "    # Initialize dataset, settings for image scrambling and feedback\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "    # features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)\n",
        "    # labels is num_data_points x 1 (values 0-9 or 0/1 depending)\n",
        "    self.feedback_type = feedback_type\n",
        "    self.rng = np.random.default_rng(seed)\n",
        "    #sample_order = np.arange(self.features.shape[0])\n",
        "    sample_order = self.rng.permutation(self.features.shape[0])\n",
        "    self.features = self.features[sample_order]\n",
        "    self.labels = self.labels[sample_order]\n",
        "    self.extra_labels = extra_labels[sample_order]\n",
        "    # initialize game state\n",
        "    self.current_index = 0\n",
        "    self.current_image = None\n",
        "    self.previous_image = None\n",
        "    self.score = 0\n",
        "    self.best_possible_score = 0\n",
        "    self.successful_captures = 0\n",
        "    self.failed_captures = 0\n",
        "    # Initialize widgets\n",
        "    self.strike_buttons = [widgets.Button(description=str(i)) for i in range(10)]\n",
        "    # Bind event handlers\n",
        "    for btn in self.strike_buttons:\n",
        "      btn.on_click(self.on_class_selected)\n",
        "    self.score_display = widgets.Output()\n",
        "    self.feedback_display = widgets.Output()\n",
        "\n",
        "    # Initialize the figure for image display\n",
        "    self.fig, self.ax = plt.subplots(figsize=(4, 4))\n",
        "    remove_ip_clutter(self.fig)\n",
        "    self.prev_fig, self.prev_ax = plt.subplots(figsize=(4, 4))\n",
        "    remove_ip_clutter(self.prev_fig)\n",
        "    self.show_next_image()\n",
        "\n",
        "    # Arrange widgets in a layout\n",
        "    current_buttons = widgets.VBox([self.fig.canvas,\n",
        "                                    widgets.HBox(self.strike_buttons[0:3]),\n",
        "                                    widgets.HBox(self.strike_buttons[3:6]),\n",
        "                                    widgets.HBox(self.strike_buttons[6:10])])\n",
        "    previous_feedback = widgets.VBox([self.prev_fig.canvas, self.feedback_display])\n",
        "    self.ui = widgets.HBox([previous_feedback, current_buttons, self.score_display])\n",
        "\n",
        "  def show_next_image(self):\n",
        "    # Display the next image\n",
        "    image = self.features[self.current_index]\n",
        "\n",
        "    if len(image) == 64:\n",
        "        image = image.reshape(8, 8)\n",
        "    elif len(image) == 1:\n",
        "      scalar_value = image.flatten()[0]\n",
        "      # Initialize the 8x8 array with -6 (black)\n",
        "      image = np.full((8, 8), -6.0)\n",
        "      # Set the first ring to 6 (white)\n",
        "      image[0, 0] = 6\n",
        "      # Set the second ring to 6 (white)\n",
        "      image[1:-1, 1:-1] = 6\n",
        "      # Set the third (inner ring) back to -6 (black)\n",
        "      image[2:-2, 2:-2] = -6\n",
        "      # Assuming scalar_value is already in the range -6 to 6\n",
        "      #print(scalar_value)\n",
        "      image[3:-3, 3:-3] = scalar_value\n",
        "    else:\n",
        "      raise ValueError(f'Unexpected image shape: {image.shape}')\n",
        "    if self.current_image is not None:\n",
        "      self.previous_image = self.current_image\n",
        "    image = np.flipud(image)\n",
        "    self.current_image = image\n",
        "    # Display the image\n",
        "    #print(image)\n",
        "    self.fig.clf()\n",
        "    self.prev_fig.clf()\n",
        "    self.ax = self.fig.add_subplot(111)\n",
        "    self.prev_ax = self.prev_fig.add_subplot(111)\n",
        "    self.ax.set_xlim(-.5, 7.5)\n",
        "    self.ax.set_ylim(-0.5, 7.5)\n",
        "    self.prev_ax.set_xlim(-.5, 7.5)\n",
        "    self.prev_ax.set_ylim(-0.5, 7.5)\n",
        "    self.ax.set_aspect('equal')\n",
        "    self.prev_ax.set_aspect('equal')\n",
        "    self.ax.axis('off')\n",
        "    self.prev_ax.axis('off')\n",
        "    self.ax.imshow(self.current_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
        "    if self.previous_image is not None:\n",
        "      self.prev_ax.imshow(self.previous_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
        "    self.ax.set_title('Current Sensory Input')\n",
        "    self.prev_ax.set_title('Previous Sensory Input')\n",
        "    self.fig.canvas.draw()\n",
        "    self.prev_fig.canvas.draw()\n",
        "\n",
        "  def on_class_selected(self, button):\n",
        "    # freeze buttons while we process\n",
        "    for btn in self.strike_buttons:\n",
        "      btn.disabled=True\n",
        "    selected_class = int(button.description)\n",
        "    correct_class = int(np.squeeze(self.labels[self.current_index]))\n",
        "    if selected_class == correct_class:\n",
        "      self.score += 1\n",
        "      self.successful_captures += 1\n",
        "      feedback = f\"Your last choice {selected_class}, was correct\"\n",
        "    else:\n",
        "      self.score -= 1\n",
        "      self.failed_captures += 1\n",
        "      feedback = f\"Your last choice {selected_class}, was incorrect. The correct choice was {correct_class}.\"\n",
        "\n",
        "    # show feedback\n",
        "    with self.feedback_display:\n",
        "      clear_output(wait=True)\n",
        "      #print(self.labels[self.current_index])\n",
        "      #print(self.extra_labels[self.current_index])\n",
        "      print(feedback)\n",
        "\n",
        "    # Show score\n",
        "    with self.score_display:\n",
        "      clear_output(wait=True)\n",
        "      average_score = self.score / (self.current_index+1)\n",
        "      print(f'Total Score: {self.score}')\n",
        "      print(f'Number of Trials: {self.current_index + 1}')\n",
        "      print(f'Successful Captures: {self.successful_captures}')\n",
        "      print(f'Failed Captures: {self.failed_captures}')\n",
        "      print(f'Average Score Per Trial: {average_score:.2f}')\n",
        "\n",
        "    # Prepare the next image\n",
        "    self.current_index += 1\n",
        "    #print(self.current_index)\n",
        "    self.show_next_image()\n",
        "    # Re-enable buttons\n",
        "    for btn in self.strike_buttons:\n",
        "      btn.disabled=False\n",
        "\n",
        "\n",
        "scramble_multi_hard = InteractiveMNISTPredator_Multi(\n",
        "    features=Xs, labels=y, feedback_type='both')\n",
        "display(scramble_multi_hard.fig.canvas)\n",
        "display(scramble_multi_hard.prev_fig.canvas)\n",
        "clear_output()\n",
        "display(scramble_multi_hard.ui)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HqPf3kNxH5Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found the binary discrimination task with these inputs basically intractable, so for us at least this 10-fold discrimination task is even more impossible. Let's see though if we can adapt our behaviour generating network and our learning rules to this new multi-class scenario."
      ],
      "metadata": {
        "id": "AyKMnGsyO7Iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We model this creature's sensory-behaviour much as before but with a few small but important differences. As before $\\mathbf{x}$ is the raw sensory input (column vector) of length 64 in a given episode. Each element $x_i$ of $\\mathbf{x}$ corresponds to the activation level of a single photosensitive neuron.\n",
        "\n",
        "These input neurons are then connected by synapses to a **multiple** output neurons. The activation level, as a vector $\\mathbf{z}$, of these output neurons is computed as\n",
        "$$\\mathbf{z} = \\mathbf{Wx}$$\n",
        "Here $\\mathbf{W}$ is a matrix of synaptic weights between the input neurons and the various output neurons. In this case where there are 65 inputs (We have done our usual trick of hiding the baseline activation level by augmenting $\\mathbf{x}$) and 10 outputs $\\mathbf{W}$ has shape 10x66. So\n",
        "$$ z_i = \\sum_{j=1}^{65} w_{ij} x_j$$\n",
        "\n",
        "(Notation reminder: $w_{ij}$ is the weight connecting the $j^{th}$ input to the $i^{th}$ output and is the element in the $i^{th}$ row and $j^{th}$ column of $\\mathbf{W}$.)\n",
        "\n",
        "We need some way of then translating these activation levels, $\\mathbf{z}$, into actions. The generalization of the logistic sigmoid to multi-class situations like this is softmax normalization. Applying softmax normalization we get that the probability of taking action $i$ is given by\n",
        "$$ \\Pr \\{\\text{action }i\\} = \\text{softmax}(z_i) = \\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)} $$\n",
        "\n",
        "(Note that when there are only two possible actions, and one action has a fixed activation of $0$, then softmax normalization becomes equivalent to applying the logistic sigmoid of the variable (non-zero) activation level, to determin action probabilities.)\n",
        "\n",
        "Like the logistic sigmoid, softmaz normalization can also have it's variabilility modified by a temperature parameter, $\\tau$, where high temperatures cause the distribution to be closer to a uniform distribution, and low temperatures cause the highest activation value action to be chosen with near certainty.\n",
        "$$ \\Pr \\{\\text{action }i\\} = \\text{softmax}_{\\tau}(z_i) = \\frac{\\exp(z_i /\\tau)}{\\sum_k \\exp(z_k / \\tau)} $$"
      ],
      "metadata": {
        "id": "z5g9V9gHPhRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This motivates a the following modification of our parameter evaluation fucntion. The logic is basically the same as before, but with some added complexity around sampling from more than two possible options."
      ],
      "metadata": {
        "id": "rEs0YjB9eCwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward-Prediction-Error Multi-Class Episodic Update Rule\n",
        "\n",
        "$$\\Delta W_{ij} = s \\cdot (r-z_i) \\cdot x_j \\quad \\text{if action }i\\text{ taken}$$\n",
        "$$\\Delta W_{ij} = 0 \\quad \\text{if some action other than }i\\text{ taken}$$\n",
        "Or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W}_{i\\cdot} = s \\cdot (r-z_i) \\cdot \\mathbf{x}$$"
      ],
      "metadata": {
        "id": "mFHWveVz7L1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cheating variant of this learns as though all actions were taken and experienced instead of working with just the sampled action and reward."
      ],
      "metadata": {
        "id": "qlI-9f5U8BbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action-Probability-Reinforcement Mulit-Class Episodic Update Rule\n",
        "\n",
        "$$ \\Delta W_{ij} = s \\cdot r \\cdot \\frac{\\partial}{\\partial z_i}\\text{softmax}(z_k) \\cdot x_j \\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
        "\n",
        "or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s \\cdot r \\cdot \\frac{\\partial}{\\partial \\mathbf{z}}\\text{softmax}(z_k) \\cdot \\mathbf{x}^T \\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
        "\n",
        "In the vector formulation $\\mathbf{x}^T$ is a row vector of length 65, and $\\frac{\\partial}{\\partial \\mathbf{z}}\\text{softmax}(z_k)$ is a col vector of length 10, so their matrix outer product gives a matrix of shape 10 by 65 (the shape of $\\mathbf{W}$.) Now much like the sigmoid function which it is a generalization of $\\text{softmax}$ also has a convinient derivative, specifically\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial z_i}\\text{softmax}(z_k) =\n",
        "\\begin{cases}\n",
        "-\\pi(a_k)\\cdot \\pi(a_i) & \\text{if } i \\neq k \\\\\n",
        "\\pi(a_i)\\cdot (1 - \\pi(a_i)) & \\text{if } i = k\n",
        "\\end{cases}\n",
        "$$\n",
        "Here $\\pi(a_i)$ is the output of the softmax normalization corresponding to the probability of selecting action $a_i$, given the vector of output activations $\\mathbf{z}$. This is saying that if we increase the activation of a given action, say $z_k$, we increase the probability that that action, $a_k$ is taken at a rate of $\\pi(a_k)\\cdot (1 - \\pi(a_k))$ while simultaneously decreasing the probability (hence the negative sign) with which other actions $a_i$, $i\\neq k$, are taken at a rate of $-\\pi(a_k)\\cdot \\pi(a_i)$. Substituting this into our update rule we have\n",
        "\n",
        "$$\n",
        "\\Delta W_{ij} \\begin{cases}\n",
        "- s \\cdot r \\cdot \\pi(a_k)\\cdot \\pi(a_i) \\cdot x_j & \\text{if } i \\neq k \\\\\n",
        "s \\cdot r \\cdot \\pi(a_i)\\cdot (1 - \\pi(a_i)) \\cdot x_j & \\text{if } i = k\n",
        "\\end{cases}\n",
        "\\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
        "\n",
        "What this is saying is that if an action is rewarding, the weights will shift to make selection of the that action given those inputs more likely next time, and if an action is not rewarding the reverse is true, the weights will shift to make alternative actions more likely to be selected next time.\n",
        "\n",
        "One important way in which this action-probability-reinforcement differs from the reward-prediction-error update is that every weight in the network is potentially updated after a given episode, in contrast to only the weights making a prediction about the reward associated with the action sampled."
      ],
      "metadata": {
        "id": "3TOOt83m8bVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expected Reward Gradient Update Rule\n",
        "\n",
        "In our previous analysis we saw that the Expected-Reward-Gradient update is equivalent to the Action-Log-Probability-Reinforcement so extending to this multi-class scenario we have\n",
        "\n",
        "$$ \\Delta W_{ij} = s \\cdot r \\cdot \\frac{\\partial}{\\partial z_i}\\log(\\text{softmax}(z_k)) \\cdot x_j \\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
        "\n",
        "Now $\\log(\\text{softmax})$ also hase a very convenient derivative.\n",
        "$$\n",
        "\\frac{\\partial}{\\partial z_i}\\log(\\text{softmax}(z_k)) =\n",
        "\\begin{cases}\n",
        "- \\pi(a_i) & \\text{if } i \\neq k \\\\\n",
        "(1 - \\pi(a_i)) & \\text{if } i = k\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "So putting this into our update rule we have\n",
        "\n",
        "$$\n",
        "\\Delta W_{ij} \\begin{cases}\n",
        "- s \\cdot r \\cdot \\pi(a_i) \\cdot x_j & \\text{if } i \\neq k \\\\\n",
        "s \\cdot r \\cdot (1 - \\pi(a_i)) \\cdot x_j & \\text{if } i = k\n",
        "\\end{cases}\n",
        "\\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
        "\n",
        "This has basically the same affect as Action-Probability-Reinforcement, but with different scaling terms."
      ],
      "metadata": {
        "id": "UpoBaqTUDAkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more complicated versions of this update rule that take into account how to more precisely update all the parameters of $\\mathbf{W}$ to increment the probability of taking a certain action, by not only increasing the corresponding activation level, but also by slightly decreasing the activation level of all other alternatives. For now, to keep our exposition clear and tractable we ignore these more complicated alternatives. Just note that the above rule does not"
      ],
      "metadata": {
        "id": "cmuE4SwF9bGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def np_softmax(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = x / tau\n",
        "  # Shift x by subtracting the max value to prevent overflow in exp\n",
        "  x_shifted = x_scaled - np.max(x_scaled, axis=0, keepdims=True)\n",
        "  exps = np.exp(x_shifted)\n",
        "  # Normalize the exponentials while maintaining batch structure\n",
        "  softmax_output = exps / np.sum(exps, axis=0, keepdims=True)\n",
        "  return softmax_output\n",
        "\n",
        "def eval_params_multi(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z = np.dot(W, x)  # num_action x batch\n",
        "  action_probs = np_softmax(z, tau)  # num_action x batch\n",
        "  cumulative_probs = np.cumsum(action_probs, axis=0)\n",
        "  random_samples = rng.random(size=cumulative_probs.shape[1])\n",
        "  sampled_actions = (cumulative_probs > random_samples).argmax(axis=0)\n",
        "  # calculate which actions were correct and compute reward\n",
        "  correct = sampled_actions == y\n",
        "  r = np.zeros_like(y)\n",
        "  r[correct] = 1\n",
        "  r[~correct] = -1  # sampled reward\n",
        "  # Create an outcomes matrix and calculate expected reward\n",
        "  outcomes_matrix = -np.ones_like(action_probs)\n",
        "  # Set reward to +1 at the position of the correct action for each sample\n",
        "  outcomes_matrix[y, np.arange(y.size)] = 1\n",
        "  r_exp = np.sum(action_probs * outcomes_matrix, axis=0)  # expected reward\n",
        "  return z, action_probs, sampled_actions, r, outcomes_matrix, r_exp"
      ],
      "metadata": {
        "id": "BtWeUs_seIH6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to adapt our update rules to this new multi-class problem (though note that perturb-measure-step doesn't need any adaption at all, because it is agnostic to the underlying mechanism of behaviour generation, it needs no modifcation at all, it simply needs to use this new evaluation function.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vKVXnuaXp7WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def np_softmax(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = x / tau\n",
        "  # Shift x by subtracting the max value to prevent overflow in exp\n",
        "  x_shifted = x_scaled - np.max(x_scaled, axis=0, keepdims=True)\n",
        "  exps = np.exp(x_shifted)\n",
        "  # Normalize the exponentials while maintaining batch structure\n",
        "  softmax_output = exps / np.sum(exps, axis=0, keepdims=True)\n",
        "  return softmax_output\n",
        "\n",
        "def eval_params_multi(W, x, y, tau=1, rng=None):\n",
        "  # W shape is num_action x input\n",
        "  # x shape is input x batch\n",
        "  # y shape is 1 x batch\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z = np.dot(W, x)  # num_action x batch\n",
        "  action_probs = np_softmax(z, tau)  # num_action x batch\n",
        "  cumulative_probs = np.cumsum(action_probs, axis=0)\n",
        "  random_samples = rng.random(size=cumulative_probs.shape[1])\n",
        "  sampled_actions = (cumulative_probs > random_samples).argmax(axis=0)\n",
        "  # calculate which actions were correct and compute reward\n",
        "  correct = sampled_actions == y\n",
        "  r = np.zeros_like(y)\n",
        "  r[correct] = 1\n",
        "  r[~correct] = -1  # sampled reward\n",
        "  # Create an outcomes matrix and calculate expected reward\n",
        "  outcomes_matrix = -np.ones_like(action_probs)\n",
        "  # Set reward to +1 at the position of the correct action for each sample\n",
        "  outcomes_matrix[y, np.arange(y.size)] = 1\n",
        "  r_exp = np.sum(action_probs * outcomes_matrix, axis=0)  # expected reward\n",
        "  return z, action_probs, sampled_actions, r, outcomes_matrix, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           tau=1.0,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.0001):\n",
        "  z, action_probs, sampled_actions, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
        "  # learn only from actual received rewards\n",
        "  errors = np.zeros_like(z)\n",
        "  actual_errors = r - z[sampled_actions, np.arange(len(sampled_actions))]\n",
        "  errors[sampled_actions, np.arange(len(sampled_actions))] = actual_errors\n",
        "  # implicit sum over batch here in this matrix multiplication\n",
        "  update = errors @ x.T  # errors is num_actions x batch, x.T is batch x num_features,\n",
        "  update /= x.shape[1]  # Divide by the number of samples in batch to make the sum an average\n",
        "  W_new = W + learning_rate * update  # Apply learning rate to update step\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y,\n",
        "                     cheat=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  z, action_probs, sampled_actions, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
        "  num_actions, batch_size = action_probs.shape\n",
        "  num_features = x.shape[0]\n",
        "  sampled_action_probs = action_probs[sampled_actions, np.arange(batch_size)]  # Shape [batch_size]\n",
        "  broadcast_sampled_action_probs = np.broadcast_to(sampled_action_probs.reshape(1,batch_size), (num_actions, batch_size))\n",
        "  # Compute updates for case when row (i) of W does not correspond to the sampled action\n",
        "  delta_W = np.zeros((num_actions, num_features, batch_size))\n",
        "  delta_W -= r[np.newaxis,:,:] * broadcast_sampled_action_probs[:,np.newaxis,:] * action_probs[:,np.newaxis,:] * x[np.newaxis,:,:]  # Shape [num_actions, num_features, batch_size]\n",
        "  # now compute updates for case when row (i) of W does correspond to the sampled action\n",
        "  mask = np.arange(num_actions)[:, None] == sampled_actions[None, :]  # [num_actions, batch_size]\n",
        "  mask = np.broadcast_to(mask[:,np.newaxis,:], delta_W.shape) # [num_actions, num_features, batch_size]\n",
        "  positive_update = r[np.newaxis,:,:] * (action_probs * (1 - action_probs))[:,np.newaxis,:] * x[np.newaxis,:,:]  # [num_features, num_actions, batch_size]\n",
        "  # Use positive update where appropriate\n",
        "  delta_W[mask] = positive_update[mask]\n",
        "  W_new = W + learning_rate * delta_W\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.01,\n",
        "                         cheat=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # evaluate perturbation using expected reward, avg over the mini-batch\n",
        "    directional_grad_est = (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
        "  else: #realishtic\n",
        "    # evaluate perturbation using sampled rewards, avg over the mini-batch\n",
        "    directional_grad_est = (np.mean(r_perturb - r)) / perturbation_scale\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "gXG6cgPXq0YO",
        "outputId": "bce00686-e3c1-4ebc-915d-37b06ef20c6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-19-3b8237295023>, line 69)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-3b8237295023>\"\u001b[0;36m, line \u001b[0;32m69\u001b[0m\n\u001b[0;31m    return delta_W\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.rand(10,65)"
      ],
      "metadata": {
        "id": "5jRNkK3eWWHI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W.shape"
      ],
      "metadata": {
        "id": "KY3U9gLqWazo",
        "outputId": "0b816ef4-8941-4c83-bd12-d98a9ede2463",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = Xs_aug[:20].T"
      ],
      "metadata": {
        "id": "lwznRC3vWdtS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "ZdXSODr2WiID",
        "outputId": "28ca29a8-7fd1-4f2c-a5aa-c721d4bd6358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(65, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = data_set.data.targets.values"
      ],
      "metadata": {
        "id": "kdldfi9yWnBU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_batch = y[:20].T"
      ],
      "metadata": {
        "id": "QK4RKUUDXKB1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_batch.shape"
      ],
      "metadata": {
        "id": "9_hJ-j65XN4P",
        "outputId": "015bcdfa-bd05-43ec-8367-39fc00362662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_params_multi(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z = np.dot(W, x)  # num_action x batch\n",
        "  action_probs = np_softmax(z, tau)  # num_action x batch\n",
        "  cumulative_probs = np.cumsum(action_probs, axis=0)\n",
        "  random_samples = rng.random(size=cumulative_probs.shape[1])\n",
        "  sampled_actions = (cumulative_probs > random_samples).argmax(axis=0)\n",
        "  # calculate which actions were correct and compute reward\n",
        "  correct = sampled_actions == y\n",
        "  r = np.zeros_like(y)\n",
        "  r[correct] = 1\n",
        "  r[~correct] = -1  # sampled reward\n",
        "  # Create an outcomes matrix and calculate expected reward\n",
        "  outcomes_matrix = -np.ones_like(action_probs)\n",
        "  # Set reward to +1 at the position of the correct action for each sample\n",
        "  outcomes_matrix[y, np.arange(y.size)] = 1\n",
        "  r_exp = np.sum(action_probs * outcomes_matrix, axis=0)  # expected reward\n",
        "  return z, action_probs, sampled_actions, r, outcomes_matrix, r_exp"
      ],
      "metadata": {
        "id": "KsOUusAcWU4W"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z, action_probs, sampled_actions, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y_batch)"
      ],
      "metadata": {
        "id": "JKilSsOWXRSA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.shape"
      ],
      "metadata": {
        "id": "_QQJtj1_XgqK",
        "outputId": "231a6970-66a9-44e1-e32a-f3149efdfb87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_actions.shape"
      ],
      "metadata": {
        "id": "8mAsr3HhXiCy",
        "outputId": "fccd2239-9bbc-49cd-cffc-ea0b0acea9cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_probs.shape"
      ],
      "metadata": {
        "id": "pFlrdkyOctUs",
        "outputId": "46665d05-cbb9-4590-8abf-ea59bfacd7af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "cf_qPba9Xml5",
        "outputId": "171f7989-a4b2-4d0e-f972-e386bf39150b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [7],\n",
              "       ...,\n",
              "       [8],\n",
              "       [9],\n",
              "       [8]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prob_of_sampled = action_probs[sampled_actions, np.arange(len(sampled_actions))]"
      ],
      "metadata": {
        "id": "uz8_-jVyXxsY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_of_sampled.shape"
      ],
      "metadata": {
        "id": "0lcjQa3_Xzok",
        "outputId": "2304ec66-77ac-437a-d8fd-b91d3410b278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(r.shape)"
      ],
      "metadata": {
        "id": "j5715WEzd6ad",
        "outputId": "a694b59f-31de-40d4-ce36-ff1435578f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def np_softmax(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = x / tau\n",
        "  # Shift x by subtracting the max value to prevent overflow in exp\n",
        "  x_shifted = x_scaled - np.max(x_scaled, axis=0, keepdims=True)\n",
        "  exps = np.exp(x_shifted)\n",
        "  # Normalize the exponentials while maintaining batch structure\n",
        "  softmax_output = exps / np.sum(exps, axis=0, keepdims=True)\n",
        "  return softmax_output\n",
        "\n",
        "def eval_params_multi(W, x, y, tau=1, rng=None):\n",
        "  # W shape is num_action x input\n",
        "  # x shape is input x batch\n",
        "  # y shape is 1 x batch\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  z = np.dot(W, x)  # num_action x batch\n",
        "  action_probs = np_softmax(z, tau)  # num_action x batch\n",
        "  cumulative_probs = np.cumsum(action_probs, axis=0)\n",
        "  random_samples = rng.random(size=cumulative_probs.shape[1])\n",
        "  sampled_actions = (cumulative_probs > random_samples).argmax(axis=0)\n",
        "  # calculate which actions were correct and compute reward\n",
        "  correct = sampled_actions == y\n",
        "  r = np.zeros_like(y)\n",
        "  r[correct] = 1\n",
        "  r[~correct] = -1  # sampled reward\n",
        "  # Create an outcomes matrix and calculate expected reward\n",
        "  outcomes_matrix = -np.ones_like(action_probs)\n",
        "  # Set reward to +1 at the position of the correct action for each sample\n",
        "  outcomes_matrix[y, np.arange(y.size)] = 1\n",
        "  r_exp = np.sum(action_probs * outcomes_matrix, axis=0)  # expected reward\n",
        "  return z, action_probs, sampled_actions, r, outcomes_matrix, r_exp\n",
        "z, action_probs, sampled_actions, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y_batch)\n",
        "num_actions, batch_size = action_probs.shape\n",
        "num_features = x.shape[0]\n",
        "sampled_action_probs = action_probs[sampled_actions, np.arange(batch_size)]  # Shape [batch_size]\n",
        "broadcast_sampled_action_probs = np.broadcast_to(sampled_action_probs.reshape(1,batch_size), (num_actions, batch_size))\n",
        "# Compute updates for case when row (i) of W does not correspond to the sampled action\n",
        "delta_W = np.zeros((num_actions, num_features, batch_size))\n",
        "delta_W -= r[np.newaxis,:,:] * broadcast_sampled_action_probs[:,np.newaxis,:] * action_probs[:,np.newaxis,:] * x[np.newaxis,:,:]  # Shape [num_actions, num_features, batch_size]\n",
        "# now compute updates for case when row (i) of W does correspond to the sampled action\n",
        "mask = np.arange(num_actions)[:, None] == sampled_actions[None, :]  # [num_actions, batch_size]\n",
        "mask = np.broadcast_to(mask[:,np.newaxis,:], delta_W.shape) # [num_actions, num_features, batch_size]\n",
        "positive_update = r[np.newaxis,:,:] * (action_probs * (1 - action_probs))[:,np.newaxis,:] * x[np.newaxis,:,:]  # [num_features, num_actions, batch_size]\n",
        "# Use positive update where appropriate\n",
        "delta_W[mask] = positive_update[mask]\n",
        "\n",
        "print(delta_W.shape)"
      ],
      "metadata": {
        "id": "ZXs_OWvpa-ZT",
        "outputId": "ae026a60-7335-42b8-e1d6-c8372584a5ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 65, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_delta_W_computation():\n",
        "  # Seed for reproducibility\n",
        "  rng = np.random.default_rng(seed=42)\n",
        "\n",
        "  # Setup sample data\n",
        "  num_actions = 3\n",
        "  num_features = 5\n",
        "  batch_size = 4\n",
        "\n",
        "  # Initialize weights W randomly\n",
        "  W = rng.normal(size=(num_actions, num_features))\n",
        "\n",
        "  # Create random inputs and labels\n",
        "  x = rng.normal(size=(num_features, batch_size))\n",
        "  y = rng.integers(low=0, high=num_actions, size=(1,batch_size))\n",
        "\n",
        "  # Parameters\n",
        "  tau = 1.0  # Default temperature\n",
        "\n",
        "  # Execute the eval_params_multi to get necessary variables\n",
        "  z, action_probs, sampled_actions, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
        "\n",
        "  num_actions, batch_size = action_probs.shape\n",
        "  num_features = x.shape[0]\n",
        "  sampled_action_probs = action_probs[sampled_actions, np.arange(batch_size)]  # Shape [batch_size]\n",
        "  broadcast_sampled_action_probs = np.broadcast_to(sampled_action_probs.reshape(1,batch_size), (num_actions, batch_size))\n",
        "  # Compute updates for case when row (i) of W does not correspond to the sampled action\n",
        "  delta_W = np.zeros((num_actions, num_features, batch_size))\n",
        "  delta_W -= r[np.newaxis,:,:] * broadcast_sampled_action_probs[:,np.newaxis,:] * action_probs[:,np.newaxis,:] * x[np.newaxis,:,:]  # Shape [num_actions, num_features, batch_size]\n",
        "  # now compute updates for case when row (i) of W does correspond to the sampled action\n",
        "  mask = np.arange(num_actions)[:, None] == sampled_actions[None, :]  # [num_actions, batch_size]\n",
        "  mask = np.broadcast_to(mask[:,np.newaxis,:], delta_W.shape) # [num_actions, num_features, batch_size]\n",
        "  positive_update = r[np.newaxis,:,:] * (action_probs * (1 - action_probs))[:,np.newaxis,:] * x[np.newaxis,:,:]  # [num_features, num_actions, batch_size]\n",
        "  # Use positive update where appropriate\n",
        "  delta_W[mask] = positive_update[mask]\n",
        "  # Testing delta_W computation\n",
        "  delta_W_test = np.zeros_like(delta_W)\n",
        "  for i in range(batch_size):\n",
        "    for j in range(num_actions):\n",
        "      if j == sampled_actions[i]:\n",
        "        delta_W_test[j,:,i] = r[0,i] * action_probs[j, i] * (1 - action_probs[j, i]) * x[:, i]\n",
        "      else:\n",
        "        delta_W_test[j,:,i] -= r[0,i] * action_probs[sampled_actions[i], i] * action_probs[j, i] * x[:, i]\n",
        "\n",
        "  # Check if computed and tested delta_W are almost equal\n",
        "  assert np.allclose(delta_W, delta_W_test), \"Delta W computation mismatch\"\n",
        "\n",
        "  print(\"Delta W computation test passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_delta_W_computation()"
      ],
      "metadata": {
        "id": "I1y41XA0jSBl",
        "outputId": "455a7bdb-eacf-4bc9-af3a-5d2fa280c358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delta W computation test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_probs.shape"
      ],
      "metadata": {
        "id": "cHqAii9mYFgg",
        "outputId": "0be45f16-483f-4528-e8be-3ed1fc0673df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(action_probs * prob_of_sampled).shape"
      ],
      "metadata": {
        "id": "eX5zuERVYKSq",
        "outputId": "fef00da1-9476-4d90-f862-affd8b7f1825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.T.shape"
      ],
      "metadata": {
        "id": "9jnb8gs0YZ8o",
        "outputId": "11378a4b-b012-42c6-e4fa-9900684fbd69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "((action_probs * prob_of_sampled) @ x.T).shape"
      ],
      "metadata": {
        "id": "kkq9_8gMYhBb",
        "outputId": "20139270-3e3e-4d53-cd9e-64b36c869778",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 65)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def action_prob_step(W, x, y,\n",
        "                     cheat=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  z, action_probs, sampled_actions, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
        "\n",
        "  # learn only from the actual action taken and reward recieved\n",
        "  # Compute prob of the action which was sampled\n",
        "  prob_of_sampled = action_probs[sampled_actions, np.arange(len(sampled_actions))]\n",
        "  # Case i != k\n",
        "  negative_update = - r * prob_of_sampled * action_probs * x.T\n",
        "  # Case i == k\n",
        "  positive_update = r * prob_of_sampled * (1 - prob_of_sampled) * x[:, None]\n",
        "\n",
        "  # Create a mask for the batch indicating the sampled action\n",
        "  mask = (np.arange(z.shape[0])[:, None] == sampled_actions)\n",
        "  # Combine updates using the mask\n",
        "  delta_W = np.where(mask, positive_update, negative_update)"
      ],
      "metadata": {
        "id": "3UOrJRYLWOCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = np.random.rand(3, 5)  # 3 classes, 5 batch instances\n",
        "P = P / P.sum(axis=0, keepdims=True)  # Normalizing to get valid softmax probabilities\n",
        "\n",
        "# Transpose P to [B, K] for easier manipulation\n",
        "P = P.T\n",
        "\n",
        "# Create a batch of identity matrices scaled by each instance in P\n",
        "B, K = P.shape\n",
        "I = np.eye(K).reshape(1, K, K)  # Create a single [K, K] identity matrix\n",
        "diag_p = I * P[:, :, None]  # Broadcasting to shape [B, K, K]\n",
        "\n",
        "# Compute the outer products using einsum\n",
        "outer_p = np.einsum('bi,bj->bij', P, P)  # Resulting shape [B, K, K]\n",
        "\n",
        "# Compute the Jacobian for each instance in the batch\n",
        "J_batch = diag_p - outer_p\n",
        "\n",
        "print(\"Batch of Jacobians:\")\n",
        "print(J_batch)"
      ],
      "metadata": {
        "id": "-wPkjVbe1APM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mini_batch_size = Xs_aug.T.shape[0]\n",
        "cooling_rate = 0.04\n",
        "W_init = np.random.rand(10,65)\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "\n"
      ],
      "metadata": {
        "id": "wOBU73KPnPrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.dot(W_init, batch_x )"
      ],
      "metadata": {
        "id": "08lBz6ylnwXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.shape"
      ],
      "metadata": {
        "id": "td1rz2QcoCLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_probs = np_softmax(z)"
      ],
      "metadata": {
        "id": "VambgXIwoFwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_probs.shape"
      ],
      "metadata": {
        "id": "QCjhkg38oI6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(a_probs[:,0])"
      ],
      "metadata": {
        "id": "QFsFgecHoMbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cum_probs = np.cumsum(a_probs, axis=0)"
      ],
      "metadata": {
        "id": "5c7tWk7NoVvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cum_probs.shape"
      ],
      "metadata": {
        "id": "G33ap2VJocZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cum_probs[:,0]"
      ],
      "metadata": {
        "id": "YqxMEgx-oeVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_samples = rng.random(size=cum_probs.shape[1])"
      ],
      "metadata": {
        "id": "1zcGmbZ3orZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_samples.shape"
      ],
      "metadata": {
        "id": "5YGwC7VKouzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_actions = (cum_probs > random_samples).argmax(axis=0)"
      ],
      "metadata": {
        "id": "diIzpH_ApDx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(sampled_actions == y.T)"
      ],
      "metadata": {
        "id": "0CDOVB_9pF9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1000\n",
        "num_steps = 0\n",
        "mini_batch_size = Xs_aug.T.shape[0]\n",
        "cooling_rate = 0.04\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "cheat_alg_lrs = {'Reward Prediction': 0.0015,\n",
        "                 'Action Probability': 0.08,\n",
        "                 'Perturb Measure': 0.0008}\n",
        "cheat_actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "cheat_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = cheat_alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      if alg_name == 'Reward Prediction':\n",
        "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
        "      else:\n",
        "        tau = 1.0\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      r = np.mean(r)\n",
        "      r_exp_full = np.mean(r_exp_full)\n",
        "      cheat_actual_reward_results[alg_name].append(r)\n",
        "      cheat_exp_reward_results[alg_name].append(r_exp_full)\n",
        "    num_steps += 1\n",
        "    if num_steps > 1000:\n",
        "      break\n",
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    eval = np.array(cheat_exp_reward_results[alg_name])\n",
        "    eval = np.cumsum(eval)\n",
        "    eval = eval / (np.arange(len(eval)) + 1)\n",
        "    ax.plot(eval, label=f'{alg_name}-(Cheating Full, Batch)')\n",
        "    ax.set_title(f'Cumulative per Episode Average of Expected (Full Batch) Reward')\n",
        "    ax.set_xlabel('Learning Episodes')\n",
        "    ax.set_ylabel('Cumulative Avg. Expected (Full Batch) Reward')\n",
        "    ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    eval = cheat_actual_reward_results[alg_name]\n",
        "    eval = np.cumsum(eval)\n",
        "    eval = eval / (np.arange(len(eval)) + 1)\n",
        "    ax.plot(eval, label=f'{alg_name} (Cheating Full Batch)')\n",
        "    ax.set_title(f'Cumulative Average of Actual Reward Per Learning Episode')\n",
        "    ax.set_xlabel('Learning Episodes')\n",
        "    ax.set_ylabel('Cumulative Avg. Actual Reward')\n",
        "    ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "D4R0-e1-bB-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  strike_prob = np_sigmoid(a, tau)\n",
        "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
        "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  r_exp = strike_prob * r_all # + (1-out_spike_prob) * 0 # expected reward\n",
        "  return a, strike_prob, strike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           cheat=False,\n",
        "                           tau=1.0,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.0001):\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn regardless of whether organism actually strikes or not\n",
        "    update = (r_all - a) * x\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    update = (r-a) * strike * x\n",
        "  # average the update over all elements in the batch\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y, r_hat=0,\n",
        "                     cheat=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  a, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if cheat:\n",
        "    # learn using expected reward\n",
        "    update = (r_exp - r_hat) * (strike_prob) * (1 -  strike_prob) * x\n",
        "  else: #realishtic\n",
        "    # learn only from actual recieved rewards\n",
        "    # reward of zero recieved when not striking\n",
        "    update = (r-r_hat) * (strike_prob) * (1 - strike_prob) * x\n",
        "  update = np.mean(update, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp"
      ],
      "metadata": {
        "id": "7qeIrz4zHzZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "#cooling_rate = 0.00001\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "cheat_alg_lrs = {'Reward Prediction': 0.0001,\n",
        "                 'Action Probability': 0.003,\n",
        "                 'Perturb Measure': 0.0000001}\n",
        "cheat_actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "cheat_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
        "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr = cheat_alg_lrs[alg_name]\n",
        "      W = W_s[alg_name]\n",
        "      #if alg_name == 'Reward Prediction':\n",
        "      #  tau = 1/((num_steps+10.0) * cooling_rate)\n",
        "      #else:\n",
        "      #  tau = 1.0\n",
        "      #if alg_name == 'Perturb Measure':\n",
        "      #  new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, perturbation_scale=0.01, cheat=False, rng=learn_rng, learning_rate=lr)\n",
        "      #else:\n",
        "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, cheat=False, rng=learn_rng, learning_rate=lr)\n",
        "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "      W_s[alg_name] = new_W\n",
        "      r = np.mean(r)\n",
        "      r_exp_full = np.mean(r_exp_full)\n",
        "      cheat_actual_reward_results[alg_name].append(r)\n",
        "      cheat_exp_reward_results[alg_name].append(r_exp_full)\n",
        "    num_steps += 1\n",
        "with plt.xkcd():\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    exp_r = np.array(cheat_exp_reward_results[alg_name])\n",
        "    cum_exp_r = np.cumsum(exp_r)\n",
        "    avg_exp_r = cum_exp_r / (np.arange(len(exp_r)) + 1)\n",
        "    ax.plot(avg_exp_r, label=f'{alg_name}-(Less Cheating)')\n",
        "    ax.set_title(f'Cumulative per Episode Average of Expected (Full Batch) Reward')\n",
        "    ax.set_xlabel('Learning Episodes')\n",
        "    ax.set_ylabel('Cumulative Avg. Expected (Full Batch) Reward')\n",
        "    ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(10, 6))\n",
        "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for alg_name in alg_names:\n",
        "    r = cheat_actual_reward_results[alg_name]\n",
        "    cum_r = np.cumsum(r)\n",
        "    avg_r = cum_r / (np.arange(len(r)) + 1)\n",
        "    ax.plot(avg_r, label=f'{alg_name}-(Less Cheating)')\n",
        "    ax.set_title(f'Cumulative Average of Actual Reward Per Learning Episode')\n",
        "    ax.set_xlabel('Learning Episodes')\n",
        "    ax.set_ylabel('Cumulative Avg. Actual Reward')\n",
        "    ax.legend()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Dy8kagpMmQU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cheat_exp_cum_avg_r"
      ],
      "metadata": {
        "id": "YCRcEd9mreQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "for alg_name in alg_names:\n",
        "  smoothed_values = cheat_cum_avg_r[alg_name]\n",
        "  ax.plot(smoothed_values, label=f'{alg_name}')\n",
        "  ax.set_title(f'Cumulative Reward')\n",
        "  ax.set_xlabel('Steps')\n",
        "  ax.set_ylabel('Cumulative Avg. Reward per Episode')\n",
        "  ax.legend()\n",
        "\n",
        "# Ensure the layout is clean and no subplot titles or axes are overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "75xBUppoS0Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spike = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  r = np.where(y == 1, out_spike, -out_spike) # sampled reward\n",
        "  r_all = 2*y-1 # reward when striking in all cases\n",
        "  #expected reward\n",
        "  r_exp = out_spike_prob * r_all # + (1-out_spike_prob) * 0\n",
        "  return a, out_spike_prob, out_spike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           reinforce_exp=False,\n",
        "                           tau=1.0,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_all-a) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-a) * out_spike * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y, r_hat=0,\n",
        "                     reinforce_exp=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_exp-r_hat) * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-r_hat) * out_spike * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.0001,\n",
        "                         reinforce_exp=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    directional_grad_est = (r_exp_perturb - r_exp) / perturbation_scale\n",
        "  else:\n",
        "    directional_grad_est = (r_perturb - r) / perturbation_scale\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 1\n",
        "num_steps = 0\n",
        "mini_batch_size = 1\n",
        "W_init = np.zeros((1,65))\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
        "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
        "lr_ranges = {'Reward Prediction': [0.0002, 0.0001, 0.000005],\n",
        "             'Action Probability': [0.02, 0.008, 0.002],\n",
        "             'Perturb Measure': [0.017, 0.016, 0.015]}\n",
        "reward_results = {alg_name: {lr: [] for lr in lr_ranges[alg_name]} for alg_name in alg_names}\n",
        "smoothed_results = {alg_name: {lr: [] for lr in lr_ranges[alg_name]} for alg_name in alg_names}\n",
        "W_s = {alg_name: {lr: W_init for lr in lr_ranges[alg_name]} for alg_name in alg_names}\n",
        "#alpha = 0.05\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
        "      lr_range = lr_ranges[alg_name]\n",
        "      for lr_idx, lr in enumerate(lr_range):\n",
        "        W = W_s[alg_name][lr]\n",
        "        new_W, r, r_exp = alg_func(W, batch_x, batch_y, reinforce_exp=True, rng=learn_rng, learning_rate=lr)\n",
        "        _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
        "        W_s[alg_name][lr] = new_W\n",
        "        reward_results[alg_name][lr].append(np.mean(r_exp_full))\n",
        "        smoothed_results[alg_name][lr].append(np.mean(reward_results[alg_name][lr]))\n",
        "    num_steps += 1\n",
        "n_algorithms = len(alg_names)\n",
        "cmap = plt.cm.viridis\n",
        "fig, axes = plt.subplots(n_algorithms, 1, figsize=(10, 6 * n_algorithms), sharex=True)\n",
        "theoretical_max = np.sum(y1 == 1) / len(y1)\n",
        "for i, alg_name in enumerate(alg_names):\n",
        "  ax = axes[i] if n_algorithms > 1 else axes\n",
        "  learning_rates = np.array(lr_ranges[alg_name])\n",
        "  norm = plt.Normalize(learning_rates.min(), learning_rates.max())\n",
        "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
        "  for lr in learning_rates:\n",
        "    smoothed_values = smoothed_results[alg_name][lr]\n",
        "    ax.plot(smoothed_values, label=f'LR={lr}', color=cmap(norm(lr)))\n",
        "    # Set the title, labels and legend\n",
        "  ax.set_title(f'{alg_name}')\n",
        "  ax.set_xlabel('Steps')\n",
        "  ax.set_ylabel('Cumulative Expected Reward per Episode')\n",
        "  ax.legend()\n",
        "\n",
        "# Ensure the layout is clean and no subplot titles or axes are overlapping\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ETlggGbatIZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMIda-42WXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So as a caveat, direct comparison of learning algorithms is difficult. Maybe different meta-parmeters (learning rates, tua values) would make one algorithm come out better than the other. Here where we are focused on aquiring pretty good behaviour as quickly as possible (as contrasted with obtaining as good as possible final performance, with-in a feasible"
      ],
      "metadata": {
        "id": "wu4hhkg-vG0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y1 == 0)"
      ],
      "metadata": {
        "id": "MuYJH9guU5ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y1 == 1)"
      ],
      "metadata": {
        "id": "X82CBfzatGW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(y1 == 1) / len(y1)"
      ],
      "metadata": {
        "id": "s4o2SqIvU9Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_ap"
      ],
      "metadata": {
        "id": "a9vheRl2tJAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def np_sigmoid(x, tau=1):\n",
        "  # high tau more exploration, low tau very little exploration\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "def eval_params(W, x, y, tau=1, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spike = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  r = np.where(y == 1, out_spike, -out_spike)\n",
        "  r_all = 2*y-1\n",
        "  r_exp = out_spike_prob * r_all\n",
        "  return a, out_spike_prob, out_spike, r, r_all, r_exp\n",
        "\n",
        "def reward_prediction_step(W, x, y,\n",
        "                           reinforce_exp=False,\n",
        "                           tau=1,\n",
        "                           rng=None,\n",
        "                           learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_all-a) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-a) * out_spike * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def action_prob_step(W, x, y, r_hat=0,\n",
        "                     reinforce_exp=False,\n",
        "                     tau=1,\n",
        "                     rng=None,\n",
        "                     learning_rate=0.001):\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    update = np.mean((r_exp-r_hat) * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  else:\n",
        "    update = np.mean((r-r_hat) * out_spike * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "  W_new = W + update * learning_rate\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "def perturb_measure_step(W, x, y,\n",
        "                         perturbation_scale=0.001,\n",
        "                         reinforce_exp=False,\n",
        "                         tau=1,\n",
        "                         rng=None,\n",
        "                         learning_rate=0.001):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a, out_spike_prob, out_spike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  perturbed_W = W + test_perturbation\n",
        "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
        "  if reinforce_exp:\n",
        "    directional_grad_est = (r_exp_perturb - r_exp) / perturbation_scale\n",
        "  else:\n",
        "    directional_grad_est = (r_perturb - r) / perturbation_scale\n",
        "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
        "  W_new = W + update\n",
        "  return W_new, r, r_exp\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 2\n",
        "lr_rp = 0.001\n",
        "lr_ap = 0.005\n",
        "lr_pm = 0.005\n",
        "perturbation_scale = 0.0001\n",
        "mini_batch_size = 1\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W_rp = W_init\n",
        "W_ap = W_init\n",
        "W_pm = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "num_steps = 0\n",
        "rewards_rp = []\n",
        "rewards_ap = []\n",
        "rewards_pm = []\n",
        "smoothed_rp = []\n",
        "smoothed_ap = []\n",
        "smoothed_pm = []\n",
        "alpha = 0.001\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W_rp, r_rp, r_rp_exp = reward_prediction_step(W_rp, batch_x, batch_y, reinforce_exp=True, tau=tau, rng=learn_rng, learning_rate=lr_rp)\n",
        "    W_ap, r_ap, r_ap_exp = action_prob_step(W_ap, batch_x, batch_y, reinforce_exp=True, tau=tau, rng=learn_rng, learning_rate=lr_ap)\n",
        "    W_pm, r_pm, r_pm_exp = perturb_measure_step(W_pm, batch_x, batch_y, reinforce_exp=True, tau=1, rng=learn_rng, learning_rate=lr_pm)\n",
        "    num_steps += 1\n",
        "    rewards_rp.append(np.mean(r_rp))\n",
        "    rewards_ap.append(np.mean(r_ap))\n",
        "    rewards_pm.append(np.mean(r_pm))\n",
        "\n",
        "    if len(smoothed_rp) == 0:\n",
        "      smoothed_rp.append(rewards_rp[-1])\n",
        "      smoothed_ap.append(rewards_ap[-1])\n",
        "      smoothed_pm.append(rewards_pm[-1])\n",
        "    else:\n",
        "      smoothed_rp.append(alpha * rewards_rp[-1] + (1 - alpha) * smoothed_rp[-1])\n",
        "      smoothed_ap.append(alpha * rewards_ap[-1] + (1 - alpha) * smoothed_ap[-1])\n",
        "      smoothed_pm.append(alpha * rewards_pm[-1] + (1 - alpha) * smoothed_pm[-1])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))  # Create a figure and an axes.\n",
        "ax.plot(smoothed_rp, label='Reward Prediction', color='blue', alpha=0.5)\n",
        "ax.plot(smoothed_ap, label='Action Probability', color='red', alpha=0.5)\n",
        "ax.plot(smoothed_pm, label='Perturb Measure', color='green', alpha=0.5)\n",
        "\n",
        "ax.set_title('(Smoothed) Average Rewards')  # Title of the plot\n",
        "ax.set_xlabel('Steps')  # Label for the x-axis\n",
        "ax.set_ylabel('Average Reward')  # Label for the y-axis\n",
        "ax.legend()  # Add a legend\n",
        "\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "854bVhbjOuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)"
      ],
      "metadata": {
        "id": "EY7zU206eGh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "print(Xs_aug.shape)\n",
        "print(Xs_aug[:1])"
      ],
      "metadata": {
        "id": "S05lN-I9cmgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = 0.2 * (learn_rng.random(size=(1,65)) - 0.5)\n",
        "print(W)\n",
        "x = Xs_aug[:5].T\n",
        "print(x)"
      ],
      "metadata": {
        "id": "T9zsErHTiveg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.dot(W, x)\n",
        "print(a)\n",
        "out_spike_prob = np_sigmoid(a)\n",
        "print(out_spike_prob)"
      ],
      "metadata": {
        "id": "JH63UEmYdFKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_spikes = np.array(learn_rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "print(out_spikes)"
      ],
      "metadata": {
        "id": "eOmGv02Ae_vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y1[:5].T\n",
        "print(y)"
      ],
      "metadata": {
        "id": "ovnnZwjzf_Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " R = np.where(y == 1, out_spikes, -out_spikes)\n",
        " print(R)"
      ],
      "metadata": {
        "id": "CIWRpmzmf1A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_R = np.where(y == 1, 1, -1)\n",
        "print(all_R)"
      ],
      "metadata": {
        "id": "8Vyb4wuDhzL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = np.mean(np.abs(all_R - a))\n",
        "print(mae)"
      ],
      "metadata": {
        "id": "JpuLMy4fiBSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "id": "8nfWOFHbgGL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y-a"
      ],
      "metadata": {
        "id": "VX66Mh_Ubul9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_R - a"
      ],
      "metadata": {
        "id": "u5Ie8RIobxDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(R)"
      ],
      "metadata": {
        "id": "hOXHtaKahQxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_pass(W, x, y, tau=1, rng=None, learning_rate=0.001, verbose=False):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spikes = np.array(learn_rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  all_R = np.where(y == 1, 1, -1)\n",
        "  mae = np.mean(np.abs(all_R - a))\n",
        "  if verbose:\n",
        "    R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "    did_strike = out_spikes == 1\n",
        "    did_not_strike = out_spikes == 0\n",
        "    should_strike = y == 1\n",
        "    should_not_strike = y == 0\n",
        "    TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "    FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "    FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "    TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "    R[TP] = 1\n",
        "    R[FP] = -1\n",
        "    R[FN] = 0\n",
        "    R[TN] = 0\n",
        "    TPs = np.sum(TP)\n",
        "    FPs = np.sum(FP)\n",
        "    FNs = np.sum(FN)\n",
        "    TNs = np.sum(TN)\n",
        "    confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    print(f'Mean Absolute Error: {mae}')\n",
        "    return None\n",
        "  else:\n",
        "    R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "    # only learn from strikes taken\n",
        "    update = np.mean((R-a) * out_spikes * x, axis=1, keepdims=True).T\n",
        "    # always learn\n",
        "    #update = np.mean((all_R-a) * x, axis=1, keepdims=True).T\n",
        "    W_new = W + update * learning_rate\n",
        "    return W_new, a, out_spikes, R, mae\n",
        "\n",
        "forward_backward_pass(W, x, y, tau=1, rng=learn_rng, learning_rate=0.00001)"
      ],
      "metadata": {
        "id": "_Yfp2tPvi3Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_backward_pass(W, x, y, tau=1, rng=learn_rng, learning_rate=0.00001)"
      ],
      "metadata": {
        "id": "voz_8TAZamtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 200\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "W_init = 0.2 * (learn_rng.random(size=(1,65)) - 0.5)\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W, _, _, _, _ = forward_backward_pass(W, batch_x, batch_y, tau=tau, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "  if epoch % 10 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_y = y1.T\n",
        "    _, _, _, batch_R, batch_mae = forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "    print(f'Epoch {epoch} | Reward Sample: {np.sum(batch_R)} | Strike Reward MAE: {batch_mae} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "forward_backward_pass(W, Xs_aug.T, y1.T, tau=0.0001, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "yz6fy__PgN3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems to get in the neighbourhood of 2300, so not as good as perturb measure step. But why? Because here we are minimizing a prediction error, which is only closely correleated with correctly choosing the right action as often as possible. So while this prediction mis-match could be a nice way for learning to occur, if we really want to maximize reward, we need to make sure that reward is what we are most directly maximizing. To do this we need to change our update rule so that we directly increase the probability of striking when a strike happens and it is rewarded, and decrease the probability of striking when it happens and a negative reward results."
      ],
      "metadata": {
        "id": "Znyp5dxHdbXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Action Probability Reinforcement\n",
        "\n",
        "$$ \\Delta W_i = s r \\sigma'(a) x_i$$\n",
        "\n",
        "or in vector form\n",
        "\n",
        "$$ \\Delta \\mathbf{W} = s r \\sigma'(a) \\mathbf{x}$$"
      ],
      "metadata": {
        "id": "G7wZDUgWewB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal here is to more directly associate action with reward, as contrasted with doing this inderectly by tying action probabilities to a reward prediction."
      ],
      "metadata": {
        "id": "FwgSmE63f2xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward_pass(W, x, y, tau=1, rng=None, learning_rate=0.001, verbose=False):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  a = np.dot(W, x)\n",
        "  out_spike_prob = np_sigmoid(a, tau)\n",
        "  out_spikes = np.array(learn_rng.random(size=out_spike_prob.shape) < out_spike_prob, int)\n",
        "  all_R = np.where(y == 1, 1, -1)\n",
        "  if verbose:\n",
        "    R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "    did_strike = out_spikes == 1\n",
        "    did_not_strike = out_spikes == 0\n",
        "    should_strike = y == 1\n",
        "    should_not_strike = y == 0\n",
        "    TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "    FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "    FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "    TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "    R[TP] = 1\n",
        "    R[FP] = -1\n",
        "    R[FN] = 0\n",
        "    R[TN] = 0\n",
        "    TPs = np.sum(TP)\n",
        "    FPs = np.sum(FP)\n",
        "    FNs = np.sum(FN)\n",
        "    TNs = np.sum(TN)\n",
        "    confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "    # only learn from strikes taken\n",
        "    # reward probability based updated\n",
        "    update = np.mean(R * out_spikes * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "    # always learn\n",
        "    #update = np.mean(all_R * (out_spike_prob) * (1 - out_spike_prob) * x, axis=1, keepdims=True).T\n",
        "    W_new = W + update * learning_rate\n",
        "    return W_new, a, out_spikes, R\n",
        "\n",
        "forward_backward_pass(W, x, y, tau=1, rng=learn_rng, learning_rate=0.00001)"
      ],
      "metadata": {
        "id": "eUvs2c-Y9S4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 200\n",
        "learning_rate = 0.0002\n",
        "mini_batch_size = 1\n",
        "W_init = 0.2 * (learn_rng.random(size=(1,65)) - 0.5)\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_y = y1.T\n",
        "forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = 1/(epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W, _, _, _ = forward_backward_pass(W, batch_x, batch_y, tau=tau, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "  if epoch % 20 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_y = y1.T\n",
        "    _, _, _, batch_R = forward_backward_pass(W, batch_x, batch_y, tau=0.0001, rng=learn_rng, learning_rate=learning_rate, verbose=False)\n",
        "    print(f'Epoch {epoch} | Reward Sample: {np.sum(batch_R)} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "forward_backward_pass(W, Xs_aug.T, y1.T, tau=0.0001, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "hboWkIa-g73C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is basically the same as perturb measure step, except that it doesn't require a seperate purturbation mode"
      ],
      "metadata": {
        "id": "7hUcs6nfi_81"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KbxBKtlPi_RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to see how a simple stochastic spiking network can learn to solve this problem\n",
        "\n",
        "def np_sigmoid(x, tau=1):\n",
        "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x/tau))\n",
        "\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def delta_rule_batch(W, x, y, tau, learning_rate, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise, used delta rule\n",
        "  based on reward perdiction to compute updates\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activation level given input\n",
        "  a = np.dot(W, x) # 1 x batch\n",
        "  # strike probability\n",
        "  out_spike_prob = np_sigmoid(a, tau) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  out_spikes = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)  # 1 x batch\n",
        "  R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "  did_strike = out_spikes == 1\n",
        "  did_not_strike = out_spikes == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "  updates = learning_rate * (R-a) * out_spikes * x #inuput dim x batch\n",
        "  mean_squared_reward_prediction_errors = np.mean((y - a)**2)\n",
        "  if verbose:\n",
        "    TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "    FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "    FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "    TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "    R[TP] = 1\n",
        "    R[FP] = -1\n",
        "    R[FN] = 0\n",
        "    R[TN] = 0\n",
        "    TPs = np.sum(TP)\n",
        "    FPs = np.sum(FP)\n",
        "    FNs = np.sum(FN)\n",
        "    TNs = np.sum(TN)\n",
        "    confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), updates, mean_squared_reward_prediction_errors\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "W_init = np.zeros((1,65))\n",
        "tau_init = 5\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  tau = tau_init/(10*epoch+1)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_y = y1[batch_indices].T\n",
        "    _, W_update, _ = delta_rule_batch(W, batch_x, batch_y, tau, learning_rate, rng=learn_rng)\n",
        "    W = W + W_update.T\n",
        "  if epoch % 2 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_y = y1.T\n",
        "    batch_reward, _, mean_squared_reward_prediction_errors = delta_rule_batch(W, batch_x, batch_y, tau, learning_rate, rng=learn_rng)\n",
        "    print(f'Epoch {epoch} | Batch Reward: {batch_reward} | Avg. Prediction Error {mean_squared_reward_prediction_errors}|Time elapsed: {elapsed_time:.2f} seconds')\n",
        "delta_rule_batch(W, Xs_aug.T, y1.T, tau, learning_rate, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "b2iezpbdL0oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to see how a simple stochastic spiking network can learn to solve this problem\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -200, 200) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def update_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise, input and output\n",
        "  neurons are spiking\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # input spike probs\n",
        "  squash_x = np_sigmoid(x) # inputs x batch\n",
        "  # activation level given input\n",
        "  a = np.dot(W, squash_x) # 1 x batch\n",
        "  # strike probability\n",
        "  out_spike_prob = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  out_spikes = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)  # 1 x batch\n",
        "  R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "  did_strike = out_spikes == 1\n",
        "  did_not_strike = out_spikes == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  updates = learning_rate * R * squash_x #inuput dim x batch\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix, updates\n",
        "\n",
        "\n",
        "  return replicated_x\n",
        "\n",
        "def compact_update_stochastic_spiking_batch(W, x, y, learning_rate, rng=None):\n",
        "  \"\"\"\n",
        "  Evaluates parameters of a simple behaviour circuit given inputs and target outputs.\n",
        "  Optimized for performance by focusing solely on update computations.\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "        weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "    learning_rate: scalar, the learning rate for updates\n",
        "\n",
        "  Returns:\n",
        "    updates: gradient updates for weight adjustments\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  squash_x = np_sigmoid(x)\n",
        "  a = np.dot(W, x)  # 1 x batch\n",
        "  out_spike_prob = np_sigmoid(a)  # Logistic sigmoid function\n",
        "  out_spikes = (rng.random(out_spike_prob.shape) < out_spike_prob).astype(int)\n",
        "  # Compute rewards\n",
        "  R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "  # Compute updates\n",
        "  updates = learning_rate * R * squash_x\n",
        "  return updates\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "input_replications = 100\n",
        "W_init = np.zeros((1,65*input_replications))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "batch_y = y1.T\n",
        "update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W_update = compact_update_stochastic_spiking_batch(W, batch_x, batch_y, learning_rate, rng=learn_rng)\n",
        "    W = W + W_update.T\n",
        "  if epoch % 2 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1.T\n",
        "    batch_reward, confusion_matrix, W_update = update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng)\n",
        "    print(f'Epoch {epoch} completed | Batch Reward Sample: {batch_reward}|Time elapsed: {elapsed_time:.2f} seconds')\n",
        "update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "xcotlVflIrdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run this cell to see how a simple stochastic spiking network can learn to solve this problem\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -200, 200) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def update_stochastic_spiking_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise, input and output\n",
        "  neurons are spiking\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # input spike probs\n",
        "  in_spike_probs = np_sigmoid(x) # inputs x batch\n",
        "  # sampled spikes\n",
        "  in_spikes = np.array(rng.random(size=x.shape) < in_spike_probs, int)\n",
        "  # activation level given input spikes\n",
        "  a = np.dot(W, in_spikes) # 1 x batch\n",
        "  # strike probability\n",
        "  out_spike_prob = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  out_spikes = np.array(rng.random(size=out_spike_prob.shape) < out_spike_prob, int)  # 1 x batch\n",
        "  R = np.zeros(out_spikes.shape) # 1 x batch\n",
        "  did_strike = out_spikes == 1\n",
        "  did_not_strike = out_spikes == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  updates = learning_rate * R * out_spikes * in_spikes #inuput dim x batch\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix, updates\n",
        "\n",
        "\n",
        "  return replicated_x\n",
        "\n",
        "def compact_update_stochastic_spiking_batch(W, x, y, learning_rate, rng=None):\n",
        "  \"\"\"\n",
        "  Evaluates parameters of a simple behaviour circuit given inputs and target outputs.\n",
        "  Optimized for performance by focusing solely on update computations.\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "        weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "    learning_rate: scalar, the learning rate for updates\n",
        "\n",
        "  Returns:\n",
        "    updates: gradient updates for weight adjustments\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # Compute input spikes\n",
        "  in_spike_probs = np_sigmoid(x)  # Logistic sigmoid function\n",
        "  in_spikes = (rng.random(x.shape) < in_spike_probs).astype(int)\n",
        "  # Compute output spikes\n",
        "  a = np.dot(W, in_spikes)  # 1 x batch\n",
        "  out_spike_prob = np_sigmoid(a)  # Logistic sigmoid function\n",
        "  out_spikes = (rng.random(out_spike_prob.shape) < out_spike_prob).astype(int)\n",
        "  # Compute rewards\n",
        "  R = np.where(y == 1, out_spikes, -out_spikes)\n",
        "  # Compute updates\n",
        "  updates = learning_rate * (R * out_spikes) * in_spikes\n",
        "  return updates\n",
        "\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 100\n",
        "learning_rate = 0.0001\n",
        "mini_batch_size = 1\n",
        "input_replications = 100\n",
        "W_init = np.zeros((1,65*input_replications))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "batch_x = Xs_aug.T\n",
        "batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "batch_y = y1.T\n",
        "update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng, verbose=True)\n",
        "for epoch in range(num_epochs):\n",
        "  learn_rng.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_x = Xs_aug[batch_indices].T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1[batch_indices].T\n",
        "    W_update = compact_update_stochastic_spiking_batch(W, batch_x, batch_y, learning_rate, rng=learn_rng)\n",
        "    W = W + W_update.T\n",
        "  if epoch % 2 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    batch_x = Xs_aug.T\n",
        "    batch_x = np.repeat(batch_x, input_replications, axis=0)\n",
        "    batch_y = y1.T\n",
        "    batch_reward, confusion_matrix, W_update = update_stochastic_spiking_batch(W, batch_x, batch_y, rng=learn_rng)\n",
        "    print(f'Epoch {epoch} completed | Batch Reward Sample: {batch_reward}|Time elapsed: {elapsed_time:.2f} seconds')\n",
        "update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "KC-J3m1AhRZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "id": "FPta8uE7A0xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewards = []\n",
        "for _ in range(100):\n",
        "  batch_reward, _, _ = update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng)\n",
        "  rewards.append(batch_reward)\n",
        "\n",
        "# Convert list to numpy array for better handling in matplotlib\n",
        "rewards_array = np.array(rewards)\n",
        "\n",
        "# Plotting the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(rewards_array, bins=30, color='blue', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eTT3vID_62vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.zeros((1,65))"
      ],
      "metadata": {
        "id": "OeSPA7D98lRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "update_stochastic_spiking_batch(W, Xs_aug.T, y1.T, rng=learn_rng, verbose=True)"
      ],
      "metadata": {
        "id": "Ws4BK0Qd8XwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "id": "aMujOt-k8esw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_single_action(W, x, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  pre_spike_probs = np_sigmoid(x)\n",
        "  input_spikes = np.array(rng.random(size=x.shape) < pre_spike_probs, int)\n",
        "  # activaation\n",
        "  output_activation = np.dot(W,pre_spikes)\n",
        "  # strike probability, does the output spike\n",
        "  output_spike_prob = np_sigmoid(a)\n",
        "  sampled_action = rng.random() < y_hat\n",
        "  return sampled_action, input_spikes\n",
        "\n",
        "def stochastic_single_reward(a, y):\n",
        "  if a == 1:\n",
        "    if y == 1\n",
        "      R = 1\n",
        "    elif y == 0:\n",
        "      R = -1\n",
        "    else:\n",
        "      raise ValueError(f'Unknown target: {y}')\n",
        "  elif a == 0:\n",
        "    R = 0\n",
        "  else:\n",
        "    raise ValueError(f'Unknown action: {a}')\n",
        "  return R\n",
        "\n",
        "def stochastic_single_update(W, x, y, rng=None):\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  sampled_action, input_spikes = stochastic_single_action(W, x, rng)\n",
        "  reward = stochastic_single_reward(sampled_action, y)\n",
        "  W += learning_rate * sampled_action * input_spikes * reward\n",
        "  return W\n"
      ],
      "metadata": {
        "id": "QA745wjh4lR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The organism recieves a reward of 1 for striking at the right time and a penalty of -1 for striking at the wrong time."
      ],
      "metadata": {
        "id": "9fnPqbxRKPhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO for students: Complete the lines with ... set the appropriate rewards for\n",
        "# for the evaluations function\n",
        "raise NotImplementedError(\"Exercise: Set the reward for different outcomes\")\n",
        "################################################################################\n",
        "\n",
        "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
        "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
        "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
        "# After augmentation there is one extra column of features\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def eval_params_stochastic_single(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) np.array) sensory input\n",
        "    y: (outputs(1) np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x)\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a)\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random() < y_hat\n",
        "  if y_sample == 1: #organism strikes\n",
        "    if y == 1: #prey is present\n",
        "      R = ...\n",
        "    else: # prey is not present\n",
        "      R = ...\n",
        "  else: # organism does not strike\n",
        "    R = ...\n",
        "  if verbose:\n",
        "    print(f'Probability of striking: {y_hat}')\n",
        "    action_string = 'Strike' if y_sample == 1 else 'No Strike'\n",
        "    print(f'Action taken: {action_string}')\n",
        "    target_string = 'Strike' if y == 1 else 'No Strike'\n",
        "    print(f'Correct Action: {target_string}')\n",
        "    print(f'Reward recieved: {R}')\n",
        "  else:\n",
        "    return R\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "eval_params_stochastic_single(W_test, Xs_aug[0], y1[0], verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "dAB-xsyQNzFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
        "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
        "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
        "# After augmentation there is one extra column of features\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def eval_params_stochastic_single(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) np.array) sensory input\n",
        "    y: (outputs(1) np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x)\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a)\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random() < y_hat\n",
        "  if y_sample == 1: #organism strikes\n",
        "    if y == 1: #prey is present\n",
        "      R = 1\n",
        "    else: # prey is not present\n",
        "      R = -1\n",
        "  else: # organism does not strike\n",
        "    R = 0\n",
        "  if verbose:\n",
        "    print(f'Probability of striking: {y_hat}')\n",
        "    action_string = 'Strike' if y_sample == 1 else 'No Strike'\n",
        "    print(f'Action taken: {action_string}')\n",
        "    target_string = 'Strike' if y == 1 else 'No Strike'\n",
        "    print(f'Correct Action: {target_string}')\n",
        "    print(f'Reward recieved: {R}')\n",
        "  else:\n",
        "    return R\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "eval_params_stochastic_single(W_test, Xs_aug[0], y1[0], verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "mz00q3D8h1kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that evaluates the reward over a single experience. We can use numpy broadcasting to apply this same reward calculation efficiently to many, even all, the input-out pairs in our data set. We call this **batch** evaluation."
      ],
      "metadata": {
        "id": "gPh6sBUTnZnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO for students: Complete the lines with ... to compute the number of\n",
        "# True Positives, False Positives, True Negative and False Negatives in the batch\n",
        "raise NotImplementedError(\"Exercise: Compute the number of different Outcomes\")\n",
        "################################################################################\n",
        "\n",
        "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random(size=y_hat.shape) < y_hat  # 1 x batch\n",
        "  R = np.zeros(y_sample.shape)\n",
        "  did_strike = y_sample == ...\n",
        "  did_not_strike = y_sample == ...\n",
        "  should_strike = y == ...\n",
        "  should_not_strike = y == ...\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
        "# for broadcasting to work\n",
        "print('Evaluation 1')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)\n",
        "print('\\nEvaluation 2')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "CI-p0If_8ucJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to_remove solution\n",
        "\n",
        "# As a little trick to keep our code cleaner we 'hide' our bias term.\n",
        "# We to do this by augmenting the features to include a feature that always has the value '1'.\n",
        "# Then, the 'weight' associated with this feature, which always has a value of '1', effectively serves as the bias term.\n",
        "# After augmentation there is one extra column of features\n",
        "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
        "\n",
        "def np_sigmoid(x):\n",
        "  x = np.clip(x, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random(size=y_hat.shape) < y_hat  # 1 x batch\n",
        "  R = np.zeros(y_sample.shape)\n",
        "  did_strike = y_sample == 1\n",
        "  did_not_strike = y_sample == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix\n",
        "\n",
        "eval_rng = np.random.default_rng(0)\n",
        "W_test = np.zeros((1,65))\n",
        "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
        "# for broadcasting to work\n",
        "print('Evaluation 1')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)\n",
        "print('\\nEvaluation 2')\n",
        "eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=True, rng=eval_rng)"
      ],
      "metadata": {
        "id": "pMYaMDiYp9TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the two evaluations give different total rewards, even though the exact same synaptic weights $\\mathbf{W}$ are being used, on the exact same batch of inputs $\\mathbf{x}$ and prey presence indicators $y$. This is expected given the inherent stochasticity in the organisms behaviour. This stochastic evaluation of the synaptic weights will make things difficult for the perturb-measure-step alogorithm though, because it relies upon precise function evaluations to get good estimates of the rate of improvement in a given direction in parameter space. We can overcome this stochastic evaluation issue though by using our knowledge of how the different probabilities of striking or not determine the expected, or average reward. By directly evaluating expected reward we can recover a precise, deterministic evaluation function."
      ],
      "metadata": {
        "id": "SNMQj2pq5c9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_params_expectation_batch(W, x, y, verbose=False):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R_exp: the expected reward obtained over the batch given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # Expected true positives (TPs) and false positives (FPs)\n",
        "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
        "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
        "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
        "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
        "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
        "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "             [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {R_exp}')\n",
        "    return None\n",
        "  else:\n",
        "    return R_exp, confusion_matrix\n",
        "\n",
        "W_test = np.zeros((1,65))\n",
        "# Xs_aug and y1 are batch x 65 and batch x 1, function wants transpose of this shape\n",
        "# for broadcasting to work\n",
        "print('Evaluation 1')\n",
        "eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=True)\n",
        "print('\\nEvaluation 2')\n",
        "eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "ZDlv-zwC6hkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that as hoped, the evaluation of parameters using expected reward, is consistent, as it should be. As a sanity check we see that the distribution of stochastic evaluations is roughly symmetric, and centered around this expectation, with the average of many such stochastic evaluations becoming close to our calculated expected value."
      ],
      "metadata": {
        "id": "JACJHFAi90df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown **Run this cell** to visualize the distribution of stochastic parameter evaluation, relative to the expectation.\n",
        "def eval_params_expectation_batch(W, x, y, verbose=False):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R_exp: the expected reward obtained over the batch given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # Expected true positives (TPs) and false positives (FPs)\n",
        "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
        "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
        "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
        "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
        "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
        "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "             [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {R_exp}')\n",
        "    return None\n",
        "  else:\n",
        "    return R_exp, confusion_matrix\n",
        "\n",
        "def eval_params_stochastic_batch(W, x, y, verbose=False, rng=None):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W: (outputs(1) x inputs(65) np.array)\n",
        "       weights between sensory neurons and output neuron\n",
        "    x: (input(65) x batch np.array) sensory input\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "\n",
        "  Returns:\n",
        "    R: the reward obtained given the parameters, inputs and targets\n",
        "  \"\"\"\n",
        "  if rng is None:\n",
        "    rng = np.random.default_rng()\n",
        "  # activaation\n",
        "  a = np.dot(W,x) # 1 x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(a) # 1 x batch\n",
        "  # what the organism actually does\n",
        "  # rng.random is a sample from the uniform distribution on [0,1)\n",
        "  y_sample = rng.random(size=y_hat.shape) < y_hat  # 1 x batch\n",
        "  R = np.zeros(y_sample.shape)\n",
        "  did_strike = y_sample == 1\n",
        "  did_not_strike = y_sample == 0\n",
        "  should_strike = y == 1\n",
        "  should_not_strike = y == 0\n",
        "  TP = np.logical_and(did_strike, should_strike) # True Positive\n",
        "  FP = np.logical_and(did_strike, should_not_strike) # False Positive\n",
        "  FN = np.logical_and(did_not_strike, should_strike) # False Negative\n",
        "  TN = np.logical_and(did_not_strike, should_not_strike) # True Negative\n",
        "  R[TP] = 1\n",
        "  R[FP] = -1\n",
        "  R[FN] = 0\n",
        "  R[TN] = 0\n",
        "  TPs = np.sum(TP)\n",
        "  FPs = np.sum(FP)\n",
        "  FNs = np.sum(FN)\n",
        "  TNs = np.sum(TN)\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "                 [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {np.sum(R)}')\n",
        "    return None\n",
        "  else:\n",
        "    return np.sum(R), confusion_matrix\n",
        "\n",
        "W_test = np.zeros((1,65))\n",
        "exp_reward, _ = eval_params_expectation_batch(W_test, Xs_aug.T, y1.T, verbose=False)\n",
        "\n",
        "# Generate stochastic rewards\n",
        "stochastic_rewards = []\n",
        "for _ in range(500):  # Simulate 100 times to create a distribution\n",
        "  r, _ = eval_params_stochastic_batch(W_test, Xs_aug.T, y1.T, verbose=False)\n",
        "  stochastic_rewards.append(r)\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "remove_ip_clutter(fig)\n",
        "ax.hist(stochastic_rewards, bins=20, alpha=0.75, label='Stochastic Evaluations')\n",
        "ax.axvline(x=exp_reward, color='r', linestyle='dashed', linewidth=2, label=f'Expected Reward: {exp_reward}')\n",
        "ax.set_title('Comparison of Stochastic Evaluations and Expected Reward')\n",
        "ax.set_xlabel('Reward')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fGcMioTH9zxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As yet another sanity check we calculate the expected reward when striking and not striking with equal probability, in all circumstances, which is what we expect from a $\\mathbf{W}$ of all zeros."
      ],
      "metadata": {
        "id": "q_M7oYlFC9y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are this many cases where striking is good\n",
        "np.sum(y1 == 1)"
      ],
      "metadata": {
        "id": "k1lP9VvCPLNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And this many cases where striking is bad\n",
        "np.sum(y1 == 0)"
      ],
      "metadata": {
        "id": "FjR08i1kPSoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# W = 0 should strike half the time no matter what,\n",
        "# in which case would expect a reward of\n",
        "(2829 - 2791) / 2"
      ],
      "metadata": {
        "id": "fKcCR599Piys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That all checks out, so now that we have some confidence in our evaluation function let's see if perturb-measure-step is able to find a good set of values for $\\mathbf{W}$ using the batch expected value version of parameter evaluation. Run the training loop. The process will take a minute or two to complete, while its running inspect the code and see if it makes sense to you."
      ],
      "metadata": {
        "id": "EK8zI-aSDTJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 60000\n",
        "step_scale = 0.002\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "for step in range(num_steps):\n",
        "  R_current, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  R_test, _ = eval_params_expectation_batch(W + test_perturbation, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 3000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Expected Total Batch Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "DPCBERK3Q0kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best possible score is 2829, and perturb measure step is able to discover network parameters that achieve a score of roughly 2500, in 60,000 steps. This is pretty good, better than we the humans were able to do in terms of figuring out the pattern, before we got bored of the problem in a few minutes. This is all well and good as an optimization exercise, but if we want to connect this form of optimization back to our inspirational cartoon of neural behaviour as a kind of learning, there are a few issues. Three major issues stand out as ways in which the perturb-measure-step training loop above deviates from a process that is simple and local enough to serve as plausible (even if very abstract) model of a physiological syanptic plasticity processes. These key issues are:\n",
        "1. The organism's striking behaviour is stochastic but expected reward outcomes, not actual obtained reward outcomes are used to drive updates to the synaptic parameters $\\mathbf{W}$.\n",
        "2. The evaluation of a given synaptic configuration is based on performance over all of the 5620 of distcint input-outpur pairs (sensory-pattern, prey-presence) in the data set that defines the \"environment\" of this learning problem. Physiologically viable would be evaluations over a single, or at least relatively few, stimulus-response-reward episodes.\n",
        "3. Evaluations are performed in seperate perturbation and non-perturbation modes in the training loop above. A single mode of evaluation that operated fully \"online\" and in congunction with the ongoing generation of behaviour is more simple and easy to imagine physiological implementations of.\n",
        "\n",
        "There are of course many other ways in which the this cartoon learning system deviates from what might plausibly be implemented in an actual simple neural system, but these have more to do with the abstractness of the model, and can concievibly be remedied with careful choices about how to make the model more concrete so as to map nicely onto measurable phyiological features of neural plasticity. In contrast, for the critical points oulined above, it is difficult to imagine how any of these key issues can be overcome physiologically, without invoking additional complex neural circuits and processes, the orgin of which also need to be explained. So for this sequence and the next we focus on addressing these core issues, different-modes, batch versus single experience based reward, and expected versus actual recieved (stochastic) reward.\n",
        "\n",
        "In the rest of this sequence we will adapt the base perturb-measure-step update rule to address each of these three issues. But first let's just get a bit of a sense of how these issues impinge upon the fantasy of using perturb-measure-step as an algorithm that might feasibly be used by a living organism to update the connection strengths of this simple network determining behaviour in response to stimulus. In each interation in the vanilla perturb-measure-step training loop implemented above the organism first evaluates its current parameters based on the expected reward over all 5620 possible experiences. It then perturbs its synaptic parameters and evaluates its performance again on all 5620 experiences, these two evaluations are compared to determine $\\Delta R$ and this together with the pertrubations $\\Delta W_i$ determine the synaptic connections update according to\n",
        "\n",
        "$$ W_i' = W_i + s \\ \\Delta W_i \\ \\Delta R $$\n",
        "where\n",
        "$$\\Delta R = R(\\mathbf{W} + \\Delta \\mathbf{W}) - R(\\mathbf{W})$$\n",
        "and $R(\\mathbf{W})$ is our reward/evaluation function.\n",
        "Note that $s$ needs to be carefully choosen to account both for the average size of the perturbation $\\| \\mathbf{W} \\|$, the appropriately level of scaling given the expected alignment of a random perturbation with the gradient given the dimensionality of $\\mathbf{W}$, and the relative scale of the gradient. While this is can be a challenge in practical applications, the \"dialing-in\" of meta-parameters of learning algorithms is something that we expect evolution to be quite good at.\n",
        "\n",
        "This process would require the organism somehow integrate all of the reward outcomes of all 5620 experiences, remember this aggregated outcome, then integrate up the reward outcomes of another 5620 experiences accumulated while in 'perturbation' mode, and then update its parameters based on a comparison of the remembered and the recently accumlated aggregate reward outcome. This all seems a bit complicated, and difficult to implement with simple, primarily local, synaptic plasticity mechanisms. What we would like intead is a version of perturb-measure-step that updates its synaptic weights as a result of every reward experience, doesn't rely on an expected reward calculation, and that does not have a seperate perturbation mode.\n",
        "\n",
        "Removing the seperate perturbation mode is perhaps the easiest issue to address so we take this on first."
      ],
      "metadata": {
        "id": "T-Qw7kZ7T8Gp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.2 Stochastic-Step-Measure-Step\n",
        "\n",
        "One simple way to avoid a seperate perturbation evaluation is to simply compress the perturbation and the update into a single step. We call such an update method step-measure-step as contrasted with perturb measure step. This new update rule looks like this.\n",
        "\n",
        "\n",
        "\n",
        "$$\\ W_{i}(t+1)= W_i(t) + s \\ (W_i(t) - W_i(t-1)) \\ \\ R(\\mathbf{W}(t)) - R(\\mathbf{W}(t-1)) + \\xi_{i}(t)$$\n",
        "\n",
        "Previously we used $\\Delta$ to denote the perturbation and did not directly reference the parameter update. Because now we are combining the perturbation and the parameter update we use $\\Delta$ to denote this combined change, that is\n",
        "$$ \\ W_{i}(t+1) - W_i(t) = \\Delta W_i(t) $$\n",
        "\n",
        "Then the above simplifies\n",
        "\n",
        "$$ \\Delta W_i(t) = s \\ \\Delta W_i(t-1) \\ \\Delta R(t) + \\xi_i(t) $$\n",
        "Here $\\Delta R(t) = R(\\mathbf{W}(t)) - R(\\mathbf{W}(t-1))$\n",
        "\n",
        "Let's see if\n"
      ],
      "metadata": {
        "id": "Rdp1m0JJV2ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Stochastic-Step-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 100000\n",
        "init_step_scale = 0.003 * np.sqrt(65)\n",
        "init_noise_scale = 0.0005\n",
        "total_scale = 1.0\n",
        "#later_step_scale = 0.002 * np.sqrt(65)\n",
        "#later_noise_scale = 0.0001\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "delta_W = np.zeros((1,65))\n",
        "R, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "start_time = time.time()\n",
        "step_scale = init_step_scale\n",
        "noise_scale = init_noise_scale\n",
        "for step in range(num_steps):\n",
        "  R_old = R\n",
        "  R, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "  delta_R = R - R_old\n",
        "  delta_W_noise = learn_rng.normal(0, noise_scale, size=(1,65))\n",
        "  delta_W = total_scale * (step_scale * delta_W * delta_R + delta_W_noise)\n",
        "  W += delta_W\n",
        "\n",
        "  if step == 0 or (step + 1) % 10000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    #print(f'Delta R: {delta_R}')\n",
        "    #print(f'Delta W: {delta_W}')\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Total Expected Reward: {R:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "    #if step > 40000:\n",
        "    #  step_scale = later_step_scale\n",
        "    #  noise_scale = later_noise_scale\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "5Q6yqqAJWCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this trick of smushing the perturbation and the update into a single step seems to work okay, though not as well as doing the very careful perturbation, but it shows that much of the correct pattern response to stimulus can be discovered through this mode. This particular idea for smushing together the perturbation and the update was is known as ALOPEX (an acronym from \"ALgorithms Of Pattern EXtraction\",  first proposed by Tzanakou and Harth in 1974.) This is one way of not having a seperate perturbation and non-perturbation mode. There are other ways, such as leveraging the stochasticity of relative spike timings (ref seung), and using reward signals directly (or above some anticipated baseline) and we will discuss these later. For now, it is enough to know that this issue can be mitigated, though with some apparrent cost to overall performance (relative to perturb-measure-step). Let's move on to the next issue."
      ],
      "metadata": {
        "id": "-73AjlBda7kA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.3 Full-Batch versus Mini-Batch Perturb-Measure-Step\n",
        "\n",
        "Another issue in terms of physiological viability was parameter evaluation based on performance across all possible experiences. In a simple model of learning and synaptic plasticity, we would like learning to be driven by the outcomes of a single stimulus-response-reward experience.\n",
        "\n",
        "This relates to an important idea in machine learning. The idea of the mini-batch. Even though we are ulitmately interested is the performance of the parameters over all of the input-target pairs in the data set, given how computers operate, it is typically possible to generate a sample evaluation based on a small random sample of the input-target pairs in the data set. Such a samlpe is called a mini-batch of the data. (As contrasted with the entire data-set which is called a full-batch or just batch). Evaluations on these mini-batches provides an estimate of the desired full-batch evaluation. In the extreme case a mini-batch can consist of a single input-output pair. In ML the idea of using mini-batches developed in the context of Gradient Descent optimization (which we haven't covered yet). So even though the mini-batch idea applies to all kinds of optimzation processes, for historical reasons,  learning with mini-batches of size 1 is called \"Stochastic Gradient Descent\", and learning with mini-batches of size greater than 1 but less than the full batch is called \"Mini-Batch Stochastic Gradient Descent\", and learning with the full batch is just called Gradient Descent. In practical ML settings, whenever the data-set becomes sufficiently large, mini-batches are almost always used, as they allow for the most efficient use of computational resources. Here we explicitly seperate the mini-batch idea from Gradient Descent, and so use the terms full-batch, mini-batch, and singleton based learning and evaluations. Let's see what having a mini-batch of different sizes does to our learning rates. For now we test out this mini-batch idea on perturb-measure-step."
      ],
      "metadata": {
        "id": "HIOigwyZeYwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mini-Batch(10) Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "step_scale = 0.002\n",
        "mini_batch_size = 10\n",
        "num_epochs = 400\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for epoch in range(num_epochs):\n",
        "  np.random.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_Xs = Xs_aug[batch_indices].T\n",
        "    batch_y1 = y1[batch_indices].T\n",
        "    R_current, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
        "    raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "    unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "    test_perturbation = unit_test_perturb * perturbation_scale\n",
        "    R_test, _ = eval_params_expectation_batch(W + test_perturbation, batch_Xs, batch_y1)\n",
        "    directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "    W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if epoch == 0 or (epoch + 1) % 20 == 0:\n",
        "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Expected Total Batch Reward: {total_expected_reward:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "b_Q3ceuPUUkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So with a mini-batch of size 10, this works really quite well, basically just as well as evaluating the parameters on the entire experience, what about when we scale down to the extreme case of a mini-batch of size 1?"
      ],
      "metadata": {
        "id": "pjayFNS7ilBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Singleton Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "step_scale = 0.002\n",
        "mini_batch_size = 1\n",
        "num_epochs = 80\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for epoch in range(num_epochs):\n",
        "  np.random.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_Xs = Xs_aug[batch_indices].T\n",
        "    batch_y1 = y1[batch_indices].T\n",
        "    R_current, _ = eval_params_expectation_batch(W, batch_Xs, batch_y1)\n",
        "    raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "    unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "    test_perturbation = unit_test_perturb * perturbation_scale\n",
        "    R_test, _ = eval_params_expectation_batch(W + test_perturbation, batch_Xs, batch_y1)\n",
        "    directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "    W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Expected Total Batch Reward: {total_expected_reward:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "zChE6Ud1i0dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(65) * 0.002"
      ],
      "metadata": {
        "id": "xj0WOGHEXSWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So learning from a single experience using perturb-measure-step doesn't appear to be a problem at all. If anything it leads to faster (in terms of computational time) learning, since evaluations on a single (or small mini-batch of) input-output pair(s) are much quicker to compute than evaluations of the parameters based on the entirety of the avialble data-set.\n",
        "\n",
        "Now, unfortunately, the smushing together of the update step and the perturbation step, does not mix well with the mini-batch idea. The key issue here, is that in the mini-batch training loops above **the same** mini-batch was used to evaluate both the base parameters and the perturbed parameters alowing for a precise estimate of the rate of improvement in the direction of the test perturbation. If two **different** mini-batches are used to estimate the reward at two different points in the parameter space (as happens in when we try to use step-measure-step with mini-batches), the noise introduced by using different mini-batches makes the estimate the degree of improvement between the two points much noisier. While learning is still possible with this extra noise, it will make learning too slow to be practical both in an ML context and in the context of rapdily aquiring adaptive behaviours.\n",
        "\n",
        "Finally we turn our attention to using actual sampled rewards, instead of computed expectation of reward."
      ],
      "metadata": {
        "id": "0BZyRrdxj35_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.4 Recieved Reward Perturb-Measure-Step\n",
        "\n",
        "Later on in this book we will see how an organism might develop an internal model of how their actions interact with the dynamics of the environment to change the state of the environment and produce rewards, but for the moment we are focused on arriving at the simplest viable learning circuit that can rapidly learn good behaviour on the strike-no-stike problem, while invoking only simplest imaginable physiological mechanisms of plasticity. So we wish to exclude the computation of expected reward, and instead focus on the actual rewards recieved by the organism and how they might drive synaptic weight changes. In the training loop below we implement perturb-measure-step but using stochastic evaluation. Let's see how it goes"
      ],
      "metadata": {
        "id": "ye0f3r02J2he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampled-Reward Stochastic-Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 30000\n",
        "step_scale = 0.000005\n",
        "dimensional_scale_factor = np.sqrt(65)\n",
        "perturbation_scale = 0.01 # std of gaussian test perturbations\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "start_time = time.time()\n",
        "for step in range(num_steps):\n",
        "  R_current, _ = eval_params_stochastic_batch(W, Xs_aug.T, y1.T)\n",
        "  raw_test_perturb = learn_rng.standard_normal(size=(1,65))\n",
        "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
        "  test_perturbation = unit_test_perturb * perturbation_scale\n",
        "  R_test, _ = eval_params_stochastic_batch(W + test_perturbation, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  W += step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 3000 == 0:\n",
        "    R_exp, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Expected Total Batch Reward: {R_exp:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "SveIOTt2LWB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.4.5 Putting them all together\n",
        "\n",
        "So there are simple ideas that overcome our key issues with perturb-measure-step as implemented initially. We can effectively learn from single experiences, we can effectively learn from actual rewards resulting from actual rewards made (not expectated reward given hypothetical probabilities of taking given actions), and we can effectively use the difference in reward between two episodes for learning instead of requiring a seperate perturbation evaluations mode of operation for the network. All of these are lovely ideas, however, they do not combine particularly well, at least not in a straightforward way. Learning is still possible when all of these ideas are combined, but improvement is so slow as to be totally impractical. As can be seen in the figure below, (or if you want to burn a bunch of cpu cycles you can run the cell below"
      ],
      "metadata": {
        "id": "Jb1aF07qNrlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampled-Reward Singleton Stochastic-Step-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_epochs = 10000\n",
        "mini_batch_size = 1\n",
        "step_scale =  0.000001\n",
        "noise_scale = 0.0000005\n",
        "W_init = np.zeros((1,65))\n",
        "W = W_init\n",
        "delta_W = np.zeros((1,65))\n",
        "R, _ = eval_params_stochastic_batch(W, Xs_aug.T, y1.T)\n",
        "start_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "  np.random.shuffle(indices)\n",
        "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
        "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
        "    batch_Xs = Xs_aug[batch_indices].T\n",
        "    batch_y1 = y1[batch_indices].T\n",
        "    R_old = R\n",
        "    R, _ = eval_params_stochastic_batch(W, batch_Xs, batch_y1)\n",
        "    delta_R = R - R_old\n",
        "    delta_W_noise = learn_rng.normal(0, noise_scale, size=(1,65))\n",
        "    delta_W = total_scale * (step_scale * delta_W * delta_R + delta_W_noise)\n",
        "    W += delta_W\n",
        "  if epoch == 0 or (epoch + 1) % 10 == 0:\n",
        "    total_expected_reward, _ = eval_params_expectation_batch(W, Xs_aug.T, y1.T)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs} completed | Expected Total Batch Reward: {total_expected_reward:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "eval_params_expectation_batch(W, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "k-k30JNENuXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1.3.6 Is Solving High Dimensional Optimization Problem Necissary for Adaptive Behaviour? Hard Yes.\n",
        "\n",
        "From a machine learning perspective, when thinking about the performance of a model, there are several things that might be limiting performance. Maybe the data is fundamentally difficult, e.g. there are identical inputs with different correct outputs, i.e. $\\mathbf{x} = \\mathbf{x}$, but $y = 0 $ and $y'=1$. Another possibility is that learning algorithm being used is not effective given the model archetecture and the available data. Yet a third possibility is that archetecture of the model itstelf is not complex or flexible enough to capture the relevent patterns in the data. This last reason happens to be the case here.\n",
        "\n",
        "One reason for having a large, flexible brain is to produce complex behaviour that is contingent on the state of the world in complex ways. The simple strike-no-strike network we've been using, consists of a single layer of synaptic connections between the sensory inputs and the motor output. In a shallow network structure like this a sensory input either directly inhibits or promotes the striking behaviour, there is no possibility for a sensory input to promote striking in some sensory contexts, but inhibit it in others. This is a deficincy. Context is important, loud thumping and roaring noises at a concert means the band is good, similar noises when camping in the wilderness means a large animal is nearby. A shallow network does not allow for such context dependence.\n",
        "\n",
        "How do we allow for context dependence, or put another way, interactions between the inputs (beyond simply summing them together). As a starting point our current sensory-behavioural circuit is effectively equivalent to logistic regression, i.e. each sensory input element (feature in ML parlance) can either inhibit or potentiate striking behaviour to varying degrees, but there is no possibility for conditional interaction between features. By 'conditional interaction,' we mean a scenario where, for instance, feature 1 typically inhibits the behavior, except when feature 2 is positive, under which condition feature 1 becomes potentiating. These kinds of feature interactions are impossible in the current model. One way to allow for such interactions is to augment the base set of features with composite features, e.g. incorporate all the pairwise products of the existing feature set, so that instead of 65 features (bias included) we have $(\\frac{64 \\cdot 63}{2} + 64 + 64 + 1) = 2145$ features to work with. This could work, but what if we want something that depends on the interaction of more than 2 features, adding higher order polynomial terms will quickly make the problem intractable (Reference appendix section on why hidden layers not polynomials if we do that). If we had some mechanistically grounded understanding or hypothesis about the relationship between the features and label could might be able to cherry pick some small subset of higher order interaction terms, but the ML mindset is in large part about automating the feature selection processes based on the data alone. In turns out that instead of resorting the regression on polynomial terms to capture feature interactions, there is a much more compact and expressive way of allowing for feature interactions. The idea is to allow for feature interactions to emerge as needed in a 'hidden' computational layer of our highly abstracted neurons.\n",
        "\n",
        "Simply adding an additional layer to the network, consisting of some intermediate (hidden) neurons between the inputs and the outputs, allows for context dependent inhibition and promotion of striking are possible. There are theorems showing that a much as polynomials of arbitrarily high degree are a kind of universal function approximator (e.g. taylor series), simiarly networks with a singal (but potentially very large hidden layers) are a kind of universal function approximator. With this in mind, let's see if adding a hidden layer to our network improves performance.\n",
        "\n",
        "Our new strike-no-strike network with 1 hidden layer is structured as follows"
      ],
      "metadata": {
        "id": "u1lGLbkNVUj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before $\\mathbf{x}$ is the raw sensory input (vector) in a given episode and each element of $\\mathbf{x}$ corresponds to the activation level and firing rate of a single photosensitive neuron.\n",
        "These input neurons are then connected by synapses to a 'hidden layer' of intermediate computational neurons, say 10 of them. The activation level of these hidden layer neurons is computed as\n",
        "$$\\mathbf{h} = \\sigma(\\mathbf{W}_{in} \\cdot \\mathbf{x})$$\n",
        "Now $\\mathbf{W}_{in}$ is a matrix of synaptic weights between the input neurons and the hidden layer neurons, and $\\cdot$ denotes standard matrix vector multiplication. (In this case $\\mathbf{W}$ has shape $10 \\times 65$. Each the values in the $i^{th}$ row of $\\mathbf{W}_{in}$ given the sign and strength of the connections coming into the $i^{th}$ element of $h$ and similarly each value in the $j^{th}$ column of $\\mathbf{W}_{in}$ corresponds to connection strengths coming out of the $j^{th}$ sensory input neuron.)  We still us $\\sigma$ to represent the standard logistic sigmoid function, but in this case applied elementwise the vector output of the product $\\mathbf{W}_{in} \\cdot \\mathbf{x}$. Then, much as before our striking probability is computed as\n",
        "$$a = \\mathbf{W}_{out} \\cdot \\mathbf{h}$$\n",
        "and\n",
        "$$ \\Pr \\{\\text{strike}\\} = \\sigma(a) $$\n",
        "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma(a)$$\n",
        "Here $\\mathbf{W}_{out}$ has shape $1  \\times 10$.\n"
      ],
      "metadata": {
        "id": "CvsVIoAvVNh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to re-write a new eval params function for this new model, let's do it. To keep things quick and simple we will just use our expecation based evaluation"
      ],
      "metadata": {
        "id": "j9tt8sJofITF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_params_batch_expectation_hidden(W_in, W_out, x, y, verbose=False):\n",
        "  \"\"\"\n",
        "  evaluates parameters of simple behaviour circuit given inputs and target\n",
        "  outputs, use numpy broadcasting to be fast and concise\n",
        "  Args:\n",
        "    W_in: (hidden_neurons x inputs(65) np.array)\n",
        "           weights between sensory neurons and hidden layer neurons\n",
        "    W_out: (output(1) x hidden_neurons np.array)\n",
        "           weights between hidden layer neurons and output\n",
        "    x: (input(64) x batch np.array) sensory input\n",
        "       (can be single input, mini-batch of inputs or the whole batch of inputs)\n",
        "    y: (outputs(1) x batch np.array) target behavioural output\n",
        "       (can be a single target, mini-batch of targets, or whole batch),\n",
        "       needs to correspond to input\n",
        "\n",
        "  Returns:\n",
        "    R_exp: the average/expected total reward obtained given the parameters, over the\n",
        "           (mini-)batch of inputs and targets. (mini-batch could be size 1)\n",
        "  \"\"\"\n",
        "  # activation\n",
        "  h = np_sigmoid(np.dot(W_in,x)) # hidden_neurons x batch\n",
        "  # strike probability\n",
        "  y_hat = np_sigmoid(np.dot(W_out,h)) # 1 x batch\n",
        "  # Expected true positives (TPs) and false positives (FPs)\n",
        "  TPs = np.sum(y_hat * y)  # Sum of strike probabilities where true label is 1\n",
        "  FPs = np.sum(y_hat * (1 - y))  # Sum of strike probabilities where true label is 0\n",
        "  # Expected false negatives (FN_e) and true negatives (TN_e)\n",
        "  FNs = np.sum((1 - y_hat) * y)  # Sum of no strike probabilities where true label is 1\n",
        "  TNs = np.sum((1 - y_hat) * (1 - y))  # Sum of no strike probabilities where true label is 0\n",
        "  R_exp = 1 * TPs + 0 * FNs + -1 * FPs + 0 * TNs\n",
        "  confusion_matrix = np.array([[TPs, FNs], [FPs, TNs]])\n",
        "  if verbose:\n",
        "    table = [[\"Should Strike\", TPs, FNs],\n",
        "             [\"Shouldn't Strike\", FPs, TNs]]\n",
        "    headers = [\"\", \"Did Strike\", \"Didn't Strike\"]\n",
        "    print(\"Confusion_matrix: \")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"grid\"))\n",
        "    print(f'Total Reward: {R_exp}')\n",
        "    return None\n",
        "  else:\n",
        "    return R_exp, confusion_matrix\n"
      ],
      "metadata": {
        "id": "wequmEMQfJMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've got a more complicated circuit with more parameters, how much longer does it take us to evaluate this circuit compared to our previous one?"
      ],
      "metadata": {
        "id": "Eiem7eE-fw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.zeros((1,65))\n",
        "%timeit eval_params_expectation_batch(W, Xs_aug.T, y1.T)"
      ],
      "metadata": {
        "id": "vyebIZ8Vf3bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W_in = np.zeros((10,65))\n",
        "W_out = np.zeros((1,10))\n",
        "%timeit eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)"
      ],
      "metadata": {
        "id": "htd8pC1uf55M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roughly 5x longer per function evaluation, which means not only will we likely need more iterations of our algorithm because it is harder to find good parameters in high dimensions (we have 660 parameters now, which is a lot more than 65), but also each of those steps will take longer to process because function evaluations are also more costly. It will all be worth it if we can get better final performance though."
      ],
      "metadata": {
        "id": "Px0KZWgFgK1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 10 Hidden Units - Perturb-Measure-Step Training Loop\n",
        "llearn_rng = np.random.default_rng(0)\n",
        "num_steps = 20000\n",
        "step_scale = 0.025\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "num_hidden_units = 10\n",
        "# initializing both layers as zero leads to some issues, so we\n",
        "# use a Xavier/Glorot random initialization scheme\n",
        "in_init = np.sqrt(6 / (65 + num_hidden_units))\n",
        "W_in_init = learn_rng.uniform(-in_init, in_init, size=(num_hidden_units, 65))\n",
        "out_init = np.sqrt(6 / (10 + 1))\n",
        "W_out_init = learn_rng.uniform(-out_init, out_init, size=(1, num_hidden_units))\n",
        "flat_params = np.concatenate((W_in_init.flatten(), W_out_init.flatten()))\n",
        "dimensional_scale_factor = np.sqrt(len(flat_params))\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for step in range(num_steps):\n",
        "  W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "  W_in = W_in.reshape((num_hidden_units,65))\n",
        "  W_out = W_out.reshape((1,num_hidden_units))\n",
        "  R_current, _ = eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)\n",
        "  raw_param_perturb = learn_rng.standard_normal(size=len(flat_params))\n",
        "  unit_param_perturb = raw_param_perturb / np.linalg.norm(raw_param_perturb.flatten())\n",
        "  test_perturbation = unit_param_perturb * perturbation_scale\n",
        "  perturbed_flat_params = flat_params + test_perturbation\n",
        "  W_in_test, W_out_test = np.split(perturbed_flat_params, [65 * num_hidden_units])\n",
        "  W_in_test = W_in_test.reshape((num_hidden_units,65))\n",
        "  W_out_test = W_out_test.reshape((1,num_hidden_units))\n",
        "  R_test, _ = eval_params_batch_expectation_hidden(W_in_test, W_out_test, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  flat_params += step_scale * dimensional_scale_factor * directional_grad_est * unit_param_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 1000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Current Total Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "W_in = W_in.reshape((num_hidden_units,65))\n",
        "W_out = W_out.reshape((1,num_hidden_units))\n",
        "eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T, verbose=True)\n"
      ],
      "metadata": {
        "id": "MDoAqfWghM--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this new, more complex circuit is great. We're much closer to the theoretical maximum performance of 2829, maybe with a few more hidden units, and a little longer training time we could have perfect discrimination. Let's see what happens when we go up to 20 hidden units. As a heads up this is going to take awhile (about 3 minutes) so you should read ahead while waiting for this training loop to complete"
      ],
      "metadata": {
        "id": "m4hKKsmKnjrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 20 Hidden Units - Perturb-Measure-Step Training Loop\n",
        "learn_rng = np.random.default_rng(0)\n",
        "num_steps = 20000\n",
        "step_scale = 0.025\n",
        "perturbation_scale = 0.0001 # std of gaussian test perturbations\n",
        "num_hidden_units = 20\n",
        "# initializing both layers as zero leads to some issues, so we\n",
        "# use a Xavier/Glorot random initialization scheme\n",
        "in_init = np.sqrt(6 / (65 + num_hidden_units))\n",
        "W_in_init = learn_rng.uniform(-in_init, in_init, size=(num_hidden_units, 65))\n",
        "out_init = np.sqrt(6 / (10 + 1))\n",
        "W_out_init = learn_rng.uniform(-out_init, out_init, size=(1, num_hidden_units))\n",
        "flat_params = np.concatenate((W_in_init.flatten(), W_out_init.flatten()))\n",
        "dimensional_scale_factor = np.sqrt(len(flat_params))\n",
        "start_time = time.time()\n",
        "indices = np.arange(Xs_aug.shape[0])\n",
        "for step in range(num_steps):\n",
        "  W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "  W_in = W_in.reshape((num_hidden_units,65))\n",
        "  W_out = W_out.reshape((1,num_hidden_units))\n",
        "  R_current, _ = eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T)\n",
        "  raw_param_perturb = learn_rng.standard_normal(size=len(flat_params))\n",
        "  unit_param_perturb = raw_param_perturb / np.linalg.norm(raw_param_perturb.flatten())\n",
        "  test_perturbation = unit_param_perturb * perturbation_scale\n",
        "  perturbed_flat_params = flat_params + test_perturbation\n",
        "  W_in_test, W_out_test = np.split(perturbed_flat_params, [65 * num_hidden_units])\n",
        "  W_in_test = W_in_test.reshape((num_hidden_units,65))\n",
        "  W_out_test = W_out_test.reshape((1,num_hidden_units))\n",
        "  R_test, _ = eval_params_batch_expectation_hidden(W_in_test, W_out_test, Xs_aug.T, y1.T)\n",
        "  directional_grad_est = (R_test - R_current) / perturbation_scale\n",
        "  flat_params += step_scale * dimensional_scale_factor * directional_grad_est * unit_param_perturb\n",
        "\n",
        "  if step == 0 or (step + 1) % 1000 == 0:\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f'Step {step + 1}/{num_steps} completed | Current Total Reward: {R_current:.6f} | Time elapsed: {elapsed_time:.2f} seconds')\n",
        "W_in, W_out = np.split(flat_params, [65 * num_hidden_units])\n",
        "W_in = W_in.reshape((num_hidden_units,65))\n",
        "W_out = W_out.reshape((1,num_hidden_units))\n",
        "eval_params_batch_expectation_hidden(W_in, W_out, Xs_aug.T, y1.T, verbose=True)"
      ],
      "metadata": {
        "id": "LV67Oyy1nxSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This more complex circuit gets us even closer to the theoretical maximum performance of 2829, but it's taking longer to get there. This is a consequence of both, the function evaluations required at each step being slower (for the more complex function) and because more iterations are needed to effectively search the higher dimensional space for a good configuration of $\\mathbf{W}_{in}$ and $\\mathbf{W}_{out}$. With even more hidden units and more time we can likely learn perfect discrimination, but it will take even longer (more than 3 minutes!)."
      ],
      "metadata": {
        "id": "DoYe61pfdTzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the toy neural circuit models in this sequence are a far cry from actual neural circuits, they still provide insight into possible mechanisms of synaptic plasticity the brain. We can imagine a scenario where synaptic strengths between neurons in a circuit undergo small, transient perturbations. The brain might integrate and compare the performance of these perturbations over a learning episode (for example, a day) to previous performance levels. (We leave aside the specifics of how this integration and comparison occur for now.)\n",
        "\n",
        "If performance improves with a perturbation, synaptic changes could be consolidated in the direction of the perturbation, proportionate to the degree of improvement. Conversely, if performance worsens, changes might be consolidated in the opposite direction, also proportional to the performance decrease. This concept, while still vague, suggests a mechanism of synaptic adjustment based on performance feedback.\n",
        "\n",
        "One critical point to consider is the scalability of such a learning process. The number of learning episodes required for effective optimization grows with the number of parameters in a neural circuit. This implies that 'perturb-measure-step' plasticity cannot be the primary mechanism driving neural plasticity in large, complex neural circuits that learn rapidly. This limitation is critical, the lifetimes of most animals simply aren't long enough to accommodate the number of learning iterations needed for extensive optimization.\n",
        "\n",
        "However, as demonstrated above, a more complex circuit achieved significantly better performance in the discrimination task, so large complex circuits can be useful. This suggests that even if empirical evidence of perturbation-based learning in the brain exists and its physiological implementation is understood, such processes are unlikely to be the primary drivers of neural plasticity for complex and challenging behaviors.\n",
        "\n",
        "(One counterargument in favor of simple learning rules is that extensive learning might not be necessary if genetic predisposition starts the circuit off close to an optimal parameter configuration. Then subsequently, relatively slow learning processes could 'fine-tune' the neural circuit's configuration. However, as noted in our earlier discussions on evolution, changing environments necessitate that a significant portion of behavior must emerge from learning, thereby limiting the extent to which genetic predispositions can facilitate efficient and adaptive learning.)"
      ],
      "metadata": {
        "id": "4s0X34-jgHZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sequence we looked at how a simple neural circuit, and then a slightly more complex one were able to solve the strike-no-strike discrimination problem, when trained using perturb-measure-step. We found that several of the \"unrealistic\" aspects of perturb-measure-step (full batch learning, a seperate perturbation mode, used expected instead of actual reward to inform learning) could be mitigated, but only at the cost of slowing down learning (though using mini-batches in isolation, when evaluating the perturbation and current parameters on the same mini-batch was actually a speed up). Additionally we saw that using a more complex network (with hidden layers of increasing size) allowed for better perfomance on the discrimination task, but this extra performance came at a cost, function evaluation was more complex and there were more parameters to be optimized, and searching higher-dimensional spaces for good parameter configurations is always harder (more places to look, more directions to try, in addition to more compute required to for each test point evaluation).\n",
        "\n",
        "In the next sequence we will introduce a different perspective on neural plasticity, one in which synaptic plasticity is driven between a mismatch between an expectation or prediction about reward, and the actual recieved reward."
      ],
      "metadata": {
        "id": "B-UDcsvvhF9V"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "P2C1_Sequence5",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "34a32a0c41f949a3aec009f0102dff77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a5ccb4a5b0f44469ef1b2f8647ce042",
              "IPY_MODEL_c089bf3c2c2644b79bb7398d8f9f0654",
              "IPY_MODEL_837beff7259248f296be658bfc819c19"
            ],
            "layout": "IPY_MODEL_1812b5f4bf0a46e98666fd4ffae62718"
          }
        },
        "4a5ccb4a5b0f44469ef1b2f8647ce042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f490ae96c11e4c53adc0495291134e30",
              "IPY_MODEL_50a932b6ca194e878aa2439acc61e8bd"
            ],
            "layout": "IPY_MODEL_53225a74db974128962d5a9f0a9c027e"
          }
        },
        "c089bf3c2c2644b79bb7398d8f9f0654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5009d9c3bd12452ab7d9331a15bbe0b8",
              "IPY_MODEL_d6c2ee1a77ec47ecad7ef230a81a8afe"
            ],
            "layout": "IPY_MODEL_52ce7f7ead1548c596a5e95c13682f1c"
          }
        },
        "837beff7259248f296be658bfc819c19": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f812f2cf6abb4acf993e9d11dae9df1a",
            "msg_id": "",
            "outputs": []
          }
        },
        "1812b5f4bf0a46e98666fd4ffae62718": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f490ae96c11e4c53adc0495291134e30": {
          "model_module": "jupyter-matplotlib",
          "model_name": "MPLCanvasModel",
          "model_module_version": "^0.11",
          "state": {
            "_cursor": "default",
            "_data_url": "data:image/png;base64,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",
            "_dom_classes": [],
            "_figure_label": "Figure 2",
            "_image_mode": "full",
            "_message": "",
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "MPLCanvasModel",
            "_rubberband_height": 0,
            "_rubberband_width": 0,
            "_rubberband_x": 0,
            "_rubberband_y": 0,
            "_size": [
              400,
              400
            ],
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "MPLCanvasView",
            "capture_scroll": false,
            "footer_visible": false,
            "header_visible": false,
            "layout": "IPY_MODEL_cef706152f2d4e27910731dbeedd98ba",
            "pan_zoom_throttle": 33,
            "resizable": false,
            "toolbar": "IPY_MODEL_ef1f14514b9644a4906c56c225e8a524",
            "toolbar_position": "left",
            "toolbar_visible": false
          }
        },
        "50a932b6ca194e878aa2439acc61e8bd": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4c56a51b69ee4a12b25d0850e4c17f10",
            "msg_id": "",
            "outputs": []
          }
        },
        "53225a74db974128962d5a9f0a9c027e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5009d9c3bd12452ab7d9331a15bbe0b8": {
          "model_module": "jupyter-matplotlib",
          "model_name": "MPLCanvasModel",
          "model_module_version": "^0.11",
          "state": {
            "_cursor": "default",
            "_data_url": "data:image/png;base64,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",
            "_dom_classes": [],
            "_figure_label": "Figure 1",
            "_image_mode": "full",
            "_message": "",
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "MPLCanvasModel",
            "_rubberband_height": 0,
            "_rubberband_width": 0,
            "_rubberband_x": 0,
            "_rubberband_y": 0,
            "_size": [
              400,
              400
            ],
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "MPLCanvasView",
            "capture_scroll": false,
            "footer_visible": false,
            "header_visible": false,
            "layout": "IPY_MODEL_d1f0d445b89e4a898008b881a107c676",
            "pan_zoom_throttle": 33,
            "resizable": false,
            "toolbar": "IPY_MODEL_6666e485a1a34bdd8057edf14c2fd120",
            "toolbar_position": "left",
            "toolbar_visible": false
          }
        },
        "d6c2ee1a77ec47ecad7ef230a81a8afe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e220128611f94fae80285b7bcc61bc9f",
              "IPY_MODEL_3f346cd8d42f4d10a51a7c3304e9883a"
            ],
            "layout": "IPY_MODEL_9999c189fa824bcea60a11a929fe6294"
          }
        },
        "52ce7f7ead1548c596a5e95c13682f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f812f2cf6abb4acf993e9d11dae9df1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c56a51b69ee4a12b25d0850e4c17f10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e220128611f94fae80285b7bcc61bc9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Strike",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cc682e57c98a46e28d8611773ee55e3c",
            "style": "IPY_MODEL_607e23142ce845a1a355ecf39663d1d1",
            "tooltip": ""
          }
        },
        "3f346cd8d42f4d10a51a7c3304e9883a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "No Strike",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cd492da82f97434393682df764cc8458",
            "style": "IPY_MODEL_418b7e4a07e94b10859402be50595bd3",
            "tooltip": ""
          }
        },
        "9999c189fa824bcea60a11a929fe6294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef706152f2d4e27910731dbeedd98ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1f14514b9644a4906c56c225e8a524": {
          "model_module": "jupyter-matplotlib",
          "model_name": "ToolbarModel",
          "model_module_version": "^0.11",
          "state": {
            "_current_action": "",
            "_dom_classes": [],
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "ToolbarModel",
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "ToolbarView",
            "button_style": "",
            "collapsed": true,
            "layout": "IPY_MODEL_3bb7b68c6628478c9bd7b36b20f4477d",
            "orientation": "vertical",
            "toolitems": [
              [
                "Home",
                "Reset original view",
                "home",
                "home"
              ],
              [
                "Back",
                "Back to previous view",
                "arrow-left",
                "back"
              ],
              [
                "Forward",
                "Forward to next view",
                "arrow-right",
                "forward"
              ],
              [
                "Pan",
                "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect",
                "arrows",
                "pan"
              ],
              [
                "Zoom",
                "Zoom to rectangle\nx/y fixes axis",
                "square-o",
                "zoom"
              ],
              [
                "Download",
                "Download plot",
                "floppy-o",
                "save_figure"
              ]
            ]
          }
        },
        "d1f0d445b89e4a898008b881a107c676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6666e485a1a34bdd8057edf14c2fd120": {
          "model_module": "jupyter-matplotlib",
          "model_name": "ToolbarModel",
          "model_module_version": "^0.11",
          "state": {
            "_current_action": "",
            "_dom_classes": [],
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "ToolbarModel",
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "ToolbarView",
            "button_style": "",
            "collapsed": true,
            "layout": "IPY_MODEL_002359b4762b462f9a017c7415c8d1ba",
            "orientation": "vertical",
            "toolitems": [
              [
                "Home",
                "Reset original view",
                "home",
                "home"
              ],
              [
                "Back",
                "Back to previous view",
                "arrow-left",
                "back"
              ],
              [
                "Forward",
                "Forward to next view",
                "arrow-right",
                "forward"
              ],
              [
                "Pan",
                "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect",
                "arrows",
                "pan"
              ],
              [
                "Zoom",
                "Zoom to rectangle\nx/y fixes axis",
                "square-o",
                "zoom"
              ],
              [
                "Download",
                "Download plot",
                "floppy-o",
                "save_figure"
              ]
            ]
          }
        },
        "cc682e57c98a46e28d8611773ee55e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607e23142ce845a1a355ecf39663d1d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "cd492da82f97434393682df764cc8458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "418b7e4a07e94b10859402be50595bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3bb7b68c6628478c9bd7b36b20f4477d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "002359b4762b462f9a017c7415c8d1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5b32f47ed2437996e88f6a02cbf49b": {
          "model_module": "jupyter-matplotlib",
          "model_name": "MPLCanvasModel",
          "model_module_version": "^0.11",
          "state": {
            "_cursor": "default",
            "_data_url": "data:image/png;base64,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",
            "_dom_classes": [],
            "_figure_label": "Figure 3",
            "_image_mode": "full",
            "_message": "",
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "MPLCanvasModel",
            "_rubberband_height": 0,
            "_rubberband_width": 0,
            "_rubberband_x": 0,
            "_rubberband_y": 0,
            "_size": [
              400,
              400
            ],
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "MPLCanvasView",
            "capture_scroll": false,
            "footer_visible": false,
            "header_visible": false,
            "layout": "IPY_MODEL_4cdcf1aaf236412eb6c82218b8662b79",
            "pan_zoom_throttle": 33,
            "resizable": false,
            "toolbar": "IPY_MODEL_e3e6749470b74012b2494374781757f2",
            "toolbar_position": "left",
            "toolbar_visible": false
          }
        },
        "4cdcf1aaf236412eb6c82218b8662b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3e6749470b74012b2494374781757f2": {
          "model_module": "jupyter-matplotlib",
          "model_name": "ToolbarModel",
          "model_module_version": "^0.11",
          "state": {
            "_current_action": "",
            "_dom_classes": [],
            "_model_module": "jupyter-matplotlib",
            "_model_module_version": "^0.11",
            "_model_name": "ToolbarModel",
            "_view_count": null,
            "_view_module": "jupyter-matplotlib",
            "_view_module_version": "^0.11",
            "_view_name": "ToolbarView",
            "button_style": "",
            "collapsed": true,
            "layout": "IPY_MODEL_b01c1968fdcf449292325d5eb61b128d",
            "orientation": "vertical",
            "toolitems": [
              [
                "Home",
                "Reset original view",
                "home",
                "home"
              ],
              [
                "Back",
                "Back to previous view",
                "arrow-left",
                "back"
              ],
              [
                "Forward",
                "Forward to next view",
                "arrow-right",
                "forward"
              ],
              [
                "Pan",
                "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect",
                "arrows",
                "pan"
              ],
              [
                "Zoom",
                "Zoom to rectangle\nx/y fixes axis",
                "square-o",
                "zoom"
              ],
              [
                "Download",
                "Download plot",
                "floppy-o",
                "save_figure"
              ]
            ]
          }
        },
        "b01c1968fdcf449292325d5eb61b128d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}