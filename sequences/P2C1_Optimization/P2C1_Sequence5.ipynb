{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P2C1_Optimization/P2C1_Sequence5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_Optimization/P2C1_Sequence5.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **2.1.5: Learning Behaviour Quickly**\n",
    "\n",
    "### Objective: Investigate a new, simple optimization procedure with clear physiological interpretation.\n",
    "\n",
    "In this sequence we will:\n",
    "\n",
    "* Investigate new plasticty rules that are able to aquire adaptive behaviour more rapidly. We continue to work with the strike-no-strike problem where the decision to strike or not is made on the basis of 64 sensory inputs (features) to, and striking is beneficial or costly depending on whether or not prey is actual present.\n",
    "\n",
    "* Compare these new plasticity rules to perturb-measure-step both in terms of learning rate and physilogical plausibility.\n",
    "\n",
    "* Understand the efficacy of these new learning rules using an optimization perspective.\n",
    "\n",
    "* Test these new learning rules on a harder learning problem, still with 64 sensory features, but now with 10 possible behavioural responses to see how increasing the scale of the problem affect the rate at which adaptive behaviour is learned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**. It will take a minute. Read ahead while the dependencies are loading.\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz ucimlrepo > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import asyncio\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "  return\n",
    "\n",
    "data_set = fetch_ucirepo(id=80)\n",
    "X = data_set.data.features.values\n",
    "# Translate the data to have a minimum of 0\n",
    "X_translated = X - X.min()\n",
    "# Scale the data to have a range from 0 to 12 (which is 6 - (-6))\n",
    "scaling_factor = 12 / (X.max() - X.min())\n",
    "X_scaled = X_translated * scaling_factor\n",
    "# Finally, shift the data to be centered between -6 and 6\n",
    "X_final = X_scaled - 6\n",
    "\n",
    "y = data_set.data.targets.values\n",
    "rng = np.random.default_rng(seed=2021)\n",
    "scramble_permutation = rng.permutation(X.shape[1])\n",
    "Xs = X_final[:, scramble_permutation]\n",
    "Xs_aug = np.hstack([Xs, np.ones((Xs.shape[0],1))])\n",
    "y1 = y % 2\n",
    "y2 = np.array(y >= 5, dtype=y.dtype)\n",
    "simple_index = ((y.flatten()==1) | (y.flatten()==0))\n",
    "X_simple = Xs[simple_index]\n",
    "y1_simple = y1[simple_index]\n",
    "# if you only had one feature which would likely be best for discrimination\n",
    "epsilon = 10\n",
    "class_a_sep = np.mean(X_simple[y1_simple.flatten() == 1, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 1, :], axis=0) + epsilon)\n",
    "class_b_sep = np.mean(X_simple[y1_simple.flatten() == 0, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 0, :], axis=0) + epsilon)\n",
    "best_feature = np.argmax(class_a_sep - class_b_sep)\n",
    "#print(f'Best feature is {best_feature}')\n",
    "X_simple_1_feature = X_simple[:, [best_feature]]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P2C1_S5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.1 Learning Strike-No-Strike Quickly with Reward-Prediction-Error and Action-Probability-Reinforcement\n",
    "\n",
    "Recall from the previous sequence that the cartoon organism that inspires this problem can be thought of as having 64 photo-sensitive receptors, and based on the combination of inputs from these receptors it must decide whether to strike or not. The organism pays a cost of one if it strikes when it shouldn't (prey is absent) and recieves a reward of one if it strikes when it should (prey is present). It receives no cost or reward when it does not strike. To get a sense of this discrimination problem try it yourself by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to try out the more complex 'strike-no-strike' discrimination task.\n",
    "\n",
    "class InteractiveMNISTPredator():\n",
    "  def __init__(self,\n",
    "               features=Xs,\n",
    "               labels=y1,\n",
    "               extra_labels=y,\n",
    "               feedback_type='on_strike_only', seed=123):\n",
    "    # Initialize dataset, settings for image scrambling and feedback\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    "    # features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)\n",
    "    # labels is num_data_points x 1 (values 0-9 or 0/1 depending)\n",
    "    self.feedback_type = feedback_type\n",
    "    self.rng = np.random.default_rng(seed)\n",
    "    #sample_order = np.arange(self.features.shape[0])\n",
    "    sample_order = self.rng.permutation(self.features.shape[0])\n",
    "    self.features = self.features[sample_order]\n",
    "    self.labels = self.labels[sample_order]\n",
    "    self.extra_labels = extra_labels[sample_order]\n",
    "    # initialize game state\n",
    "    self.current_index = 0\n",
    "    self.current_image = None\n",
    "    self.previous_image = None\n",
    "    self.score = 0\n",
    "    self.best_possible_score = 0\n",
    "    self.successful_strikes = 0\n",
    "    self.failed_strikes = 0\n",
    "    self.non_strikes = 0\n",
    "    # Initialize widgets\n",
    "    self.strike_button = widgets.Button(description='Strike')\n",
    "    self.no_strike_button = widgets.Button(description='No Strike')\n",
    "    self.score_display = widgets.Output()\n",
    "    self.feedback_display = widgets.Output()\n",
    "\n",
    "    # Initialize the figure for image display\n",
    "    self.fig, self.ax = plt.subplots(figsize=(4, 4))\n",
    "    remove_ip_clutter(self.fig)\n",
    "    self.prev_fig, self.prev_ax = plt.subplots(figsize=(4, 4))\n",
    "    remove_ip_clutter(self.prev_fig)\n",
    "    self.show_next_image()\n",
    "    # Bind event handlers\n",
    "    self.strike_button.on_click(self.on_strike_clicked)\n",
    "    self.no_strike_button.on_click(self.on_no_strike_clicked)\n",
    "\n",
    "    # Arrange widgets in a layout\n",
    "    buttons_layout = widgets.HBox([self.strike_button, self.no_strike_button])\n",
    "    current_buttons = widgets.VBox([self.fig.canvas, buttons_layout])\n",
    "    previous_feedback = widgets.VBox([self.prev_fig.canvas, self.feedback_display])\n",
    "    self.ui = widgets.HBox([previous_feedback, current_buttons, self.score_display])\n",
    "\n",
    "  def show_next_image(self):\n",
    "    # Display the next image\n",
    "    image = self.features[self.current_index]\n",
    "\n",
    "    if len(image) == 64:\n",
    "        image = image.reshape(8, 8)\n",
    "    elif len(image) == 1:\n",
    "      scalar_value = image.flatten()[0]\n",
    "      # Initialize the 8x8 array with -6 (black)\n",
    "      image = np.full((8, 8), -6.0)\n",
    "      # Set the first ring to 6 (white)\n",
    "      image[0, 0] = 6\n",
    "      # Set the second ring to 6 (white)\n",
    "      image[1:-1, 1:-1] = 6\n",
    "      # Set the third (inner ring) back to -6 (black)\n",
    "      image[2:-2, 2:-2] = -6\n",
    "      # Assuming scalar_value is already in the range -6 to 6\n",
    "      #print(scalar_value)\n",
    "      image[3:-3, 3:-3] = scalar_value\n",
    "    else:\n",
    "      raise ValueError(f'Unexpected image shape: {image.shape}')\n",
    "    if self.current_image is not None:\n",
    "      self.previous_image = self.current_image\n",
    "    image = np.flipud(image)\n",
    "    self.current_image = image\n",
    "    # Display the image\n",
    "    #print(image)\n",
    "    self.fig.clf()\n",
    "    self.prev_fig.clf()\n",
    "    self.ax = self.fig.add_subplot(111)\n",
    "    self.prev_ax = self.prev_fig.add_subplot(111)\n",
    "    self.ax.set_xlim(-.5, 7.5)\n",
    "    self.ax.set_ylim(-0.5, 7.5)\n",
    "    self.prev_ax.set_xlim(-.5, 7.5)\n",
    "    self.prev_ax.set_ylim(-0.5, 7.5)\n",
    "    self.ax.set_aspect('equal')\n",
    "    self.prev_ax.set_aspect('equal')\n",
    "    self.ax.axis('off')\n",
    "    self.prev_ax.axis('off')\n",
    "    self.ax.imshow(self.current_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
    "    if self.previous_image is not None:\n",
    "      self.prev_ax.imshow(self.previous_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
    "    self.ax.set_title('Current Sensory Input')\n",
    "    self.prev_ax.set_title('Previous Sensory Input')\n",
    "    self.fig.canvas.draw()\n",
    "    self.prev_fig.canvas.draw()\n",
    "\n",
    "  def on_strike_clicked(self, button):\n",
    "    self.process_decision('Strike')\n",
    "\n",
    "  def on_no_strike_clicked(self, button):\n",
    "    self.process_decision('No Strike')\n",
    "\n",
    "  def process_decision(self, decision):\n",
    "    # freeze buttons while we process\n",
    "    self.strike_button.disabled = True\n",
    "    self.no_strike_button.disabled = True\n",
    "\n",
    "    # Process the user's decision, update score, and provide feedback\n",
    "    correct_action = 'Strike' if self.labels[self.current_index] == 1 else 'No Strike'\n",
    "    if decision == 'Strike':\n",
    "      if decision == correct_action:\n",
    "        self.score += 1\n",
    "        self.successful_strikes += 1\n",
    "      else:\n",
    "        self.score -= 1\n",
    "        self.failed_strikes += 1\n",
    "    elif decision == 'No Strike':\n",
    "      self.non_strikes += 1\n",
    "      # no strike means no gain or loss\n",
    "    else:\n",
    "      raise ValueError(f'Unknown decision: {decision}')\n",
    "\n",
    "    # Show feedback and score\n",
    "    if (self.feedback_type == 'both' or\n",
    "      (self.feedback_type == 'on_strike_only' and decision == 'Strike')):\n",
    "      # Show informative feedback\n",
    "      feedback = f'Your last choice: {decision}\\nCorrect last choice: {correct_action}'\n",
    "    else:\n",
    "      # Show uninformative feedback\n",
    "      feedback = 'Feedback only available after striking.'\n",
    "    with self.feedback_display:\n",
    "      clear_output(wait=True)\n",
    "      #print(self.labels[self.current_index])\n",
    "      #print(self.extra_labels[self.current_index])\n",
    "      print(feedback)\n",
    "\n",
    "    # Show score\n",
    "    with self.score_display:\n",
    "      clear_output(wait=True)\n",
    "      average_score = self.score / (self.current_index+1)\n",
    "      print(f'Total Score: {self.score}')\n",
    "      print(f'Number of Trials: {self.current_index + 1}')\n",
    "      print(f'Successful Strikes: {self.successful_strikes}')\n",
    "      print(f'Failed Strikes: {self.failed_strikes}')\n",
    "      print(f'Non-Strikes: {self.non_strikes}')\n",
    "      print(f'Average Score Per Trial: {average_score:.2f}')\n",
    "\n",
    "    # Prepare the next image\n",
    "    self.current_index += 1\n",
    "    #print(self.current_index)\n",
    "    self.show_next_image()\n",
    "    # Re-enable buttons\n",
    "    self.strike_button.disabled = False\n",
    "    self.no_strike_button.disabled = False\n",
    "\n",
    "\n",
    "scramble_bin_hard = InteractiveMNISTPredator(features=Xs,\n",
    "                                             labels=y1,\n",
    "                                             feedback_type='both')\n",
    "display(scramble_bin_hard.fig.canvas)\n",
    "display(scramble_bin_hard.prev_fig.canvas)\n",
    "clear_output()\n",
    "display(scramble_bin_hard.ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We model this creature's sensory-behaviour system as follows. $\\mathbf{x}$ is the raw sensory input (column vector) of length 64 in a given episode. Each element $x_i$ of $\\mathbf{x}$ corresponds to the activation level of a single photosensitive neuron. We visualize an example sensory input as an $8 \\times 8$ grid in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# visualizing the example we see that lower values correspond to darker pixels\n",
    "# and higher values correspond to lighter values\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "remove_ip_clutter(fig)\n",
    "ax.imshow(Xs[0].reshape(8,8), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These input neurons are then connected by synapses to a single output neuron. The activation level, $z$, of this output neuron is computed as\n",
    "$$z = \\mathbf{Wx} + b$$\n",
    "Here, $b$ is the (scalar) bias, or baseline activation level of the output neuron, and $\\mathbf{W}$ is a matrix of synaptic weights between the input neurons and the single output neuron. (In this case where there is only one output neuron so $\\mathbf{W}$ has shape 1x64 so could also be thought of as a row vector.)  \n",
    "\n",
    "To simplify exposition and coding the input $\\mathbf{x}$ is augmented to have a feature which is always 1, and then the bias terms can be treated as the weight connecting to this constant valued feature. That is\n",
    "\n",
    "$$z = \\mathbf{Wx}$$\n",
    "\n",
    "And now $\\mathbf{W}$ has shape 1x65. The probability of striking is determined by the activation level of the output neuron, together with a temperature parameter $\\tau$ which determines how exploratory the behaviour of the organism is, specifically:\n",
    "$$ \\Pr \\{\\text{strike}\\} = \\sigma(z;\\tau) $$\n",
    "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma(z;\\tau)$$\n",
    "\n",
    "Here $$\\sigma(z;\\tau): \\frac{1}{1+\\exp(-z / \\tau)} = \\frac{\\exp(z/ \\tau)}{1+\\exp(z / \\tau)}$$ is the logistic (sigmoid) function, with temperature hyper-parameter $\\tau$. High values of $\\tau$ make striking and not striking have nearly equal probability regardless of the value of $z$, whereas very low values of $\\tau$ mean that even a very slightly positive $z$ value will correspond to near certainty of striking, and a very slight negative value will result in an almost certain chance of not striking. In other words $\\tau$ determines how responsive the striking probabilities are to changes in $z$.\n",
    "\n",
    "Instead of thinking of $z$ as simply just a way of determining the striking probability, we could also treat $z$ as a kind of prediction or expectation of the reward that will occur if the striking action is taken given the current sensory experience $\\mathbf{x}$. As the expectation of reward recieved increases, the probability of taking the striking action also increases, if the expectation of reward recieved the probability of taking the striking action decreases, according the the temperature scaled softmax function above.\n",
    "\n",
    "Taking this perspective, where $z$ as the organism's internal representation of the expected reward when taking the striking action, then the organism could compare its expectation of reward with the reward it actually receives, and use the difference between expectation and reality to update the synaptic weights that had a causal impact on this expectation in such a way that the expected reward more closely aligns with the received reward. Given our current network model of the sensory-behaviour system a rule that does this is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Reward-Prediction-Error Episodic Update Rule\n",
    "\n",
    "$$\\Delta W_i = s \\cdot (r-z) \\cdot x_i $$\n",
    "\n",
    "Or in vector form\n",
    "\n",
    "$$ \\Delta \\mathbf{W} = s \\cdot (r-z) \\cdot \\mathbf{x}^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is saying that if the received reward is greater than the predicted reward, then it would be better if $z$ had been larger, which means that in the cases where $x_i$ was positive, the weight from $x_i$ should be increased, in proportion to the strength of activation of $x_i$ and in the case that $x_i$ was negative, the weight from $x_i$ should be decreased, again in proportion to the strength of the activation of $x_i$. Conversely if, the reward received is less than predicted then it would have been better if $z$ was smaller and so, in the case where $x_i$ is positive the weight from $x_i$ should be decreased, in proportion to the strength of the activation of $x_i$ and in the case that $x_i$ is negative, the weight from $x_i$ should be decreased. (Note that in the vector formulation above we $\\mathbf{x}^T$ since $\\mathbf{x}$ is column vector (65, 1) whereas $\\mathbf{W}$ is a row vector (1, 65), and the shape and orientation of $\\Delta \\mathbf{W}$ should match the shape of $\\mathbf{W}$.)\n",
    "\n",
    "This rule can be understood as shifting the weights $\\mathbf{W}$ in the direction that minimizes the squared error of the reward prediction.\n",
    "\n",
    "$$ z = \\mathbf{W}\\mathbf{x}$$\n",
    "$$\\begin{align} \\nabla_\\mathbf{W} (r-z)^2 & = 2 \\ (r-z) \\ \\nabla_{\\mathbf{W}}z \\\\\n",
    "& = 2 (r-z) \\mathbf{x}^T\n",
    "\\end{align}$$\n",
    "The factor of 2 is subsumed in the step scaling factor $s$.\n",
    "\n",
    "We call this an episodic update rule because it can concievably be implemented after a single experience of sensory input, action selection, and resultant reward.\n",
    "\n",
    "In this scenario the prediction pertains to the reward when striking, so this difference between expected and received reward simply does not apply to the case where no striking occurs, so no learning occurs without striking. Note that the $\\tau$ parameter in the sigmoid striking probability function determines how exploratory the behaviour is independently of the predicted reward. $\\tau$ is very important. During the earlier episodes of the organism's life it should be more exploratory (higher $\\tau$), but once it has learned good predictions of reward across a representative range of experiences it can settle into simply exploiting the knowledge it has acquired (lower $\\tau$). Deciding when and how to transition from exploratory behaviours to more directly reward maximizing behaviours is known as an exploration-exploitation trade-off, and is a central challenge in Reinforcement Learning. We just mention this in passing now, but will dive more deeply into the exploration-exploitation tradeoff in sequence (blah).\n",
    "\n",
    "There is also a \"cheating\" version of this update rule, where an update to the parameters is always made based on the $r_{\\text{strike}}$, the reward that the organism would recieve if it strikes, regardless of whether it actually strikes or not, i.e. it gets to know what the right answer was regardless of what action it takes. This could be implemented by the organism having some additional, post-hoc prey detection that augments the organism's basic behavioural inputs with additional teaching signals. These additional signals allow the organism to learn from situations where it's like \"Oh I wish I had struck at the food that is now swimming past me\" and also situations where it's like \"Oh there really was no food there, good thing I didn't strike\". Such mechanism likely exist, but are more complex than the simple case of reinforcing actions taken based on intrinsic reward recieved. Our focus for now is this simple case. (We could approximate such a system by \"rewarding\" not striking when no prey is present with 1 and giving a penalty of 1 when an organism fails to strike when prey is present, but this leaves open the question of if and how such \"learning scaffolding\" rewards need to be and can be kept seperate from 'actual' rewards that count towards fitness in the evolutionary sense. This is a complex topic that we will revist throught the book, in particular in sequences (blah)).\n",
    "\n",
    "From an evolutionary and adaptive behaviour perspective we are interested in how the organism can quickly learn to take rewarding actions. This suggest an alternative updating scheme where the reward recieved directly reinforces the probability of taking the action that produced it. To do this we need know how a change in $z$ translates into an increase (or decrease) in the probability of striking, so that we can increase (decrease) action probabilites in proportion to the rewards recieved as a result of particular action choices. We call this the Action-Probability-Reinforcement update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Action Probability Reinforcement Episodic Update Rule\n",
    "\n",
    "$$ \\Delta W_i = s \\cdot r \\cdot \\sigma'(z) \\cdot x_i$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$ \\Delta \\mathbf{W} = s \\cdot r \\cdot \\sigma'(z) \\cdot \\mathbf{x}$$\n",
    "\n",
    "Here $\\sigma'$ is the derivative of the sigmoid striking probability function. We no longer need to explicitly control exploration versus exploitation with $\\tau$, since this update rule acts directly on action probabilities, so we simply set $\\tau =1$ here for simplicity. The above rule uses the raw rewards to reinforce actions. The probability of striking is given by $\\sigma(z)$ so the way to change $\\mathbf{W}$ to increase the probability of striking is to shift the weights in the direction of the gradient of the probability of taking the action given by $\\sigma'(z)$. Using the defination of $\\sigma$ above and the chain rule from calculus we can derive that $\\sigma'(z) = \\sigma(z) (1-\\sigma(z))$. This is telling us what we already know intuitively about the sigmoid function, which is that it is montonically increasing, and relatively flat when probalities are close to zero or one, and steepest when probabilities are close to even, that is 0.5.\n",
    "\n",
    "The above rule can be understood as shifting weights $\\mathbf{W}$ so that the probability of striking in a particular instance ($\\mathbf{x}$) is increased (or decreased in the case of negative reward) in proportion to the rewards that result from taking the striking action.\n",
    "$$ \\Pr\\{\\text{strike}|\\mathbf{x}\\} = \\sigma(\\mathbf{W}\\mathbf{x})$$\n",
    "$$\\begin{align} \\Delta \\mathbf{W} &= s \\cdot \\nabla_\\mathbf{W} \\Pr\\{\\text{strike}| \\mathbf{x}\\} \\cdot r \\\\\n",
    "& = s \\cdot \\sigma'(z) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx} \\cdot r \\\\\n",
    "& = s \\cdot \\sigma'(z) \\cdot \\mathbf{x} \\cdot r \\\\\n",
    "& = s \\cdot \\sigma(z) (1 - \\sigma(z)) \\cdot \\mathbf{x} \\cdot r\n",
    "\\end{align}$$\n",
    "\n",
    "Again we call this an episodic update rule because it can concievably be applied after a single episode consisting of sensory experience, action selection, and resultant reward.\n",
    "\n",
    "Again, this rule also has a cheating variant. The normal variant exclusively uses the actual reward $r$ obtained from sampling an action, and experiencing the subsequent reward. In contrast, the cheating variant (often used in ML contexts) uses the expected reward $\\mathbb{E}[r]$ given the probability distribution over possible actions instead of utilizing a particular sample from the distribution of possible actions and the particular resultant reward. From a practical ML perspective it is sometimes much more efficient to directly compute expected rewards than to sample actions and rewards stochastically, and in these cases using expectations can greatly accelerate learning. However, this cheating variant is only episodic if it is able to somehow consider all possible actions and all possible outcomes within that single episode, certainly possible, but adds the complexity of trying to learn from hypothetical outcomes, much as in the cheating version of Reward-Prediction-Error update rule.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Complete the coding exercise below to see how these two different udpated rules, reward-prediction-error and action-probability-reinforcement, compare with our perturb-measure-step rule, both when using \"realistic\" episodic and cheating variants. Note the that the `eval_params` function we use below can be thought of as containing both an action selection component that depends on the sensory inputs $\\mathbf{x}$ and the parameters $\\mathbf{W}$ and then a reward component that calculates the appropriate reward given the action selected and the true state of the environment encoded in $y$, i.e. the presense or absence of prey which is as yet unobserved by organisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Replace ... in the lines below with one of the following\n",
    "# options, each option get's used exactly once\n",
    "# a) r * (strike_prob) * (1 - strike_prob) * x\n",
    "# b) (r_all - z) * x\n",
    "# c) (np.mean(r_perturb - r)) / perturbation_scale\n",
    "# d) r_exp * (strike_prob) * (1 -  strike_prob) * x\n",
    "# e) (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
    "# f) (r-z) * strike * x\n",
    "# This will implement different update rules driven by: Reward Prediction Error,\n",
    "# Reinforcement of Action ProbabilitiesTrue Positives, or Peturb-Measure\n",
    "# estimate of the gradient of expected reward. Both in a strict, experience\n",
    "# driven way, and in \"cheating\" a bit way.\n",
    "raise NotImplementedError(\"Exercise: Implement different update rules to compare\")\n",
    "################################################################################\n",
    "\n",
    "def np_sigmoid(x, tau=1):\n",
    "  # high tau more exploration, low tau very little exploration\n",
    "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
    "  return 1 / (1 + np.exp(-x/tau))\n",
    "\n",
    "def eval_params(W, x, y, tau=1, rng=None):\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "  - W (ndarray, shape: (1, n_inputs)): Connective strength weights between inputs and the output.\n",
    "  - x (ndarray, shape: (n_inputs, batch)): Input features, col is a sample, each row an input feature type.\n",
    "  - y (ndarray, shape: (1, batch)): Binary indication of prey presence, used to determine the reward.\n",
    "  - tau (float, optional): Temperature parameter for the sigmoid function, controlling its steepness.\n",
    "    A higher tau value leads to a steeper function.\n",
    "  - rng (np.random.Generator, optional): NumPy random number generator instance for reproducibility.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z = np.dot(W, x) # 1 x batch = 1 x n_inputs @ n_inputs x batch\n",
    "  strike_prob = np_sigmoid(z, tau) # 1 x batch\n",
    "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
    "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
    "  r_all = 2*y-1 # reward when striking in all cases\n",
    "  r_exp = strike_prob * r_all # + (1-strike_prob) * 0 # expected reward\n",
    "  return z, strike_prob, strike, r, r_all, r_exp\n",
    "\n",
    "def reward_prediction_step(W, x, y,\n",
    "                           cheat=False,\n",
    "                           tau=1.0,\n",
    "                           rng=None,\n",
    "                           learning_rate=0.0001):\n",
    "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
    "  if cheat:\n",
    "    # learn regardless of whether organism actually strikes or not\n",
    "    update = ...\n",
    "  else: # properly episodic\n",
    "    # learn only from actual recieved rewards\n",
    "    update = ...\n",
    "  # average the update over all elements in the batch\n",
    "  update = np.mean(update, axis=1, keepdims=True).T\n",
    "  W_new = W + update * learning_rate\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def action_prob_step(W, x, y,\n",
    "                     cheat=False,\n",
    "                     tau=1,\n",
    "                     rng=None,\n",
    "                     learning_rate=0.001):\n",
    "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
    "  if cheat:\n",
    "    # learn using expected reward\n",
    "    update = ...\n",
    "  else: # properly episodic\n",
    "    # learn only from actual recieved rewards\n",
    "    # reward of zero recieved when not striking\n",
    "    update = ...\n",
    "  update = np.mean(update, axis=1, keepdims=True).T\n",
    "  W_new = W + update * learning_rate\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def perturb_measure_step(W, x, y,\n",
    "                         perturbation_scale=0.01,\n",
    "                         cheat=False,\n",
    "                         tau=1,\n",
    "                         rng=None,\n",
    "                         learning_rate=0.001):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
    "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
    "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "  test_perturbation = unit_test_perturb * perturbation_scale\n",
    "  perturbed_W = W + test_perturbation\n",
    "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
    "  if cheat:\n",
    "    # evaluate perturbation using expected reward, avg over the mini-batch\n",
    "    directional_grad_est = ...\n",
    "  else: # more episodic (still evaluate peturb and base on same experiences)\n",
    "    # evaluate perturbation using sampled rewards, avg over the mini-batch\n",
    "    directional_grad_est = ...\n",
    "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
    "  W_new = W + update\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "################################################################################\n",
    "# Exercise Complete, simulations and plotting logic follow\n",
    "################################################################################\n",
    "# simulation\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 1\n",
    "num_steps = 0\n",
    "mini_batch_size = 1\n",
    "cooling_rate = 0.04\n",
    "W_init = np.zeros((1,65))\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "batch_x = Xs_aug.T\n",
    "batch_y = y1.T\n",
    "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
    "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
    "# cheating simulation\n",
    "cheat_alg_lrs = {'Reward Prediction': 0.0001,\n",
    "                 'Action Probability': 0.008,\n",
    "                 'Perturb Measure': 0.0016}\n",
    "cheat_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "cheat_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = cheat_alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=True, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      cheat_reward_results[alg_name].append(np.mean(r))\n",
    "      cheat_exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
    "    num_steps += 1\n",
    "    if num_steps > 1000:\n",
    "      break\n",
    "\n",
    "num_steps = 0\n",
    "real_alg_lrs = {'Reward Prediction': 0.0001,\n",
    "                'Action Probability': 0.003,\n",
    "                'Perturb Measure': 0.0000002}\n",
    "# realishtic simulation\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "actual_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = real_alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      actual_reward_results[alg_name].append(np.mean(r))\n",
    "      actual_exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
    "    num_steps += 1\n",
    "    if num_steps > 1000:\n",
    "      break\n",
    "# plotting\n",
    "with plt.xkcd():\n",
    "  # Create subplots with a shared x-axis\n",
    "  #fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8), sharex=True)\n",
    "  fig, ax1 = plt.subplots(figsize=(8,6))\n",
    "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
    "\n",
    "  # Colors for algorithms\n",
    "  colors = {'Reward Prediction': 'b', 'Action Probability': 'g', 'Perturb Measure': 'r'}\n",
    "\n",
    "  # First subplot for expected rewards\n",
    "  ax1.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  for alg_name in alg_names:\n",
    "    eval_cheat = np.array(cheat_exp_reward_results[alg_name])\n",
    "    eval_cheat = np.cumsum(eval_cheat)\n",
    "    eval_cheat = eval_cheat / (np.arange(len(eval_cheat)) + 1)\n",
    "    ax1.plot(eval_cheat, linestyle='--', color=colors[alg_name], label=f'{alg_name}-Cheating')\n",
    "\n",
    "    eval_real = np.array(actual_exp_reward_results[alg_name])\n",
    "    eval_real = np.cumsum(eval_real)\n",
    "    eval_real = eval_real / (np.arange(len(eval_real)) + 1)\n",
    "    ax1.plot(eval_real, linestyle='-', color=colors[alg_name], label=f'{alg_name}')\n",
    "\n",
    "  ax1.set_title('Cumulative per Episode Average of\\nExpected (Full Batch) Reward')\n",
    "  ax1.set_ylabel('Cumulative Avg. Expected Reward')\n",
    "  ax1.legend()\n",
    "  # Second subplot for actual rewards\n",
    "  # ax2.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  # for alg_name in alg_names:\n",
    "  #  eval_cheat = np.array(cheat_reward_results[alg_name])\n",
    "  #  eval_cheat = np.cumsum(eval_cheat)\n",
    "  #  eval_cheat = eval_cheat / (np.arange(len(eval_cheat)) + 1)\n",
    "  #  ax2.plot(eval_cheat, linestyle='--', color=colors[alg_name], label=f'{alg_name}-Cheating')\n",
    "\n",
    "  #  eval_real = np.array(actual_reward_results[alg_name])\n",
    "  #  eval_real = np.cumsum(eval_real)\n",
    "  #  eval_real = eval_real / (np.arange(len(eval_real)) + 1)\n",
    "  #  ax2.plot(eval_real, linestyle='-', color=colors[alg_name], label=f'{alg_name}-Realishtic')\n",
    "\n",
    "  #ax2.set_title('Cumulative Average of Actual Reward Per Learning Episode')\n",
    "  #ax2.set_xlabel('Learning Episodes')\n",
    "  #ax2.set_ylabel('Cumulative Avg. Actual Reward')\n",
    "  #ax2.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def np_sigmoid(x, tau=1):\n",
    "  # high tau more exploration, low tau very little exploration\n",
    "  x_scaled = np.clip(x/tau, -500, 500) #prevent overflow, fine because sigmoid saturates\n",
    "  return 1 / (1 + np.exp(-x/tau))\n",
    "\n",
    "def eval_params(W, x, y, tau=1, rng=None):\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "  - W (ndarray, shape: (1, n_inputs)): Connective strength weights between inputs and the output.\n",
    "  - x (ndarray, shape: (n_inputs, batch)): Input features, col is a sample, each row an input feature type.\n",
    "  - y (ndarray, shape: (1, batch)): Binary indication of prey presence, used to determine the reward.\n",
    "  - tau (float, optional): Temperature parameter for the sigmoid function, controlling its steepness.\n",
    "    A higher tau value leads to a steeper function.\n",
    "  - rng (np.random.Generator, optional): NumPy random number generator instance for reproducibility.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z = np.dot(W, x) # 1 x batch = 1 x n_inputs @ n_inputs x batch\n",
    "  strike_prob = np_sigmoid(z, tau) # 1 x batch\n",
    "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
    "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
    "  r_all = 2*y-1 # reward when striking in all cases\n",
    "  r_exp = strike_prob * r_all # + (1-strike_prob) * 0 # expected reward\n",
    "  return z, strike_prob, strike, r, r_all, r_exp\n",
    "\n",
    "def reward_prediction_step(W, x, y,\n",
    "                           cheat=False,\n",
    "                           tau=1.0,\n",
    "                           rng=None,\n",
    "                           learning_rate=0.0001):\n",
    "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
    "  if cheat:\n",
    "    # learn regardless of whether organism actually strikes or not\n",
    "    update = (r_all - z) * x\n",
    "  else: # properly episodic\n",
    "    # learn only from actual recieved rewards\n",
    "    update = (r-z) * strike * x\n",
    "  # average the update over all elements in the batch\n",
    "  update = np.mean(update, axis=1, keepdims=True).T\n",
    "  W_new = W + update * learning_rate\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def action_prob_step(W, x, y,\n",
    "                     cheat=False,\n",
    "                     tau=1,\n",
    "                     rng=None,\n",
    "                     learning_rate=0.001):\n",
    "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
    "  if cheat:\n",
    "    # learn using expected reward\n",
    "    update = r_exp * (strike_prob) * (1 -  strike_prob) * x\n",
    "  else: # properly episodic\n",
    "    # learn only from actual recieved rewards\n",
    "    # reward of zero recieved when not striking\n",
    "    update = r * (strike_prob) * (1 - strike_prob) * x\n",
    "  update = np.mean(update, axis=1, keepdims=True).T\n",
    "  W_new = W + update * learning_rate\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def perturb_measure_step(W, x, y,\n",
    "                         perturbation_scale=0.01,\n",
    "                         cheat=False,\n",
    "                         tau=1,\n",
    "                         rng=None,\n",
    "                         learning_rate=0.001):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
    "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
    "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "  test_perturbation = unit_test_perturb * perturbation_scale\n",
    "  perturbed_W = W + test_perturbation\n",
    "  _, _, _, r_perturb, r_all_perturb, r_exp_perturb = eval_params(perturbed_W, x, y, tau, rng)\n",
    "  if cheat:\n",
    "    # evaluate perturbation using expected reward, avg over the mini-batch\n",
    "    directional_grad_est = (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
    "  else: # more episodic (still evaluate peturb and base on same experiences)\n",
    "    # evaluate perturbation using sampled rewards, avg over the mini-batch\n",
    "    directional_grad_est = (np.mean(r_perturb - r)) / perturbation_scale\n",
    "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
    "  W_new = W + update\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "################################################################################\n",
    "# Exercise Complete, simulations and plotting logic follow\n",
    "################################################################################\n",
    "# simulation\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 1\n",
    "num_steps = 0\n",
    "mini_batch_size = 1\n",
    "cooling_rate = 0.04\n",
    "W_init = np.zeros((1,65))\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "batch_x = Xs_aug.T\n",
    "batch_y = y1.T\n",
    "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
    "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
    "# cheating simulation\n",
    "cheat_alg_lrs = {'Reward Prediction': 0.0001,\n",
    "                 'Action Probability': 0.008,\n",
    "                 'Perturb Measure': 0.0016}\n",
    "cheat_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "cheat_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = cheat_alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=True, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      cheat_reward_results[alg_name].append(np.mean(r))\n",
    "      cheat_exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
    "    num_steps += 1\n",
    "    if num_steps > 1000:\n",
    "      break\n",
    "\n",
    "num_steps = 0\n",
    "real_alg_lrs = {'Reward Prediction': 0.0001,\n",
    "                'Action Probability': 0.003,\n",
    "                'Perturb Measure': 0.0000002}\n",
    "# realishtic simulation\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "actual_exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = real_alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      actual_reward_results[alg_name].append(np.mean(r))\n",
    "      actual_exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
    "    num_steps += 1\n",
    "    if num_steps > 1000:\n",
    "      break\n",
    "# plotting\n",
    "with plt.xkcd():\n",
    "  # Create subplots with a shared x-axis\n",
    "  #fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8), sharex=True)\n",
    "  fig, ax1 = plt.subplots(figsize=(8,6))\n",
    "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
    "\n",
    "  # Colors for algorithms\n",
    "  colors = {'Reward Prediction': 'b', 'Action Probability': 'g', 'Perturb Measure': 'r'}\n",
    "\n",
    "  # First subplot for expected rewards\n",
    "  ax1.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  for alg_name in alg_names:\n",
    "    eval_cheat = np.array(cheat_exp_reward_results[alg_name])\n",
    "    eval_cheat = np.cumsum(eval_cheat)\n",
    "    eval_cheat = eval_cheat / (np.arange(len(eval_cheat)) + 1)\n",
    "    ax1.plot(eval_cheat, linestyle='--', color=colors[alg_name], label=f'{alg_name}-Cheating')\n",
    "\n",
    "    eval_real = np.array(actual_exp_reward_results[alg_name])\n",
    "    eval_real = np.cumsum(eval_real)\n",
    "    eval_real = eval_real / (np.arange(len(eval_real)) + 1)\n",
    "    ax1.plot(eval_real, linestyle='-', color=colors[alg_name], label=f'{alg_name}')\n",
    "\n",
    "  ax1.set_title('Cumulative per Episode Average of\\nExpected (Full Batch) Reward')\n",
    "  ax1.set_ylabel('Cumulative Avg. Expected Reward')\n",
    "  ax1.set_xlabel('Learning Episodes')\n",
    "  ax1.legend()\n",
    "  # Second subplot for actual rewards\n",
    "  # ax2.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  # for alg_name in alg_names:\n",
    "  #  eval_cheat = np.array(cheat_reward_results[alg_name])\n",
    "  #  eval_cheat = np.cumsum(eval_cheat)\n",
    "  #  eval_cheat = eval_cheat / (np.arange(len(eval_cheat)) + 1)\n",
    "  #  ax2.plot(eval_cheat, linestyle='--', color=colors[alg_name], label=f'{alg_name}-Cheating')\n",
    "\n",
    "  #  eval_real = np.array(actual_reward_results[alg_name])\n",
    "  #  eval_real = np.cumsum(eval_real)\n",
    "  #  eval_real = eval_real / (np.arange(len(eval_real)) + 1)\n",
    "  #  ax2.plot(eval_real, linestyle='-', color=colors[alg_name], label=f'{alg_name}-Realishtic')\n",
    "\n",
    "  #ax2.set_title('Cumulative Average of Actual Reward Per Learning Episode')\n",
    "  #ax2.set_xlabel('Learning Episodes')\n",
    "  #ax2.set_ylabel('Cumulative Avg. Actual Reward')\n",
    "  #ax2.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Looking at the plots produced by the coding exercise we can see that Reward-Prediction-Error and Action-Probability-Reinforcement both learn to (fairly) effectively discriminate between when prey is and is not present much more quickly, i.e. with fewer learning episodes, than Perturb-Measure-Step. And, this difference in number of learning episodes required to learn \"good\" behaviour is much greater for the episodic variants than for the cheating variants.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Although all the learning algorithms learn less quickly when not allowed to cheat, Perturb-Measure-Step suffers the most.\n",
    "\n",
    "Let's take a moment to think about how the Episodic vs. Cheating variants differ for each of these update rules.\n",
    "* **reward-prediction-error**: In episodic mode can only learn when it strikes and there is an actual reward recieved (plus or minus one, to contrast with the prediction). When cheating always learns as though it had made a strike;\n",
    "* **action-probability-reinforcement**: In episodic mode only reinforces the actual action taken by the reward recieved.  When cheating all actions are reinforced by expected reward in proportion to the probability of having taken that action/\n",
    "* **perturb-measure-step**: In episodic compares an actual reward recieved under the base evaluation mode against the actual reward recieved under the perturbation mode to evaluate the perturbation. When cheating this evaluation is made using expected reward, not sampled reward.\n",
    "\n",
    "Why does this cause such a big slow down for perturb-measure-step? When learning from a single experience, and using actual sampled rewards, very few outcomes actually result in effective learning for perturb-measure-step, as can be seen in the table below:\n",
    "\n",
    "| Case | Correct<br>Action | Perturb<br>Action | Base<br>Action | Perturb - Base<br>Reward<br>Difference | Learning<br>Happens | P(Strike, Perturb)<br> greater than<br>P(Strike, Base) | Learning is<br>Helpful |\n",
    "|:-------------:|:-------------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n",
    "|1 | Strike    | Strike    | Strike    | 0  | No  | - | | -   |\n",
    "|2 | Strike    | Strike    | No-Strike | 1  | Yes | Yes | Yes |\n",
    "|3 | Strike    | Strike    | No-Strike | 1  | Yes | No  | No  |\n",
    "|4 | Strike    | No-Strike | Strike    | -1 | Yes | Yes | No  |\n",
    "|5 | Strike    | No-Strike | Strike    | -1 | Yes | No  | Yes |\n",
    "|6 | Strike    | No-Strike | No-Strike | 0  | No  | -   | -   |\n",
    "|7 | No-Strike | Strike    | Strike    | 0  | No  | -   | -   |\n",
    "|8 | No-Strike | Strike    | No-Strike | -1 | Yes | Yes | Yes |\n",
    "|9 | No-Strike | Strike    | No-Strike | -1 | Yes | No  | No  |\n",
    "|10| No-Strike | No-Strike | Strike    | 1  | Yes | Yes | No  |\n",
    "|11| No-Strike | No-Strike | Strike    | 1  | Yes | No  | Yes |\n",
    "|12| No-Strike | No-Strike | No-Strike | 0  | No  | -   | -   |\n",
    "\n",
    "This is a bit dense, but what it is saying is that first of all, there are many cases where no learning occurs because both the pertubation and the base mode sample the same action and recieve the same rewards, so there is no reward difference at all to drive learning. Note that for small perturbations the action probabilities of the base and perturbed networks will be very similar, so these no learning cases will be the most commen case by far as perturbation sizes become increasinly small. Then, even in the cases where the perturbation mode and the base mode do sample different actions, it is possible that the sampled actions will run counter to the way the perturbation have shifted the probability of striking. Case 3 in the table above is an example of this. Under the perturbation the probability of striking is not greater than (but rather less than) the probability of striking with the base parameters, however, despite this, the less likely outcome where the perturb network samples striking, and the base network samples not-striking occurs. Because the sample is not aligned with the underlying shift in probabilities caused by the perturbation, learning from this difference is counterproductive. Now on average, learning will help shift probabilities towards striking in the right situations. However, it will do so only slowly, as many learning episodes will result in no learning, or counterproductive learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.2 Perturb-Measure-Step is Slow Because of Poor Gradient Alignment\n",
    "Although Perturb-Measure-Step suffered the most from using strictly episodic learning, even when learning on the full batch instead of just one example at a time, and using expected rewards, we still see that perturb-measure-step is slow compared to reward-prediction-error and action-probability-reinforcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** - to see how perturb-measure-step is slow relative to reward-prediction-error and action-probability-reinforcement, even when using full-batch learning and expected rewards.\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 10000\n",
    "num_steps = 0\n",
    "mini_batch_size = Xs_aug.T.shape[0]\n",
    "cooling_rate = 0.04\n",
    "W_init = np.zeros((1,65))\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "batch_x = Xs_aug.T\n",
    "batch_y = y1.T\n",
    "alg_names = ['Reward Prediction', 'Action Probability', 'Perturb Measure']\n",
    "alg_funcs = [reward_prediction_step, action_prob_step, perturb_measure_step]\n",
    "alg_lrs = {'Reward Prediction': 0.0015,\n",
    "                 'Action Probability': 0.08,\n",
    "                 'Perturb Measure': 0.001}\n",
    "actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      r = np.mean(r)\n",
    "      r_exp_full = np.mean(r_exp_full)\n",
    "      actual_reward_results[alg_name].append(r)\n",
    "      exp_reward_results[alg_name].append(r_exp_full)\n",
    "    num_steps += 1\n",
    "  if num_steps > 10000:\n",
    "    break\n",
    "with plt.xkcd():\n",
    "  fig, ax = plt.subplots(figsize=(10, 6))\n",
    "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
    "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  for alg_name in alg_names:\n",
    "    eval = np.array(exp_reward_results[alg_name])\n",
    "    eval = np.cumsum(eval)\n",
    "    eval = eval / (np.arange(len(eval)) + 1)\n",
    "    ax.plot(eval, label=f'{alg_name}-(Cheating Full, Batch)')\n",
    "    ax.set_title(f'Cumulative per Episode Average of Expected (Full Batch) Reward')\n",
    "    ax.set_xlabel('Learning Episodes')\n",
    "    ax.set_ylabel('Cumulative Avg. Expected (Full Batch) Reward')\n",
    "    ax.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  #fig, ax = plt.subplots(figsize=(10, 6))\n",
    "  #theoretical_max = np.sum(y1 == 1) / len(y1)\n",
    "  #ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  #for alg_name in alg_names:\n",
    "  #  eval = actual_reward_results[alg_name]\n",
    "  #  eval = np.cumsum(eval)\n",
    "  #  eval = eval / (np.arange(len(eval)) + 1)\n",
    "  #  ax.plot(eval, label=f'{alg_name} (Cheating Full Batch)')\n",
    "  #  ax.set_title(f'Cumulative Average of Actual Reward Per Learning Episode')\n",
    "  #  ax.set_xlabel('Learning Episodes')\n",
    "  #  ax.set_ylabel('Cumulative Avg. Actual Reward')\n",
    "  #  ax.legend()\n",
    "  #plt.tight_layout()\n",
    "  #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the previous sequence looking at optimization in higher dimensions, analysis showed that the expected improvement from our perturb-measure-step was proportional to the square of the magnitude of the gradient, diveded by $N$ the number of dimensions in the optimization problem, so in our current model of striking behaviour, the number of parameters ($65$) to be optimized gives:\n",
    "\n",
    "$$\n",
    "\\text{Perturb-Measure-Step Expected Improvement per Step} \\propto \\frac{\\| \\mathbf{g} \\|^2}{n}$$\n",
    "\n",
    "And a bit of further analysis found that by thoughtfully scaling the step size to compensate for the low level of alignment of a random test direction with the gradient (best direction in parameters space for improvement) this could be improved to\n",
    "\n",
    "$$\n",
    "\\text{Scaled-Perturb-Measure-Step Expected Improvement per Step} \\propto \\frac{\\| \\mathbf{g} \\|^2}{\\sqrt{n}}$$\n",
    "\n",
    "What this tells us is that with perturb-measure-step learning will always be kind of a zigg-zagging through parameter space, with parameter update steps being almost perpendicular to the direction of maximal improvement (the gradient $\\mathbf{g}$) in high dimensional spaces\n",
    "\n",
    "With this in mind, let's look compute this gradient, $\\mathbf{g}$, for our evaluation function, applied to the entire batch of data. Then we can see how our new update rules, reward-prediction-error and action-probability-reinforcement, relate to the gradient. To do that, we need to translate our evaluation function from python code into math symbols. In code our evaluation function was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def eval_params(W, x, y, tau=1, rng=None):\n",
    "  \"\"\"\n",
    "  Parameters:\n",
    "  - W (ndarray, shape: (1, n_inputs)): Connective strength weights between inputs and the output.\n",
    "  - x (ndarray, shape: (n_inputs, batch)): Input features, col is a sample, each row an input feature type.\n",
    "  - y (ndarray, shape: (1, batch)): Binary indication of prey presence, used to determine the reward.\n",
    "  - tau (float, optional): Temperature parameter for the sigmoid function, controlling its steepness.\n",
    "    A higher tau value leads to a steeper function.\n",
    "  - rng (np.random.Generator, optional): NumPy random number generator instance for reproducibility.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z = np.dot(W, x)\n",
    "  strike_prob = np_sigmoid(z, tau)\n",
    "  strike = np.array(rng.random(size=strike_prob.shape) < strike_prob, int) # 1->did, 0->didn't\n",
    "  r = np.where(y == 1, strike, -strike) # sampled reward\n",
    "  r_all = 2*y-1 # reward when striking in all cases\n",
    "  r_exp = strike_prob * r_all # + (1-strike_prob) * 0 # expected reward\n",
    "  return z, strike_prob, strike, r, r_all, r_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We will leave aside the issues of having a stochastic evaluation function for the moment. Randomness in the evaluation can come from both which particular samples from the full batch we use in a given mini-batch, from randomness in behaviour, i.e. whether or not the organism strikes. To simplify things here we eliminate both these sources of randomness by 1) evaluating performance over the full batch of data, and 2) looking at expected reward given the striking probabilities. Then we can write:\n",
    "\n",
    "$$ \\mathbb{E}[R(\\mathbf{W}, \\mathbf{x}, y)] = \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\frac{1}{n} \\sum_{a\\in\\mathcal{A}} \\Pr(a | \\mathbf{x}) \\cdot r(a|y)$$\n",
    "\n",
    "This defines the criteria by which we evaluate the parameters and the resulting behavioural performance of the organism. Here $\\mathcal{D}$ is the set of all $n$ data points in the full-batch and $\\mathcal{A}$ is the set of actions $\\{ \\text{strike}, \\text{no-strike} \\}$. Note that $R(\\mathbf{W}, \\mathbf{x}, y)$ is a random variable that depends on which actions the organism (probablistically) selects in which situations, whereas $r(a|y)$ is deterministic (in our example, in the more general case this might also be a random variable resulting from further stochasticity in the environment, but we are not considering this more general case just yet). The above expression corresponds to the mean of `r_exp` in the code block above, when eval params is applided to the whole batch of data. When writing about the probability of selecting actions we will typically us the notation:\n",
    "\n",
    "$$\\pi_{\\mathbf{W}}(a|\\mathbf{x})$$\n",
    "\n",
    "to emphasize this is the probability of an action being selected by an organism with ***policy*** $\\pi$ parameterized by weights $\\mathbf{W}$ given experience $\\mathbf{x}$. (In the case where the parameters of the policy consist of more than a single weight matrix/vector we typically use $\\theta$ to denote all the parameters of the policy function and write $\\pi_{\\theta}$. To keep notation compact we will often just write $\\pi(a)$ when there is no need to explicitly emphasize the dependence on the paramters and the inputs.)\n",
    "\n",
    "The gradient with respect to $\\mathbf{W}$ of this expected reward expression is the *direction* of parameter change that will cause the greatest increase in expected reward for a small change in $\\mathbf{W}$, scaled by the *rate* of improvement in expected reward given a small change in $\\mathbf{W}$ in that direction. The gradient is a vector, it has direction and magnitude! Let's compute this gradient.\n",
    "\n",
    "$$\\begin{align} \\nabla_{\\mathbf{W}} \\mathbb{E}[R(\\mathbf{W}, \\mathbf{x}, y)] &= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\frac{1}{n} \\sum_{a\\in\\mathcal{A}} \\nabla_{\\mathbf{W}} \\pi_{\\mathbf{W}}(a | \\mathbf{x}) \\cdot r(a|y) \\\\\n",
    "&= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\sum_{a\\in\\mathcal{A}} \\frac{1}{n} \\cdot \\pi_{\\mathbf{W}}(a | \\mathbf{x}) \\cdot r(a|y) \\cdot \\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(a|\\mathbf{x})) \\end{align}$$\n",
    "\n",
    "Math trick alert! this rearangement of the terms comes from the application of the chain rule to:\n",
    "$$\\frac{\\mathrm{d}}{\\mathrm{d}x}\\log(f(x)) = \\frac{f'(x)}{f(x) } \\iff \\frac{\\mathrm{d}}{\\mathrm{d}x} f(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\log(f(x)) \\cdot f(x)$$\n",
    "\n",
    "There is a really good reason for organizing the gradient of the expected reward like this, since this now correspondes with how the organisms experiences the environment given its current policy as determined by parameters $\\mathbf{W}$. Specifically, the organism encounters a data point $(\\mathbf{x}, y)$ from the data set, each point being equally likely to be encountered, so it encounters this particular point with probability $\\frac{1}{n}$. Then, in response to the experience of $\\mathbf{x}$ the ***policy*** of the organism as parameterized by $\\mathbf{W}$ selects or samples an action according to\n",
    "\n",
    "$$\\begin{align}\n",
    "\\pi_{\\mathbf{W}}(\\text{strike} | \\mathbf{x}) &= \\sigma(\\mathbf{Wx}) \\\\\n",
    "\\pi_{\\mathbf{W}}(\\text{no-strike} | \\mathbf{x}) &= 1 - \\sigma(\\mathbf{Wx})\n",
    "\\end{align}$$\n",
    "\n",
    "Once the organism has selected an action, the response/state of the environment determines the reward that the organism experiences according $r(a|y)$. Thus, the terms in the double sum over possible data points encountered and actions taken, can be understood as accounting for probability of that a particular environmental state occurs, $\\frac{1}{n}$, the probability that a particular action is selected in response to the organism's sensory experience of that particular state $\\pi_{\\mathbf{W}}(a | \\mathbf{x})$ given the current policy, the reward that results from taking that particular action in that particular situation, $r(a|y)$, and lastly a gradient term $\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(a|\\mathbf{x}))$\n",
    "\n",
    "What this tells us, is that if the organism were to just go about its life experiencing $\\mathbf{x}$'s, selecting actions according to its policy, experiencing rewards, and then in response to these experiences of sensation-action-reward shift the parameters of its policy in the direction of $\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(a|\\mathbf{x}))$ scaled by the reward experienced, it would actually be implementing (an episodic approximation of) gradient ascent on the expected reward. This is perfect, because expected reward is ***exactly*** the quantity the organism should be trying to optimize (presuming that evolultion has effectively aligned the intrinsic rewards experienced by the organism with reproductive success)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Okay so let's dig into this gradient term for our particular policy. Let's look at the case when $a = \\text{strike}$ first, expanding using our definitions and the chain rule for derivatives\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(\\text{strike}|\\mathbf{x})) &= \\nabla_{\\mathbf{W}} \\log(\\sigma(\\mathbf{Wx})) \\\\\n",
    "&= \\frac{1}{\\sigma(\\mathbf{Wx})} \\cdot  \\nabla_{\\mathbf{W}} \\sigma(\\mathbf{Wx}) \\\\\n",
    "&= \\frac{1}{\\sigma(\\mathbf{Wx})} \\cdot \\sigma(\\mathbf{Wx}) \\cdot (1-\\sigma(\\mathbf{Wx})) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx} \\\\\n",
    "&= (1-\\sigma(\\mathbf{Wx})) \\cdot \\mathbf{x}^T\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Math Exercise**\n",
    "\n",
    "Complete the gradient calculation for the derivative of the log-probability of not striking:\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\nabla_{\\mathbf{W}} \\log(\\pi_{\\mathbf{W}}(\\text{no-strike}|\\mathbf{x})) &= \\nabla_{\\mathbf{W}} \\log(1 - \\sigma(\\mathbf{Wx})) \\\\\n",
    "&= \\dots \\\\%\\frac{1}{1 - \\sigma(\\mathbf{Wx})} \\cdot  \\nabla_{\\mathbf{W}} (1 - \\sigma(\\mathbf{Wx})) \\\\\n",
    "&= \\dots \\\\%\\frac{1}{1 - \\sigma(\\mathbf{Wx})} \\cdot -\\sigma(\\mathbf{Wx}) \\cdot (1-\\sigma(\\mathbf{Wx})) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx} \\\\\n",
    "&= -\\sigma(\\mathbf{Wx}) \\cdot \\mathbf{x}^T\n",
    "\\end{align}$$\n",
    "\n",
    "**Answer:**\n",
    "$$\\begin{align} &= \\frac{1}{1-\\sigma(\\mathbf{Wx})} \\nabla_{\\mathbf{W}} (1-\\sigma( \\mathbf{Wx})) \\\\\n",
    "&= \\frac{-1}{1-\\sigma(\\mathbf{Wx})} \\cdot \\sigma(\\mathbf{Wx}) \\cdot (1-\\sigma(\\mathbf{Wx})) \\cdot \\nabla_{\\mathbf{W}} \\mathbf{Wx}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So if we wanted have an update rule based on following the gradient of the expected reward it would look like this\n",
    "\n",
    "## Expected Reward Gradient Update Rule\n",
    "\n",
    "$$\\Delta W_i = s \\cdot (1-\\sigma(z)) \\cdot r \\cdot x_i $$\n",
    "\n",
    "Or in vector form\n",
    "\n",
    "$$ \\Delta \\mathbf{W} = s \\cdot (1-\\sigma(z)) \\cdot r \\cdot \\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So in the cases where the organism strikes, if the reward is positive it increase weights $w_i$ that were connected to positive inputs, and decrease weights that were connected to negative inputs, so as to increase the probability of striking in the case of experiencing this particular $\\mathbf{x}$. If there is regular structure to the environment in terms of a consistent correlation between $\\mathbf{x}$ and $y$, this adaptation will not only help the organism to select the correct action the next time it experiences this exact same situations $(\\mathbf{x}, y)$, but it will also help the organism to select the correct action with higher probability on subsequent experiences which are sufficiently similar. If the reward is negative the organism decreases weights that are connected to postive inputs, and decreases weights that are connected to negative inputs so as to decrease the probability of striking in such circumstances.\n",
    "\n",
    "Now, let's compare this ideal, expected reward gradient update to our two other update rules\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta \\mathbf{W} (\\text{Reward Prediction}) &= s \\cdot (r-z) \\cdot \\mathbf{x}^T \\\\\n",
    "\\Delta \\mathbf{W} (\\text{Action Probability}) &= s \\cdot r \\cdot \\sigma(z) \\cdot (1 - \\sigma(z)) \\mathbf{x}^T \\\\\n",
    "\\Delta \\mathbf{W} (\\text{Expected Reward Gradient}) &= s \\cdot r \\cdot (1 - \\sigma(z)) \\mathbf{x}^T\n",
    "\\end{align}$$\n",
    "\n",
    "Or in terms of the policy (which gives the probability with which the sampled action $a$ was chosen):\n",
    "\n",
    "$$\\begin{align}\n",
    "\\Delta \\mathbf{W} (\\text{Reward Prediction}) &= s \\cdot (r-z) \\cdot \\mathbf{x}^T \\\\\n",
    "\\Delta \\mathbf{W} (\\text{Action Probability}) &= s \\cdot r \\cdot \\pi(a) \\cdot (1 - \\pi(a)) \\mathbf{x}^T \\\\\n",
    "\\Delta \\mathbf{W} (\\text{Expected Reward Gradient}) &= s \\cdot r \\cdot (1 - \\pi(a)) \\cdot \\mathbf{x}^T\n",
    "\\end{align}$$\n",
    "\n",
    "What each of these have in common is that they all shift the parameters in the direction defined by $\\mathbf{x}^T$ scaled by the reward $r$, modulo some scaling and/or shifting. Thus they all have the effect of increasing the probability of striking when striking is rewarded, and decreasing the probability when striking is punished (negative reward). Reward-prediction-error scales the magnitude of the learning step by using ($r-z$) instead of raw $r$ to drive learning (and disregards anything to do with probability as that is under control of the $\\tau$ parameter). Action-probability-reinforcement scales by probabilities of striking and not-striking, so that updates are smaller when the probability of taking the action is either very high (close to one) or very low (close to zero) and is largest when the probability of taking versus not taking the action is smallest. In contrast, the expected-reward-gradient episodic update is only scaled by the probability of not taking the action, so the more likely the action is the more strongly it is reinforced. In all cases though, each rule prescribes a parameter change in the ***same direction***. These rules only differ in the way the parameter update is scaled.\n",
    "\n",
    "This different scaling or weighting of the episodic update rules means that on average when applied over many iterations they will not shift the parameters in the same direction, and as a result will lead to slightly different long term learning outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___________________________\n",
    "# Loss Box: Expected-Reward-Gradient **Is** Action-Log-Probability-Reinforcement\n",
    "\n",
    "Although we have just looked at the particular case of our problem. This observation that the gradient of the expected reward with respect to the parameters of a policy, is equal to the expectation of the the reward scaled by the gradient of the action log-probabilities, is true in general\n",
    "\n",
    "$$\\begin{align} \\nabla_{\\mathbf{\\theta}} \\mathbb{E}[R(\\mathbf{\\theta}, \\mathbf{x}, y)] &= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\frac{1}{n} \\sum_{a\\in\\mathcal{A}} \\nabla_{\\mathbf{\\theta}} \\pi_{\\mathbf{\\theta}}(a | \\mathbf{x}) \\cdot r(a|y) \\\\\n",
    "&= \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}} \\sum_{a\\in\\mathcal{A}} \\frac{1}{n} \\cdot \\pi_{\\mathbf{\\theta}}(a | \\mathbf{x}) \\cdot r(a|y) \\cdot \\nabla_{\\mathbf{\\theta}} \\log(\\pi_{\\mathbf{\\theta}}(a|\\mathbf{x})) \\end{align}$$\n",
    "\n",
    "Here $\\mathcal{D}$ is the set of all $n$ data points in the full-batch and $\\mathcal{A}$ is the set of actions available to the policy. Note that $R(\\mathbf{\\theta}, \\mathbf{x}, y)$ is random variable that depends on which actions the organism (probablistically) selects in which situations, whereas $r(a|y)$ is deterministic in this case. This formulation is a beautiful thing. It tells us that an organism can maximize their expected reward, simply by doing what they do, experiencing their rewards and then shifting the parameters of their policy in the direction of the gradient of the log probability of the action just taken, scaled by the resultant reward. How this derivative is computed and applied physiologically is a rich topic which we address in part later.\n",
    "\n",
    "Putting this in terms of an episodic or single experience update rule for the parameters we have\n",
    "\n",
    "$$\\Delta \\theta = s \\cdot r(a|y) \\cdot \\nabla_{\\theta} \\log(\\pi_\\theta(a|\\mathbf{x})$$\n",
    "\n",
    "We will use action-log-probability-reinforcement and expected-reward-gradient interchangably to refer to the same update rule, depending on which aspect we wish to emphasize. Most important though, they are one and the same.\n",
    "________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to see a comparison the different algorithms - reward-prediction-error, action-probability-reinforcement and expected-reward-gradient.\n",
    "\n",
    "def action_log_prob_step(W, x, y,\n",
    "                         cheat=False,\n",
    "                         tau=1,\n",
    "                         rng=None,\n",
    "                         learning_rate=0.001):\n",
    "  z, strike_prob, strike, r, r_all, r_exp = eval_params(W, x, y, tau, rng)\n",
    "  if cheat:\n",
    "    # learn using expected reward\n",
    "    update = r_exp * (1 -  strike_prob) * x\n",
    "  else: # properly episodic\n",
    "    # learn only from actual recieved rewards\n",
    "    # reward of zero recieved when not striking\n",
    "    update = r * (1 - strike_prob) * x\n",
    "  update = np.mean(update, axis=1, keepdims=True).T\n",
    "  W_new = W + update * learning_rate\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 1\n",
    "num_steps = 0\n",
    "mini_batch_size = 1\n",
    "cooling_rate = 0.04\n",
    "W_init = np.zeros((1,65))\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "batch_x = Xs_aug.T\n",
    "batch_y = y1.T\n",
    "alg_names = ['Reward Prediction', 'Action Probability', 'Expected Reward']\n",
    "alg_funcs = [reward_prediction_step, action_prob_step, action_log_prob_step]\n",
    "alg_lrs = {'Reward Prediction': 0.0001,\n",
    "           'Action Probability': 0.003,\n",
    "           'Expected Reward': 0.001}\n",
    "actual_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, cheat=False, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      r = np.mean(r)\n",
    "      r_exp_full = np.mean(r_exp_full)\n",
    "      actual_reward_results[alg_name].append(r)\n",
    "      exp_reward_results[alg_name].append(r_exp_full)\n",
    "    num_steps += 1\n",
    "    if num_steps > 2000:\n",
    "      break\n",
    "with plt.xkcd():\n",
    "  fig, ax = plt.subplots(figsize=(10, 6))\n",
    "  theoretical_max = np.sum(y1 == 1) / len(y1)\n",
    "  ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  for alg_name in alg_names:\n",
    "    eval = np.array(exp_reward_results[alg_name])\n",
    "    eval = np.cumsum(eval)\n",
    "    eval = eval / (np.arange(len(eval)) + 1)\n",
    "    ax.plot(eval, label=f'{alg_name}')\n",
    "    ax.set_title(f'Cumulative per Episode Average of Expected (Full Batch) Reward')\n",
    "    ax.set_xlabel('Learning Episodes')\n",
    "    ax.set_ylabel('Cumulative Avg. Expected (Full Batch) Reward')\n",
    "    ax.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Comparing these three different update rules, we see that they all have relatively similar performance in terms of speed of learning. This stems from the fact that they all are leveraging knowledge of how the behaviour is produced by the network to inform the weight updates. This knowledge manifests as the term $\\mathbf{x}^T$ in each of these update rules, since this determines the causal impact of changes in the weight parameters $\\mathbf{W}$ on downstream probabilities of the selected action being taken or not, given the current sensory input $\\mathbf{x}$. This should be contrasted with perturb-measure-step which makes no use of such knowledge. Perturb-measure-step assumes nothing about the structure of the evaluation function, both the behaviour generating aspect of parameter evaluation, and the reward response aspect of the evaluation function. A useful analogy (which we formalize later in sequence ref) is of observations as kind of currency to be spent on making better inferences about the world. Different learning algorithms can all recieve the same data-points, but how they \"spend\" this data on inference is what differentiates them. The same information is always present in the data, but the model archetecture and the learning algorithm determine how efficiently that information in that data is used (or not) to improve some objective. Loosely speaking, within this spending data analogy, perturb-measure-step is spending a lot of its data on inference about how the paramaters generate reward, through behaviour selection. In contrast these other update rules, take the structure of the behaviour generating function as a given, which allows them to spend all of their data on learning the association between sensory inputs and rewarding behaviour, i.e. directly improving behaviour, without \"wasting\" any data on implicit inference about how behaviour is generated, (as this knowledge is already embedded in the update rule). This is very abstract.\n",
    "\n",
    "To see a concrete example of this suppose that on a particular episode $x_0$ has a value of zero. Because this $x_0$ input will have had no causal impact on the action selected in that episode none of the gradient aligned update rules will make an update to $w_0$. In contrast, perturb-measure-step may well update $w_0$, so long as the perturbed evaluation is different from the baseline evaluation. This is because, based on a single learning episode, perturb-measure-step can only make a rough guess based on correlations as to the causal impact of any one parameter on the outcome, as it does not know anything about how the parameters $\\mathbf{W}$ interact with the inputs $\\mathbf{x}$ to generate behaviour and resultant reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.4.3 Learning To Do More Than One Thing\n",
    "\n",
    "So far we have just focused on the case where the organism has to choose between doing something, in our cartoon example striking at prey, or refraining from doing that. There is only one thing to do, and it is either done or not done. Now we want to look at a more general kind of problem where there are many possible actions to choose from. We envision a cartoon scenario much like the first one, but now instead of either striking or not striking, we imagine that there are 10 disctinct prey types, each requiring a different capture technique. If the predator selects the correct capture technique for the particular prey type encountered, they are successful and get a reward of one, but if they select the wrong capture technique they recieve a reward of zero. (How these specific techniques are learned, and what it would even mean to learn specific stereotyped action patterns from a sensory motor perspective, we leave aside for now, and focus solely on the problem of the predator learning to associate the correct discrete capture technique with the correct prey type based on sensory inputs.)\n",
    "\n",
    "To get a sense of this discrimination problem run the cell below and try it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to try out this new 10-fold discrimination task. Click the buttons to try different prey-capture techniques.\n",
    "\n",
    "class InteractiveMNISTPredator_Multi():\n",
    "  def __init__(self,\n",
    "               features=Xs,\n",
    "               labels=y,\n",
    "               extra_labels=y,\n",
    "               feedback_type='on_strike_only', seed=123):\n",
    "    # Initialize dataset, settings for image scrambling and feedback\n",
    "    self.features = features\n",
    "    self.labels = labels\n",
    "    # features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)\n",
    "    # labels is num_data_points x 1 (values 0-9 or 0/1 depending)\n",
    "    self.feedback_type = feedback_type\n",
    "    self.rng = np.random.default_rng(seed)\n",
    "    #sample_order = np.arange(self.features.shape[0])\n",
    "    sample_order = self.rng.permutation(self.features.shape[0])\n",
    "    self.features = self.features[sample_order]\n",
    "    self.labels = self.labels[sample_order]\n",
    "    self.extra_labels = extra_labels[sample_order]\n",
    "    # initialize game state\n",
    "    self.current_index = 0\n",
    "    self.current_image = None\n",
    "    self.previous_image = None\n",
    "    self.score = 0\n",
    "    self.best_possible_score = 0\n",
    "    self.successful_captures = 0\n",
    "    self.failed_captures = 0\n",
    "    # Initialize widgets\n",
    "    # Define button labels as techniques\n",
    "    capture_techniques = [\"Lure\", \"Chase\", \"Stalk\", \"Snare\", \"Pounce\", \"Strike\", \"Slash\", \"Crush\", \"Pin\", \"Bite\"]\n",
    "    self.technique_to_index = {technique: i for i, technique in enumerate(capture_techniques)}\n",
    "    button_style = widgets.ButtonStyle(width='80px')  # Adjust the width as needed\n",
    "    self.strike_buttons = [widgets.Button(description=technique, style=button_style) for technique in capture_techniques]\n",
    "    # Bind event handlers\n",
    "    for btn in self.strike_buttons:\n",
    "      btn.on_click(self.on_class_selected)\n",
    "    self.score_display = widgets.Output()\n",
    "    self.feedback_display = widgets.Output()\n",
    "\n",
    "    # Initialize the figure for image display\n",
    "    self.fig, self.ax = plt.subplots(figsize=(4, 4))\n",
    "    remove_ip_clutter(self.fig)\n",
    "    self.prev_fig, self.prev_ax = plt.subplots(figsize=(4, 4))\n",
    "    remove_ip_clutter(self.prev_fig)\n",
    "    self.show_next_image()\n",
    "\n",
    "    # Arrange widgets in a layout\n",
    "    current_buttons = widgets.VBox([self.fig.canvas,\n",
    "                                    widgets.HBox(self.strike_buttons[0:3]),\n",
    "                                    widgets.HBox(self.strike_buttons[3:6]),\n",
    "                                    widgets.HBox(self.strike_buttons[6:10])])\n",
    "    previous_feedback = widgets.VBox([self.prev_fig.canvas, self.feedback_display])\n",
    "    self.ui = widgets.HBox([previous_feedback, current_buttons, self.score_display])\n",
    "\n",
    "  def show_next_image(self):\n",
    "    # Display the next image\n",
    "    image = self.features[self.current_index]\n",
    "\n",
    "    if len(image) == 64:\n",
    "        image = image.reshape(8, 8)\n",
    "    elif len(image) == 1:\n",
    "      scalar_value = image.flatten()[0]\n",
    "      # Initialize the 8x8 array with -6 (black)\n",
    "      image = np.full((8, 8), -6.0)\n",
    "      # Set the first ring to 6 (white)\n",
    "      image[0, 0] = 6\n",
    "      # Set the second ring to 6 (white)\n",
    "      image[1:-1, 1:-1] = 6\n",
    "      # Set the third (inner ring) back to -6 (black)\n",
    "      image[2:-2, 2:-2] = -6\n",
    "      # Assuming scalar_value is already in the range -6 to 6\n",
    "      #print(scalar_value)\n",
    "      image[3:-3, 3:-3] = scalar_value\n",
    "    else:\n",
    "      raise ValueError(f'Unexpected image shape: {image.shape}')\n",
    "    if self.current_image is not None:\n",
    "      self.previous_image = self.current_image\n",
    "    image = np.flipud(image)\n",
    "    self.current_image = image\n",
    "    # Display the image\n",
    "    #print(image)\n",
    "    self.fig.clf()\n",
    "    self.prev_fig.clf()\n",
    "    self.ax = self.fig.add_subplot(111)\n",
    "    self.prev_ax = self.prev_fig.add_subplot(111)\n",
    "    self.ax.set_xlim(-.5, 7.5)\n",
    "    self.ax.set_ylim(-0.5, 7.5)\n",
    "    self.prev_ax.set_xlim(-.5, 7.5)\n",
    "    self.prev_ax.set_ylim(-0.5, 7.5)\n",
    "    self.ax.set_aspect('equal')\n",
    "    self.prev_ax.set_aspect('equal')\n",
    "    self.ax.axis('off')\n",
    "    self.prev_ax.axis('off')\n",
    "    self.ax.imshow(self.current_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
    "    if self.previous_image is not None:\n",
    "      self.prev_ax.imshow(self.previous_image, cmap='gray', vmin=-6, vmax=6, origin='upper')\n",
    "    self.ax.set_title('Current Sensory Input')\n",
    "    self.prev_ax.set_title('Previous Sensory Input')\n",
    "    self.fig.canvas.draw()\n",
    "    self.prev_fig.canvas.draw()\n",
    "\n",
    "  def on_class_selected(self, button):\n",
    "    # freeze buttons while we process\n",
    "    for btn in self.strike_buttons:\n",
    "      btn.disabled=True\n",
    "    selected_class = self.technique_to_index[button.description]\n",
    "    correct_class = int(np.squeeze(self.labels[self.current_index]))\n",
    "    selected_description = button.description\n",
    "    correct_description = self.strike_buttons[correct_class].description\n",
    "    if selected_class == correct_class:\n",
    "      self.score += 1\n",
    "      self.successful_captures += 1\n",
    "      feedback = f\"Your last choice, '{selected_description}', was correct!\"\n",
    "    else:\n",
    "      self.score -= 1\n",
    "      self.failed_captures += 1\n",
    "      feedback = f\"Your last choice, '{selected_description}', was incorrect. The correct technique was '{correct_description}'.\"\n",
    "    # show feedback\n",
    "    with self.feedback_display:\n",
    "      clear_output(wait=True)\n",
    "      print(feedback)\n",
    "    # Show score\n",
    "    with self.score_display:\n",
    "      clear_output(wait=True)\n",
    "      average_score = self.score / (self.current_index+1)\n",
    "      print(f'Total Score: {self.score}')\n",
    "      print(f'Number of Trials: {self.current_index + 1}')\n",
    "      print(f'Successful Captures: {self.successful_captures}')\n",
    "      print(f'Failed Captures: {self.failed_captures}')\n",
    "      print(f'Average Score Per Trial: {average_score:.2f}')\n",
    "    # Prepare the next image\n",
    "    self.current_index += 1\n",
    "    #print(self.current_index)\n",
    "    self.show_next_image()\n",
    "    # Re-enable buttons\n",
    "    for btn in self.strike_buttons:\n",
    "      btn.disabled=False\n",
    "\n",
    "\n",
    "scramble_multi_hard = InteractiveMNISTPredator_Multi(\n",
    "    features=Xs, labels=y, feedback_type='both')\n",
    "display(scramble_multi_hard.fig.canvas)\n",
    "display(scramble_multi_hard.prev_fig.canvas)\n",
    "clear_output()\n",
    "display(scramble_multi_hard.ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We found the binary discrimination task with these inputs basically intractable, so for us at least this 10-fold discrimination task is even more impossible. Let's see though if we can adapt our behaviour generating network and our learning rules to this new multi-class scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We model this organism's sensory-behaviour much as before with a few small, but important, differences. As before $\\mathbf{x}$ is the raw sensory input (column vector) of length 64 in a given episode. Each element $x_i$ of $\\mathbf{x}$ corresponds to the activation level of a single photosensitive neuron.\n",
    "\n",
    "These input neurons are then connected by synapses to **multiple** output neurons, each one corresponding to a different possible action selection. The activation level, as a vector $\\mathbf{z}$, of these output neurons is computed as\n",
    "$$\\mathbf{z} = \\mathbf{Wx}$$\n",
    "Here $\\mathbf{W}$ is a matrix of synaptic weights between the input neurons and the various output neurons. In this case where there are 65 inputs (we have done our usual trick of hiding the baseline activation level by augmenting $\\mathbf{x}$) and 10 outputs, so $\\mathbf{W}$ has shape 10 x 65. So\n",
    "$$ z_i = \\sum_{j=1}^{65} w_{ij} x_j$$\n",
    "\n",
    "**Notation reminder**: $w_{ij}$ is the weight connecting the $j^{th}$ input to the $i^{th}$ output and is the element in the $i^{th}$ row and $j^{th}$ column of $\\mathbf{W}$.)\n",
    "\n",
    "To convert these activation levels, $\\mathbf{z}$, into probabilities of actions, we use softmax normalization. Softmax extends the logistic sigmoid function to handle multiple classes.\n",
    "Much like the logistic sigmoid turns any real number into a probability over a binary outcome (strike or no-strike), the softmax function turns any vector of real values into a vector of probabilities over different outcomes\n",
    "$$\\text{softmax}(\\mathbf{z}) = \\frac{(\\exp{z_0}, \\dots, \\exp(z_i), \\dots, \\exp(z_n))}{\\sum_k \\exp(z_k)}$$\n",
    "\n",
    "We use a subscript to refere to a particular element of the softmax output so that probability of choosing a particular action $i$ can be expressed as:\n",
    "$$ \\Pr \\{\\text{action }i\\} = \\text{softmax}_i(\\mathbf{z}) = \\frac{\\exp(z_i)}{\\sum_k \\exp(z_k)} $$\n",
    "\n",
    "When there are only two possible actions, and one action has a fixed activation of $0$, then softmax normalization is equivalent to applying the logistic sigmoid of the variable (non-zero) activation level, to determine action probabilities. (This was shown in sequence (blah).)\n",
    "\n",
    "Like the logistic sigmoid, softmax normalization can also have its variabilility modified by a temperature hyper-parameter, $\\tau$, where high temperatures cause the distribution to be closer to a uniform distribution, and low temperatures cause the highest activation value action to be chosen with near certainty.\n",
    "$$ \\Pr \\{\\text{action }i\\} = \\text{softmax}_i(\\mathbf{z};\\tau) = \\frac{\\exp(z_i /\\tau)}{\\sum_k \\exp(z_k / \\tau)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This motivates the following modifications of our update rules to the multi-class forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Reward-Prediction-Error Multi-Class Episodic Update Rule\n",
    "\n",
    "$$\\Delta W_{ij} = s \\cdot (r-z_i) \\cdot x_j \\quad \\text{if action }i\\text{ taken}$$\n",
    "$$\\Delta W_{ij} = 0 \\quad \\text{if some action other than }i\\text{ taken}$$\n",
    "Or in vector form\n",
    "\n",
    "$$ \\Delta \\mathbf{W}_{i,:} = s \\cdot (r-z_i) \\cdot \\mathbf{x}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, $\\mathbf{W}_{i,:}$ denotes the $i^{th}$ row of $\\mathbf{W}$. Using similar notation we would use $\\mathbf{W}_{:,j}$ denotes the $j^{th}$ column of $\\mathbf{W}$. Note that $\\mathbf{x}$ is transposed so that it becomes a row vector, and it's orientation matches that of $\\mathbf{W}_{i,:}$ which is a row vector of the same length as $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Action-Probability-Reinforcement Mulit-Class Episodic Update Rule\n",
    "\n",
    "$$ \\Delta W_{ij} = s \\cdot r \\cdot \\frac{\\partial}{\\partial z_i}\\text{softmax}_k(\\mathbf{z}) \\cdot x_j \\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$ \\Delta \\mathbf{W} = s \\cdot r \\cdot \\left(\\nabla_{\\mathbf{z}} \\text{softmax}_k(\\mathbf{z})\\right) \\mathbf{x}^T \\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
    "\n",
    "In the vector formulation $\\mathbf{x}^T$ is a row vector of length 65, and $\\frac{\\partial}{\\partial \\mathbf{z}}\\text{softmax}(z_k)$ is a col vector of length 10, so their matrix outer product gives a matrix of shape 10 by 65 (the shape of $\\mathbf{W}$.) Now much like the sigmoid function which it is a generalization of $\\text{softmax}$ also has a convinient derivative, specifically\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_i}\\text{softmax}(\\mathbf{z})_k =\n",
    "\\begin{cases}\n",
    "-\\pi(a_k)\\cdot \\pi(a_i) & \\text{if } i \\neq k \\\\\n",
    "\\pi(a_i)\\cdot (1 - \\pi(a_i)) & \\text{if } i = k\n",
    "\\end{cases}\n",
    "$$\n",
    "Here $\\pi(a_i)$ is just another way of denoting output of the softmax normalization corresponding to the probability of selecting action $a_i$, that is $ \\text{softmax} (\\mathbf{z})_i$, given the vector of output activations $\\mathbf{z}$, but writen to ephasize that is the probability of an action being taken given a policy. This is saying that if we increase the activation of a given action, say $z_k$, we increase the probability that that action, $a_k$ is taken at a rate of $\\pi(a_k)\\cdot (1 - \\pi(a_k))$ while simultaneously decreasing the probability (hence the negative sign) with which other actions $a_i$, $i\\neq k$, are taken at a rate of $-\\pi(a_k)\\cdot \\pi(a_i)$. Substituting this into our update rule we have\n",
    "\n",
    "$$\n",
    "\\Delta W_{ij} \\begin{cases}\n",
    "- s \\cdot r \\cdot \\pi(a_k)\\cdot \\pi(a_i) \\cdot x_j & \\text{if } i \\neq k \\\\\n",
    "s \\cdot r \\cdot \\pi(a_i)\\cdot (1 - \\pi(a_i)) \\cdot x_j & \\text{if } i = k\n",
    "\\end{cases}\n",
    "\\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
    "\n",
    "What this is saying is that if an action is rewarding, the weights will shift to make selection of that action given those inputs more likely given these (or similar) sensory inputs, and if an action is not rewarding the reverse is true, the weights will shift to make alternative actions more likely to be selected in given these (or similar) sensory inputs.\n",
    "\n",
    "One important way in which this action-probability-reinforcement differs from the reward-prediction-error update is that every weight in the network is potentially updated after a given episode, in contrast to only the weights making a prediction about the reward associated with the action sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Expected-Reward-Gradient Multi-Class Episodic Update Rule\n",
    "\n",
    "In our previous analysis we saw that the expected-reward-gradient update is equivalent to the action-log-probability-reinforcement update so extending to this insight our multi-class scenario we have\n",
    "\n",
    "$$ \\Delta W_{ij} = s \\cdot r \\cdot \\frac{\\partial}{\\partial z_i}\\log(\\text{softmax}(z_k)) \\cdot x_j \\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
    "\n",
    "Now $\\log(\\text{softmax})$ also hase a very convenient derivative.\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_i}\\log(\\text{softmax}(z_k)) =\n",
    "\\begin{cases}\n",
    "- \\pi(a_i) & \\text{if } i \\neq k \\\\\n",
    "(1 - \\pi(a_i)) & \\text{if } i = k\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "So putting this into our update rule we have\n",
    "\n",
    "$$\n",
    "\\Delta W_{ij} \\begin{cases}\n",
    "- s \\cdot r \\cdot \\pi(a_i) \\cdot x_j & \\text{if } i \\neq k \\\\\n",
    "s \\cdot r \\cdot (1 - \\pi(a_i)) \\cdot x_j & \\text{if } i = k\n",
    "\\end{cases}\n",
    "\\quad \\text{where } k \\text{ corresponds to the action taken}$$\n",
    "\n",
    "This has basically the same affect as action-probability-reinforcement, but with different scaling terms. The primary differnce between the two is that action-probability reinforcement scales down parameter changes both when actions have a very high, and when they have a very low probability, in contrast, action-log-probability-reinforcement only scales down parameter changes in proportion to the probability of not taking the selected action. The practical effect of this difference in scaling is that action-probability-reinforcement discounts updates driven by actions that were unlikely to be selected relative to action-log-probability-reinforcement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we know what our multi-class update rules are, we can implement and compare them. Before we do that though, we need to update our parameter evaluation function to this new multi-class setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def np_softmax(x, tau=1):\n",
    "  # high tau more exploration, low tau very little exploration\n",
    "  x_scaled = x / tau\n",
    "  # Shift x by subtracting the max value to prevent overflow in exp\n",
    "  x_shifted = x_scaled - np.max(x_scaled, axis=0, keepdims=True)\n",
    "  exps = np.exp(x_shifted)\n",
    "  # Normalize the exponentials while maintaining batch structure\n",
    "  softmax_output = exps / np.sum(exps, axis=0, keepdims=True)\n",
    "  return softmax_output\n",
    "\n",
    "def eval_params_multi(W, x, y, tau=1, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z = np.dot(W, x)  # num_action x batch\n",
    "  action_probs = np_softmax(z, tau)  # num_action x batch\n",
    "  cumulative_probs = np.cumsum(action_probs, axis=0)\n",
    "  random_samples = rng.random(size=cumulative_probs.shape[1])\n",
    "  sampled_actions = (cumulative_probs > random_samples).argmax(axis=0)\n",
    "  # calculate which actions were correct and compute reward\n",
    "  correct = sampled_actions == y\n",
    "  r = np.zeros_like(y)\n",
    "  r[correct] = 1\n",
    "  r[~correct] = -1  # sampled reward\n",
    "  # Create an outcomes matrix and calculate expected reward\n",
    "  outcomes_matrix = -np.ones_like(action_probs)\n",
    "  # Set reward to +1 at the position of the correct action for each sample\n",
    "  outcomes_matrix[y, np.arange(y.size)] = 1\n",
    "  r_exp = np.sum(action_probs * outcomes_matrix, axis=0)  # expected reward\n",
    "  return z, action_probs, sampled_actions, r, outcomes_matrix, r_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have our evaluation function we can adapt our update rules to the new multi-class problem. Interestingly perturb-measure-step doesn't need any adaption at all, because it is agnostic to the underlying mechanism of behaviour generation, the only modification it needs is to make use of this new evaluation function, and optimize over the appropriate number of parameters.\n",
    "\n",
    "In the coding exercise below implement these multi-class adaptions of reward-prediction-error, action-probability-reinforcement, and action-log-probability-reinforcement.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Replace ... in the lines below with one of the following\n",
    "# options, each option get's used exactly once\n",
    "# a1) r - z[a, np.arange(len(a))]\n",
    "# a2) errors @ x.T\n",
    "# b1) r[0,b] * pi[i,b] * (1 - pi[i,b]) * x[j,b]\n",
    "# b2) -r[0,b] * pi_a[b] * pi[i,b] * x[j,b]\n",
    "# c1) r[0,b] * (1 - pi[i,b]) * x[j,b]\n",
    "# c2) -r[0,b] * pi[i,b] * x[j,b]\n",
    "# This will implement different multi-class update rules driven by:\n",
    "# reward-prediction-error, action-probability-reinforcement,\n",
    "# and action-log-probability-reinforcement\n",
    "raise NotImplementedError(\"Exercise: Implement different multi-class update rules to compare\")\n",
    "################################################################################\n",
    "\n",
    "def np_softmax(x, tau=1):\n",
    "  # high tau more exploration, low tau very little exploration\n",
    "  x_scaled = x / tau\n",
    "  # Shift x by subtracting the max value to prevent overflow in exp\n",
    "  x_shifted = x_scaled - np.max(x_scaled, axis=0, keepdims=True)\n",
    "  exps = np.exp(x_shifted)\n",
    "  # Normalize the exponentials while maintaining batch structure\n",
    "  softmax_output = exps / np.sum(exps, axis=0, keepdims=True)\n",
    "  return softmax_output\n",
    "\n",
    "def eval_params_multi(W, x, y, tau=1, rng=None):\n",
    "  # W shape is num_action x input\n",
    "  # x shape is input x batch\n",
    "  # y shape is 1 x batch\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z = np.dot(W, x)  # num_action x batch\n",
    "  pi = np_softmax(z, tau)  # num_action x batch\n",
    "  cumulative_probs = np.cumsum(pi, axis=0)\n",
    "  random_samples = rng.random(size=cumulative_probs.shape[1])\n",
    "  #sampled actions\n",
    "  a = (cumulative_probs > random_samples).argmax(axis=0)\n",
    "  # calculate which actions were correct and compute reward\n",
    "  correct = a == y\n",
    "  r = np.zeros_like(y)\n",
    "  r[correct] = 1\n",
    "  r[~correct] = -1  # sampled reward\n",
    "  # Create an outcomes matrix to calculate expected reward\n",
    "  outcomes_matrix = -np.ones_like(pi)\n",
    "  # Set reward to +1 at the position of the correct action for each sample\n",
    "  outcomes_matrix[np.squeeze(y), np.arange(z.shape[1])] = 1\n",
    "  r_exp = np.sum(pi * outcomes_matrix, axis=0)  # expected reward\n",
    "  return z, pi, a, r, outcomes_matrix, r_exp\n",
    "\n",
    "def reward_prediction_multi(W, x, y,\n",
    "                            tau=1.0,\n",
    "                            rng=None,\n",
    "                            learning_rate=0.0001):\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  # learn only from actual received rewards\n",
    "  errors = np.zeros_like(z)\n",
    "  actual_errors = ...\n",
    "  errors[a, np.arange(len(a))] = actual_errors\n",
    "  # implicit sum over batch here in this matrix multiplication\n",
    "  update = ...  # errors is num_actions x batch, x.T is batch x num_features, update is num_actions x num_features\n",
    "  update /= x.shape[1]  # Divide by the number of samples in batch to make the sum an average\n",
    "  W_new = W + learning_rate * update  # Apply learning rate to update step\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def action_prob_multi(W, x, y,\n",
    "                      tau=1,\n",
    "                      rng=None,\n",
    "                      learning_rate=0.001,\n",
    "                      test=False):\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  num_actions, batch_size = pi.shape\n",
    "  num_features = x.shape[0]\n",
    "  pi_a = pi[a, np.arange(len(a))]  # [batch_size]\n",
    "  broadcast_pi_a = np.broadcast_to(pi_a.reshape(1,batch_size), (num_actions, batch_size))\n",
    "  # Compute updates for case when row (i) of W does not correspond to the sampled action\n",
    "  delta_W = np.zeros((num_actions, num_features, batch_size))\n",
    "  delta_W -= r[np.newaxis,:,:] * broadcast_pi_a[:,np.newaxis,:] * pi[:,np.newaxis,:] * x[np.newaxis,:,:]  # Shape [num_actions, num_features, batch_size]\n",
    "  # now compute updates for case when row (i) of W does correspond to the sampled action\n",
    "  mask = np.arange(num_actions)[:, None] == a[None, :]  # [num_actions, batch_size]\n",
    "  mask = np.broadcast_to(mask[:,np.newaxis,:], delta_W.shape) # [num_actions, num_features, batch_size]\n",
    "  positive_update = r[np.newaxis,:,:] * (pi * (1 - pi))[:,np.newaxis,:] * x[np.newaxis,:,:]  # [num_features, num_actions, batch_size]\n",
    "  # Use positive update where appropriate\n",
    "  delta_W[mask] = positive_update[mask]\n",
    "  # average over the elements of the mini-batch\n",
    "  delta_W = np.mean(delta_W, axis=2)\n",
    "  W_new = W + learning_rate * delta_W\n",
    "  if test:\n",
    "    # as a sanity check on all the clever broadcasting and array operations\n",
    "    # check against bog simple for loop implementation\n",
    "    delta_W_test = np.zeros_like(delta_W)\n",
    "    for b in range(batch_size):\n",
    "      for i in range(num_actions):\n",
    "        for j in range(num_features):\n",
    "          if i == a[b]:\n",
    "            delta_W_test[i,j,b] = ...\n",
    "          else:\n",
    "            delta_W_test[i,j,b] = ...\n",
    "    assert np.allclose(delta_W, delta_W_test)\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def action_log_prob_multi(W, x, y,\n",
    "                          tau=1,\n",
    "                          rng=None,\n",
    "                          learning_rate=0.001,\n",
    "                          test=False):\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  num_actions, batch_size = pi.shape\n",
    "  num_features = x.shape[0]\n",
    "  # Compute updates for case when row (i) of W does not correspond to the sampled action\n",
    "  a_1hot = np.zeros_like(pi)  # [num_actions, batch_size]\n",
    "  a_1hot[a, np.arange(batch_size)] = 1\n",
    "  pi_term = a_1hot - pi\n",
    "  delta_W = r[np.newaxis,:,:] * pi_term[:,np.newaxis,:] * x[np.newaxis,:,:]  # Shape [num_actions, num_features, batch_size]\n",
    "  # average over the elements of the mini-batch\n",
    "  delta_W = np.mean(delta_W, axis=2)\n",
    "  W_new = W + learning_rate * delta_W\n",
    "  if test:\n",
    "    # as a sanity check on all the clever broadcasting and array operations\n",
    "    # check against bog simple for loop implementation\n",
    "    delta_W_test = np.zeros_like(delta_W)\n",
    "    for b in range(batch_size):\n",
    "      for i in range(num_actions):\n",
    "        for j in range(num_features):\n",
    "          if i == a[b]:\n",
    "            delta_W_test[i,j,b] = ...\n",
    "          else:\n",
    "            delta_W_test[i,j,b] = ...\n",
    "    assert np.allclose(delta_W, delta_W_test)\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "############### Exercise Complete ###############\n",
    "##### Simulation and Plotting Logic Follows #####\n",
    "\n",
    "def always_cheat_perturb_measure_multi(W, x, y,\n",
    "                                      perturbation_scale=0.0001,\n",
    "                                      tau=1,\n",
    "                                      rng=None,\n",
    "                                      learning_rate=0.001):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
    "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "  test_perturbation = unit_test_perturb * perturbation_scale\n",
    "  perturbed_W = W + test_perturbation\n",
    "  _, _, _, r_perturb, _, r_exp_perturb = eval_params_multi(perturbed_W, x, y, tau, rng)\n",
    "  directional_grad_est = (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
    "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
    "  W_new = W + update\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "# simulation\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 10000\n",
    "num_steps = 0\n",
    "mini_batch_size = 281\n",
    "cooling_rate = 0.01\n",
    "W_init = np.zeros((10,65))\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "batch_x = Xs_aug.T\n",
    "batch_y = y1.T\n",
    "alg_names = ['Reward Prediction', 'Action Probability', 'Action Log Probability', 'Perturb Measure']\n",
    "alg_funcs = [reward_prediction_multi, action_prob_multi, action_log_prob_multi, always_cheat_perturb_measure_multi]\n",
    "alg_lrs = {'Reward Prediction': 0.0001,\n",
    "           'Action Probability': 0.008,\n",
    "           'Action Log Probability': 0.004,\n",
    "           'Perturb Measure': 0.01}\n",
    "reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      # perturb-measure-step uses r_exp everything else just uses r\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params_multi(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      reward_results[alg_name].append(np.mean(r))\n",
    "      exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
    "    num_steps += 1\n",
    "  if num_steps > 2000:\n",
    "    break\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "theoretical_max = 1.0\n",
    "# Colors for algorithms\n",
    "colors = {'Reward Prediction': 'blue', 'Action Probability': 'green',\n",
    "          'Action Log Probability': 'orange', 'Perturb Measure': 'red'}\n",
    "\n",
    "# First subplot for expected rewards\n",
    "ax1.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "for alg_name in alg_names:\n",
    "  eval = np.array(exp_reward_results[alg_name])\n",
    "  eval = np.cumsum(eval)\n",
    "  eval = eval / (np.arange(len(eval)) + 1)\n",
    "  ax1.plot(eval, linestyle='-', color=colors[alg_name], label=f'{alg_name}')\n",
    "  ax1.set_title('Cumulative per Episode Average of\\nExpected (Full Batch) Reward')\n",
    "  ax1.set_ylabel('Cumulative Avg. Expected Reward')\n",
    "  ax1.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def np_softmax(x, tau=1):\n",
    "  # high tau more exploration, low tau very little exploration\n",
    "  x_scaled = x / tau\n",
    "  # Shift x by subtracting the max value to prevent overflow in exp\n",
    "  x_shifted = x_scaled - np.max(x_scaled, axis=0, keepdims=True)\n",
    "  exps = np.exp(x_shifted)\n",
    "  # Normalize the exponentials while maintaining batch structure\n",
    "  softmax_output = exps / np.sum(exps, axis=0, keepdims=True)\n",
    "  return softmax_output\n",
    "\n",
    "def eval_params_multi(W, x, y, tau=1, rng=None):\n",
    "  # W shape is num_action x input\n",
    "  # x shape is input x batch\n",
    "  # y shape is 1 x batch\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z = np.dot(W, x)  # num_action x batch\n",
    "  pi = np_softmax(z, tau)  # num_action x batch\n",
    "  cumulative_probs = np.cumsum(pi, axis=0)\n",
    "  random_samples = rng.random(size=cumulative_probs.shape[1])\n",
    "  #sampled actions\n",
    "  a = (cumulative_probs > random_samples).argmax(axis=0)\n",
    "  # calculate which actions were correct and compute reward\n",
    "  correct = a == y\n",
    "  r = np.zeros_like(y)\n",
    "  r[correct] = 1\n",
    "  r[~correct] = -1  # sampled reward\n",
    "  # Create an outcomes matrix to calculate expected reward\n",
    "  outcomes_matrix = -np.ones_like(pi)\n",
    "  # Set reward to +1 at the position of the correct action for each sample\n",
    "  outcomes_matrix[np.squeeze(y), np.arange(z.shape[1])] = 1\n",
    "  r_exp = np.sum(pi * outcomes_matrix, axis=0)  # expected reward\n",
    "  return z, pi, a, r, outcomes_matrix, r_exp\n",
    "\n",
    "def reward_prediction_multi(W, x, y,\n",
    "                            tau=1.0,\n",
    "                            rng=None,\n",
    "                            learning_rate=0.0001):\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  # learn only from actual received rewards\n",
    "  errors = np.zeros_like(z)\n",
    "  actual_errors = r - z[a, np.arange(len(a))]\n",
    "  errors[a, np.arange(len(a))] = actual_errors\n",
    "  # implicit sum over batch here in this matrix multiplication\n",
    "  update = errors @ x.T  # errors is num_actions x batch, x.T is batch x num_features, update is num_actions x num_features\n",
    "  update /= x.shape[1]  # Divide by the number of samples in batch to make the sum an average\n",
    "  W_new = W + learning_rate * update  # Apply learning rate to update step\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def action_prob_multi(W, x, y,\n",
    "                      tau=1,\n",
    "                      rng=None,\n",
    "                      learning_rate=0.001,\n",
    "                      test=False):\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  num_actions, batch_size = pi.shape\n",
    "  num_features = x.shape[0]\n",
    "  pi_a = pi[a, np.arange(len(a))]  # [batch_size]\n",
    "  broadcast_pi_a = np.broadcast_to(pi_a.reshape(1,batch_size), (num_actions, batch_size))\n",
    "  # Compute updates for case when row (i) of W does not correspond to the sampled action\n",
    "  delta_W = np.zeros((num_actions, num_features, batch_size))\n",
    "  delta_W -= r[np.newaxis,:,:] * broadcast_pi_a[:,np.newaxis,:] * pi[:,np.newaxis,:] * x[np.newaxis,:,:]  # Shape [num_actions, num_features, batch_size]\n",
    "  # now compute updates for case when row (i) of W does correspond to the sampled action\n",
    "  mask = np.arange(num_actions)[:, None] == a[None, :]  # [num_actions, batch_size]\n",
    "  mask = np.broadcast_to(mask[:,np.newaxis,:], delta_W.shape) # [num_actions, num_features, batch_size]\n",
    "  positive_update = r[np.newaxis,:,:] * (pi * (1 - pi))[:,np.newaxis,:] * x[np.newaxis,:,:]  # [num_features, num_actions, batch_size]\n",
    "  # Use positive update where appropriate\n",
    "  delta_W[mask] = positive_update[mask]\n",
    "  # average over the elements of the mini-batch\n",
    "  delta_W = np.mean(delta_W, axis=2)\n",
    "  W_new = W + learning_rate * delta_W\n",
    "  if test:\n",
    "    # as a sanity check on all the clever broadcasting and array operations\n",
    "    # check against bog simple for loop implementation\n",
    "    delta_W_test = np.zeros_like(delta_W)\n",
    "    for b in range(batch_size):\n",
    "      for i in range(num_actions):\n",
    "        for j in range(num_features):\n",
    "          if i == a[b]:\n",
    "            delta_W_test[i,j,b] = r[0,b] * pi[i,b] * (1 - pi[i,b]) * x[j,b]\n",
    "          else:\n",
    "            delta_W_test[i,j,b] = -r[0,b] * pi_a[b] * pi[i,b] * x[j,b]\n",
    "    assert np.allclose(delta_W, delta_W_test)\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "def action_log_prob_multi(W, x, y,\n",
    "                          tau=1,\n",
    "                          rng=None,\n",
    "                          learning_rate=0.001,\n",
    "                          test=False):\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  num_actions, batch_size = pi.shape\n",
    "  num_features = x.shape[0]\n",
    "  # Compute updates for case when row (i) of W does not correspond to the sampled action\n",
    "  a_1hot = np.zeros_like(pi)  # [num_actions, batch_size]\n",
    "  a_1hot[a, np.arange(batch_size)] = 1\n",
    "  pi_term = a_1hot - pi\n",
    "  delta_W = r[np.newaxis,:,:] * pi_term[:,np.newaxis,:] * x[np.newaxis,:,:]  # Shape [num_actions, num_features, batch_size]\n",
    "  # average over the elements of the mini-batch\n",
    "  delta_W = np.mean(delta_W, axis=2)\n",
    "  W_new = W + learning_rate * delta_W\n",
    "  if test:\n",
    "    # as a sanity check on all the clever broadcasting and array operations\n",
    "    # check against bog simple for loop implementation\n",
    "    delta_W_test = np.zeros_like(delta_W)\n",
    "    for b in range(batch_size):\n",
    "      for i in range(num_actions):\n",
    "        for j in range(num_features):\n",
    "          if i == a[b]:\n",
    "            delta_W_test[i,j,b] = r[0,b] * (1 - pi[i,b]) * x[j,b]\n",
    "          else:\n",
    "            delta_W_test[i,j,b] = -r[0,b] * pi[i,b] * x[j,b]\n",
    "    assert np.allclose(delta_W, delta_W_test)\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "############### Exercise Complete ###############\n",
    "##### Simulation and Plotting Logic Follows #####\n",
    "\n",
    "def always_cheat_perturb_measure_multi(W, x, y,\n",
    "                                      perturbation_scale=0.0001,\n",
    "                                      tau=1,\n",
    "                                      rng=None,\n",
    "                                      learning_rate=0.001):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  z, pi, a, r, outcomes_matrix, r_exp = eval_params_multi(W, x, y, tau, rng)\n",
    "  raw_test_perturb = learn_rng.standard_normal(size=(W.shape))\n",
    "  unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb.flatten())\n",
    "  test_perturbation = unit_test_perturb * perturbation_scale\n",
    "  perturbed_W = W + test_perturbation\n",
    "  _, _, _, r_perturb, _, r_exp_perturb = eval_params_multi(perturbed_W, x, y, tau, rng)\n",
    "  directional_grad_est = (np.mean(r_exp_perturb - r_exp)) / perturbation_scale\n",
    "  update = learning_rate * directional_grad_est * unit_test_perturb\n",
    "  W_new = W + update\n",
    "  return W_new, r, r_exp\n",
    "\n",
    "# simulation\n",
    "learn_rng = np.random.default_rng(0)\n",
    "num_epochs = 10000\n",
    "num_steps = 0\n",
    "mini_batch_size = 281\n",
    "cooling_rate = 0.01\n",
    "W_init = np.zeros((10,65))\n",
    "indices = np.arange(Xs_aug.shape[0])\n",
    "batch_x = Xs_aug.T\n",
    "batch_y = y1.T\n",
    "alg_names = ['Reward Prediction', 'Action Probability', 'Action Log Probability', 'Perturb Measure']\n",
    "alg_funcs = [reward_prediction_multi, action_prob_multi, action_log_prob_multi, always_cheat_perturb_measure_multi]\n",
    "alg_lrs = {'Reward Prediction': 0.0001,\n",
    "           'Action Probability': 0.008,\n",
    "           'Action Log Probability': 0.004,\n",
    "           'Perturb Measure': 0.01}\n",
    "reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "exp_reward_results = {alg_name: [] for alg_name in alg_names}\n",
    "W_s = {alg_name: W_init.copy() for alg_name in alg_names}\n",
    "for epoch in range(num_epochs):\n",
    "  learn_rng.shuffle(indices)\n",
    "  for batch_step in range(0, Xs_aug.shape[0], mini_batch_size):\n",
    "    batch_indices = indices[batch_step:batch_step+mini_batch_size]\n",
    "    batch_x = Xs_aug[batch_indices].T\n",
    "    batch_y = y1[batch_indices].T\n",
    "    for alg_name, alg_func in zip(alg_names, alg_funcs):\n",
    "      lr = alg_lrs[alg_name]\n",
    "      W = W_s[alg_name]\n",
    "      if alg_name == 'Reward Prediction':\n",
    "        tau = 1/((num_steps+10.0) * cooling_rate)\n",
    "      else:\n",
    "        tau = 1.0\n",
    "      # perturb-measure-step uses r_exp everything else just uses r\n",
    "      new_W, r, r_exp = alg_func(W, batch_x, batch_y, tau=tau, rng=learn_rng, learning_rate=lr)\n",
    "      _, _, _, _, _, r_exp_full = eval_params_multi(W, Xs_aug.T, y1.T, tau=0.00001, rng=learn_rng)\n",
    "      W_s[alg_name] = new_W\n",
    "      reward_results[alg_name].append(np.mean(r))\n",
    "      exp_reward_results[alg_name].append(np.mean(r_exp_full))\n",
    "    num_steps += 1\n",
    "  if num_steps > 2000:\n",
    "    break\n",
    "\n",
    "with plt.xkcd():\n",
    "  fig, ax1 = plt.subplots()\n",
    "  theoretical_max = 1.0\n",
    "  # Colors for algorithms\n",
    "  colors = {'Reward Prediction': 'blue', 'Action Probability': 'green',\n",
    "            'Action Log Probability': 'orange', 'Perturb Measure': 'red'}\n",
    "\n",
    "  # First subplot for expected rewards\n",
    "  ax1.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')\n",
    "  for alg_name in alg_names:\n",
    "    eval = np.array(exp_reward_results[alg_name])\n",
    "    eval = np.cumsum(eval)\n",
    "    eval = eval / (np.arange(len(eval)) + 1)\n",
    "    ax1.plot(eval, linestyle='-', color=colors[alg_name], label=f'{alg_name}')\n",
    "\n",
    "    ax1.set_title('Cumulative per Episode Average of\\nExpected (Full Batch) Reward')\n",
    "    ax1.set_ylabel('Cumulative Avg. Expected Reward')\n",
    "    ax1.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this harder problem each of the gradient aligned update rules (reward-prediction-error, action-probability-reinforcement, action-log-probability-reinforcement) all are able to achieve an average reward per episode of roughly 0.75, which means they choose the \"correct\" action roughly 88% of the time. Eventually perturb-measure-step will also be able to achieve this level of performance, but learning is slow with perturb-measure-step. In the simulation above perturb-measure-step was allowed to \"cheat\" in the sense that it used the expected reward (not sampled reward) to evaluate perturbed versus base parameters. Also in the simulation above, to further help perturb-meausure-step each 'epsiode' consisted of mini-batch of 281 experiences, that is $(\\mathbf{x}, y)$ pairs. In contrast the other learning rules adapted using sampled actions and the resulting rewards.\n",
    "\n",
    "This further highlights some of differences between perturb-measure-step and these other gradient aligned methods noted earlier. Each of these gradient oriented update rules needed to be recomputed for this new multi-class output. In contrast the perturb-measure-step update rule needed basically no adaption at all. A new evaluation function could simply be swapped into perturb-measure-step. This difference in ease of implementation in code is important, as it stems from the crucial difference between these different classes of update methods. Perturb-measure-step is what is called a zeroeth-order method, in that it requires absolutly zero information about the underlying structure of the evaluation function being optimized, only the ability to use the evaluation function to evaluate parameters. Optimization processes like this are sometime called \"black box\", since they only see inputs to an evaluation function (the parameter weights $\\mathbf{W}$ in our case), and some (potentially noisy) evaluation of those parameters in the form of a reward. In contrast reward-prediction-error, action-probability-reinforcement, and action-log-probability-reinforcement all belong a class of optimization mehtods known as first-order methods. First-order refers to the use of knowledge of the **structure** of the parameter evaluation function, and in particular knoweldge in the form of derivatives (of the first-order) of various parts of the evaluation function with respect to the parameters. Second order methods also exist which not only take into account the first derivatives of the evaluation function with respect to the parameters, i.e. local linear rates of change, but also second derivatives, i.e. local curvature, to inform parameter updates. We will learn more about second order methods in sequence (blah).\n",
    "\n",
    "So perturb-measure-step is robust in the sense that it can be readily applied to any optimization problem without any condideration of the particular structure of the problem, but this generality comes at the cost of compartively slow learning. In contrast, first order methods can find good solutions much more quickly, but implementation of these methods is more problem specific, as at least some of the structure of the evaluation function is used to inform the update rule. In the rules we looked at the gradient of the activations with respect to the weights, $\\nabla_{\\mathbf{W}}\\mathbf{z}$, was critical to improving behaviour, either through improving a reward prediction, or directly reinforcing the probabilities (log-probailities) of actions taken with reward recieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Conclusion\n",
    "We have now learned about an important way of speeding up learning. That is, by making parameter updates that are (mostly) aligned with the gradient of the function (in this case expected reward) that we are trying to improve through learning. Doing this, however, requires using update rules that somehow utilize implicit knowledge of the structure of the function being optimized. Our gradient aligned methods made use of implicit knowledge about how the sensory inputs interacted with the parameter weights to cause action selection. The \"cheating\" variants of all of our update rules made use of knowledge of how actions combined with the true state of the environment to compute the \"true\" expected reward given the action probabilities, instead of simply relying on an estimate of this expected reward based on sampled actions and resultant rewards. How these both of these kinds of \"knowledge about structure\", that is \"knowledge of self in the form of how parameters generate actions from sensory inputs\" and \"knowledge of the world in the form of how actions generate rewards\" can be used to inform local synaptic plasticity update rules in the brain is an important issue that leave aside for the moment. We do emphasize however, that biological plausibility needs to be considered both from the micro perspective of electro-physiological mechanisms and also from the macro perspective of behavioural learning efficacy. Animals acquire adaptive behaviours rapidly, often after only a few dozen learning episodes for simple associations. There are likely many complimentary process at play allowing for this rapid learning. The point though is that any plasticity rule which does not support this kind of rapid learning, is not plausible at the macro level of explaining learning behaviour, and so must be discounted no matter how empirically well supported the underlying electro-physiological mechanisms are at the micro level.\n",
    "\n",
    "In the next sequence we shift our focus from learning speed, to \"final\" performance. Here we kept our model archetecture fixed and to compare the relative rates of improvement driven by different update rules. In the next sequence we will keep our basic update rule fixed and focus the highest level of performance possible given different models a sensory-behaviour system, in particular looking at what more complex (more parameter) models are able to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "_____\n",
    "# Bio Box: Biological Plausibility Constrained Above and Below\n",
    "\n",
    "We need to think of biological plausibility of learning mechanisms as constrained on two ends.\n",
    "\n",
    "1. From below, by the plausibility of the electro-physiological mechanisms of neural plasticity,\n",
    "2. From above, by the plausibility of the efficacy of the learning rule implemented by those mechanisms.\n",
    "\n",
    "From the perspective of the constraints of electro-physiological mechansims alone an algorithm like perturb-measure-step is very appealing. Perturb-measure-step implements a kind of optimization with no knowledge of the structure of the network that generates the behaviour or the environmental dynamics that then provide reward contingent on behaviour. This makes it easy to reconcile such a learning rule with the observed facts of synaptic plasticity, e.g. Hebbian learning modulated by global reward signals . However, this comes at the cost taking relatively many learning trials to discover good parameters. Thus perturb-measure-step runs into an efficacu constraint when considering optimization of large and complex neural ciruits. Is is simply too slow to account for the synaptic plasticty underlying the **rapid** aquisition of adaptive behaviours observed (near unviserally) in animals. Thus while algorithms roughly like perturb-measure-step are likely implemented and useful for configuring many small and simple neural circuits, they cannot account for all of synaptic plasticity, and in particular the adaptive plasticity in large networks.\n",
    "\n",
    "From the perspective of plausible efficacy, update rules which do make use of knowledge of structure of the network that generates behaviour and/or the environmental dynamics that then provide reward, are more appealing, because they are effective and quick, even in an online, episodic context where learning is driven exclusively by actual experiences (not hypothetical distributions of experiences and expectated values of reward.). However, from the perpsective electro-physiological mechanisms, the question of how knowledge of the structure of the behaviour generating network and/or the dynamics of the environment is implemented on the level of electro-physiological mechanisms is a challenge. However, this is a challenge that needs to be embraced: as networks become large, perturb-measure-step and related methods are non-starters in terms of candidate learning mechanisms because they are simply not **effective** enough.\n",
    "\n",
    "A physiologically simple mechanism that predicts learning on the time course of millions of learning episodes, when behaviourally we see learning over a time course of dozens of episodes, is not biologically plausible. Practical efficacy and eltro-physiological mechansims are **both** necissary constraints on any update rule that purports to describe the dynamics of synaptic plasticit used by the brain to rapidly aquire adaptive behaviours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Algo Box: Zeroeth Order Methods\n",
    "We have primarily used propose-accept-reject and perturb-measure-step as our work-horse zeroeth order methods in examples because of their conceptual simplicity. Zeroeth order methods are important and useful tools in that they make no assumptions, and require no knowledge of the underlying strcuture of the optimization problem. They build up this knowledge purely by inference/experience, evaluating the function to be optimized many times. This situation of having no knowledge, or at least no helpful knowledge, does indeed occur in many practical settings, and in such situations zeroeth order methods are the only option available.\n",
    "\n",
    "The best of such methods extend build on the core ideas of propose-accept-reject and perturb-measure-step, by keeping track of a population of sample points, and then using the relative evaluations of these samples points to inform the selection of new sample points. Such methods make inferences (either implicitly through geometry e.g. in the Nedler-Mead algorithm or explicitly as in the case of Covariance Matrix Adaptation - Evolutionary Strategy, CMA-ES), about both the direction greatest improvement (gradient) which informs which general direction in parameter space new test points should be sampled from relative to the current population of test points, and the curvature of the evaluation fucntion in the neighbourhood of the current test population, which informs how dispersed from the current population of test points new test points should be. High curvature in a given direction means small steps need to be taken in that direction to avoid overshooting as the gradient in that direction is rapidly changing, whereas low curvature in a given direction means that large steps can be taken with lower risk of overshooting as the gradient is relatively stable in that direction.\n",
    "\n",
    "Covariance Matrix Adaption - Evolutionay Strategy is one such state of the art zeoreth order methods, and as the name implies, takes some of its inspiration from the evolutionary processes of natural selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M4\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P2C1_Sequence5",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
