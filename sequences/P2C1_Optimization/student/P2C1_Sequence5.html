
<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>2.1.5: Learning Behaviour Quickly — Modern Foundations of Neuroscience</title>
<!-- Loaded before other Sphinx assets -->
<link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet"/>
<link href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css" rel="stylesheet"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2" rel="preload" type="font/woff2"/>
<link as="font" crossorigin="" href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2" rel="preload" type="font/woff2"/>
<link href="../../../_static/pygments.css" rel="stylesheet" type="text/css">
<link href="../../../_static/styles/sphinx-book-theme.css" rel="stylesheet" type="text/css">
<link href="../../../_static/togglebutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/copybutton.css" rel="stylesheet" type="text/css">
<link href="../../../_static/mystnb.css" rel="stylesheet" type="text/css">
<link href="../../../_static/sphinx-thebe.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" rel="stylesheet" type="text/css"/>
<link href="../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" rel="stylesheet" type="text/css"/>
<!-- Pre-loaded scripts that we'll load fully later -->
<link as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf" rel="preload"/>
<script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
<script src="../../../_static/jquery.js"></script>
<script src="../../../_static/underscore.js"></script>
<script src="../../../_static/doctools.js"></script>
<script src="../../../_static/togglebutton.js"></script>
<script src="../../../_static/clipboard.min.js"></script>
<script src="../../../_static/copybutton.js"></script>
<script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
<script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
<script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
<script async="async" src="../../../_static/sphinx-thebe.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<link href="../../../genindex.html" rel="index" title="Index">
<link href="../../../search.html" rel="search" title="Search"/>
<link href="P2C1_Sequence4.html" rel="prev" title="2.1.4: Learning Behaviour as a Form of High Dimensional Optimization"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="None" name="docsearch:language"/>
<!-- Google Analytics -->
</link></link></link></link></link></link></head>
<body data-offset="60" data-spy="scroll" data-target="#bd-toc-nav">
<!-- Checkboxes to toggle the left sidebar -->
<input aria-label="Toggle navigation sidebar" class="sidebar-toggle" id="__navigation" name="__navigation" type="checkbox"/>
<label class="overlay overlay-navbar" for="__navigation">
<div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input aria-label="Toggle in-page Table of Contents" class="sidebar-toggle" id="__page-toc" name="__page-toc" type="checkbox"/>
<label class="overlay overlay-pagetoc" for="__page-toc">
<div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>
<div class="container-fluid" id="banner"></div>
<div class="container-xl">
<div class="row">
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
<div class="bd-sidebar__content">
<div class="bd-sidebar__top"><div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../../index.html">
<h1 class="site-logo" id="site-title">Modern Foundations of Neuroscience</h1>
</a>
</div><form action="../../../search.html" class="bd-search d-flex align-items-center" method="get">
<i class="icon fas fa-search"></i>
<input aria-label="Search this book..." autocomplete="off" class="form-control" id="search-input" name="q" placeholder="Search this book..." type="search"/>
</form><nav aria-label="Main" class="bd-links" id="bd-docs-nav">
<div class="bd-toc-item active">
<ul class="nav bd-sidenav bd-sidenav__home-link">
<li class="toctree-l1">
<a class="reference internal" href="../../intro.html">
                    Introduction
                </a>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Behaviour, Environments and Optimization, Evolution and Learning
 </span>
</p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../P1C1_BehaviourAsPolicy/student/P1C1_Title.html">
   Behaviour As Policy (P1C1)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox">
<label for="toctree-checkbox-1">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C1_BehaviourAsPolicy/student/P1C1_Sequence1.html">
<strong>
      1.1.1: Gridworld Introduction
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C1_BehaviourAsPolicy/student/P1C1_Sequence2.html">
<strong>
      1.1.2: Thinking of What Receptors and Muscles Do as a Policy
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C1_BehaviourAsPolicy/student/P1C1_Sequence3.html">
<strong>
      1.1.3: Abstraction and Reasoning with Policies
     </strong>
</a>
</li>
</ul>
</input></li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../P1C2_OptimizationAndEnvironment/student/P1C2_Title.html">
   Optimization And Environment (P1C2)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox">
<label for="toctree-checkbox-2">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C2_OptimizationAndEnvironment/student/P1C2_Sequence1.html">
<strong>
      1.2.1: Optimization Makes you Better
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C2_OptimizationAndEnvironment/student/P1C2_Sequence2.html">
<strong>
      1.2.2: What’s Good Depends on the Niche
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C2_OptimizationAndEnvironment/student/P1C2_Sequence3.html">
<strong>
      1.2.3: Normative Thinking
     </strong>
</a>
</li>
</ul>
</input></li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../P1C3_RealEvolution/student/P1C3_Title.html">
   Real Evolution (P1C3)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox">
<label for="toctree-checkbox-3">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C3_RealEvolution/student/P1C3_Sequence1.html">
<strong>
      1.3.1: Populations and Inheritance
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C3_RealEvolution/student/P1C3_Sequence2.html">
<strong>
      1.3.2: Why Sex
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C3_RealEvolution/student/P1C3_Sequence3.html">
<strong>
      1.3.3: Competition and Interaction Transform Optimization into Game Theory
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C3_RealEvolution/student/P1C3_Sequence4.html">
<strong>
      1.3.4: Learning and Evolution
     </strong>
</a>
</li>
</ul>
</input></li>
<li class="toctree-l1 has-children">
<a class="reference internal" href="../../P1C4_LifeIsRL/student/P1C4_Title.html">
   Life Is RL (P1C4)
  </a>
<input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
<label for="toctree-checkbox-4">
<i class="fas fa-chevron-down">
</i>
</label>
<ul>
<li class="toctree-l2">
<a class="reference internal" href="../../P1C4_LifeIsRL/student/P1C4_Sequence1.html">
<strong>
      1.4.1: Thinking of Life as a Markov Decision Process
     </strong>
</a>
</li>
</ul>
</li>
</ul>
<p class="caption">
<span class="caption-text">
  Supervised Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children">
<a class="reference internal" href="P2C1_Title.html">
   Optimization (P2C1)
  </a>
<input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
<label for="toctree-checkbox-5">
<i class="fas fa-chevron-down">
</i>
</label>
<ul class="current">
<li class="toctree-l2">
<a class="reference internal" href="P2C1_Sequence1.html">
<strong>
      2.1.1: Optimization in One Dimension: Developing Intuition
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="P2C1_Sequence2.html">
<strong>
      2.1.2: Optimization in Two Dimensions: Estimating Gradients
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="P2C1_Sequence3.html">
<strong>
      2.1.3: Optimization in Higher Dimensions: The Need for Speed
     </strong>
</a>
</li>
<li class="toctree-l2">
<a class="reference internal" href="P2C1_Sequence4.html">
<strong>
      2.1.4: Learning Behaviour as a Form of High Dimensional Optimization
     </strong>
</a>
</li>
<li class="toctree-l2 current active">
<a class="current reference internal" href="#">
<strong>
      2.1.5: Learning Behaviour Quickly
     </strong>
</a>
</li>
</ul>
</li>
</ul>
</div>
</nav></div>
<div class="bd-sidebar__bottom">
<!-- To handle the deprecated key -->
<div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>
</div>
</div>
<div id="rtd-footer-container"></div>
</div>
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
<div class="header-article row sticky-top noprint">
<div class="col py-1 d-flex header-article-main">
<div class="header-article__left">
<label class="headerbtn" data-placement="right" data-toggle="tooltip" for="__navigation" title="Toggle navigation">
<span class="headerbtn__icon-container">
<i class="fas fa-bars"></i>
</span>
</label>
</div>
<div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
<button aria-label="Launch interactive content" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-rocket"></i>
</button>
<div class="menu-dropdown__content">
<ul>
</ul>
</div>
</div>
<button class="headerbtn" data-placement="bottom" data-toggle="tooltip" onclick="toggleFullScreen()" title="Fullscreen mode">
<span class="headerbtn__icon-container">
<i class="fas fa-expand"></i>
</span>
</button>
<div class="menu-dropdown menu-dropdown-repository-buttons">
<button aria-label="Source repositories" class="headerbtn menu-dropdown__trigger">
<i class="fab fa-github"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/dcownden/PerennialProblemsOfLifeWithABrain" title="Source repository">
<span class="headerbtn__icon-container">
<i class="fab fa-github"></i>
</span>
<span class="headerbtn__text-container">repository</span>
</a>
</li>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="https://github.com/dcownden/PerennialProblemsOfLifeWithABrain/issues/new?title=Issue%20on%20page%20%2Fsequences/P2C1_Optimization/student/P2C1_Sequence5.html&amp;body=Your%20issue%20content%20here." title="Open an issue">
<span class="headerbtn__icon-container">
<i class="fas fa-lightbulb"></i>
</span>
<span class="headerbtn__text-container">open issue</span>
</a>
</li>
</ul>
</div>
</div>
<div class="menu-dropdown menu-dropdown-download-buttons">
<button aria-label="Download this page" class="headerbtn menu-dropdown__trigger">
<i class="fas fa-download"></i>
</button>
<div class="menu-dropdown__content">
<ul>
<li>
<a class="headerbtn" data-placement="left" data-toggle="tooltip" href="../../../_sources/sequences/P2C1_Optimization/student/P2C1_Sequence5.ipynb" title="Download source file">
<span class="headerbtn__icon-container">
<i class="fas fa-file"></i>
</span>
<span class="headerbtn__text-container">.ipynb</span>
</a>
</li>
<li>
<button class="headerbtn" data-placement="left" data-toggle="tooltip" onclick="printPdf(this)" title="Print to PDF">
<span class="headerbtn__icon-container">
<i class="fas fa-file-pdf"></i>
</span>
<span class="headerbtn__text-container">.pdf</span>
</button>
</li>
</ul>
</div>
</div>
<label class="headerbtn headerbtn-page-toc" for="__page-toc">
<span class="headerbtn__icon-container">
<i class="fas fa-list"></i>
</span>
</label>
</div>
</div>
<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
<div class="tocsection onthispage pt-5 pb-3">
<i class="fas fa-list"></i> Contents
    </div>
<nav aria-label="Page" id="bd-toc-nav">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
<strong>
    2.1.5: Learning Behaviour Quickly
   </strong>
</a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#objective-investigate-a-new-simple-optimization-procedure-with-clear-physiological-interpretation">
     Objective: Investigate a new, simple optimization procedure with clear physiological interpretation.
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#dependencies-imports-and-setup">
     Dependencies, Imports and Setup
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#learning-strike-no-strike-quickly-with-reward-prediction-error-and-action-probability-reinforcement">
   2.1.4.1 Learning Strike-No-Strike Quickly with Reward-Prediction-Error and Action-Probability-Reinforcement
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#reward-prediction-error-episodic-update-rule">
     Reward-Prediction-Error Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#action-probability-reinforcement-episodic-update-rule">
     Action Probability Reinforcement Episodic Update Rule
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#perturb-measure-step-is-slow-because-of-poor-gradient-alignment">
   2.1.4.2 Perturb-Measure-Step is Slow Because of Poor Gradient Alignment
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#expected-reward-gradient-update-rule">
     Expected Reward Gradient Update Rule
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#loss-box-expected-reward-gradient-is-action-log-probability-reinforcement">
   Loss Box: Expected-Reward-Gradient
   <strong>
    Is
   </strong>
   Action-Log-Probability-Reinforcement
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#learning-to-do-more-than-one-thing">
   2.1.4.3 Learning To Do More Than One Thing
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#reward-prediction-error-multi-class-episodic-update-rule">
     Reward-Prediction-Error Multi-Class Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#action-probability-reinforcement-mulit-class-episodic-update-rule">
     Action-Probability-Reinforcement Mulit-Class Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#expected-reward-gradient-multi-class-episodic-update-rule">
     Expected-Reward-Gradient Multi-Class Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#conclusion">
     Conclusion
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bio-box-biological-plausibility-constrained-above-and-below">
   Bio Box: Biological Plausibility Constrained Above and Below
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#algo-box-zeroeth-order-methods">
   Algo Box: Zeroeth Order Methods
  </a>
</li>
</ul>
</nav>
</div>
</div>
<div class="article row">
<div class="col pl-md-3 pl-lg-5 content-container">
<!-- Table of contents that is only displayed when printing the page -->
<div class="onlyprint" id="jb-print-docs-body">
<h1>2.1.5: Learning Behaviour Quickly</h1>
<!-- Table of contents -->
<div id="print-main-content">
<div id="jb-print-toc">
<div>
<h2> Contents </h2>
</div>
<nav aria-label="Page">
<ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#">
<strong>
    2.1.5: Learning Behaviour Quickly
   </strong>
</a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#objective-investigate-a-new-simple-optimization-procedure-with-clear-physiological-interpretation">
     Objective: Investigate a new, simple optimization procedure with clear physiological interpretation.
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#setup">
   Setup
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#dependencies-imports-and-setup">
     Dependencies, Imports and Setup
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#learning-strike-no-strike-quickly-with-reward-prediction-error-and-action-probability-reinforcement">
   2.1.4.1 Learning Strike-No-Strike Quickly with Reward-Prediction-Error and Action-Probability-Reinforcement
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#reward-prediction-error-episodic-update-rule">
     Reward-Prediction-Error Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#action-probability-reinforcement-episodic-update-rule">
     Action Probability Reinforcement Episodic Update Rule
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#perturb-measure-step-is-slow-because-of-poor-gradient-alignment">
   2.1.4.2 Perturb-Measure-Step is Slow Because of Poor Gradient Alignment
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#expected-reward-gradient-update-rule">
     Expected Reward Gradient Update Rule
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#loss-box-expected-reward-gradient-is-action-log-probability-reinforcement">
   Loss Box: Expected-Reward-Gradient
   <strong>
    Is
   </strong>
   Action-Log-Probability-Reinforcement
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#learning-to-do-more-than-one-thing">
   2.1.4.3 Learning To Do More Than One Thing
  </a>
<ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#reward-prediction-error-multi-class-episodic-update-rule">
     Reward-Prediction-Error Multi-Class Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#action-probability-reinforcement-mulit-class-episodic-update-rule">
     Action-Probability-Reinforcement Mulit-Class Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#expected-reward-gradient-multi-class-episodic-update-rule">
     Expected-Reward-Gradient Multi-Class Episodic Update Rule
    </a>
</li>
<li class="toc-h2 nav-item toc-entry">
<a class="reference internal nav-link" href="#conclusion">
     Conclusion
    </a>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#bio-box-biological-plausibility-constrained-above-and-below">
   Bio Box: Biological Plausibility Constrained Above and Below
  </a>
</li>
<li class="toc-h1 nav-item toc-entry">
<a class="reference internal nav-link" href="#algo-box-zeroeth-order-methods">
   Algo Box: Zeroeth Order Methods
  </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<main id="main-content" role="main">
<div>
<p><a href="https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P2C1_Optimization/student/P2C1_Sequence5.ipynb" target="_blank"><img alt="Open In Colab" src="https://colab.research.google.com/assets/colab-badge.svg"/></a>   <a href="https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_Optimization/student/P2C1_Sequence5.ipynb" target="_blank"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg"/></a></p>
<p>The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do.</p>
<hr class="docutils"/>
<div class="section" id="learning-behaviour-quickly">
<h1><strong>2.1.5: Learning Behaviour Quickly</strong><a class="headerlink" href="#learning-behaviour-quickly" title="Permalink to this headline">¶</a></h1>
<div class="section" id="objective-investigate-a-new-simple-optimization-procedure-with-clear-physiological-interpretation">
<h2>Objective: Investigate a new, simple optimization procedure with clear physiological interpretation.<a class="headerlink" href="#objective-investigate-a-new-simple-optimization-procedure-with-clear-physiological-interpretation" title="Permalink to this headline">¶</a></h2>
<p>In this sequence we will:</p>
<ul class="simple">
<li><p>Investigate new plasticty rules that are able to aquire adaptive behaviour more rapidly. We continue to work with the strike-no-strike problem where the decision to strike or not is made on the basis of 64 sensory inputs (features) to, and striking is beneficial or costly depending on whether or not prey is actual present.</p></li>
<li><p>Compare these new plasticity rules to perturb-measure-step both in terms of learning rate and physilogical plausibility.</p></li>
<li><p>Understand the efficacy of these new learning rules using an optimization perspective.</p></li>
<li><p>Test these new learning rules on a harder learning problem, still with 64 sensory features, but now with 10 possible behavioural responses to see how increasing the scale of the problem affect the rate at which adaptive behaviour is learned.</p></li>
</ul>
</div>
</div>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>Run the following cell to setup and install the various dependencies and helper functions for this sequence.</p>
<div class="section" id="dependencies-imports-and-setup">
<h2>Dependencies, Imports and Setup<a class="headerlink" href="#dependencies-imports-and-setup" title="Permalink to this headline">¶</a></h2>
<p>You don’t need to worry about how this code works – but you do need to <strong>run the cell</strong>. It will take a minute. Read ahead while the dependencies are loading.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @title Dependencies, Imports and Setup</span>
<span class="c1"># @markdown You don't need to worry about how this code works – but you do need to **run the cell**. It will take a minute. Read ahead while the dependencies are loading.</span>
<span class="o">!</span>apt<span class="w"> </span>install<span class="w"> </span>libgraphviz-dev<span class="w"> </span>&gt;<span class="w"> </span>/dev/null<span class="w"> </span><span class="m">2</span>&gt;<span class="w"> </span>/dev/null<span class="w"> </span>#colab
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>ipympl<span class="w"> </span>pygraphviz<span class="w"> </span>vibecheck<span class="w"> </span>datatops<span class="w"> </span>jupyterquiz<span class="w"> </span>ucimlrepo<span class="w"> </span>&gt;<span class="w"> </span>/dev/null<span class="w"> </span><span class="m">2</span>&gt;<span class="w"> </span>/dev/null<span class="w"> </span>#google.colab

<span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">requests.exceptions</span> <span class="kn">import</span> <span class="n">RequestException</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>
<span class="kn">from</span> <span class="nn">matplotlib.lines</span> <span class="kn">import</span> <span class="n">Line2D</span>
<span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">LogNorm</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">gridspec</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pygraphviz</span> <span class="k">as</span> <span class="nn">pgv</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>
<span class="kn">from</span> <span class="nn">enum</span> <span class="kn">import</span> <span class="n">Enum</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="kn">import</span> <span class="n">tabulate</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">clear_output</span><span class="p">,</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">HTML</span><span class="p">,</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">jupyterquiz</span> <span class="kn">import</span> <span class="n">display_quiz</span>
<span class="kn">from</span> <span class="nn">vibecheck</span> <span class="kn">import</span> <span class="n">DatatopsContentReviewContainer</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">ucimlrepo</span> <span class="kn">import</span> <span class="n">fetch_ucirepo</span>
<span class="kn">from</span> <span class="nn">IPython.core.magic</span> <span class="kn">import</span> <span class="n">register_cell_magic</span>

<span class="nd">@register_cell_magic</span>
<span class="k">def</span> <span class="nf">skip</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">cell</span><span class="p">):</span>
  <span class="k">return</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="n">fetch_ucirepo</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">values</span>
<span class="c1"># Translate the data to have a minimum of 0</span>
<span class="n">X_translated</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="c1"># Scale the data to have a range from 0 to 12 (which is 6 - (-6))</span>
<span class="n">scaling_factor</span> <span class="o">=</span> <span class="mi">12</span> <span class="o">/</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">X_translated</span> <span class="o">*</span> <span class="n">scaling_factor</span>
<span class="c1"># Finally, shift the data to be centered between -6 and 6</span>
<span class="n">X_final</span> <span class="o">=</span> <span class="n">X_scaled</span> <span class="o">-</span> <span class="mi">6</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">data_set</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">values</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2021</span><span class="p">)</span>
<span class="n">scramble_permutation</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Xs</span> <span class="o">=</span> <span class="n">X_final</span><span class="p">[:,</span> <span class="n">scramble_permutation</span><span class="p">]</span>
<span class="n">Xs_aug</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">Xs</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">Xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))])</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">y</span> <span class="o">%</span> <span class="mi">2</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">simple_index</span> <span class="o">=</span> <span class="p">((</span><span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">))</span>
<span class="n">X_simple</span> <span class="o">=</span> <span class="n">Xs</span><span class="p">[</span><span class="n">simple_index</span><span class="p">]</span>
<span class="n">y1_simple</span> <span class="o">=</span> <span class="n">y1</span><span class="p">[</span><span class="n">simple_index</span><span class="p">]</span>
<span class="c1"># if you only had one feature which would likely be best for discrimination</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">class_a_sep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_simple</span><span class="p">[</span><span class="n">y1_simple</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_simple</span><span class="p">[</span><span class="n">y1_simple</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="n">class_b_sep</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_simple</span><span class="p">[</span><span class="n">y1_simple</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_simple</span><span class="p">[</span><span class="n">y1_simple</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span>
<span class="n">best_feature</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">class_a_sep</span> <span class="o">-</span> <span class="n">class_b_sep</span><span class="p">)</span>
<span class="c1">#print(f'Best feature is {best_feature}')</span>
<span class="n">X_simple_1_feature</span> <span class="o">=</span> <span class="n">X_simple</span><span class="p">[:,</span> <span class="p">[</span><span class="n">best_feature</span><span class="p">]]</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">"ignore"</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s2">"matplotlib"</span><span class="p">)</span>
<span class="c1"># random seed settings and</span>
<span class="c1"># getting torch to use gpu if it's there</span>


<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed_torch</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Function that controls randomness. NumPy and random modules must be imported.</span>

<span class="sd">  Args:</span>
<span class="sd">    seed : Integer</span>
<span class="sd">      A non-negative integer that defines the random state. Default is `None`.</span>
<span class="sd">    seed_torch : Boolean</span>
<span class="sd">      If `True` sets the random seed for pytorch tensors, so pytorch module</span>
<span class="sd">      must be imported. Default is `True`.</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">32</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">seed_torch</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Random seed </span><span class="si">{</span><span class="n">seed</span><span class="si">}</span><span class="s1"> has been set.'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">seed_worker</span><span class="p">(</span><span class="n">worker_id</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  DataLoader will reseed workers following randomness in</span>
<span class="sd">  multi-process data loading algorithm.</span>

<span class="sd">  Args:</span>
<span class="sd">    worker_id: integer</span>
<span class="sd">      ID of subprocess to seed. 0 means that</span>
<span class="sd">      the data will be loaded in the main process</span>
<span class="sd">      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">worker_seed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">initial_seed</span><span class="p">()</span> <span class="o">%</span> <span class="mi">2</span><span class="o">**</span><span class="mi">32</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>
  <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">worker_seed</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">set_device</span><span class="p">():</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Set the device. CUDA if available, CPU otherwise</span>

<span class="sd">  Args:</span>
<span class="sd">    None</span>

<span class="sd">  Returns:</span>
<span class="sd">    Nothing</span>
<span class="sd">  """</span>
  <span class="n">device</span> <span class="o">=</span> <span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">"cpu"</span>
  <span class="k">if</span> <span class="n">device</span> <span class="o">!=</span> <span class="s2">"cuda"</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"This notebook isn't using and doesn't need a GPU. Good."</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"GPU is enabled in this notebook but not needed."</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"If possible, in the menu under `Runtime` -&gt; "</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"`Change runtime type.`  select `CPU`"</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">device</span>


<span class="n">SEED</span> <span class="o">=</span> <span class="mi">2021</span>
<span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">SEED</span><span class="p">)</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">set_device</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">printmd</span><span class="p">(</span><span class="n">string</span><span class="p">):</span>
  <span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">string</span><span class="p">))</span>


<span class="c1"># the different utility .py files used in this notebook</span>
<span class="n">filenames</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1"># just run the code straight out of the response, no local copies needed!</span>
<span class="k">for</span> <span class="n">filename</span> <span class="ow">in</span> <span class="n">filenames</span><span class="p">:</span>
  <span class="n">url</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s1">'</span>
  <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
  <span class="c1"># Check that we got a valid response</span>
  <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
    <span class="n">code</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="n">exec</span><span class="p">(</span><span class="n">code</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Failed to download </span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

<span class="c1"># environment contingent imports</span>
<span class="k">try</span><span class="p">:</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">'Running in colab'</span><span class="p">)</span>
  <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">output</span>
  <span class="n">output</span><span class="o">.</span><span class="n">enable_custom_widget_manager</span><span class="p">()</span>
  <span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">data_table</span>
  <span class="n">data_table</span><span class="o">.</span><span class="n">disable_dataframe_formatter</span><span class="p">()</span>
  <span class="c1">#from google.colab import output as colab_output</span>
  <span class="c1">#colab_output.enable_custom_widget_manager()</span>
  <span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">True</span>
<span class="k">except</span><span class="p">:</span>
  <span class="n">IN_COLAB</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="nb">print</span><span class="p">(</span><span class="s1">'Not running in colab'</span><span class="p">)</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'
<span class="o">%</span><span class="k">matplotlib</span> widget
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ioff</span><span class="p">()</span> <span class="c1">#need to use plt.show() or display explicitly</span>
<span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">'matplotlib.font_manager'</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">ERROR</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">remove_ip_clutter</span><span class="p">(</span><span class="n">fig</span><span class="p">):</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">header_visible</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">toolbar_visible</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">resizable</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">footer_visible</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">content_review</span><span class="p">(</span><span class="n">notebook_section</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">DatatopsContentReviewContainer</span><span class="p">(</span>
    <span class="s2">""</span><span class="p">,</span>  <span class="c1"># No text prompt</span>
    <span class="n">notebook_section</span><span class="p">,</span>
    <span class="p">{</span>
      <span class="s2">"url"</span><span class="p">:</span> <span class="s2">"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab"</span><span class="p">,</span>
      <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"neuro_book"</span><span class="p">,</span>
      <span class="s2">"user_key"</span><span class="p">:</span> <span class="s2">"xuk960xj"</span><span class="p">,</span>
    <span class="p">},</span>
  <span class="p">)</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
<span class="n">feedback_prefix</span> <span class="o">=</span> <span class="s2">"P2C1_S5"</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Random seed 2021 has been set.
This notebook isn't using and doesn't need a GPU. Good.
Running in colab
Not running in colab
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="learning-strike-no-strike-quickly-with-reward-prediction-error-and-action-probability-reinforcement">
<h1>2.1.4.1 Learning Strike-No-Strike Quickly with Reward-Prediction-Error and Action-Probability-Reinforcement<a class="headerlink" href="#learning-strike-no-strike-quickly-with-reward-prediction-error-and-action-probability-reinforcement" title="Permalink to this headline">¶</a></h1>
<p>Recall from the previous sequence that the cartoon organism that inspires this problem can be thought of as having 64 photo-sensitive receptors, and based on the combination of inputs from these receptors it must decide whether to strike or not. The organism pays a cost of one if it strikes when it shouldn’t (prey is absent) and recieves a reward of one if it strikes when it should (prey is present). It receives no cost or reward when it does not strike. To get a sense of this discrimination problem try it yourself by running the code cell below.</p>
<p><strong>Run this cell</strong> to try out the more complex ‘strike-no-strike’ discrimination task.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown **Run this cell** to try out the more complex 'strike-no-strike' discrimination task.</span>

<span class="k">class</span> <span class="nc">InteractiveMNISTPredator</span><span class="p">():</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">features</span><span class="o">=</span><span class="n">Xs</span><span class="p">,</span>
               <span class="n">labels</span><span class="o">=</span><span class="n">y1</span><span class="p">,</span>
               <span class="n">extra_labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
               <span class="n">feedback_type</span><span class="o">=</span><span class="s1">'on_strike_only'</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
    <span class="c1"># Initialize dataset, settings for image scrambling and feedback</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="c1"># features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)</span>
    <span class="c1"># labels is num_data_points x 1 (values 0-9 or 0/1 depending)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feedback_type</span> <span class="o">=</span> <span class="n">feedback_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1">#sample_order = np.arange(self.features.shape[0])</span>
    <span class="n">sample_order</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">sample_order</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">sample_order</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">extra_labels</span> <span class="o">=</span> <span class="n">extra_labels</span><span class="p">[</span><span class="n">sample_order</span><span class="p">]</span>
    <span class="c1"># initialize game state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">best_possible_score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">successful_strikes</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">failed_strikes</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">non_strikes</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Initialize widgets</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strike_button</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">'Strike'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">no_strike_button</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">'No Strike'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">score_display</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Output</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feedback_display</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Output</span><span class="p">()</span>

    <span class="c1"># Initialize the figure for image display</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">remove_ip_clutter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">remove_ip_clutter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">show_next_image</span><span class="p">()</span>
    <span class="c1"># Bind event handlers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strike_button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">on_strike_clicked</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">no_strike_button</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">on_no_strike_clicked</span><span class="p">)</span>

    <span class="c1"># Arrange widgets in a layout</span>
    <span class="n">buttons_layout</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">HBox</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">strike_button</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">no_strike_button</span><span class="p">])</span>
    <span class="n">current_buttons</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">VBox</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">,</span> <span class="n">buttons_layout</span><span class="p">])</span>
    <span class="n">previous_feedback</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">VBox</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_display</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ui</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">HBox</span><span class="p">([</span><span class="n">previous_feedback</span><span class="p">,</span> <span class="n">current_buttons</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_display</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">show_next_image</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Display the next image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="o">==</span> <span class="mi">64</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">scalar_value</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
      <span class="c1"># Initialize the 8x8 array with -6 (black)</span>
      <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="o">-</span><span class="mf">6.0</span><span class="p">)</span>
      <span class="c1"># Set the first ring to 6 (white)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="c1"># Set the second ring to 6 (white)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="c1"># Set the third (inner ring) back to -6 (black)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">6</span>
      <span class="c1"># Assuming scalar_value is already in the range -6 to 6</span>
      <span class="c1">#print(scalar_value)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">scalar_value</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Unexpected image shape: </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flipud</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span> <span class="o">=</span> <span class="n">image</span>
    <span class="c1"># Display the image</span>
    <span class="c1">#print(image)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">'equal'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">'equal'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">'upper'</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">'upper'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Current Sensory Input'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Previous Sensory Input'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">on_strike_clicked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">button</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">process_decision</span><span class="p">(</span><span class="s1">'Strike'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">on_no_strike_clicked</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">button</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">process_decision</span><span class="p">(</span><span class="s1">'No Strike'</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">process_decision</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">decision</span><span class="p">):</span>
    <span class="c1"># freeze buttons while we process</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strike_button</span><span class="o">.</span><span class="n">disabled</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">no_strike_button</span><span class="o">.</span><span class="n">disabled</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># Process the user's decision, update score, and provide feedback</span>
    <span class="n">correct_action</span> <span class="o">=</span> <span class="s1">'Strike'</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="s1">'No Strike'</span>
    <span class="k">if</span> <span class="n">decision</span> <span class="o">==</span> <span class="s1">'Strike'</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">decision</span> <span class="o">==</span> <span class="n">correct_action</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">successful_strikes</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">-=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failed_strikes</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">decision</span> <span class="o">==</span> <span class="s1">'No Strike'</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">non_strikes</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="c1"># no strike means no gain or loss</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Unknown decision: </span><span class="si">{</span><span class="n">decision</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

    <span class="c1"># Show feedback and score</span>
    <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feedback_type</span> <span class="o">==</span> <span class="s1">'both'</span> <span class="ow">or</span>
      <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feedback_type</span> <span class="o">==</span> <span class="s1">'on_strike_only'</span> <span class="ow">and</span> <span class="n">decision</span> <span class="o">==</span> <span class="s1">'Strike'</span><span class="p">)):</span>
      <span class="c1"># Show informative feedback</span>
      <span class="n">feedback</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">'Your last choice: </span><span class="si">{</span><span class="n">decision</span><span class="si">}</span><span class="se">\n</span><span class="s1">Correct last choice: </span><span class="si">{</span><span class="n">correct_action</span><span class="si">}</span><span class="s1">'</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Show uninformative feedback</span>
      <span class="n">feedback</span> <span class="o">=</span> <span class="s1">'Feedback only available after striking.'</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_display</span><span class="p">:</span>
      <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="c1">#print(self.labels[self.current_index])</span>
      <span class="c1">#print(self.extra_labels[self.current_index])</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">feedback</span><span class="p">)</span>

    <span class="c1"># Show score</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_display</span><span class="p">:</span>
      <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">average_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Total Score: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">score</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of Trials: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Successful Strikes: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">successful_strikes</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Failed Strikes: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">failed_strikes</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Non-Strikes: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">non_strikes</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Average Score Per Trial: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

    <span class="c1"># Prepare the next image</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1">#print(self.current_index)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">show_next_image</span><span class="p">()</span>
    <span class="c1"># Re-enable buttons</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strike_button</span><span class="o">.</span><span class="n">disabled</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">no_strike_button</span><span class="o">.</span><span class="n">disabled</span> <span class="o">=</span> <span class="kc">False</span>


<span class="n">scramble_bin_hard</span> <span class="o">=</span> <span class="n">InteractiveMNISTPredator</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">Xs</span><span class="p">,</span>
                                             <span class="n">labels</span><span class="o">=</span><span class="n">y1</span><span class="p">,</span>
                                             <span class="n">feedback_type</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">scramble_bin_hard</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">scramble_bin_hard</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">)</span>
<span class="n">clear_output</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">scramble_bin_hard</span><span class="o">.</span><span class="n">ui</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "caebc221c70a4b6d9d7328323993add4"}
</script></div>
</div>
<p>We model this creature’s sensory-behaviour system as follows. <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the raw sensory input (column vector) of length 64 in a given episode. Each element <span class="math notranslate nohighlight">\(x_i\)</span> of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> corresponds to the activation level of a single photosensitive neuron. We visualize an example sensory input as an <span class="math notranslate nohighlight">\(8 \times 8\)</span> grid in the code cell below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># visualizing the example we see that lower values correspond to darker pixels</span>
<span class="c1"># and higher values correspond to lighter values</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">remove_ip_clutter</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Xs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "7b82334797bd454db215da66df2295c7"}
</script></div>
</div>
<p>These input neurons are then connected by synapses to a single output neuron. The activation level, <span class="math notranslate nohighlight">\(z\)</span>, of this output neuron is computed as
$<span class="math notranslate nohighlight">\(z = \mathbf{Wx} + b\)</span><span class="math notranslate nohighlight">\(
Here, \)</span>b<span class="math notranslate nohighlight">\( is the (scalar) bias, or baseline activation level of the output neuron, and \)</span>\mathbf{W}<span class="math notranslate nohighlight">\( is a matrix of synaptic weights between the input neurons and the single output neuron. (In this case where there is only one output neuron so \)</span>\mathbf{W}$ has shape 1x64 so could also be thought of as a row vector.)</p>
<p>To simplify exposition and coding the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is augmented to have a feature which is always 1, and then the bias terms can be treated as the weight connecting to this constant valued feature. That is</p>
<div class="math notranslate nohighlight">
\[z = \mathbf{Wx}\]</div>
<p>And now <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> has shape 1x65. The probability of striking is determined by the activation level of the output neuron, together with a temperature parameter <span class="math notranslate nohighlight">\(\tau\)</span> which determines how exploratory the behaviour of the organism is, specifically:
$<span class="math notranslate nohighlight">\( \Pr \{\text{strike}\} = \sigma(z;\tau) \)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\( \Pr \{\text{no strike}\} = 1 - \sigma(z;\tau)\)</span>$</p>
<p>Here $<span class="math notranslate nohighlight">\(\sigma(z;\tau): \frac{1}{1+\exp(-z / \tau)} = \frac{\exp(z/ \tau)}{1+\exp(z / \tau)}\)</span><span class="math notranslate nohighlight">\( is the logistic (sigmoid) function, with temperature hyper-parameter \)</span>\tau<span class="math notranslate nohighlight">\(. High values of \)</span>\tau<span class="math notranslate nohighlight">\( make striking and not striking have nearly equal probability regardless of the value of \)</span>z<span class="math notranslate nohighlight">\(, whereas very low values of \)</span>\tau<span class="math notranslate nohighlight">\( mean that even a very slightly positive \)</span>z<span class="math notranslate nohighlight">\( value will correspond to near certainty of striking, and a very slight negative value will result in an almost certain chance of not striking. In other words \)</span>\tau<span class="math notranslate nohighlight">\( determines how responsive the striking probabilities are to changes in \)</span>z$.</p>
<p>Instead of thinking of <span class="math notranslate nohighlight">\(z\)</span> as simply just a way of determining the striking probability, we could also treat <span class="math notranslate nohighlight">\(z\)</span> as a kind of prediction or expectation of the reward that will occur if the striking action is taken given the current sensory experience <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. As the expectation of reward recieved increases, the probability of taking the striking action also increases, if the expectation of reward recieved the probability of taking the striking action decreases, according the the temperature scaled softmax function above.</p>
<p>Taking this perspective, where <span class="math notranslate nohighlight">\(z\)</span> as the organism’s internal representation of the expected reward when taking the striking action, then the organism could compare its expectation of reward with the reward it actually receives, and use the difference between expectation and reality to update the synaptic weights that had a causal impact on this expectation in such a way that the expected reward more closely aligns with the received reward. Given our current network model of the sensory-behaviour system a rule that does this is:</p>
<div class="section" id="reward-prediction-error-episodic-update-rule">
<h2>Reward-Prediction-Error Episodic Update Rule<a class="headerlink" href="#reward-prediction-error-episodic-update-rule" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[\Delta W_i = s \cdot (r-z) \cdot x_i \]</div>
<p>Or in vector form</p>
<div class="math notranslate nohighlight">
\[ \Delta \mathbf{W} = s \cdot (r-z) \cdot \mathbf{x}^T\]</div>
<p>This is saying that if the received reward is greater than the predicted reward, then it would be better if <span class="math notranslate nohighlight">\(z\)</span> had been larger, which means that in the cases where <span class="math notranslate nohighlight">\(x_i\)</span> was positive, the weight from <span class="math notranslate nohighlight">\(x_i\)</span> should be increased, in proportion to the strength of activation of <span class="math notranslate nohighlight">\(x_i\)</span> and in the case that <span class="math notranslate nohighlight">\(x_i\)</span> was negative, the weight from <span class="math notranslate nohighlight">\(x_i\)</span> should be decreased, again in proportion to the strength of the activation of <span class="math notranslate nohighlight">\(x_i\)</span>. Conversely if, the reward received is less than predicted then it would have been better if <span class="math notranslate nohighlight">\(z\)</span> was smaller and so, in the case where <span class="math notranslate nohighlight">\(x_i\)</span> is positive the weight from <span class="math notranslate nohighlight">\(x_i\)</span> should be decreased, in proportion to the strength of the activation of <span class="math notranslate nohighlight">\(x_i\)</span> and in the case that <span class="math notranslate nohighlight">\(x_i\)</span> is negative, the weight from <span class="math notranslate nohighlight">\(x_i\)</span> should be decreased. (Note that in the vector formulation above we <span class="math notranslate nohighlight">\(\mathbf{x}^T\)</span> since <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is column vector (65, 1) whereas <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is a row vector (1, 65), and the shape and orientation of <span class="math notranslate nohighlight">\(\Delta \mathbf{W}\)</span> should match the shape of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.)</p>
<p>This rule can be understood as shifting the weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> in the direction that minimizes the squared error of the reward prediction.</p>
<div class="math notranslate nohighlight">
\[ z = \mathbf{W}\mathbf{x}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align} \nabla_\mathbf{W} (r-z)^2 &amp; = 2 \ (r-z) \ \nabla_{\mathbf{W}}z \\
&amp; = 2 (r-z) \mathbf{x}^T
\end{align}\end{split}\]</div>
<p>The factor of 2 is subsumed in the step scaling factor <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>We call this an episodic update rule because it can concievably be implemented after a single experience of sensory input, action selection, and resultant reward.</p>
<p>In this scenario the prediction pertains to the reward when striking, so this difference between expected and received reward simply does not apply to the case where no striking occurs, so no learning occurs without striking. Note that the <span class="math notranslate nohighlight">\(\tau\)</span> parameter in the sigmoid striking probability function determines how exploratory the behaviour is independently of the predicted reward. <span class="math notranslate nohighlight">\(\tau\)</span> is very important. During the earlier episodes of the organism’s life it should be more exploratory (higher <span class="math notranslate nohighlight">\(\tau\)</span>), but once it has learned good predictions of reward across a representative range of experiences it can settle into simply exploiting the knowledge it has acquired (lower <span class="math notranslate nohighlight">\(\tau\)</span>). Deciding when and how to transition from exploratory behaviours to more directly reward maximizing behaviours is known as an exploration-exploitation trade-off, and is a central challenge in Reinforcement Learning. We just mention this in passing now, but will dive more deeply into the exploration-exploitation tradeoff in sequence (blah).</p>
<p>There is also a “cheating” version of this update rule, where an update to the parameters is always made based on the <span class="math notranslate nohighlight">\(r_{\text{strike}}\)</span>, the reward that the organism would recieve if it strikes, regardless of whether it actually strikes or not, i.e. it gets to know what the right answer was regardless of what action it takes. This could be implemented by the organism having some additional, post-hoc prey detection that augments the organism’s basic behavioural inputs with additional teaching signals. These additional signals allow the organism to learn from situations where it’s like “Oh I wish I had struck at the food that is now swimming past me” and also situations where it’s like “Oh there really was no food there, good thing I didn’t strike”. Such mechanism likely exist, but are more complex than the simple case of reinforcing actions taken based on intrinsic reward recieved. Our focus for now is this simple case. (We could approximate such a system by “rewarding” not striking when no prey is present with 1 and giving a penalty of 1 when an organism fails to strike when prey is present, but this leaves open the question of if and how such “learning scaffolding” rewards need to be and can be kept seperate from ‘actual’ rewards that count towards fitness in the evolutionary sense. This is a complex topic that we will revist throught the book, in particular in sequences (blah)).</p>
<p>From an evolutionary and adaptive behaviour perspective we are interested in how the organism can quickly learn to take rewarding actions. This suggest an alternative updating scheme where the reward recieved directly reinforces the probability of taking the action that produced it. To do this we need know how a change in <span class="math notranslate nohighlight">\(z\)</span> translates into an increase (or decrease) in the probability of striking, so that we can increase (decrease) action probabilites in proportion to the rewards recieved as a result of particular action choices. We call this the Action-Probability-Reinforcement update rule</p>
</div>
<div class="section" id="action-probability-reinforcement-episodic-update-rule">
<h2>Action Probability Reinforcement Episodic Update Rule<a class="headerlink" href="#action-probability-reinforcement-episodic-update-rule" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[ \Delta W_i = s \cdot r \cdot \sigma'(z) \cdot x_i\]</div>
<p>or in vector form</p>
<div class="math notranslate nohighlight">
\[ \Delta \mathbf{W} = s \cdot r \cdot \sigma'(z) \cdot \mathbf{x}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\sigma'\)</span> is the derivative of the sigmoid striking probability function. We no longer need to explicitly control exploration versus exploitation with <span class="math notranslate nohighlight">\(\tau\)</span>, since this update rule acts directly on action probabilities, so we simply set <span class="math notranslate nohighlight">\(\tau =1\)</span> here for simplicity. The above rule uses the raw rewards to reinforce actions. The probability of striking is given by <span class="math notranslate nohighlight">\(\sigma(z)\)</span> so the way to change <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> to increase the probability of striking is to shift the weights in the direction of the gradient of the probability of taking the action given by <span class="math notranslate nohighlight">\(\sigma'(z)\)</span>. Using the defination of <span class="math notranslate nohighlight">\(\sigma\)</span> above and the chain rule from calculus we can derive that <span class="math notranslate nohighlight">\(\sigma'(z) = \sigma(z) (1-\sigma(z))\)</span>. This is telling us what we already know intuitively about the sigmoid function, which is that it is montonically increasing, and relatively flat when probalities are close to zero or one, and steepest when probabilities are close to even, that is 0.5.</p>
<p>The above rule can be understood as shifting weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> so that the probability of striking in a particular instance (<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>) is increased (or decreased in the case of negative reward) in proportion to the rewards that result from taking the striking action.
$<span class="math notranslate nohighlight">\( \Pr\{\text{strike}|\mathbf{x}\} = \sigma(\mathbf{W}\mathbf{x})\)</span><span class="math notranslate nohighlight">\(
\)</span><span class="math notranslate nohighlight">\(\begin{align} \Delta \mathbf{W} &amp;= s \cdot \nabla_\mathbf{W} \Pr\{\text{strike}| \mathbf{x}\} \cdot r \\
&amp; = s \cdot \sigma'(z) \cdot \nabla_{\mathbf{W}} \mathbf{Wx} \cdot r \\
&amp; = s \cdot \sigma'(z) \cdot \mathbf{x} \cdot r \\
&amp; = s \cdot \sigma(z) (1 - \sigma(z)) \cdot \mathbf{x} \cdot r
\end{align}\)</span>$</p>
<p>Again we call this an episodic update rule because it can concievably be applied after a single episode consisting of sensory experience, action selection, and resultant reward.</p>
<p>Again, this rule also has a cheating variant. The normal variant exclusively uses the actual reward <span class="math notranslate nohighlight">\(r\)</span> obtained from sampling an action, and experiencing the subsequent reward. In contrast, the cheating variant (often used in ML contexts) uses the expected reward <span class="math notranslate nohighlight">\(\mathbb{E}[r]\)</span> given the probability distribution over possible actions instead of utilizing a particular sample from the distribution of possible actions and the particular resultant reward. From a practical ML perspective it is sometimes much more efficient to directly compute expected rewards than to sample actions and rewards stochastically, and in these cases using expectations can greatly accelerate learning. However, this cheating variant is only episodic if it is able to somehow consider all possible actions and all possible outcomes within that single episode, certainly possible, but adds the complexity of trying to learn from hypothetical outcomes, much as in the cheating version of Reward-Prediction-Error update rule.</p>
<p>Complete the coding exercise below to see how these two different udpated rules, reward-prediction-error and action-probability-reinforcement, compare with our perturb-measure-step rule, both when using “realistic” episodic and cheating variants. Note the that the <code class="docutils literal notranslate"><span class="pre">eval_params</span></code> function we use below can be thought of as containing both an action selection component that depends on the sensory inputs <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> and then a reward component that calculates the appropriate reward given the action selected and the true state of the environment encoded in <span class="math notranslate nohighlight">\(y\)</span>, i.e. the presense or absence of prey which is as yet unobserved by organisms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">################################################################################</span>
<span class="c1"># TODO for students: Replace ... in the lines below with one of the following</span>
<span class="c1"># options, each option get's used exactly once</span>
<span class="c1"># a) r * (strike_prob) * (1 - strike_prob) * x</span>
<span class="c1"># b) (r_all - z) * x</span>
<span class="c1"># c) (np.mean(r_perturb - r)) / perturbation_scale</span>
<span class="c1"># d) r_exp * (strike_prob) * (1 -  strike_prob) * x</span>
<span class="c1"># e) (np.mean(r_exp_perturb - r_exp)) / perturbation_scale</span>
<span class="c1"># f) (r-z) * strike * x</span>
<span class="c1"># This will implement different update rules driven by: Reward Prediction Error,</span>
<span class="c1"># Reinforcement of Action ProbabilitiesTrue Positives, or Peturb-Measure</span>
<span class="c1"># estimate of the gradient of expected reward. Both in a strict, experience</span>
<span class="c1"># driven way, and in "cheating" a bit way.</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Implement different update rules to compare"</span><span class="p">)</span>
<span class="c1">################################################################################</span>

<span class="k">def</span> <span class="nf">np_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="c1"># high tau more exploration, low tau very little exploration</span>
  <span class="n">x_scaled</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">tau</span><span class="p">,</span> <span class="o">-</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> <span class="c1">#prevent overflow, fine because sigmoid saturates</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">/</span><span class="n">tau</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Parameters:</span>
<span class="sd">  - W (ndarray, shape: (1, n_inputs)): Connective strength weights between inputs and the output.</span>
<span class="sd">  - x (ndarray, shape: (n_inputs, batch)): Input features, col is a sample, each row an input feature type.</span>
<span class="sd">  - y (ndarray, shape: (1, batch)): Binary indication of prey presence, used to determine the reward.</span>
<span class="sd">  - tau (float, optional): Temperature parameter for the sigmoid function, controlling its steepness.</span>
<span class="sd">    A higher tau value leads to a steeper function.</span>
<span class="sd">  - rng (np.random.Generator, optional): NumPy random number generator instance for reproducibility.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># 1 x batch = 1 x n_inputs @ n_inputs x batch</span>
  <span class="n">strike_prob</span> <span class="o">=</span> <span class="n">np_sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span> <span class="c1"># 1 x batch</span>
  <span class="n">strike</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">strike_prob</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="c1"># 1-&gt;did, 0-&gt;didn't</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="o">-</span><span class="n">strike</span><span class="p">)</span> <span class="c1"># sampled reward</span>
  <span class="n">r_all</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="o">-</span><span class="mi">1</span> <span class="c1"># reward when striking in all cases</span>
  <span class="n">r_exp</span> <span class="o">=</span> <span class="n">strike_prob</span> <span class="o">*</span> <span class="n">r_all</span> <span class="c1"># + (1-strike_prob) * 0 # expected reward</span>
  <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_all</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="k">def</span> <span class="nf">reward_prediction_step</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                           <span class="n">cheat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                           <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">):</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_all</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">cheat</span><span class="p">:</span>
    <span class="c1"># learn regardless of whether organism actually strikes or not</span>
    <span class="n">update</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">else</span><span class="p">:</span> <span class="c1"># properly episodic</span>
    <span class="c1"># learn only from actual recieved rewards</span>
    <span class="n">update</span> <span class="o">=</span> <span class="o">...</span>
  <span class="c1"># average the update over all elements in the batch</span>
  <span class="n">update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">update</span> <span class="o">*</span> <span class="n">learning_rate</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="k">def</span> <span class="nf">action_prob_step</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                     <span class="n">cheat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                     <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                     <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_all</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">cheat</span><span class="p">:</span>
    <span class="c1"># learn using expected reward</span>
    <span class="n">update</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">else</span><span class="p">:</span> <span class="c1"># properly episodic</span>
    <span class="c1"># learn only from actual recieved rewards</span>
    <span class="c1"># reward of zero recieved when not striking</span>
    <span class="n">update</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">update</span> <span class="o">*</span> <span class="n">learning_rate</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="k">def</span> <span class="nf">perturb_measure_step</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                         <span class="n">perturbation_scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                         <span class="n">cheat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_all</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="n">raw_test_perturb</span> <span class="o">=</span> <span class="n">learn_rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
  <span class="n">unit_test_perturb</span> <span class="o">=</span> <span class="n">raw_test_perturb</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">raw_test_perturb</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
  <span class="n">test_perturbation</span> <span class="o">=</span> <span class="n">unit_test_perturb</span> <span class="o">*</span> <span class="n">perturbation_scale</span>
  <span class="n">perturbed_W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">test_perturbation</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_perturb</span><span class="p">,</span> <span class="n">r_all_perturb</span><span class="p">,</span> <span class="n">r_exp_perturb</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">perturbed_W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">cheat</span><span class="p">:</span>
    <span class="c1"># evaluate perturbation using expected reward, avg over the mini-batch</span>
    <span class="n">directional_grad_est</span> <span class="o">=</span> <span class="o">...</span>
  <span class="k">else</span><span class="p">:</span> <span class="c1"># more episodic (still evaluate peturb and base on same experiences)</span>
    <span class="c1"># evaluate perturbation using sampled rewards, avg over the mini-batch</span>
    <span class="n">directional_grad_est</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">update</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">directional_grad_est</span> <span class="o">*</span> <span class="n">unit_test_perturb</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">update</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="c1">################################################################################</span>
<span class="c1"># Exercise Complete, simulations and plotting logic follow</span>
<span class="c1">################################################################################</span>
<span class="c1"># simulation</span>
<span class="n">learn_rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mini_batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cooling_rate</span> <span class="o">=</span> <span class="mf">0.04</span>
<span class="n">W_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">65</span><span class="p">))</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span>
<span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span>
<span class="n">alg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Reward Prediction'</span><span class="p">,</span> <span class="s1">'Action Probability'</span><span class="p">,</span> <span class="s1">'Perturb Measure'</span><span class="p">]</span>
<span class="n">alg_funcs</span> <span class="o">=</span> <span class="p">[</span><span class="n">reward_prediction_step</span><span class="p">,</span> <span class="n">action_prob_step</span><span class="p">,</span> <span class="n">perturb_measure_step</span><span class="p">]</span>
<span class="c1"># cheating simulation</span>
<span class="n">cheat_alg_lrs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Reward Prediction'</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>
                 <span class="s1">'Action Probability'</span><span class="p">:</span> <span class="mf">0.008</span><span class="p">,</span>
                 <span class="s1">'Perturb Measure'</span><span class="p">:</span> <span class="mf">0.0016</span><span class="p">}</span>
<span class="n">cheat_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">cheat_exp_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">W_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="n">W_init</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="n">learn_rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">batch_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mini_batch_size</span><span class="p">):</span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">batch_step</span><span class="p">:</span><span class="n">batch_step</span><span class="o">+</span><span class="n">mini_batch_size</span><span class="p">]</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">alg_func</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alg_names</span><span class="p">,</span> <span class="n">alg_funcs</span><span class="p">):</span>
      <span class="n">lr</span> <span class="o">=</span> <span class="n">cheat_alg_lrs</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="n">W</span> <span class="o">=</span> <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">alg_name</span> <span class="o">==</span> <span class="s1">'Reward Prediction'</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">((</span><span class="n">num_steps</span><span class="o">+</span><span class="mf">10.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">cooling_rate</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="n">new_W</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">alg_func</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">cheat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_exp_full</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">)</span>
      <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_W</span>
      <span class="n">cheat_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
      <span class="n">cheat_exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r_exp_full</span><span class="p">))</span>
    <span class="n">num_steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">num_steps</span> <span class="o">&gt;</span> <span class="mi">1000</span><span class="p">:</span>
      <span class="k">break</span>

<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">real_alg_lrs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Reward Prediction'</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>
                <span class="s1">'Action Probability'</span><span class="p">:</span> <span class="mf">0.003</span><span class="p">,</span>
                <span class="s1">'Perturb Measure'</span><span class="p">:</span> <span class="mf">0.0000002</span><span class="p">}</span>
<span class="c1"># realishtic simulation</span>
<span class="n">W_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="n">W_init</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">actual_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">actual_exp_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="n">learn_rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">batch_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mini_batch_size</span><span class="p">):</span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">batch_step</span><span class="p">:</span><span class="n">batch_step</span><span class="o">+</span><span class="n">mini_batch_size</span><span class="p">]</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">alg_func</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alg_names</span><span class="p">,</span> <span class="n">alg_funcs</span><span class="p">):</span>
      <span class="n">lr</span> <span class="o">=</span> <span class="n">real_alg_lrs</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="n">W</span> <span class="o">=</span> <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">alg_name</span> <span class="o">==</span> <span class="s1">'Reward Prediction'</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">((</span><span class="n">num_steps</span><span class="o">+</span><span class="mf">10.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">cooling_rate</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="n">new_W</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">alg_func</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">cheat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_exp_full</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">)</span>
      <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_W</span>
      <span class="n">actual_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
      <span class="n">actual_exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r_exp_full</span><span class="p">))</span>
    <span class="n">num_steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">num_steps</span> <span class="o">&gt;</span> <span class="mi">1000</span><span class="p">:</span>
      <span class="k">break</span>
<span class="c1"># plotting</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="c1"># Create subplots with a shared x-axis</span>
  <span class="c1">#fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 8), sharex=True)</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
  <span class="n">theoretical_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y1</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>

  <span class="c1"># Colors for algorithms</span>
  <span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Reward Prediction'</span><span class="p">:</span> <span class="s1">'b'</span><span class="p">,</span> <span class="s1">'Action Probability'</span><span class="p">:</span> <span class="s1">'g'</span><span class="p">,</span> <span class="s1">'Perturb Measure'</span><span class="p">:</span> <span class="s1">'r'</span><span class="p">}</span>

  <span class="c1"># First subplot for expected rewards</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">theoretical_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Max Possible Avg. Reward per Episode'</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">:</span>
    <span class="n">eval_cheat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cheat_exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">])</span>
    <span class="n">eval_cheat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">eval_cheat</span><span class="p">)</span>
    <span class="n">eval_cheat</span> <span class="o">=</span> <span class="n">eval_cheat</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eval_cheat</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eval_cheat</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">alg_name</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">alg_name</span><span class="si">}</span><span class="s1">-Cheating'</span><span class="p">)</span>

    <span class="n">eval_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">actual_exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">])</span>
    <span class="n">eval_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">eval_real</span><span class="p">)</span>
    <span class="n">eval_real</span> <span class="o">=</span> <span class="n">eval_real</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">eval_real</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eval_real</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">alg_name</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">alg_name</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>

  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Cumulative per Episode Average of</span><span class="se">\n</span><span class="s1">Expected (Full Batch) Reward'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Cumulative Avg. Expected Reward'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="c1"># Second subplot for actual rewards</span>
  <span class="c1"># ax2.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')</span>
  <span class="c1"># for alg_name in alg_names:</span>
  <span class="c1">#  eval_cheat = np.array(cheat_reward_results[alg_name])</span>
  <span class="c1">#  eval_cheat = np.cumsum(eval_cheat)</span>
  <span class="c1">#  eval_cheat = eval_cheat / (np.arange(len(eval_cheat)) + 1)</span>
  <span class="c1">#  ax2.plot(eval_cheat, linestyle='--', color=colors[alg_name], label=f'{alg_name}-Cheating')</span>

  <span class="c1">#  eval_real = np.array(actual_reward_results[alg_name])</span>
  <span class="c1">#  eval_real = np.cumsum(eval_real)</span>
  <span class="c1">#  eval_real = eval_real / (np.arange(len(eval_real)) + 1)</span>
  <span class="c1">#  ax2.plot(eval_real, linestyle='-', color=colors[alg_name], label=f'{alg_name}-Realishtic')</span>

  <span class="c1">#ax2.set_title('Cumulative Average of Actual Reward Per Learning Episode')</span>
  <span class="c1">#ax2.set_xlabel('Learning Episodes')</span>
  <span class="c1">#ax2.set_ylabel('Cumulative Avg. Actual Reward')</span>
  <span class="c1">#ax2.legend()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
<p><a class="reference external" href="https://github.com/dcownden/PerennialProblemsOfLifeWithABrain/tree/main//sequences/P2C1_Optimization/solutions/P2C1_Sequence5_Solution_9c812782.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_Optimization/static/P2C1_Sequence5_Solution_9c812782_1.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_Optimization/static/P2C1_Sequence5_Solution_9c812782_1.png" style="width: 800.0px; height: 600.0px;"/></a>
<p>Looking at the plots produced by the coding exercise we can see that Reward-Prediction-Error and Action-Probability-Reinforcement both learn to (fairly) effectively discriminate between when prey is and is not present much more quickly, i.e. with fewer learning episodes, than Perturb-Measure-Step. And, this difference in number of learning episodes required to learn “good” behaviour is much greater for the episodic variants than for the cheating variants.</p>
<p>Although all the learning algorithms learn less quickly when not allowed to cheat, Perturb-Measure-Step suffers the most.</p>
<p>Let’s take a moment to think about how the Episodic vs. Cheating variants differ for each of these update rules.</p>
<ul class="simple">
<li><p><strong>reward-prediction-error</strong>: In episodic mode can only learn when it strikes and there is an actual reward recieved (plus or minus one, to contrast with the prediction). When cheating always learns as though it had made a strike;</p></li>
<li><p><strong>action-probability-reinforcement</strong>: In episodic mode only reinforces the actual action taken by the reward recieved.  When cheating all actions are reinforced by expected reward in proportion to the probability of having taken that action/</p></li>
<li><p><strong>perturb-measure-step</strong>: In episodic compares an actual reward recieved under the base evaluation mode against the actual reward recieved under the perturbation mode to evaluate the perturbation. When cheating this evaluation is made using expected reward, not sampled reward.</p></li>
</ul>
<p>Why does this cause such a big slow down for perturb-measure-step? When learning from a single experience, and using actual sampled rewards, very few outcomes actually result in effective learning for perturb-measure-step, as can be seen in the table below:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Case</p></th>
<th class="text-align:center head"><p>Correct<br/>Action</p></th>
<th class="text-align:center head"><p>Perturb<br/>Action</p></th>
<th class="text-align:center head"><p>Base<br/>Action</p></th>
<th class="text-align:center head"><p>Perturb - Base<br/>Reward<br/>Difference</p></th>
<th class="text-align:center head"><p>Learning<br/>Happens</p></th>
<th class="text-align:center head"><p>P(Strike, Perturb)<br/> greater than<br/>P(Strike, Base)</p></th>
<th class="text-align:center head"><p>Learning is<br/>Helpful</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>-</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>2</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>3</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>No</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>4</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>-1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>5</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>-1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>Yes</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>6</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>-</p></td>
<td class="text-align:center"><p>-</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>7</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>-</p></td>
<td class="text-align:center"><p>-</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>8</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>-1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>9</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>-1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>No</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>10</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>11</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>Strike</p></td>
<td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>Yes</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>Yes</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>12</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>No-Strike</p></td>
<td class="text-align:center"><p>0</p></td>
<td class="text-align:center"><p>No</p></td>
<td class="text-align:center"><p>-</p></td>
<td class="text-align:center"><p>-</p></td>
</tr>
</tbody>
</table>
<p>This is a bit dense, but what it is saying is that first of all, there are many cases where no learning occurs because both the pertubation and the base mode sample the same action and recieve the same rewards, so there is no reward difference at all to drive learning. Note that for small perturbations the action probabilities of the base and perturbed networks will be very similar, so these no learning cases will be the most commen case by far as perturbation sizes become increasinly small. Then, even in the cases where the perturbation mode and the base mode do sample different actions, it is possible that the sampled actions will run counter to the way the perturbation have shifted the probability of striking. Case 3 in the table above is an example of this. Under the perturbation the probability of striking is not greater than (but rather less than) the probability of striking with the base parameters, however, despite this, the less likely outcome where the perturb network samples striking, and the base network samples not-striking occurs. Because the sample is not aligned with the underlying shift in probabilities caused by the perturbation, learning from this difference is counterproductive. Now on average, learning will help shift probabilities towards striking in the right situations. However, it will do so only slowly, as many learning episodes will result in no learning, or counterproductive learning.</p>
<p>Submit your feedback</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Submit your feedback</span>
<span class="n">content_review</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">feedback_prefix</span><span class="si">}</span><span class="s2">_M1"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "74824eea8a724b199395e27a816e9e17"}
</script></div>
</div>
</div>
</div>
<div class="section" id="perturb-measure-step-is-slow-because-of-poor-gradient-alignment">
<h1>2.1.4.2 Perturb-Measure-Step is Slow Because of Poor Gradient Alignment<a class="headerlink" href="#perturb-measure-step-is-slow-because-of-poor-gradient-alignment" title="Permalink to this headline">¶</a></h1>
<p>Although Perturb-Measure-Step suffered the most from using strictly episodic learning, even when learning on the full batch instead of just one example at a time, and using expected rewards, we still see that perturb-measure-step is slow compared to reward-prediction-error and action-probability-reinforcement.</p>
<p><strong>Run this cell</strong> - to see how perturb-measure-step is slow relative to reward-prediction-error and action-probability-reinforcement, even when using full-batch learning and expected rewards.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown **Run this cell** - to see how perturb-measure-step is slow relative to reward-prediction-error and action-probability-reinforcement, even when using full-batch learning and expected rewards.</span>
<span class="n">learn_rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mini_batch_size</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cooling_rate</span> <span class="o">=</span> <span class="mf">0.04</span>
<span class="n">W_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">65</span><span class="p">))</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span>
<span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span>
<span class="n">alg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Reward Prediction'</span><span class="p">,</span> <span class="s1">'Action Probability'</span><span class="p">,</span> <span class="s1">'Perturb Measure'</span><span class="p">]</span>
<span class="n">alg_funcs</span> <span class="o">=</span> <span class="p">[</span><span class="n">reward_prediction_step</span><span class="p">,</span> <span class="n">action_prob_step</span><span class="p">,</span> <span class="n">perturb_measure_step</span><span class="p">]</span>
<span class="n">alg_lrs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Reward Prediction'</span><span class="p">:</span> <span class="mf">0.0015</span><span class="p">,</span>
                 <span class="s1">'Action Probability'</span><span class="p">:</span> <span class="mf">0.08</span><span class="p">,</span>
                 <span class="s1">'Perturb Measure'</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">}</span>
<span class="n">actual_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">exp_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">W_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="n">W_init</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="n">learn_rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">batch_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mini_batch_size</span><span class="p">):</span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">batch_step</span><span class="p">:</span><span class="n">batch_step</span><span class="o">+</span><span class="n">mini_batch_size</span><span class="p">]</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">alg_func</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alg_names</span><span class="p">,</span> <span class="n">alg_funcs</span><span class="p">):</span>
      <span class="n">lr</span> <span class="o">=</span> <span class="n">alg_lrs</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="n">W</span> <span class="o">=</span> <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">alg_name</span> <span class="o">==</span> <span class="s1">'Reward Prediction'</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">((</span><span class="n">num_steps</span><span class="o">+</span><span class="mf">10.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">cooling_rate</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="n">new_W</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">alg_func</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">cheat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_exp_full</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">)</span>
      <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_W</span>
      <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
      <span class="n">r_exp_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r_exp_full</span><span class="p">)</span>
      <span class="n">actual_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
      <span class="n">exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_exp_full</span><span class="p">)</span>
    <span class="n">num_steps</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">num_steps</span> <span class="o">&gt;</span> <span class="mi">10000</span><span class="p">:</span>
    <span class="k">break</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
  <span class="n">theoretical_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y1</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">theoretical_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Max Possible Avg. Reward per Episode'</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">:</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">])</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="nb">eval</span><span class="p">)</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="nb">eval</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">eval</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">eval</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">alg_name</span><span class="si">}</span><span class="s1">-(Cheating Full, Batch)'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Cumulative per Episode Average of Expected (Full Batch) Reward'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Learning Episodes'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Cumulative Avg. Expected (Full Batch) Reward'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

  <span class="c1">#fig, ax = plt.subplots(figsize=(10, 6))</span>
  <span class="c1">#theoretical_max = np.sum(y1 == 1) / len(y1)</span>
  <span class="c1">#ax.hlines(theoretical_max, 0, num_steps, linestyle='--', color='gray', label='Max Possible Avg. Reward per Episode')</span>
  <span class="c1">#for alg_name in alg_names:</span>
  <span class="c1">#  eval = actual_reward_results[alg_name]</span>
  <span class="c1">#  eval = np.cumsum(eval)</span>
  <span class="c1">#  eval = eval / (np.arange(len(eval)) + 1)</span>
  <span class="c1">#  ax.plot(eval, label=f'{alg_name} (Cheating Full Batch)')</span>
  <span class="c1">#  ax.set_title(f'Cumulative Average of Actual Reward Per Learning Episode')</span>
  <span class="c1">#  ax.set_xlabel('Learning Episodes')</span>
  <span class="c1">#  ax.set_ylabel('Cumulative Avg. Actual Reward')</span>
  <span class="c1">#  ax.legend()</span>
  <span class="c1">#plt.tight_layout()</span>
  <span class="c1">#plt.show()</span>
</pre></div>
</div>
</div>

</div>
<p>In the previous sequence looking at optimization in higher dimensions, analysis showed that the expected improvement from our perturb-measure-step was proportional to the square of the magnitude of the gradient, diveded by <span class="math notranslate nohighlight">\(N\)</span> the number of dimensions in the optimization problem, so in our current model of striking behaviour, the number of parameters (<span class="math notranslate nohighlight">\(65\)</span>) to be optimized gives:</p>
<div class="math notranslate nohighlight">
\[
\text{Perturb-Measure-Step Expected Improvement per Step} \propto \frac{\| \mathbf{g} \|^2}{n}\]</div>
<p>And a bit of further analysis found that by thoughtfully scaling the step size to compensate for the low level of alignment of a random test direction with the gradient (best direction in parameters space for improvement) this could be improved to</p>
<div class="math notranslate nohighlight">
\[
\text{Scaled-Perturb-Measure-Step Expected Improvement per Step} \propto \frac{\| \mathbf{g} \|^2}{\sqrt{n}}\]</div>
<p>What this tells us is that with perturb-measure-step learning will always be kind of a zigg-zagging through parameter space, with parameter update steps being almost perpendicular to the direction of maximal improvement (the gradient <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>) in high dimensional spaces</p>
<p>With this in mind, let’s look compute this gradient, <span class="math notranslate nohighlight">\(\mathbf{g}\)</span>, for our evaluation function, applied to the entire batch of data. Then we can see how our new update rules, reward-prediction-error and action-probability-reinforcement, relate to the gradient. To do that, we need to translate our evaluation function from python code into math symbols. In code our evaluation function was:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">  </span><span class="sd">"""</span>
<span class="sd">  Parameters:</span>
<span class="sd">  - W (ndarray, shape: (1, n_inputs)): Connective strength weights between inputs and the output.</span>
<span class="sd">  - x (ndarray, shape: (n_inputs, batch)): Input features, col is a sample, each row an input feature type.</span>
<span class="sd">  - y (ndarray, shape: (1, batch)): Binary indication of prey presence, used to determine the reward.</span>
<span class="sd">  - tau (float, optional): Temperature parameter for the sigmoid function, controlling its steepness.</span>
<span class="sd">    A higher tau value leads to a steeper function.</span>
<span class="sd">  - rng (np.random.Generator, optional): NumPy random number generator instance for reproducibility.</span>
<span class="sd">  """</span>
  <span class="k">if</span> <span class="n">rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="n">strike_prob</span> <span class="o">=</span> <span class="n">np_sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
  <span class="n">strike</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">strike_prob</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="c1"># 1-&gt;did, 0-&gt;didn't</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="o">-</span><span class="n">strike</span><span class="p">)</span> <span class="c1"># sampled reward</span>
  <span class="n">r_all</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">y</span><span class="o">-</span><span class="mi">1</span> <span class="c1"># reward when striking in all cases</span>
  <span class="n">r_exp</span> <span class="o">=</span> <span class="n">strike_prob</span> <span class="o">*</span> <span class="n">r_all</span> <span class="c1"># + (1-strike_prob) * 0 # expected reward</span>
  <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_all</span><span class="p">,</span> <span class="n">r_exp</span>
</pre></div>
</div>
</div>
</div>
<p>We will leave aside the issues of having a stochastic evaluation function for the moment. Randomness in the evaluation can come from both which particular samples from the full batch we use in a given mini-batch, from randomness in behaviour, i.e. whether or not the organism strikes. To simplify things here we eliminate both these sources of randomness by 1) evaluating performance over the full batch of data, and 2) looking at expected reward given the striking probabilities. Then we can write:</p>
<div class="math notranslate nohighlight">
\[ \mathbb{E}[R(\mathbf{W}, \mathbf{x}, y)] = \sum_{(\mathbf{x},y)\in \mathcal{D}} \frac{1}{n} \sum_{a\in\mathcal{A}} \Pr(a | \mathbf{x}) \cdot r(a|y)\]</div>
<p>This defines the criteria by which we evaluate the parameters and the resulting behavioural performance of the organism. Here <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is the set of all <span class="math notranslate nohighlight">\(n\)</span> data points in the full-batch and <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is the set of actions <span class="math notranslate nohighlight">\(\{ \text{strike}, \text{no-strike} \}\)</span>. Note that <span class="math notranslate nohighlight">\(R(\mathbf{W}, \mathbf{x}, y)\)</span> is a random variable that depends on which actions the organism (probablistically) selects in which situations, whereas <span class="math notranslate nohighlight">\(r(a|y)\)</span> is deterministic (in our example, in the more general case this might also be a random variable resulting from further stochasticity in the environment, but we are not considering this more general case just yet). The above expression corresponds to the mean of <code class="docutils literal notranslate"><span class="pre">r_exp</span></code> in the code block above, when eval params is applided to the whole batch of data. When writing about the probability of selecting actions we will typically us the notation:</p>
<div class="math notranslate nohighlight">
\[\pi_{\mathbf{W}}(a|\mathbf{x})\]</div>
<p>to emphasize this is the probability of an action being selected by an organism with <em><strong>policy</strong></em> <span class="math notranslate nohighlight">\(\pi\)</span> parameterized by weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> given experience <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. (In the case where the parameters of the policy consist of more than a single weight matrix/vector we typically use <span class="math notranslate nohighlight">\(\theta\)</span> to denote all the parameters of the policy function and write <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span>. To keep notation compact we will often just write <span class="math notranslate nohighlight">\(\pi(a)\)</span> when there is no need to explicitly emphasize the dependence on the paramters and the inputs.)</p>
<p>The gradient with respect to <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> of this expected reward expression is the <em>direction</em> of parameter change that will cause the greatest increase in expected reward for a small change in <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, scaled by the <em>rate</em> of improvement in expected reward given a small change in <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> in that direction. The gradient is a vector, it has direction and magnitude! Let’s compute this gradient.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align} \nabla_{\mathbf{W}} \mathbb{E}[R(\mathbf{W}, \mathbf{x}, y)] &amp;= \sum_{(\mathbf{x},y)\in \mathcal{D}} \frac{1}{n} \sum_{a\in\mathcal{A}} \nabla_{\mathbf{W}} \pi_{\mathbf{W}}(a | \mathbf{x}) \cdot r(a|y) \\
&amp;= \sum_{(\mathbf{x},y)\in \mathcal{D}} \sum_{a\in\mathcal{A}} \frac{1}{n} \cdot \pi_{\mathbf{W}}(a | \mathbf{x}) \cdot r(a|y) \cdot \nabla_{\mathbf{W}} \log(\pi_{\mathbf{W}}(a|\mathbf{x})) \end{align}\end{split}\]</div>
<p>Math trick alert! this rearangement of the terms comes from the application of the chain rule to:
$<span class="math notranslate nohighlight">\(\frac{\mathrm{d}}{\mathrm{d}x}\log(f(x)) = \frac{f'(x)}{f(x) } \iff \frac{\mathrm{d}}{\mathrm{d}x} f(x) = \frac{\mathrm{d}}{\mathrm{d}x}\log(f(x)) \cdot f(x)\)</span>$</p>
<p>There is a really good reason for organizing the gradient of the expected reward like this, since this now correspondes with how the organisms experiences the environment given its current policy as determined by parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>. Specifically, the organism encounters a data point <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span> from the data set, each point being equally likely to be encountered, so it encounters this particular point with probability <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span>. Then, in response to the experience of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> the <em><strong>policy</strong></em> of the organism as parameterized by <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> selects or samples an action according to</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\pi_{\mathbf{W}}(\text{strike} | \mathbf{x}) &amp;= \sigma(\mathbf{Wx}) \\
\pi_{\mathbf{W}}(\text{no-strike} | \mathbf{x}) &amp;= 1 - \sigma(\mathbf{Wx})
\end{align}\end{split}\]</div>
<p>Once the organism has selected an action, the response/state of the environment determines the reward that the organism experiences according <span class="math notranslate nohighlight">\(r(a|y)\)</span>. Thus, the terms in the double sum over possible data points encountered and actions taken, can be understood as accounting for probability of that a particular environmental state occurs, <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span>, the probability that a particular action is selected in response to the organism’s sensory experience of that particular state <span class="math notranslate nohighlight">\(\pi_{\mathbf{W}}(a | \mathbf{x})\)</span> given the current policy, the reward that results from taking that particular action in that particular situation, <span class="math notranslate nohighlight">\(r(a|y)\)</span>, and lastly a gradient term <span class="math notranslate nohighlight">\(\nabla_{\mathbf{W}} \log(\pi_{\mathbf{W}}(a|\mathbf{x}))\)</span></p>
<p>What this tells us, is that if the organism were to just go about its life experiencing <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>’s, selecting actions according to its policy, experiencing rewards, and then in response to these experiences of sensation-action-reward shift the parameters of its policy in the direction of <span class="math notranslate nohighlight">\(\nabla_{\mathbf{W}} \log(\pi_{\mathbf{W}}(a|\mathbf{x}))\)</span> scaled by the reward experienced, it would actually be implementing (an episodic approximation of) gradient ascent on the expected reward. This is perfect, because expected reward is <em><strong>exactly</strong></em> the quantity the organism should be trying to optimize (presuming that evolultion has effectively aligned the intrinsic rewards experienced by the organism with reproductive success).</p>
<p>Okay so let’s dig into this gradient term for our particular policy. Let’s look at the case when <span class="math notranslate nohighlight">\(a = \text{strike}\)</span> first, expanding using our definitions and the chain rule for derivatives</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
\nabla_{\mathbf{W}} \log(\pi_{\mathbf{W}}(\text{strike}|\mathbf{x})) &amp;= \nabla_{\mathbf{W}} \log(\sigma(\mathbf{Wx})) \\
&amp;= \frac{1}{\sigma(\mathbf{Wx})} \cdot  \nabla_{\mathbf{W}} \sigma(\mathbf{Wx}) \\
&amp;= \frac{1}{\sigma(\mathbf{Wx})} \cdot \sigma(\mathbf{Wx}) \cdot (1-\sigma(\mathbf{Wx})) \cdot \nabla_{\mathbf{W}} \mathbf{Wx} \\
&amp;= (1-\sigma(\mathbf{Wx})) \cdot \mathbf{x}^T
\end{align}\end{split}\]</div>
<p><strong>Math Exercise</strong></p>
<p>Complete the gradient calculation for the derivative of the log-probability of not striking:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
\nabla_{\mathbf{W}} \log(\pi_{\mathbf{W}}(\text{no-strike}|\mathbf{x})) &amp;= \nabla_{\mathbf{W}} \log(1 - \sigma(\mathbf{Wx})) \\
&amp;= \dots \\%\frac{1}{1 - \sigma(\mathbf{Wx})} \cdot  \nabla_{\mathbf{W}} (1 - \sigma(\mathbf{Wx})) \\
&amp;= \dots \\%\frac{1}{1 - \sigma(\mathbf{Wx})} \cdot -\sigma(\mathbf{Wx}) \cdot (1-\sigma(\mathbf{Wx})) \cdot \nabla_{\mathbf{W}} \mathbf{Wx} \\
&amp;= -\sigma(\mathbf{Wx}) \cdot \mathbf{x}^T
\end{align}\end{split}\]</div>
<p><strong>Answer:</strong>
$<span class="math notranslate nohighlight">\(\begin{align} &amp;= \frac{1}{1-\sigma(\mathbf{Wx})} \nabla_{\mathbf{W}} (1-\sigma( \mathbf{Wx})) \\
&amp;= \frac{-1}{1-\sigma(\mathbf{Wx})} \cdot \sigma(\mathbf{Wx}) \cdot (1-\sigma(\mathbf{Wx})) \cdot \nabla_{\mathbf{W}} \mathbf{Wx}\end{align}\)</span>$</p>
<p>So if we wanted have an update rule based on following the gradient of the expected reward it would look like this</p>
<div class="section" id="expected-reward-gradient-update-rule">
<h2>Expected Reward Gradient Update Rule<a class="headerlink" href="#expected-reward-gradient-update-rule" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[\Delta W_i = s \cdot (1-\sigma(z)) \cdot r \cdot x_i \]</div>
<p>Or in vector form</p>
<div class="math notranslate nohighlight">
\[ \Delta \mathbf{W} = s \cdot (1-\sigma(z)) \cdot r \cdot \mathbf{x}\]</div>
<p>So in the cases where the organism strikes, if the reward is positive it increase weights <span class="math notranslate nohighlight">\(w_i\)</span> that were connected to positive inputs, and decrease weights that were connected to negative inputs, so as to increase the probability of striking in the case of experiencing this particular <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. If there is regular structure to the environment in terms of a consistent correlation between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, this adaptation will not only help the organism to select the correct action the next time it experiences this exact same situations <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span>, but it will also help the organism to select the correct action with higher probability on subsequent experiences which are sufficiently similar. If the reward is negative the organism decreases weights that are connected to postive inputs, and decreases weights that are connected to negative inputs so as to decrease the probability of striking in such circumstances.</p>
<p>Now, let’s compare this ideal, expected reward gradient update to our two other update rules</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\Delta \mathbf{W} (\text{Reward Prediction}) &amp;= s \cdot (r-z) \cdot \mathbf{x}^T \\
\Delta \mathbf{W} (\text{Action Probability}) &amp;= s \cdot r \cdot \sigma(z) \cdot (1 - \sigma(z)) \mathbf{x}^T \\
\Delta \mathbf{W} (\text{Expected Reward Gradient}) &amp;= s \cdot r \cdot (1 - \sigma(z)) \mathbf{x}^T
\end{align}\end{split}\]</div>
<p>Or in terms of the policy (which gives the probability with which the sampled action <span class="math notranslate nohighlight">\(a\)</span> was chosen):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\Delta \mathbf{W} (\text{Reward Prediction}) &amp;= s \cdot (r-z) \cdot \mathbf{x}^T \\
\Delta \mathbf{W} (\text{Action Probability}) &amp;= s \cdot r \cdot \pi(a) \cdot (1 - \pi(a)) \mathbf{x}^T \\
\Delta \mathbf{W} (\text{Expected Reward Gradient}) &amp;= s \cdot r \cdot (1 - \pi(a)) \cdot \mathbf{x}^T
\end{align}\end{split}\]</div>
<p>What each of these have in common is that they all shift the parameters in the direction defined by <span class="math notranslate nohighlight">\(\mathbf{x}^T\)</span> scaled by the reward <span class="math notranslate nohighlight">\(r\)</span>, modulo some scaling and/or shifting. Thus they all have the effect of increasing the probability of striking when striking is rewarded, and decreasing the probability when striking is punished (negative reward). Reward-prediction-error scales the magnitude of the learning step by using (<span class="math notranslate nohighlight">\(r-z\)</span>) instead of raw <span class="math notranslate nohighlight">\(r\)</span> to drive learning (and disregards anything to do with probability as that is under control of the <span class="math notranslate nohighlight">\(\tau\)</span> parameter). Action-probability-reinforcement scales by probabilities of striking and not-striking, so that updates are smaller when the probability of taking the action is either very high (close to one) or very low (close to zero) and is largest when the probability of taking versus not taking the action is smallest. In contrast, the expected-reward-gradient episodic update is only scaled by the probability of not taking the action, so the more likely the action is the more strongly it is reinforced. In all cases though, each rule prescribes a parameter change in the <em><strong>same direction</strong></em>. These rules only differ in the way the parameter update is scaled.</p>
<p>This different scaling or weighting of the episodic update rules means that on average when applied over many iterations they will not shift the parameters in the same direction, and as a result will lead to slightly different long term learning outcomes.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="loss-box-expected-reward-gradient-is-action-log-probability-reinforcement">
<h1>Loss Box: Expected-Reward-Gradient <strong>Is</strong> Action-Log-Probability-Reinforcement<a class="headerlink" href="#loss-box-expected-reward-gradient-is-action-log-probability-reinforcement" title="Permalink to this headline">¶</a></h1>
<p>Although we have just looked at the particular case of our problem. This observation that the gradient of the expected reward with respect to the parameters of a policy, is equal to the expectation of the the reward scaled by the gradient of the action log-probabilities, is true in general</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align} \nabla_{\mathbf{\theta}} \mathbb{E}[R(\mathbf{\theta}, \mathbf{x}, y)] &amp;= \sum_{(\mathbf{x},y)\in \mathcal{D}} \frac{1}{n} \sum_{a\in\mathcal{A}} \nabla_{\mathbf{\theta}} \pi_{\mathbf{\theta}}(a | \mathbf{x}) \cdot r(a|y) \\
&amp;= \sum_{(\mathbf{x},y)\in \mathcal{D}} \sum_{a\in\mathcal{A}} \frac{1}{n} \cdot \pi_{\mathbf{\theta}}(a | \mathbf{x}) \cdot r(a|y) \cdot \nabla_{\mathbf{\theta}} \log(\pi_{\mathbf{\theta}}(a|\mathbf{x})) \end{align}\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is the set of all <span class="math notranslate nohighlight">\(n\)</span> data points in the full-batch and <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> is the set of actions available to the policy. Note that <span class="math notranslate nohighlight">\(R(\mathbf{\theta}, \mathbf{x}, y)\)</span> is random variable that depends on which actions the organism (probablistically) selects in which situations, whereas <span class="math notranslate nohighlight">\(r(a|y)\)</span> is deterministic in this case. This formulation is a beautiful thing. It tells us that an organism can maximize their expected reward, simply by doing what they do, experiencing their rewards and then shifting the parameters of their policy in the direction of the gradient of the log probability of the action just taken, scaled by the resultant reward. How this derivative is computed and applied physiologically is a rich topic which we address in part later.</p>
<p>Putting this in terms of an episodic or single experience update rule for the parameters we have</p>
<div class="math notranslate nohighlight">
\[\Delta \theta = s \cdot r(a|y) \cdot \nabla_{\theta} \log(\pi_\theta(a|\mathbf{x})\]</div>
<p>We will use action-log-probability-reinforcement and expected-reward-gradient interchangably to refer to the same update rule, depending on which aspect we wish to emphasize. Most important though, they are one and the same.</p>
<hr class="docutils"/>
<p><strong>Run this cell</strong> to see a comparison the different algorithms - reward-prediction-error, action-probability-reinforcement and expected-reward-gradient.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown **Run this cell** to see a comparison the different algorithms - reward-prediction-error, action-probability-reinforcement and expected-reward-gradient.</span>

<span class="k">def</span> <span class="nf">action_log_prob_step</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                         <span class="n">cheat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                         <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">strike_prob</span><span class="p">,</span> <span class="n">strike</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_all</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">cheat</span><span class="p">:</span>
    <span class="c1"># learn using expected reward</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">r_exp</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span>  <span class="n">strike_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
  <span class="k">else</span><span class="p">:</span> <span class="c1"># properly episodic</span>
    <span class="c1"># learn only from actual recieved rewards</span>
    <span class="c1"># reward of zero recieved when not striking</span>
    <span class="n">update</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">strike_prob</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
  <span class="n">update</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">update</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">update</span> <span class="o">*</span> <span class="n">learning_rate</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="n">learn_rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mini_batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">cooling_rate</span> <span class="o">=</span> <span class="mf">0.04</span>
<span class="n">W_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">65</span><span class="p">))</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span>
<span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span>
<span class="n">alg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Reward Prediction'</span><span class="p">,</span> <span class="s1">'Action Probability'</span><span class="p">,</span> <span class="s1">'Expected Reward'</span><span class="p">]</span>
<span class="n">alg_funcs</span> <span class="o">=</span> <span class="p">[</span><span class="n">reward_prediction_step</span><span class="p">,</span> <span class="n">action_prob_step</span><span class="p">,</span> <span class="n">action_log_prob_step</span><span class="p">]</span>
<span class="n">alg_lrs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Reward Prediction'</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>
           <span class="s1">'Action Probability'</span><span class="p">:</span> <span class="mf">0.003</span><span class="p">,</span>
           <span class="s1">'Expected Reward'</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">}</span>
<span class="n">actual_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">exp_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">W_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="n">W_init</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="n">learn_rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">batch_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mini_batch_size</span><span class="p">):</span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">batch_step</span><span class="p">:</span><span class="n">batch_step</span><span class="o">+</span><span class="n">mini_batch_size</span><span class="p">]</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">alg_func</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alg_names</span><span class="p">,</span> <span class="n">alg_funcs</span><span class="p">):</span>
      <span class="n">lr</span> <span class="o">=</span> <span class="n">alg_lrs</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="n">W</span> <span class="o">=</span> <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">alg_name</span> <span class="o">==</span> <span class="s1">'Reward Prediction'</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">((</span><span class="n">num_steps</span><span class="o">+</span><span class="mf">10.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">cooling_rate</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="n">new_W</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">alg_func</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">cheat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_exp_full</span> <span class="o">=</span> <span class="n">eval_params</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">)</span>
      <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_W</span>
      <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
      <span class="n">r_exp_full</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r_exp_full</span><span class="p">)</span>
      <span class="n">actual_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
      <span class="n">exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r_exp_full</span><span class="p">)</span>
    <span class="n">num_steps</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">num_steps</span> <span class="o">&gt;</span> <span class="mi">2000</span><span class="p">:</span>
      <span class="k">break</span>
<span class="k">with</span> <span class="n">plt</span><span class="o">.</span><span class="n">xkcd</span><span class="p">():</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
  <span class="n">theoretical_max</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y1</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">theoretical_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Max Possible Avg. Reward per Episode'</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">:</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">])</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="nb">eval</span><span class="p">)</span>
    <span class="nb">eval</span> <span class="o">=</span> <span class="nb">eval</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">eval</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">eval</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">alg_name</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Cumulative per Episode Average of Expected (Full Batch) Reward'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">'Learning Episodes'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Cumulative Avg. Expected (Full Batch) Reward'</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
<p>Comparing these three different update rules, we see that they all have relatively similar performance in terms of speed of learning. This stems from the fact that they all are leveraging knowledge of how the behaviour is produced by the network to inform the weight updates. This knowledge manifests as the term <span class="math notranslate nohighlight">\(\mathbf{x}^T\)</span> in each of these update rules, since this determines the causal impact of changes in the weight parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> on downstream probabilities of the selected action being taken or not, given the current sensory input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. This should be contrasted with perturb-measure-step which makes no use of such knowledge. Perturb-measure-step assumes nothing about the structure of the evaluation function, both the behaviour generating aspect of parameter evaluation, and the reward response aspect of the evaluation function. A useful analogy (which we formalize later in sequence ref) is of observations as kind of currency to be spent on making better inferences about the world. Different learning algorithms can all recieve the same data-points, but how they “spend” this data on inference is what differentiates them. The same information is always present in the data, but the model archetecture and the learning algorithm determine how efficiently that information in that data is used (or not) to improve some objective. Loosely speaking, within this spending data analogy, perturb-measure-step is spending a lot of its data on inference about how the paramaters generate reward, through behaviour selection. In contrast these other update rules, take the structure of the behaviour generating function as a given, which allows them to spend all of their data on learning the association between sensory inputs and rewarding behaviour, i.e. directly improving behaviour, without “wasting” any data on implicit inference about how behaviour is generated, (as this knowledge is already embedded in the update rule). This is very abstract.</p>
<p>To see a concrete example of this suppose that on a particular episode <span class="math notranslate nohighlight">\(x_0\)</span> has a value of zero. Because this <span class="math notranslate nohighlight">\(x_0\)</span> input will have had no causal impact on the action selected in that episode none of the gradient aligned update rules will make an update to <span class="math notranslate nohighlight">\(w_0\)</span>. In contrast, perturb-measure-step may well update <span class="math notranslate nohighlight">\(w_0\)</span>, so long as the perturbed evaluation is different from the baseline evaluation. This is because, based on a single learning episode, perturb-measure-step can only make a rough guess based on correlations as to the causal impact of any one parameter on the outcome, as it does not know anything about how the parameters <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> interact with the inputs <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to generate behaviour and resultant reward.</p>
<p>Submit your feedback</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Submit your feedback</span>
<span class="n">content_review</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">feedback_prefix</span><span class="si">}</span><span class="s2">_M2"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "21f70a87638e48d2be56dcbfbfa46519"}
</script></div>
</div>
</div>
<div class="section" id="learning-to-do-more-than-one-thing">
<h1>2.1.4.3 Learning To Do More Than One Thing<a class="headerlink" href="#learning-to-do-more-than-one-thing" title="Permalink to this headline">¶</a></h1>
<p>So far we have just focused on the case where the organism has to choose between doing something, in our cartoon example striking at prey, or refraining from doing that. There is only one thing to do, and it is either done or not done. Now we want to look at a more general kind of problem where there are many possible actions to choose from. We envision a cartoon scenario much like the first one, but now instead of either striking or not striking, we imagine that there are 10 disctinct prey types, each requiring a different capture technique. If the predator selects the correct capture technique for the particular prey type encountered, they are successful and get a reward of one, but if they select the wrong capture technique they recieve a reward of zero. (How these specific techniques are learned, and what it would even mean to learn specific stereotyped action patterns from a sensory motor perspective, we leave aside for now, and focus solely on the problem of the predator learning to associate the correct discrete capture technique with the correct prey type based on sensory inputs.)</p>
<p>To get a sense of this discrimination problem run the cell below and try it yourself.</p>
<p><strong>Run this cell</strong> to try out this new 10-fold discrimination task. Click the buttons to try different prey-capture techniques.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown **Run this cell** to try out this new 10-fold discrimination task. Click the buttons to try different prey-capture techniques.</span>

<span class="k">class</span> <span class="nc">InteractiveMNISTPredator_Multi</span><span class="p">():</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">features</span><span class="o">=</span><span class="n">Xs</span><span class="p">,</span>
               <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
               <span class="n">extra_labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
               <span class="n">feedback_type</span><span class="o">=</span><span class="s1">'on_strike_only'</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">123</span><span class="p">):</span>
    <span class="c1"># Initialize dataset, settings for image scrambling and feedback</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="c1"># features is num_data_points x 64 (reshape to 8x8 for display, each cell 0-16)</span>
    <span class="c1"># labels is num_data_points x 1 (values 0-9 or 0/1 depending)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feedback_type</span> <span class="o">=</span> <span class="n">feedback_type</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1">#sample_order = np.arange(self.features.shape[0])</span>
    <span class="n">sample_order</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rng</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">sample_order</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">sample_order</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">extra_labels</span> <span class="o">=</span> <span class="n">extra_labels</span><span class="p">[</span><span class="n">sample_order</span><span class="p">]</span>
    <span class="c1"># initialize game state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">best_possible_score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">successful_captures</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">failed_captures</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Initialize widgets</span>
    <span class="c1"># Define button labels as techniques</span>
    <span class="n">capture_techniques</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Lure"</span><span class="p">,</span> <span class="s2">"Chase"</span><span class="p">,</span> <span class="s2">"Stalk"</span><span class="p">,</span> <span class="s2">"Snare"</span><span class="p">,</span> <span class="s2">"Pounce"</span><span class="p">,</span> <span class="s2">"Strike"</span><span class="p">,</span> <span class="s2">"Slash"</span><span class="p">,</span> <span class="s2">"Crush"</span><span class="p">,</span> <span class="s2">"Pin"</span><span class="p">,</span> <span class="s2">"Bite"</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">technique_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">technique</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">technique</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">capture_techniques</span><span class="p">)}</span>
    <span class="n">button_style</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">ButtonStyle</span><span class="p">(</span><span class="n">width</span><span class="o">=</span><span class="s1">'80px'</span><span class="p">)</span>  <span class="c1"># Adjust the width as needed</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span> <span class="o">=</span> <span class="p">[</span><span class="n">widgets</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="n">technique</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="n">button_style</span><span class="p">)</span> <span class="k">for</span> <span class="n">technique</span> <span class="ow">in</span> <span class="n">capture_techniques</span><span class="p">]</span>
    <span class="c1"># Bind event handlers</span>
    <span class="k">for</span> <span class="n">btn</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span><span class="p">:</span>
      <span class="n">btn</span><span class="o">.</span><span class="n">on_click</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">on_class_selected</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">score_display</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Output</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feedback_display</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">Output</span><span class="p">()</span>

    <span class="c1"># Initialize the figure for image display</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">remove_ip_clutter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">remove_ip_clutter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">show_next_image</span><span class="p">()</span>

    <span class="c1"># Arrange widgets in a layout</span>
    <span class="n">current_buttons</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">VBox</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">,</span>
                                    <span class="n">widgets</span><span class="o">.</span><span class="n">HBox</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">3</span><span class="p">]),</span>
                                    <span class="n">widgets</span><span class="o">.</span><span class="n">HBox</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="mi">6</span><span class="p">]),</span>
                                    <span class="n">widgets</span><span class="o">.</span><span class="n">HBox</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span><span class="p">[</span><span class="mi">6</span><span class="p">:</span><span class="mi">10</span><span class="p">])])</span>
    <span class="n">previous_feedback</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">VBox</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_display</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ui</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">HBox</span><span class="p">([</span><span class="n">previous_feedback</span><span class="p">,</span> <span class="n">current_buttons</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_display</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">show_next_image</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># Display the next image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="p">]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="o">==</span> <span class="mi">64</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">scalar_value</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
      <span class="c1"># Initialize the 8x8 array with -6 (black)</span>
      <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="o">-</span><span class="mf">6.0</span><span class="p">)</span>
      <span class="c1"># Set the first ring to 6 (white)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="c1"># Set the second ring to 6 (white)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6</span>
      <span class="c1"># Set the third (inner ring) back to -6 (black)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">6</span>
      <span class="c1"># Assuming scalar_value is already in the range -6 to 6</span>
      <span class="c1">#print(scalar_value)</span>
      <span class="n">image</span><span class="p">[</span><span class="mi">3</span><span class="p">:</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">scalar_value</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Unexpected image shape: </span><span class="si">{</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flipud</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_image</span> <span class="o">=</span> <span class="n">image</span>
    <span class="c1"># Display the image</span>
    <span class="c1">#print(image)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">'equal'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">'equal'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'off'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">'upper'</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">previous_image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">6</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">'upper'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Current Sensory Input'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Previous Sensory Input'</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">canvas</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">on_class_selected</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">button</span><span class="p">):</span>
    <span class="c1"># freeze buttons while we process</span>
    <span class="k">for</span> <span class="n">btn</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span><span class="p">:</span>
      <span class="n">btn</span><span class="o">.</span><span class="n">disabled</span><span class="o">=</span><span class="kc">True</span>
    <span class="n">selected_class</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">technique_to_index</span><span class="p">[</span><span class="n">button</span><span class="o">.</span><span class="n">description</span><span class="p">]</span>
    <span class="n">correct_class</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="p">]))</span>
    <span class="n">selected_description</span> <span class="o">=</span> <span class="n">button</span><span class="o">.</span><span class="n">description</span>
    <span class="n">correct_description</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span><span class="p">[</span><span class="n">correct_class</span><span class="p">]</span><span class="o">.</span><span class="n">description</span>
    <span class="k">if</span> <span class="n">selected_class</span> <span class="o">==</span> <span class="n">correct_class</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">successful_captures</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">feedback</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Your last choice, '</span><span class="si">{</span><span class="n">selected_description</span><span class="si">}</span><span class="s2">', was correct!"</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">-=</span> <span class="mi">1</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">failed_captures</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="n">feedback</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">"Your last choice, '</span><span class="si">{</span><span class="n">selected_description</span><span class="si">}</span><span class="s2">', was incorrect. The correct technique was '</span><span class="si">{</span><span class="n">correct_description</span><span class="si">}</span><span class="s2">'."</span>
    <span class="c1"># show feedback</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_display</span><span class="p">:</span>
      <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">feedback</span><span class="p">)</span>
    <span class="c1"># Show score</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">score_display</span><span class="p">:</span>
      <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="n">average_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">score</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Total Score: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">score</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Number of Trials: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">current_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Successful Captures: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">successful_captures</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Failed Captures: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">failed_captures</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">'Average Score Per Trial: </span><span class="si">{</span><span class="n">average_score</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
    <span class="c1"># Prepare the next image</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">current_index</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="c1">#print(self.current_index)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">show_next_image</span><span class="p">()</span>
    <span class="c1"># Re-enable buttons</span>
    <span class="k">for</span> <span class="n">btn</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">strike_buttons</span><span class="p">:</span>
      <span class="n">btn</span><span class="o">.</span><span class="n">disabled</span><span class="o">=</span><span class="kc">False</span>


<span class="n">scramble_multi_hard</span> <span class="o">=</span> <span class="n">InteractiveMNISTPredator_Multi</span><span class="p">(</span>
    <span class="n">features</span><span class="o">=</span><span class="n">Xs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">feedback_type</span><span class="o">=</span><span class="s1">'both'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">scramble_multi_hard</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">scramble_multi_hard</span><span class="o">.</span><span class="n">prev_fig</span><span class="o">.</span><span class="n">canvas</span><span class="p">)</span>
<span class="n">clear_output</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span><span class="n">scramble_multi_hard</span><span class="o">.</span><span class="n">ui</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "2c397117c68f4ff894e07a2238d6cc6a"}
</script></div>
</div>
<p>We found the binary discrimination task with these inputs basically intractable, so for us at least this 10-fold discrimination task is even more impossible. Let’s see though if we can adapt our behaviour generating network and our learning rules to this new multi-class scenario.</p>
<p>We model this organism’s sensory-behaviour much as before with a few small, but important, differences. As before <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is the raw sensory input (column vector) of length 64 in a given episode. Each element <span class="math notranslate nohighlight">\(x_i\)</span> of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> corresponds to the activation level of a single photosensitive neuron.</p>
<p>These input neurons are then connected by synapses to <strong>multiple</strong> output neurons, each one corresponding to a different possible action selection. The activation level, as a vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, of these output neurons is computed as
$<span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{Wx}\)</span><span class="math notranslate nohighlight">\(
Here \)</span>\mathbf{W}<span class="math notranslate nohighlight">\( is a matrix of synaptic weights between the input neurons and the various output neurons. In this case where there are 65 inputs (we have done our usual trick of hiding the baseline activation level by augmenting \)</span>\mathbf{x}<span class="math notranslate nohighlight">\() and 10 outputs, so \)</span>\mathbf{W}<span class="math notranslate nohighlight">\( has shape 10 x 65. So
\)</span><span class="math notranslate nohighlight">\( z_i = \sum_{j=1}^{65} w_{ij} x_j\)</span>$</p>
<p><strong>Notation reminder</strong>: <span class="math notranslate nohighlight">\(w_{ij}\)</span> is the weight connecting the <span class="math notranslate nohighlight">\(j^{th}\)</span> input to the <span class="math notranslate nohighlight">\(i^{th}\)</span> output and is the element in the <span class="math notranslate nohighlight">\(i^{th}\)</span> row and <span class="math notranslate nohighlight">\(j^{th}\)</span> column of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.)</p>
<p>To convert these activation levels, <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, into probabilities of actions, we use softmax normalization. Softmax extends the logistic sigmoid function to handle multiple classes.
Much like the logistic sigmoid turns any real number into a probability over a binary outcome (strike or no-strike), the softmax function turns any vector of real values into a vector of probabilities over different outcomes
$<span class="math notranslate nohighlight">\(\text{softmax}(\mathbf{z}) = \frac{(\exp{z_0}, \dots, \exp(z_i), \dots, \exp(z_n))}{\sum_k \exp(z_k)}\)</span>$</p>
<p>We use a subscript to refere to a particular element of the softmax output so that probability of choosing a particular action <span class="math notranslate nohighlight">\(i\)</span> can be expressed as:
$<span class="math notranslate nohighlight">\( \Pr \{\text{action }i\} = \text{softmax}_i(\mathbf{z}) = \frac{\exp(z_i)}{\sum_k \exp(z_k)} \)</span>$</p>
<p>When there are only two possible actions, and one action has a fixed activation of <span class="math notranslate nohighlight">\(0\)</span>, then softmax normalization is equivalent to applying the logistic sigmoid of the variable (non-zero) activation level, to determine action probabilities. (This was shown in sequence (blah).)</p>
<p>Like the logistic sigmoid, softmax normalization can also have its variabilility modified by a temperature hyper-parameter, <span class="math notranslate nohighlight">\(\tau\)</span>, where high temperatures cause the distribution to be closer to a uniform distribution, and low temperatures cause the highest activation value action to be chosen with near certainty.
$<span class="math notranslate nohighlight">\( \Pr \{\text{action }i\} = \text{softmax}_i(\mathbf{z};\tau) = \frac{\exp(z_i /\tau)}{\sum_k \exp(z_k / \tau)} \)</span>$</p>
<p>This motivates the following modifications of our update rules to the multi-class forms</p>
<div class="section" id="reward-prediction-error-multi-class-episodic-update-rule">
<h2>Reward-Prediction-Error Multi-Class Episodic Update Rule<a class="headerlink" href="#reward-prediction-error-multi-class-episodic-update-rule" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[\Delta W_{ij} = s \cdot (r-z_i) \cdot x_j \quad \text{if action }i\text{ taken}\]</div>
<div class="math notranslate nohighlight">
\[\Delta W_{ij} = 0 \quad \text{if some action other than }i\text{ taken}\]</div>
<p>Or in vector form</p>
<div class="math notranslate nohighlight">
\[ \Delta \mathbf{W}_{i,:} = s \cdot (r-z_i) \cdot \mathbf{x}^T\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\mathbf{W}_{i,:}\)</span> denotes the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>. Using similar notation we would use <span class="math notranslate nohighlight">\(\mathbf{W}_{:,j}\)</span> denotes the <span class="math notranslate nohighlight">\(j^{th}\)</span> column of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>. Note that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is transposed so that it becomes a row vector, and it’s orientation matches that of <span class="math notranslate nohighlight">\(\mathbf{W}_{i,:}\)</span> which is a row vector of the same length as <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
</div>
<div class="section" id="action-probability-reinforcement-mulit-class-episodic-update-rule">
<h2>Action-Probability-Reinforcement Mulit-Class Episodic Update Rule<a class="headerlink" href="#action-probability-reinforcement-mulit-class-episodic-update-rule" title="Permalink to this headline">¶</a></h2>
<div class="math notranslate nohighlight">
\[ \Delta W_{ij} = s \cdot r \cdot \frac{\partial}{\partial z_i}\text{softmax}_k(\mathbf{z}) \cdot x_j \quad \text{where } k \text{ corresponds to the action taken}\]</div>
<p>or in vector form</p>
<div class="math notranslate nohighlight">
\[ \Delta \mathbf{W} = s \cdot r \cdot \left(\nabla_{\mathbf{z}} \text{softmax}_k(\mathbf{z})\right) \mathbf{x}^T \quad \text{where } k \text{ corresponds to the action taken}\]</div>
<p>In the vector formulation <span class="math notranslate nohighlight">\(\mathbf{x}^T\)</span> is a row vector of length 65, and <span class="math notranslate nohighlight">\(\frac{\partial}{\partial \mathbf{z}}\text{softmax}(z_k)\)</span> is a col vector of length 10, so their matrix outer product gives a matrix of shape 10 by 65 (the shape of <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>.) Now much like the sigmoid function which it is a generalization of <span class="math notranslate nohighlight">\(\text{softmax}\)</span> also has a convinient derivative, specifically</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial}{\partial z_i}\text{softmax}(\mathbf{z})_k =
\begin{cases}
-\pi(a_k)\cdot \pi(a_i) &amp; \text{if } i \neq k \\
\pi(a_i)\cdot (1 - \pi(a_i)) &amp; \text{if } i = k
\end{cases}
\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\(\pi(a_i)\)</span> is just another way of denoting output of the softmax normalization corresponding to the probability of selecting action <span class="math notranslate nohighlight">\(a_i\)</span>, that is <span class="math notranslate nohighlight">\( \text{softmax} (\mathbf{z})_i\)</span>, given the vector of output activations <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, but writen to ephasize that is the probability of an action being taken given a policy. This is saying that if we increase the activation of a given action, say <span class="math notranslate nohighlight">\(z_k\)</span>, we increase the probability that that action, <span class="math notranslate nohighlight">\(a_k\)</span> is taken at a rate of <span class="math notranslate nohighlight">\(\pi(a_k)\cdot (1 - \pi(a_k))\)</span> while simultaneously decreasing the probability (hence the negative sign) with which other actions <span class="math notranslate nohighlight">\(a_i\)</span>, <span class="math notranslate nohighlight">\(i\neq k\)</span>, are taken at a rate of <span class="math notranslate nohighlight">\(-\pi(a_k)\cdot \pi(a_i)\)</span>. Substituting this into our update rule we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Delta W_{ij} \begin{cases}
- s \cdot r \cdot \pi(a_k)\cdot \pi(a_i) \cdot x_j &amp; \text{if } i \neq k \\
s \cdot r \cdot \pi(a_i)\cdot (1 - \pi(a_i)) \cdot x_j &amp; \text{if } i = k
\end{cases}
\quad \text{where } k \text{ corresponds to the action taken}\end{split}\]</div>
<p>What this is saying is that if an action is rewarding, the weights will shift to make selection of that action given those inputs more likely given these (or similar) sensory inputs, and if an action is not rewarding the reverse is true, the weights will shift to make alternative actions more likely to be selected in given these (or similar) sensory inputs.</p>
<p>One important way in which this action-probability-reinforcement differs from the reward-prediction-error update is that every weight in the network is potentially updated after a given episode, in contrast to only the weights making a prediction about the reward associated with the action sampled.</p>
</div>
<div class="section" id="expected-reward-gradient-multi-class-episodic-update-rule">
<h2>Expected-Reward-Gradient Multi-Class Episodic Update Rule<a class="headerlink" href="#expected-reward-gradient-multi-class-episodic-update-rule" title="Permalink to this headline">¶</a></h2>
<p>In our previous analysis we saw that the expected-reward-gradient update is equivalent to the action-log-probability-reinforcement update so extending to this insight our multi-class scenario we have</p>
<div class="math notranslate nohighlight">
\[ \Delta W_{ij} = s \cdot r \cdot \frac{\partial}{\partial z_i}\log(\text{softmax}(z_k)) \cdot x_j \quad \text{where } k \text{ corresponds to the action taken}\]</div>
<p>Now <span class="math notranslate nohighlight">\(\log(\text{softmax})\)</span> also hase a very convenient derivative.
$$
\frac{\partial}{\partial z_i}\log(\text{softmax}(z_k)) =
\begin{cases}</p>
<ul class="simple">
<li><p>\pi(a_i) &amp; \text{if } i \neq k \
(1 - \pi(a_i)) &amp; \text{if } i = k
\end{cases}
$$</p></li>
</ul>
<p>So putting this into our update rule we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Delta W_{ij} \begin{cases}
- s \cdot r \cdot \pi(a_i) \cdot x_j &amp; \text{if } i \neq k \\
s \cdot r \cdot (1 - \pi(a_i)) \cdot x_j &amp; \text{if } i = k
\end{cases}
\quad \text{where } k \text{ corresponds to the action taken}\end{split}\]</div>
<p>This has basically the same affect as action-probability-reinforcement, but with different scaling terms. The primary differnce between the two is that action-probability reinforcement scales down parameter changes both when actions have a very high, and when they have a very low probability, in contrast, action-log-probability-reinforcement only scales down parameter changes in proportion to the probability of not taking the selected action. The practical effect of this difference in scaling is that action-probability-reinforcement discounts updates driven by actions that were unlikely to be selected relative to action-log-probability-reinforcement.</p>
<p>Now that we know what our multi-class update rules are, we can implement and compare them. Before we do that though, we need to update our parameter evaluation function to this new multi-class setting</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">np_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="c1"># high tau more exploration, low tau very little exploration</span>
  <span class="n">x_scaled</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">tau</span>
  <span class="c1"># Shift x by subtracting the max value to prevent overflow in exp</span>
  <span class="n">x_shifted</span> <span class="o">=</span> <span class="n">x_scaled</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x_scaled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">exps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_shifted</span><span class="p">)</span>
  <span class="c1"># Normalize the exponentials while maintaining batch structure</span>
  <span class="n">softmax_output</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">softmax_output</span>

<span class="k">def</span> <span class="nf">eval_params_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># num_action x batch</span>
  <span class="n">action_probs</span> <span class="o">=</span> <span class="n">np_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>  <span class="c1"># num_action x batch</span>
  <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">action_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">random_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">cumulative_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="n">sampled_actions</span> <span class="o">=</span> <span class="p">(</span><span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">random_samples</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="c1"># calculate which actions were correct and compute reward</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="n">sampled_actions</span> <span class="o">==</span> <span class="n">y</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="n">r</span><span class="p">[</span><span class="n">correct</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">r</span><span class="p">[</span><span class="o">~</span><span class="n">correct</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># sampled reward</span>
  <span class="c1"># Create an outcomes matrix and calculate expected reward</span>
  <span class="n">outcomes_matrix</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">action_probs</span><span class="p">)</span>
  <span class="c1"># Set reward to +1 at the position of the correct action for each sample</span>
  <span class="n">outcomes_matrix</span><span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">r_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">action_probs</span> <span class="o">*</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># expected reward</span>
  <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">action_probs</span><span class="p">,</span> <span class="n">sampled_actions</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">r_exp</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have our evaluation function we can adapt our update rules to the new multi-class problem. Interestingly perturb-measure-step doesn’t need any adaption at all, because it is agnostic to the underlying mechanism of behaviour generation, the only modification it needs is to make use of this new evaluation function, and optimize over the appropriate number of parameters.</p>
<p>In the coding exercise below implement these multi-class adaptions of reward-prediction-error, action-probability-reinforcement, and action-log-probability-reinforcement.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">################################################################################</span>
<span class="c1"># TODO for students: Replace ... in the lines below with one of the following</span>
<span class="c1"># options, each option get's used exactly once</span>
<span class="c1"># a1) r - z[a, np.arange(len(a))]</span>
<span class="c1"># a2) errors @ x.T</span>
<span class="c1"># b1) r[0,b] * pi[i,b] * (1 - pi[i,b]) * x[j,b]</span>
<span class="c1"># b2) -r[0,b] * pi_a[b] * pi[i,b] * x[j,b]</span>
<span class="c1"># c1) r[0,b] * (1 - pi[i,b]) * x[j,b]</span>
<span class="c1"># c2) -r[0,b] * pi[i,b] * x[j,b]</span>
<span class="c1"># This will implement different multi-class update rules driven by:</span>
<span class="c1"># reward-prediction-error, action-probability-reinforcement,</span>
<span class="c1"># and action-log-probability-reinforcement</span>
<span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">"Exercise: Implement different multi-class update rules to compare"</span><span class="p">)</span>
<span class="c1">################################################################################</span>

<span class="k">def</span> <span class="nf">np_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="c1"># high tau more exploration, low tau very little exploration</span>
  <span class="n">x_scaled</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="n">tau</span>
  <span class="c1"># Shift x by subtracting the max value to prevent overflow in exp</span>
  <span class="n">x_shifted</span> <span class="o">=</span> <span class="n">x_scaled</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x_scaled</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="n">exps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x_shifted</span><span class="p">)</span>
  <span class="c1"># Normalize the exponentials while maintaining batch structure</span>
  <span class="n">softmax_output</span> <span class="o">=</span> <span class="n">exps</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">exps</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">softmax_output</span>

<span class="k">def</span> <span class="nf">eval_params_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
  <span class="c1"># W shape is num_action x input</span>
  <span class="c1"># x shape is input x batch</span>
  <span class="c1"># y shape is 1 x batch</span>
  <span class="k">if</span> <span class="n">rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># num_action x batch</span>
  <span class="n">pi</span> <span class="o">=</span> <span class="n">np_softmax</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>  <span class="c1"># num_action x batch</span>
  <span class="n">cumulative_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">random_samples</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">cumulative_probs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="c1">#sampled actions</span>
  <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">cumulative_probs</span> <span class="o">&gt;</span> <span class="n">random_samples</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="c1"># calculate which actions were correct and compute reward</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="n">a</span> <span class="o">==</span> <span class="n">y</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
  <span class="n">r</span><span class="p">[</span><span class="n">correct</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">r</span><span class="p">[</span><span class="o">~</span><span class="n">correct</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>  <span class="c1"># sampled reward</span>
  <span class="c1"># Create an outcomes matrix to calculate expected reward</span>
  <span class="n">outcomes_matrix</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
  <span class="c1"># Set reward to +1 at the position of the correct action for each sample</span>
  <span class="n">outcomes_matrix</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">r_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pi</span> <span class="o">*</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># expected reward</span>
  <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="k">def</span> <span class="nf">reward_prediction_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                            <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                            <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">):</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="c1"># learn only from actual received rewards</span>
  <span class="n">errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
  <span class="n">actual_errors</span> <span class="o">=</span> <span class="o">...</span>
  <span class="n">errors</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">))]</span> <span class="o">=</span> <span class="n">actual_errors</span>
  <span class="c1"># implicit sum over batch here in this matrix multiplication</span>
  <span class="n">update</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># errors is num_actions x batch, x.T is batch x num_features, update is num_actions x num_features</span>
  <span class="n">update</span> <span class="o">/=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Divide by the number of samples in batch to make the sum an average</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">update</span>  <span class="c1"># Apply learning rate to update step</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="k">def</span> <span class="nf">action_prob_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                      <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                      <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="n">num_actions</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">pi</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">num_features</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">pi_a</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">))]</span>  <span class="c1"># [batch_size]</span>
  <span class="n">broadcast_pi_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">pi_a</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">batch_size</span><span class="p">),</span> <span class="p">(</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span>
  <span class="c1"># Compute updates for case when row (i) of W does not correspond to the sampled action</span>
  <span class="n">delta_W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_actions</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">))</span>
  <span class="n">delta_W</span> <span class="o">-=</span> <span class="n">r</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:]</span> <span class="o">*</span> <span class="n">broadcast_pi_a</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">pi</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:]</span>  <span class="c1"># Shape [num_actions, num_features, batch_size]</span>
  <span class="c1"># now compute updates for case when row (i) of W does correspond to the sampled action</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_actions</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">a</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># [num_actions, batch_size]</span>
  <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">broadcast_to</span><span class="p">(</span><span class="n">mask</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:],</span> <span class="n">delta_W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># [num_actions, num_features, batch_size]</span>
  <span class="n">positive_update</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:]</span> <span class="o">*</span> <span class="p">(</span><span class="n">pi</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pi</span><span class="p">))[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:]</span>  <span class="c1"># [num_features, num_actions, batch_size]</span>
  <span class="c1"># Use positive update where appropriate</span>
  <span class="n">delta_W</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">positive_update</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
  <span class="c1"># average over the elements of the mini-batch</span>
  <span class="n">delta_W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">delta_W</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">delta_W</span>
  <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
    <span class="c1"># as a sanity check on all the clever broadcasting and array operations</span>
    <span class="c1"># check against bog simple for loop implementation</span>
    <span class="n">delta_W_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">delta_W</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_actions</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_features</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">a</span><span class="p">[</span><span class="n">b</span><span class="p">]:</span>
            <span class="n">delta_W_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">delta_W_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">delta_W</span><span class="p">,</span> <span class="n">delta_W_test</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="k">def</span> <span class="nf">action_log_prob_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                          <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                          <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                          <span class="n">test</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="n">num_actions</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">pi</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">num_features</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="c1"># Compute updates for case when row (i) of W does not correspond to the sampled action</span>
  <span class="n">a_1hot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>  <span class="c1"># [num_actions, batch_size]</span>
  <span class="n">a_1hot</span><span class="p">[</span><span class="n">a</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">pi_term</span> <span class="o">=</span> <span class="n">a_1hot</span> <span class="o">-</span> <span class="n">pi</span>
  <span class="n">delta_W</span> <span class="o">=</span> <span class="n">r</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:]</span> <span class="o">*</span> <span class="n">pi_term</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,:,:]</span>  <span class="c1"># Shape [num_actions, num_features, batch_size]</span>
  <span class="c1"># average over the elements of the mini-batch</span>
  <span class="n">delta_W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">delta_W</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">delta_W</span>
  <span class="k">if</span> <span class="n">test</span><span class="p">:</span>
    <span class="c1"># as a sanity check on all the clever broadcasting and array operations</span>
    <span class="c1"># check against bog simple for loop implementation</span>
    <span class="n">delta_W_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">delta_W</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_actions</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_features</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="n">a</span><span class="p">[</span><span class="n">b</span><span class="p">]:</span>
            <span class="n">delta_W_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">delta_W_test</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="o">...</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">delta_W</span><span class="p">,</span> <span class="n">delta_W_test</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="c1">############### Exercise Complete ###############</span>
<span class="c1">##### Simulation and Plotting Logic Follows #####</span>

<span class="k">def</span> <span class="nf">always_cheat_perturb_measure_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                      <span class="n">perturbation_scale</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                                      <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                      <span class="n">rng</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">rng</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
  <span class="n">z</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">outcomes_matrix</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">eval_params_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="n">raw_test_perturb</span> <span class="o">=</span> <span class="n">learn_rng</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
  <span class="n">unit_test_perturb</span> <span class="o">=</span> <span class="n">raw_test_perturb</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">raw_test_perturb</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
  <span class="n">test_perturbation</span> <span class="o">=</span> <span class="n">unit_test_perturb</span> <span class="o">*</span> <span class="n">perturbation_scale</span>
  <span class="n">perturbed_W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">test_perturbation</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_perturb</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_exp_perturb</span> <span class="o">=</span> <span class="n">eval_params_multi</span><span class="p">(</span><span class="n">perturbed_W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="p">)</span>
  <span class="n">directional_grad_est</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r_exp_perturb</span> <span class="o">-</span> <span class="n">r_exp</span><span class="p">))</span> <span class="o">/</span> <span class="n">perturbation_scale</span>
  <span class="n">update</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">directional_grad_est</span> <span class="o">*</span> <span class="n">unit_test_perturb</span>
  <span class="n">W_new</span> <span class="o">=</span> <span class="n">W</span> <span class="o">+</span> <span class="n">update</span>
  <span class="k">return</span> <span class="n">W_new</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span>

<span class="c1"># simulation</span>
<span class="n">learn_rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">num_steps</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">mini_batch_size</span> <span class="o">=</span> <span class="mi">281</span>
<span class="n">cooling_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">W_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span><span class="mi">65</span><span class="p">))</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span>
<span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span>
<span class="n">alg_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Reward Prediction'</span><span class="p">,</span> <span class="s1">'Action Probability'</span><span class="p">,</span> <span class="s1">'Action Log Probability'</span><span class="p">,</span> <span class="s1">'Perturb Measure'</span><span class="p">]</span>
<span class="n">alg_funcs</span> <span class="o">=</span> <span class="p">[</span><span class="n">reward_prediction_multi</span><span class="p">,</span> <span class="n">action_prob_multi</span><span class="p">,</span> <span class="n">action_log_prob_multi</span><span class="p">,</span> <span class="n">always_cheat_perturb_measure_multi</span><span class="p">]</span>
<span class="n">alg_lrs</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Reward Prediction'</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>
           <span class="s1">'Action Probability'</span><span class="p">:</span> <span class="mf">0.008</span><span class="p">,</span>
           <span class="s1">'Action Log Probability'</span><span class="p">:</span> <span class="mf">0.004</span><span class="p">,</span>
           <span class="s1">'Perturb Measure'</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">}</span>
<span class="n">reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">exp_reward_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="n">W_s</span> <span class="o">=</span> <span class="p">{</span><span class="n">alg_name</span><span class="p">:</span> <span class="n">W_init</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span> <span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">}</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
  <span class="n">learn_rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">batch_step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">mini_batch_size</span><span class="p">):</span>
    <span class="n">batch_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">batch_step</span><span class="p">:</span><span class="n">batch_step</span><span class="o">+</span><span class="n">mini_batch_size</span><span class="p">]</span>
    <span class="n">batch_x</span> <span class="o">=</span> <span class="n">Xs_aug</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="n">batch_y</span> <span class="o">=</span> <span class="n">y1</span><span class="p">[</span><span class="n">batch_indices</span><span class="p">]</span><span class="o">.</span><span class="n">T</span>
    <span class="k">for</span> <span class="n">alg_name</span><span class="p">,</span> <span class="n">alg_func</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">alg_names</span><span class="p">,</span> <span class="n">alg_funcs</span><span class="p">):</span>
      <span class="n">lr</span> <span class="o">=</span> <span class="n">alg_lrs</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="n">W</span> <span class="o">=</span> <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">alg_name</span> <span class="o">==</span> <span class="s1">'Reward Prediction'</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">((</span><span class="n">num_steps</span><span class="o">+</span><span class="mf">10.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">cooling_rate</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">tau</span> <span class="o">=</span> <span class="mf">1.0</span>
      <span class="c1"># perturb-measure-step uses r_exp everything else just uses r</span>
      <span class="n">new_W</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">r_exp</span> <span class="o">=</span> <span class="n">alg_func</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
      <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">r_exp_full</span> <span class="o">=</span> <span class="n">eval_params_multi</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">Xs_aug</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.00001</span><span class="p">,</span> <span class="n">rng</span><span class="o">=</span><span class="n">learn_rng</span><span class="p">)</span>
      <span class="n">W_s</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_W</span>
      <span class="n">reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
      <span class="n">exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r_exp_full</span><span class="p">))</span>
    <span class="n">num_steps</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="k">if</span> <span class="n">num_steps</span> <span class="o">&gt;</span> <span class="mi">2000</span><span class="p">:</span>
    <span class="k">break</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">theoretical_max</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="c1"># Colors for algorithms</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">'Reward Prediction'</span><span class="p">:</span> <span class="s1">'blue'</span><span class="p">,</span> <span class="s1">'Action Probability'</span><span class="p">:</span> <span class="s1">'green'</span><span class="p">,</span>
          <span class="s1">'Action Log Probability'</span><span class="p">:</span> <span class="s1">'orange'</span><span class="p">,</span> <span class="s1">'Perturb Measure'</span><span class="p">:</span> <span class="s1">'red'</span><span class="p">}</span>

<span class="c1"># First subplot for expected rewards</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">theoretical_max</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'--'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'gray'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">'Max Possible Avg. Reward per Episode'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alg_name</span> <span class="ow">in</span> <span class="n">alg_names</span><span class="p">:</span>
  <span class="nb">eval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">exp_reward_results</span><span class="p">[</span><span class="n">alg_name</span><span class="p">])</span>
  <span class="nb">eval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="nb">eval</span><span class="p">)</span>
  <span class="nb">eval</span> <span class="o">=</span> <span class="nb">eval</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">eval</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">eval</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'-'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">alg_name</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">'</span><span class="si">{</span><span class="n">alg_name</span><span class="si">}</span><span class="s1">'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">'Cumulative per Episode Average of</span><span class="se">\n</span><span class="s1">Expected (Full Batch) Reward'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">'Cumulative Avg. Expected Reward'</span><span class="p">)</span>
  <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>

</div>
<p><a class="reference external" href="https://github.com/dcownden/PerennialProblemsOfLifeWithABrain/tree/main//sequences/P2C1_Optimization/solutions/P2C1_Sequence5_Solution_71d74c35.py"><em>Click for solution</em></a></p>
<p><em>Example output:</em></p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_Optimization/static/P2C1_Sequence5_Solution_71d74c35_0.png"><img align="center" alt="Solution hint" class="align-center" src="https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P2C1_Optimization/static/P2C1_Sequence5_Solution_71d74c35_0.png" style="width: 800.0px; height: 600.0px;"/></a>
<p>In this harder problem each of the gradient aligned update rules (reward-prediction-error, action-probability-reinforcement, action-log-probability-reinforcement) all are able to achieve an average reward per episode of roughly 0.75, which means they choose the “correct” action roughly 88% of the time. Eventually perturb-measure-step will also be able to achieve this level of performance, but learning is slow with perturb-measure-step. In the simulation above perturb-measure-step was allowed to “cheat” in the sense that it used the expected reward (not sampled reward) to evaluate perturbed versus base parameters. Also in the simulation above, to further help perturb-meausure-step each ‘epsiode’ consisted of mini-batch of 281 experiences, that is <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span> pairs. In contrast the other learning rules adapted using sampled actions and the resulting rewards.</p>
<p>This further highlights some of differences between perturb-measure-step and these other gradient aligned methods noted earlier. Each of these gradient oriented update rules needed to be recomputed for this new multi-class output. In contrast the perturb-measure-step update rule needed basically no adaption at all. A new evaluation function could simply be swapped into perturb-measure-step. This difference in ease of implementation in code is important, as it stems from the crucial difference between these different classes of update methods. Perturb-measure-step is what is called a zeroeth-order method, in that it requires absolutly zero information about the underlying structure of the evaluation function being optimized, only the ability to use the evaluation function to evaluate parameters. Optimization processes like this are sometime called “black box”, since they only see inputs to an evaluation function (the parameter weights <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> in our case), and some (potentially noisy) evaluation of those parameters in the form of a reward. In contrast reward-prediction-error, action-probability-reinforcement, and action-log-probability-reinforcement all belong a class of optimization mehtods known as first-order methods. First-order refers to the use of knowledge of the <strong>structure</strong> of the parameter evaluation function, and in particular knoweldge in the form of derivatives (of the first-order) of various parts of the evaluation function with respect to the parameters. Second order methods also exist which not only take into account the first derivatives of the evaluation function with respect to the parameters, i.e. local linear rates of change, but also second derivatives, i.e. local curvature, to inform parameter updates. We will learn more about second order methods in sequence (blah).</p>
<p>So perturb-measure-step is robust in the sense that it can be readily applied to any optimization problem without any condideration of the particular structure of the problem, but this generality comes at the cost of compartively slow learning. In contrast, first order methods can find good solutions much more quickly, but implementation of these methods is more problem specific, as at least some of the structure of the evaluation function is used to inform the update rule. In the rules we looked at the gradient of the activations with respect to the weights, <span class="math notranslate nohighlight">\(\nabla_{\mathbf{W}}\mathbf{z}\)</span>, was critical to improving behaviour, either through improving a reward prediction, or directly reinforcing the probabilities (log-probailities) of actions taken with reward recieved.</p>
<p>Submit your feedback</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Submit your feedback</span>
<span class="n">content_review</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">feedback_prefix</span><span class="si">}</span><span class="s2">_M3"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "1454a0fc92e04d3d8ee0f0f73c0ce9d1"}
</script></div>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>We have now learned about an important way of speeding up learning. That is, by making parameter updates that are (mostly) aligned with the gradient of the function (in this case expected reward) that we are trying to improve through learning. Doing this, however, requires using update rules that somehow utilize implicit knowledge of the structure of the function being optimized. Our gradient aligned methods made use of implicit knowledge about how the sensory inputs interacted with the parameter weights to cause action selection. The “cheating” variants of all of our update rules made use of knowledge of how actions combined with the true state of the environment to compute the “true” expected reward given the action probabilities, instead of simply relying on an estimate of this expected reward based on sampled actions and resultant rewards. How these both of these kinds of “knowledge about structure”, that is “knowledge of self in the form of how parameters generate actions from sensory inputs” and “knowledge of the world in the form of how actions generate rewards” can be used to inform local synaptic plasticity update rules in the brain is an important issue that leave aside for the moment. We do emphasize however, that biological plausibility needs to be considered both from the micro perspective of electro-physiological mechanisms and also from the macro perspective of behavioural learning efficacy. Animals acquire adaptive behaviours rapidly, often after only a few dozen learning episodes for simple associations. There are likely many complimentary process at play allowing for this rapid learning. The point though is that any plasticity rule which does not support this kind of rapid learning, is not plausible at the macro level of explaining learning behaviour, and so must be discounted no matter how empirically well supported the underlying electro-physiological mechanisms are at the micro level.</p>
<p>In the next sequence we shift our focus from learning speed, to “final” performance. Here we kept our model archetecture fixed and to compare the relative rates of improvement driven by different update rules. In the next sequence we will keep our basic update rule fixed and focus the highest level of performance possible given different models a sensory-behaviour system, in particular looking at what more complex (more parameter) models are able to achieve.</p>
</div>
</div>
<hr class="docutils"/>
<div class="section" id="bio-box-biological-plausibility-constrained-above-and-below">
<h1>Bio Box: Biological Plausibility Constrained Above and Below<a class="headerlink" href="#bio-box-biological-plausibility-constrained-above-and-below" title="Permalink to this headline">¶</a></h1>
<p>We need to think of biological plausibility of learning mechanisms as constrained on two ends.</p>
<ol class="simple">
<li><p>From below, by the plausibility of the electro-physiological mechanisms of neural plasticity,</p></li>
<li><p>From above, by the plausibility of the efficacy of the learning rule implemented by those mechanisms.</p></li>
</ol>
<p>From the perspective of the constraints of electro-physiological mechansims alone an algorithm like perturb-measure-step is very appealing. Perturb-measure-step implements a kind of optimization with no knowledge of the structure of the network that generates the behaviour or the environmental dynamics that then provide reward contingent on behaviour. This makes it easy to reconcile such a learning rule with the observed facts of synaptic plasticity, e.g. Hebbian learning modulated by global reward signals . However, this comes at the cost taking relatively many learning trials to discover good parameters. Thus perturb-measure-step runs into an efficacu constraint when considering optimization of large and complex neural ciruits. Is is simply too slow to account for the synaptic plasticty underlying the <strong>rapid</strong> aquisition of adaptive behaviours observed (near unviserally) in animals. Thus while algorithms roughly like perturb-measure-step are likely implemented and useful for configuring many small and simple neural circuits, they cannot account for all of synaptic plasticity, and in particular the adaptive plasticity in large networks.</p>
<p>From the perspective of plausible efficacy, update rules which do make use of knowledge of structure of the network that generates behaviour and/or the environmental dynamics that then provide reward, are more appealing, because they are effective and quick, even in an online, episodic context where learning is driven exclusively by actual experiences (not hypothetical distributions of experiences and expectated values of reward.). However, from the perpsective electro-physiological mechanisms, the question of how knowledge of the structure of the behaviour generating network and/or the dynamics of the environment is implemented on the level of electro-physiological mechanisms is a challenge. However, this is a challenge that needs to be embraced: as networks become large, perturb-measure-step and related methods are non-starters in terms of candidate learning mechanisms because they are simply not <strong>effective</strong> enough.</p>
<p>A physiologically simple mechanism that predicts learning on the time course of millions of learning episodes, when behaviourally we see learning over a time course of dozens of episodes, is not biologically plausible. Practical efficacy and eltro-physiological mechansims are <strong>both</strong> necissary constraints on any update rule that purports to describe the dynamics of synaptic plasticit used by the brain to rapidly aquire adaptive behaviours!</p>
</div>
<div class="section" id="algo-box-zeroeth-order-methods">
<h1>Algo Box: Zeroeth Order Methods<a class="headerlink" href="#algo-box-zeroeth-order-methods" title="Permalink to this headline">¶</a></h1>
<p>We have primarily used propose-accept-reject and perturb-measure-step as our work-horse zeroeth order methods in examples because of their conceptual simplicity. Zeroeth order methods are important and useful tools in that they make no assumptions, and require no knowledge of the underlying strcuture of the optimization problem. They build up this knowledge purely by inference/experience, evaluating the function to be optimized many times. This situation of having no knowledge, or at least no helpful knowledge, does indeed occur in many practical settings, and in such situations zeroeth order methods are the only option available.</p>
<p>The best of such methods extend build on the core ideas of propose-accept-reject and perturb-measure-step, by keeping track of a population of sample points, and then using the relative evaluations of these samples points to inform the selection of new sample points. Such methods make inferences (either implicitly through geometry e.g. in the Nedler-Mead algorithm or explicitly as in the case of Covariance Matrix Adaptation - Evolutionary Strategy, CMA-ES), about both the direction greatest improvement (gradient) which informs which general direction in parameter space new test points should be sampled from relative to the current population of test points, and the curvature of the evaluation fucntion in the neighbourhood of the current test population, which informs how dispersed from the current population of test points new test points should be. High curvature in a given direction means small steps need to be taken in that direction to avoid overshooting as the gradient in that direction is rapidly changing, whereas low curvature in a given direction means that large steps can be taken with lower risk of overshooting as the gradient is relatively stable in that direction.</p>
<p>Covariance Matrix Adaption - Evolutionay Strategy is one such state of the art zeoreth order methods, and as the name implies, takes some of its inspiration from the evolutionary processes of natural selection.</p>
<p>Submit your feedback</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># @markdown Submit your feedback</span>
<span class="n">content_review</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">feedback_prefix</span><span class="si">}</span><span class="s2">_M4"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "928376ac893e4a6c882352c8d65337f6"}
</script></div>
</div>
</div>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {"499be0a8b43847009528356f1f78ae94": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e91f02b72e8c49d9b9d64c71ca583e9a": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "5af3c7fc309a4412bde665fd413ee67e": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Strike", "disabled": false, "icon": "", "layout": "IPY_MODEL_499be0a8b43847009528356f1f78ae94", "style": "IPY_MODEL_e91f02b72e8c49d9b9d64c71ca583e9a", "tooltip": ""}}, "8a38c0bb26504cab86f4ba4e719d473f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "66e8963e8cae4d57ad05c5ba97fa4c93": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "57809f21167f458492843b106688a9f8": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "No Strike", "disabled": false, "icon": "", "layout": "IPY_MODEL_8a38c0bb26504cab86f4ba4e719d473f", "style": "IPY_MODEL_66e8963e8cae4d57ad05c5ba97fa4c93", "tooltip": ""}}, "1d04d2a06f0e496c9a9c54ee66694457": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7a5e198c33de4c0f859b824ee531e393": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_1d04d2a06f0e496c9a9c54ee66694457", "msg_id": "", "outputs": []}}, "1886893fc83f479ba564d3d1e327dbc2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "dfa91b31d4df4932bd6d97c95035a8ec": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_1886893fc83f479ba564d3d1e327dbc2", "msg_id": "", "outputs": []}}, "f36750f3026a47d0b7a4d4e8d12c506e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d19c45ba7dca4a03a715dbf6dad6036e": {"model_name": "MPLCanvasModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_cursor": "default", "_data_url": "data:image/png;base64,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", "_dom_classes": [], "_figure_label": "Figure", "_image_mode": "diff", "_message": "", "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "MPLCanvasModel", "_rubberband_height": 0, "_rubberband_width": 0, "_rubberband_x": 0, "_rubberband_y": 0, "_size": [400.0, 400.0], "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "MPLCanvasView", "capture_scroll": false, "footer_visible": false, "header_visible": false, "layout": "IPY_MODEL_f36750f3026a47d0b7a4d4e8d12c506e", "pan_zoom_throttle": 33.0, "resizable": false, "toolbar": "IPY_MODEL_3b25a8adc05e4611970d78d3c9c885b1", "toolbar_position": "left", "toolbar_visible": false}}, "8ff0b16c7748438c8d94484f9211e647": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3b25a8adc05e4611970d78d3c9c885b1": {"model_name": "ToolbarModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_current_action": "", "_dom_classes": [], "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "ToolbarModel", "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "ToolbarView", "button_style": "", "collapsed": true, "layout": "IPY_MODEL_8ff0b16c7748438c8d94484f9211e647", "orientation": "vertical", "toolitems": [["Home", "Reset original view", "home", "home"], ["Back", "Back to previous view", "arrow-left", "back"], ["Forward", "Forward to next view", "arrow-right", "forward"], ["Pan", "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect", "arrows", "pan"], ["Zoom", "Zoom to rectangle\nx/y fixes axis", "square-o", "zoom"], ["Download", "Download plot", "floppy-o", "save_figure"]]}}, "0f4197c841e74e01bc01c9129bef4ca9": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d32344790d084aa88f9afd680c2abb5d": {"model_name": "MPLCanvasModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_cursor": "default", "_data_url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAYAAACAvzbMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbuUlEQVR4nO3deXRU9f3/8ddkDwlZWIIFYkBAEFARURZFBCQWjmyKogIJrSgtbsjRU7FYQKyKtRS0tBZBKKAgQlE4jaKQ4EHEKhQXjoCCEAwqApawxJDt8/uD31wzyUxI3uA3ap6Pc3Iqc+dz5876nJk7n1ufc84JAIAaCqvtDQAA/DQREACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQEBAJgQEACACQH5GfD5fPL5fFq/fn1tbwqAOqTOBGTKlCneC235v5iYGDVv3lyDBg3SsmXL5Jyr7U3FaRQXF+v555/XgAED1KxZM0VHRysxMVHnn3++evfurYkTJ+q1115TYWFhbW8q9P0bnClTptT2ppxVR44c0ZQpUzRlyhQdOXKktjenVkTU9gbUhiZNmnj/nZ+fr/3792v//v1avXq1FixYoJUrVyo6OroWt7Bm2rZtK0mqV69eLW/JD++LL77QgAEDtG3bNu+0qKgohYeHa/fu3frss8+0fv16PfHEE8rJydHVV19dexuLn7UjR45o6tSpkqTRo0crKSmpdjeoFtSZTyDlff31197fiRMntG3bNvXr10+S9Nprr2nSpEm1vIU1s2PHDu3YsUOXX355bW/KD6q0tFSDBw/Wtm3bVK9ePU2bNk379u1TYWGhvv32WxUUFOi9997TlClT1KpVq9reXOBnr04GpLywsDB16NBBq1atUuvWrSVJ//jHP1RSUlLLW4aKsrOztXXrVknSvHnzNGnSJKWmpsrn80mSoqOjddlll2ny5Mn67LPP1L1799rcXOBnr84HxC8mJkY33nijJOnYsWPasWOHJGnv3r3ed7h79+7V7t27dccdd6hly5aKjo5WixYtAtZTVlamF154QQMGDFCTJk0UFRWlxo0bKz09XUuWLKm0j2Xr1q3e+j/66KMqtzEjI0M+n099+/YNOP10O9ELCws1c+ZM9ejRQ8nJyYqJiVFaWpoyMjL0wQcfhLy86uycv/rqq0N+v/3dd9/pqaeeUvfu3ZWcnKzIyEg1btxY7du3V2ZmplasWFHl9a2o/LYOHjy4yvP6fL4qv4bcuHGjRo4cqbS0NMXExCgxMVGXX365pk+fruPHjwcdM3r0aPl8Po0ePVqStHz5cl199dVq0KCB6tWrp06dOmnWrFkqKysLebkvvfSS+vfvryZNmigyMlJJSUlq06aNBg0apNmzZ4fcb7N161ZlZGR425ucnKwePXpo5syZOnnyZNAxCxYskM/n8x6jOTk5GjJkiH7xi18oPDxco0eP1uuvvy6fz6eIiAh9+eWXIbdbknr27Blw/c+G8o8f55yee+45de3aVQkJCapfv766d++uxYsXhxzfokUL+Xw+LViwQMeOHdPEiRPVtm1bxcbGqlGjRhoyZIj+85//BB1b8bldncsov90tW7b0/t2yZcuAfat15qtTV0dMnjzZSXJVXeXZs2d759m4caNzzrk9e/Z4p73wwgsuPj7eSXL16tVzcXFxLi0tzRt/+PBhd9VVV3nnl+QSExMD/j1o0CB38uTJgMvt0KGDk+Tuv//+kNt2/PhxFxcX5yS5BQsWBCzzrzsnJ6fSuLy8PNexY0fvPJGRkQHbFBYW5p5++umgl1nVev169erlJLnJkycHnH706FF38cUXe+vw+XwuKSnJRUREeKeVv+2q48knn/TGfvrppzUa61daWuruueeegPskPj7ehYeHe/9u27at27t3b6WxmZmZTpLLzMx0d955p3f7JSUlBawvIyMj6GX/6le/qnS59erVCzhtz549lcbNmDHD+Xy+gMdUZGSk9++LLrrIffnll5XGzZ8/37udZ86c6a3DPz4zM9OVlZW5li1bOklu2rRpIW+37du3V3puVJd/XMXHiHPfP34mTZrkBg8e7CS5iIgIl5CQEHC7/OEPfwi67rS0NCfJzZgxw7Vt29ZJclFRUQHjw8LC3Lx58yqNLf/cDna7V7yM+fPne6cNHTrUNWrUyBvfqFEj16RJE+9v6NChNbqNfqoISDkPPPCAd57t27c75wIfZPHx8a5r167u/fff98bs3LnTOedcSUmJ92To1KmTW716tTtx4oRz7tSL/z//+U+XkpLiJLnx48cHXO706dOdJNe0aVNXWloadNsWLVrkJLm4uDh37NixgGWhXuhLSkpc165dvReNxYsXe/HavXu3u+6667wX96ysrEqXeSYBmTZtmpPkGjRo4FasWOEKCwudc6dewPfv3+8WLlzobr/99pDrDWb9+vXeNvXp08fl5eXVaLxzzk2aNMlJcikpKW727Nnu8OHDzjnnioqKXE5OjrvkkkucJNe5c+dK94U/IMnJyS4qKsrNmDHD5efnO+ecO3TokBszZoy3fevWrQsYu2HDBu/FbPr06d7l+seuWbPGZWZmuv379weMW716tbfOwYMHu88//9w559zJkyfdwoULXf369Z0k16NHD1dSUhIw1h+QmJgYFx4e7kaPHu327dvnnDv12Ni1a5dzzrknnnjCSXItWrRwZWVlQW+3CRMmOEmuY8eONbq9nateQJKTk11iYqJbsGCBKygocM4598UXX7iBAwd6t1uwNw3+F/fExESXnJzsli1b5oqLi51zzn3yySfe+iMiItyWLVsCxp5JQGoy/ueMgPx/+fn5rmnTpt6Lnv/Fo/yDJC0trdKLt9/ChQudJNeuXTt35MiRoOfZvHmz8/l8Lioqyh04cMA7PS8vz4WFhTlJbs2aNUHHpqenO0lu5MiRlZaFeqFfunSptyzYeouLi73ABHthOJOA9O/f30lyjz32WMixFv369fO2Kzw83HXv3t2NHz/eLVq06LSfSvbs2ePCw8NdbGys++CDD4Ke5+jRo6558+ZOklu5cmXAMn9Agr2Y+F166aVOkhszZkzA6f43Cenp6dW+rs45d8EFFzhJrmfPnpUC4Zxzq1at8rbp5ZdfDljmD4gkd/3114e8jG+++cZFRUU5Se7111+vtLywsNB7tx3q02pVqhMQSS47OzvoZfufl48++mil5f4Xd0lu7dq1lZYXFBS4Nm3aOEluwIABAcsIyJmr8/tAjhw5onXr1qlPnz7ed8D33nuvwsIq3zR33XWX4uPjg65n3rx5kqTf/va3SkxMDHqeSy+9VB06dFBRUZFycnK805s1a6Y+ffpIkhYtWlRp3FdffaV169ZJkkaNGlXt6/bSSy9Jkrp376709PRKyyMiIjR58mRJ0rZt2/Txxx9Xe92n4/9J41dffXXW1ilJK1eu1Lhx4xQZGanS0lJt2rRJM2fO1KhRo3T++eerRYsWmjp1qo4ePVpp7IIFC1RaWqpf/vKXuvjii4Ouv379+hoyZIgkac2aNUHPk5qaqszMzKDLBg0aJEmV9mf5b4+DBw+qtLS0OldVH330kbZv3y5JmjRpksLDwyudZ+DAgd6v75YsWRJyXRMnTgy5rHHjxrrhhhskSXPmzKm0fOXKlTp06JBiY2Nr9PiriSuuuEK9e/eudHp0dLSuvfZaSZVv04rjK+4blKTY2Fg98MADkqTXX39d+fn5Z2mLIdXRnejld3YlJyfrmmuu0ZYtWyRJI0eO1O9///ug46644oqgp5eWlurdd9+VdGrC4jnnnBPyb+fOnZKk3NzcgHVkZGRIOvVkPXHiRMCyF198UaWlpWratKmuueaaal/PzZs3S1KVY3r37u29MPnPfzZcd911kqS//vWvuuWWW/TKK6/o0KFDZ7zeuLg4zZ49W3l5eZozZ45GjRqlCy64wLsOubm5mjJlijp16qTdu3cHjN24caMk6Y033qjyPpo/f763rmAuu+wy75dfFTVt2lSS9O233wac3rdvX8XExGjr1q3q2bOn5s2bpz179lR5Xf33R0REhHr16hXyfP6foIe6/2JjY9W5c+cqL+s3v/mNJGn16tU6cOBAwLLnnntOknTTTTf9YHMdunbtGnJZqNu0PP8bsKqWlZWV6b///a9xCxFMnQxIkyZNvL9zzz1XnTt31m233abs7GwtWrQo6Ds9SUpJSQl6+rfffuv9EuZ///ufDhw4EPKvuLhYklRQUBCwjuuvv17x8fE6ceKE/vWvfwUs838qGTFiRNBPRqF88803kk59wgklJiZGjRo1Cjj/2XDrrbfq3nvvlc/n09KlSzV06FA1btxYbdq00Z133ukF2yolJUW33367Fi5cqE8++URHjhzRq6++qiuvvFKStGfPHt18880BY/yfME+cOFHlfeQPeMX7yK9+/fohtysi4tTcXP/97NeqVSvNnTtX8fHx2rRpk8aMGaPzzjtPKSkpGj58uF599dVKv9Dz3x+NGjWq8hdlzZs3Dzh/RQ0bNjzt4+aqq65S+/btVVxc7AVUknbt2uV9Wh47dmyV6zgTltu0vKoe4+WXnc3HOOpoQMpPJMzNzdWWLVs0d+7coB+hywsVlvJfSbz22mtyp/YtVflX8WevcXFxuv766yVJCxcu9E7/+OOP9eGHH0qq2ddXPwYzZ87Uzp079dhjj6l///5KSkrSrl279Le//U1dunTR+PHjz9plxcfHa9CgQXrrrbe8+3Hz5s0BP/3130+/+93vqnUfne1ji40YMUK5ubl69tlnNXz4cKWmpurgwYNatmyZhgwZol69egX96u1MhXrcVuT/FDJ37lwvZv7/7tixI/NqUEmdDMjZ1rBhQ+9dUqivParDH4js7Gzt379f0vefPjp16qQLL7ywRuvzf2LKy8sLeZ7CwkIdPnw44Px+/heeqo4pdbrvlFu3bq2JEycqKytLhw8f1qZNm7x9DLNmzdKqVatOez1qIiwsTGPGjPH+7f/KUJLOOeccSWd2H52pBg0aaOzYsVq6dKn27dunXbt26cEHH5TP59OGDRsC3lj4749Dhw6FnOshfX//hvqEXF0ZGRmqV6+edu/erezsbBUXF3tzH37ITx9ng//5crpl5W8j/3NWOrPHeF1GQM6CyMhIb0fm6tWrzevp06ePmjdvrrKyMr344ove/0rf7yOpiS5dukiStwM+mPXr13uz7i+77LKAZcnJyZJOHX8qmGPHjnk7easjLCxM3bp10/Lly3XuuedKkt58881qj6+u8j90KP/Vj38f1tq1a380B1ps1aqVHn/8cd16662SAm8P//1XUlKit956K+Q61q5dK6ny/VdTiYmJuuWWWySd2pnu3x8SGxurkSNHntG6f2jlf5QSallYWJguueQS73T/41sK/Rj/9NNPQx4osfzXghW/fqwrCMhZcscdd0iSsrKylJWVVeV5Q+0MDAsL04gRIySd+uTh/yQSHh7uvcDUhH8fwKZNm/TGG29UWl5SUqJHHnlEktSxY0d17NgxYLn/l0qhZow/9dRTId8ZV/WOOTw8XFFRUZJUo30627Ztq/Kdpl/5rwDLv2D8+te/VkREhA4dOuT9+iyUoqKikDPSLaq6PaRTO7qlwNvjoosuUvv27SVJjz76aNBfb2VlZXkzrf0v/mfC/zXWK6+8oieffFLSD7vz/Gx5++23g37lWFhYqD//+c+SpGuvvTbgesTFxXnHTAv1GP/jH/8Y8jITEhK8/66rR+NlHshpVPe33iUlJe6aa67xZsJOmzYtYFLY8ePHXXZ2ths3bpxLTEwMuZ5t27Z5l9elSxcnyfXv37/KbfSf/3QTCV944QVXVFTknHPu888/d4MGDfLGBptIOHfu3ICZwP5JcwcPHnQTJ04MmIVd8Tf+F198sbv77rtdTk6OO378uHf6/v373V133eWtN9i8g1CeeeYZFxUV5W666Sa3bNmygNnX3333nduwYYM38UySGzZsWKV1TJ061Vs+atQo9/HHH3vLiouL3datW93UqVNdamqq27BhQ8DY8jPRQyk/+7u8MWPGuBtvvNEtX748YA7QsWPH3N///ndvHsbEiRMDxpWfSDhkyBBvImFRUZFbvHixN+O6qomENZ3x75/L4v975513ajS+Iv96qpoHEmyZn/+526tXr0rLyk8kbNCggXv55Ze9iYTbt293ffr08eYMlZ8A7OefWBoZGelmz57tTWLct2+fu+2221x0dLR3tIBgc3+aNWvmJLm7777bu9y6hICcRk0mC+Xn53uzu/1/CQkJLikpKeBQFBEREVWup3PnzgHrWLJkSZXnDxUQ505NUvQfKsUft/KH3ggLC3OzZs0Kut6SkhLXu3dv77w+n88lJyc7n8/nfD6f+9Of/hTyBaD8BC//YUz8h2Lx/913331VXq+Knn322YDx0qlZ1snJyZVOT09Pd0ePHq20jrKyMvfwww8H3B+xsbGuYcOGAYczkeTefvvtgLFnEpDykxClU0c1qHgIlCuvvDIgtn4VD2WSlJTkBUeSu/DCCyvNYK9qW06n/BsHy8zziv4vAlL+UCbR0dEBh+vx+Xxuzpw5Qdd97Ngx1759+4Dng/9+iYyMdEuWLAk5kdC574+44L/c1NRUl5aW5oYPH17NW+enja+wzqKEhAStXr1aWVlZGj58uM4991ydPHlSBQUFatasmdLT0/X4448H7NgNpvz+joSEhNMeOLAqzZo10+bNmzVjxgx169ZNsbGxKigoUGpqqkaNGqUtW7bonnvuCTo2PDxc//73vzV16lS1a9dOUVFR8vl8Sk9P15tvvqn7778/5OUuXbpUU6dOVd++fdWyZUsVFRWpuLhYaWlpGj58uNatW6cZM2bU6LqMHTtWH374oaZPn67BgwerdevWCg8PV35+vurXr6/27dsrIyNDWVlZWrNmTdCfhvp8Pj3yyCP66KOPNG7cOG8OSX5+vneAwgceeEDvvPNOyHk/Fg8//LCefvppDR06VO3atVNERISOHz+ulJQU9evXT88//7zWr1+vuLi4SmPvu+8+bd68WSNHjlRqaqoKCgoUGxurbt266S9/+Yvef/99b67E2TBs2DBvnsuPfee5X3Jyst577z09+OCD3vOuQYMGGjhwoDZu3Kjbb7896Lj4+Hi9/fbbmjBhglq2bKmIiAhFRkbqhhtu0KZNmyr9FLyihx56SLNmzVKXLl0UGRmpvLw85ebm6uuvv/4hruaPjs+5Orr3B0BQK1as0LBhwxQbG6svv/zyR73/o0WLFsrNzdX8+fPP6lGCUT18AgEQ4JlnnpF0aqf8jzkeqH0EBIBnzpw5euuttxQWFqYJEybU9ubgR65O/n+iA/jeu+++q5tvvln5+fnez1HHjRunDh061O6G4UePgAB1XGFhoXJzcxUeHq7zzjtPmZmZeuihh2p7s/ATwE50AIAJ+0AAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABgQkAAACYEBABg8v8Ai13zey5VJ+cAAAAASUVORK5CYII=", "_dom_classes": [], "_figure_label": "Figure", "_image_mode": "diff", "_message": "", "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "MPLCanvasModel", "_rubberband_height": 0, "_rubberband_width": 0, "_rubberband_x": 0, "_rubberband_y": 0, "_size": [400.0, 400.0], "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "MPLCanvasView", "capture_scroll": false, "footer_visible": false, "header_visible": false, "layout": "IPY_MODEL_0f4197c841e74e01bc01c9129bef4ca9", "pan_zoom_throttle": 33.0, "resizable": false, "toolbar": "IPY_MODEL_8f699841d13e4233a32c691628ce53d6", "toolbar_position": "left", "toolbar_visible": false}}, "5947e2fb372c425f93e279d44aa505c8": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8f699841d13e4233a32c691628ce53d6": {"model_name": "ToolbarModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_current_action": "", "_dom_classes": [], "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "ToolbarModel", "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "ToolbarView", "button_style": "", "collapsed": true, "layout": "IPY_MODEL_5947e2fb372c425f93e279d44aa505c8", "orientation": "vertical", "toolitems": [["Home", "Reset original view", "home", "home"], ["Back", "Back to previous view", "arrow-left", "back"], ["Forward", "Forward to next view", "arrow-right", "forward"], ["Pan", "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect", "arrows", "pan"], ["Zoom", "Zoom to rectangle\nx/y fixes axis", "square-o", "zoom"], ["Download", "Download plot", "floppy-o", "save_figure"]]}}, "10898feaa0ce45b6a83fdcda66183b4a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6a30f5cc93d446e4841d6b0bb1eda3d3": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_5af3c7fc309a4412bde665fd413ee67e", "IPY_MODEL_57809f21167f458492843b106688a9f8"], "layout": "IPY_MODEL_10898feaa0ce45b6a83fdcda66183b4a"}}, "fbd6081c92da436a9e3f658331893051": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "4d3be602a883428e8c913d7e64e021a0": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_d19c45ba7dca4a03a715dbf6dad6036e", "IPY_MODEL_6a30f5cc93d446e4841d6b0bb1eda3d3"], "layout": "IPY_MODEL_fbd6081c92da436a9e3f658331893051"}}, "3f0847b2ac7f46e1b1086a82c576ee8c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b2871234fe654509aa4023b5392f3442": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_d32344790d084aa88f9afd680c2abb5d", "IPY_MODEL_dfa91b31d4df4932bd6d97c95035a8ec"], "layout": "IPY_MODEL_3f0847b2ac7f46e1b1086a82c576ee8c"}}, "6bd7ef0ff0c7499b8d7c3ba8698df83a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "caebc221c70a4b6d9d7328323993add4": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_b2871234fe654509aa4023b5392f3442", "IPY_MODEL_4d3be602a883428e8c913d7e64e021a0", "IPY_MODEL_7a5e198c33de4c0f859b824ee531e393"], "layout": "IPY_MODEL_6bd7ef0ff0c7499b8d7c3ba8698df83a"}}, "ffdbefa90c374f56b582030eeb2e0907": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7b82334797bd454db215da66df2295c7": {"model_name": "MPLCanvasModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_cursor": "default", "_data_url": "data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZAAAAGQCAYAAACAvzbMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZ3UlEQVR4nO3df2xV9eH/8ddllUML7S0bodRygbq6IZFfAjOIKT8EphlQCIslglslTjaYeLc4l/oPMIFbfwyB8cMYFkBxo2YRambmooCQNYS1m4hLyNQB49IKCLS9VNoLpefzxzeSb3cLXN72fU5v+3wk9w/Pve15pVGennvpvQHXdV0BAHCLevg9AACQmggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBECAgAwQkAAAEYICADACAEBABghIAAAIwQEAGCEgAAAjBAQAIARAgIAMEJAAABGCAgAwAgBAQAYISAAACMEBABgJOUC4rquYrGYXNf1ewoAdGtpfg+4VRcvXlQwGNQjjzyinj17+j3nhoYMGeL3hJvatm2b3xOSUl9f7/eEm9q9e7ffE5IyatQovyckZdKkSX5PuKnDhw/7PSEptv6HO+WuQAAAnQMBAQAYISAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBFPAnL16lWVlZWpoKBAjuOooKBAZWVlunr1qhenBwBY4MnngTz55JPavHmzHnvsMd13332qrKxUaWmpotGoNm7c6MUEAEAHsx6Qjz/+WK+88oqWLl2qdevWSZIef/xxZWVl6Xe/+51++tOfavjw4bZnAAA6mPWnsHbu3CnXdRUOh9scD4fDcl1X5eXlticAACywHpDq6mrl5OQoPz+/zfH8/Hz1799f1dXVticAACyw/hRWbW2t8vLy2r0vLy9PNTU17d4Xj8cVj8cTjsdisQ7dBwAwY/0K5NKlS3Icp937evXqpaampnbvi0QiCgaDCbdQKGRzLgAgSdYDkpGR0e6VhCQ1NzcrPT293ftKS0vV0NCQcItGozbnAgCSZP0prNtvv10fffRRu/fV1NRo9OjR7d7nOM51r1wAAP6zfgUyZswYnTlzRsePH29z/Pjx4zp79qzGjBljewIAwALrASkuLlYgENDatWvbHF+7dq0CgYCKi4ttTwAAWGD9KayRI0fqiSee0Pr163Xx4kVNmDBBlZWV2rp1qxYtWqQRI0bYngAAsMCTtzLZsGGDBg0apC1btuiNN95QXl6eVq1apWeeecaL0wMALPAkIGlpaXr22Wf17LPPenE6AIAHeDt3AIARAgIAMEJAAABGCAgAwAgBAQAYISAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMCIJ2+maMMf/vAHvyd0CUVFRX5PSEp+fr7fE25qyJAhfk9ISjgc9ntCUg4fPuz3BNwEVyAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBECAgAwQkAAAEasB6SxsVHLly/XzJkzlZubq0AgoJKSEtunBQBYZj0g586d04oVK/SPf/xDY8eOtX06AIBHrH+gVG5urk6dOqW8vDy1tLTotttus31KAIAHrF+BOI6jvLw826cBAHiMF9EBAEY67Weix+NxxePxhOOxWMyHNQCA/9Vpr0AikYiCwWDCLRQK+T0NAKBOHJDS0lI1NDQk3KLRqN/TAADqxE9hOY4jx3H8ngEAuI5OewUCAOjcCAgAwIgnT2Ft2LBB9fX1am1tlSQdOXJEK1eulCTNmjVLI0aM8GIGAKADeRKQl156Sf/973+v/fOHH36oDz/8UJI0cOBAAgIAKciTgJw4ccKL0wAAPMRrIAAAIwQEAGCEgAAAjBAQAIARAgIAMEJAAABGCAgAwAgBAQAYISAAACMEBABghIAAAIwQEACAkYDruq7fI25FLBZTMBjUoEGD1KNH5+4fbyIJmAuHw35PuKlt27b5PSEpdXV1Vr5v5/4TGADQaREQAIARAgIAMEJAAABGCAgAwAgBAQAYISAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAj1gNSXV2tcDisESNGKDMzUwMGDNADDzyg999/3/apAQAWWQ9IWVmZ3njjDd1333367W9/q2eeeUZnz57VtGnTtHnzZtunBwBYYv0TCSsrKzV27Fg5jnPtWFNTk0aNGqUvvvhCZ8+eVVpaWtLfj08kBLoHPpGw46TsJxJOmDChTTwkKT09XTNmzFBdXZ1Onz5tewIAwALf/he+trZWaWlpys7O9msCAOBrSP65ow509OhRvfXWW5o1a5b69OnT7mPi8bji8XjC8VgsZnseACAJnl+BNDQ0aO7cuUpPT9eaNWuu+7hIJKJgMJhwC4VCHq4FAFyPpwFpamrSzJkzdezYMe3atUuDBw++7mNLS0vV0NCQcItGox4uBgBcj2dPYV2+fFlz5szRwYMH9ac//UmTJ0++4eMdx0l48R0A0Hl4EpCWlhY9/PDDeu+99/Taa6+pqKjIi9MCACyyHpDW1lYtWLBAFRUVeuWVVzR//nzbpwQAeMB6QJ5++mmVl5ersLBQvXv31o4dO9rcP23aNOXk5NieAQDoYNYD8s9//lOSdODAAR04cCDh/n379hEQAEhB1gPywQcf2D4FAMAHnfvNpAAAnRYBAQAYISAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAjBAQAYMSzTyTsaD/4wQ86/ScWhsNhvyfc1OzZs/2ekJSSkhK/J9zUvn37/J6QlL59+/o9ocvo7m8WyxUIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBECAgAwQkAAAEYICADACAEBABghIAAAIwQEAGCEgAAAjBAQAIAR6wE5evSo5s2bpzvvvFN9+vRRVlaWRo8erfXr1+vy5cu2Tw8AsMT654FEo1FduHBB8+bN08CBA3X16lVVVlYqHA5r79692r17t+0JAAALrAdk+vTpmj59eptjixcvVt++fbVx40b9+9//1ne/+13bMwAAHcy310CGDBkiSaqvr/drAgDga/DsI20vXbqkS5cu6csvv9Tf//53vfDCC8rNzdWIESO8mgAA6ECeBeSFF17QihUrrv3zuHHj9Oqrryo9Pb3dx8fjccXj8YTjsVjM2kYAQPI8C8iPfvQj3X///Tp//rz27t2rf/3rXzd8+ioSibQJDgCgc/EsIHfccYfuuOMOSVJxcbFefvllTZ8+XR999JHuuuuuhMeXlpbql7/8ZcLxWCymUChkfS8A4MZ8exH9kUce0ZUrV7Rjx45273ccR1lZWe3eAAD+8y0gTU1NkqS6ujq/JgAAvgbrATl79my7xzdt2iRJuvfee21PAABYYP01kEWLFun8+fOaNGmSQqGQ6uvr9de//lV79uzR/fffr/nz59ueAACwwHpA5s2bp23btun3v/+9vvjiCzmOo6FDh+rFF1/Uk08+qbQ0z17HBwB0IOt/ehcXF6u4uNj2aQAAHuPt3AEARggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBECAgAwQkAAAEYICADACAEBABghIAAAIwHXdV2/R9yKWCymYDDo94ykZGdn+z3hpk6cOOH3hKSsXbvW7wk3tXz5cr8nJGXbtm1+T0jKkCFD/J5wU7t37/Z7QlJefvllK9+XKxAAgBECAgAwQkAAAEYICADACAEBABghIAAAIwQEAGCEgAAAjBAQAIARAgIAMEJAAABGCAgAwAgBAQAYISAAACMEBABgxJeA7N27V4FAQIFAQJ999pkfEwAAX5PnAbly5YqWLFmi3r17e31qAEAH8jwgL730ki5cuKCf/OQnXp8aANCBPA3IyZMntXLlSpWVlaXMx9ICANrnaUCeeuopDR8+XCUlJV6eFgBgQZpXJ3rnnXf09ttv69ChQwoEAjd9fDweVzweTzgei8VszAMA3CJPrkCam5u1dOlSLVy4UGPHjk3qayKRiILBYMItFApZXgsASIYnAYlEIqqrq1MkEkn6a0pLS9XQ0JBwi0ajFpcCAJJl/Smszz//XM8//7x+8YtfqLGxUY2NjZKk+vp6SVJNTY169uypQYMGtfk6x3HkOI7teQAAQ9YDcubMGcXjcZWVlamsrCzh/kmTJql3797XwgIASA3WA5Kfn69du3YlHN+5c6fKy8u1efNmDRw40PYMAEAHsx6QYDCo2bNnJxw/fPiwJGnq1KkqKCiwPQMA0MF4M0UAgBHfArJ8+XK5rsvVBwCkKK5AAABGCAgAwAgBAQAYISAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAjAdd1Xb9H3IpYLKZgMOj3DCBBdna23xOS8tWngXZ2X33kQ2c2cuRIvyf4iisQAIARAgIAMEJAAABGCAgAwAgBAQAYISAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAj1gNy4sQJBQKBdm+PP/647dMDACxJ8+pERUVF+uEPf9jmWEFBgVenBwB0MM8Ccvfdd2vBggVenQ4AYJmnr4E0NTWpqanJy1MCACzxLCDr1q1TRkaGMjIydOedd2rTpk1enRoAYIH1p7B69OihBx54QHPmzNGgQYNUW1urV199VUuWLNHx48f14osvtvt18Xhc8Xg84XgsFrM9GQCQhIDruq7XJ7169aomTpyogwcP6pNPPtG3v/3thMcsX75cK1as8HoaYCw7O9vvCUmpr6/3e0JSDh8+7PeEmxo5cqTfE3zly++BfOMb39Cvf/1rtba2as+ePe0+prS0VA0NDQm3aDTq8VoAQHs8+1tY/2vw4MGSpHPnzrV7v+M4chzHy0kAgFvg22+if/bZZ5KknJwcvyYAAL4G6wE5e/ZswrGmpiatXLlSt912m6ZPn257AgDAAutPYS1atEjnz5/XlClTNHDgQNXW1mr79u06duyYIpGIQqGQ7QkAAAusB2TGjBnavn27Nm/erAsXLqhPnz6655579PLLL2vWrFm2Tw8AsMSXv8b7dcRiMQWDQb9nAAn4a7wdi7/G2/nxdu4AACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBECAgAw4ttH2qJzKCoq8ntCUioqKvyecFOp8i63u3fv9ntCUkaNGuX3hJsqKSnxe0JStm7dauX7cgUCADBCQAAARggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBECAgAwQkAAAEYICADACAEBABghIAAAIwQEAGDEs4CcPn1aS5Ys0eDBg+U4jnJzczVz5kydPHnSqwkAgA7kyeeBfPrppyosLJTjOFq4cKFCoZDOnz+vQ4cOqa6uToMGDfJiBgCgA1kPiOu6mj9/vgYMGKADBw4oMzPT9ikBAB6wHpB9+/apqqpKb7/9tjIzM9Xc3KwePXqoZ8+etk8NALDI+msg7777riQpOztbhYWFSk9PV69evTR+/HgdPHjQ9ukBAJZYD8gnn3wiSZo7d6769u2r8vJybdy4USdPntSUKVP08ccft/t18XhcsVis3RsAwH/Wn8JqbGyUJA0bNkwVFRXXjk+ePFl33323nnvuOb355psJXxeJRLRixQrb8wAAhqxfgaSnp0uSHn300TbHhw4dqnvvvVf79+9v9+tKS0vV0NCQcItGo7YnAwCSYP0KJC8vT5KUk5OTcF9ubq6qqqra/TrHceQ4jtVtAABz1q9Axo0bJ0k6depUwn3RaFT9+/e3PQEAYIH1gBQVFSkjI0NbtmxRS0vLteNVVVWqqqrSgw8+aHsCAMAC609h9evXT6tXr1Y4HNbEiRM1b948nTt3TuvWrVO/fv20bNky2xMAABZ48lYmTz31lL71rW9pzZo1+tWvfqWMjAx9//vfVyQSUSgU8mICAKCDeRIQSVqwYIEWLFjg1ekAAJbxdu4AACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgJGA67qu3yNuRSwWUzAYVENDg7Kysvyek/ICgYDfE7qMSZMm+T0hKR988IHfE5Kydu1avyfcVDgc9ntCUmz9Mc8VCADACAEBABghIAAAIwQEAGCEgAAAjBAQAIARAgIAMEJAAABGCAgAwAgBAQAYISAAACMEBABghIAAAIwQEACAEesBKSkpUSAQuO5t1apVticAACxIs32CRYsWaerUqQnH161bp+rqaj300EO2JwAALLAekPHjx2v8+PFtjl26dEmLFy/W8OHDdc8999ieAACwwJfXQHbt2qWLFy/qxz/+sR+nBwB0AF8Csn37dqWlpWnBggV+nB4A0AGsP4X1v2pqarRnzx499NBDysnJue7j4vG44vF4wvFYLGZzHgAgSZ5fgbz++utqbW1VSUnJDR8XiUQUDAYTbqFQyJuhAIAb8jwgr732mr75zW9q5syZN3xcaWmpGhoaEm7RaNSjpQCAG/H0KayqqiodPXpUixcvluM4N3ys4zg3fQwAwD+eXoFs375dkvjbVwDQBXgWkMuXL+uPf/yj7rrrLn3ve9/z6rQAAEs8C8if//xnXbhwgasPAOgiPAvI9u3b1aNHDz366KNenRIAYJFnL6JXVFR4dSoAgAd4O3cAgBECAgAwQkAAAEYICADACAEBABghIAAAIwQEAGCEgAAAjBAQAIARAgIAMEJAAABGCAgAwIinn0jYEVzXlSTFYjGflwBttbS0+D2hS2lubvZ7Qpfhuq4CgUCHf9+A+9WfyCni1KlTCoVCfs8AgJTR0NCgrKysDv++KReQ1tZW1dbWKjMzs8OKGovFFAqFFI1GrfyQuxN+lh2Hn2XH6e4/y4788/L/l3JPYfXo0UMDBw608r2zsrK65b9cNvCz7Dj8LDsOP8uOxYvoAAAjBAQAYISAAACMEBAAgBECIslxHC1btkyO4/g9JeXxs+w4/Cw7Dj9LO1Lur/ECADoHrkAAAEYICADACAEBABghIAAAI906IFevXlVZWZkKCgrkOI4KCgpUVlamq1ev+j0tpVRXVyscDmvEiBHKzMzUgAED9MADD+j999/3e1qXsHfvXgUCAQUCAX322Wd+z0k5p0+f1pIlSzR48GA5jqPc3FzNnDlTJ0+e9Htayku598LqSE8++aQ2b96sxx57TPfdd58qKytVWlqqaDSqjRs3+j0vZZSVlWn//v2aO3eufv7zn6uxsVFbt27VtGnTtGnTJv3sZz/ze2LKunLlipYsWaLevXvryy+/9HtOyvn0009VWFgox3G0cOFChUIhnT9/XocOHVJdXZ0GDRrk98TU5nZTR44ccQOBgLt06dI2x5cuXeoGAgH3yJEjPi1LPX/729/c5ubmNscuXbrkfuc733H79u3rXrlyxadlqW/16tVu//793XA47EpyP/30U78npYzW1lZ33Lhx7qhRo9xYLOb3nC6p2z6FtXPnTrmuq3A43OZ4OByW67oqLy/3Z1gKmjBhQsIvaKWnp2vGjBmqq6vT6dOnfVqW2k6ePKmVK1eqrKxMwWDQ7zkpZ9++faqqqtJvfvMbZWZmqrm5WZcvX/Z7VpfSbQNSXV2tnJwc5efntzmen5+v/v37q7q62qdlXUdtba3S0tKUnZ3t95SU9NRTT2n48OEqKSnxe0pKevfddyVJ2dnZKiwsVHp6unr16qXx48fr4MGDPq/rGrptQGpra5WXl9fufXl5eaqpqfF4Uddy9OhRvfXWW5o1a5b69Onj95yU88477+jtt9/Whg0brHwQUHfwySefSJLmzp2rvn37qry8XBs3btTJkyc1ZcoUffzxxz4vTH3d9kX0S5cuKTMzs937evXqxWeufw0NDQ2aO3eu0tPTtWbNGr/npJzm5mYtXbpUCxcu1NixY/2ek7IaGxslScOGDVNFRcW145MnT9bdd9+t5557Tm+++aZf87qEbhuQjIwMxePxdu9rbm5Wenq6x4u6hqamJs2cOVPHjh3TX/7yFw0ePNjvSSknEomorq5OkUjE7ykp7av/hh999NE2x4cOHap7771X+/fv92NWl9Jtn8K6/fbbr/s0VU1NzXWf3sL1Xb58WXPmzNHBgwdVXl6uyZMn+z0p5Xz++ed6/vnntWjRIjU2NurEiRM6ceKE6uvrJf2/fzf5/YXkfPXfcE5OTsJ9ubm5qqur83pSl9NtAzJmzBidOXNGx48fb3P8+PHjOnv2rMaMGePTstTU0tKihx9+WO+99562bdumoqIivyelpDNnzigej6usrEz5+fnXbuvWrZMkTZo0ScOGDfN5ZWoYN26cJOnUqVMJ90WjUfXv39/rSV1Otw1IcXGxAoGA1q5d2+b42rVrFQgEVFxc7M+wFNTa2qoFCxaooqJCmzZt0vz58/2elLLy8/O1a9euhNtX/z5u3rxZO3fu9HllaigqKlJGRoa2bNmilpaWa8erqqpUVVWlBx980Md1XUO3fQ1k5MiReuKJJ7R+/XpdvHhREyZMUGVlpbZu3apFixZpxIgRfk9MGU8//bTKy8tVWFio3r17a8eOHW3unzZtWrtPIyBRMBjU7NmzE44fPnxYkjR16lQVFBR4OypF9evXT6tXr1Y4HNbEiRM1b948nTt3TuvWrVO/fv20bNkyvyemPr9/k9FPV65ccVetWuXm5+e7PXv2dPPz891Vq1bxm9O3aOLEia6k69727dvn98SUt2zZMn4T3dDrr7/ujh492nUcx+3bt6/78MMPu//5z3/8ntUl8ImEAAAj3fY1EADA10NAAABGCAgAwAgBAQAYISAAACMEBABghIAAAIwQEACAEQICADBCQAAARggIAMAIAQEAGCEgAAAjBAQAYISAAACMEBAAgBECAgAwQkAAAEb+D+FHadfwuhuPAAAAAElFTkSuQmCC", "_dom_classes": [], "_figure_label": "Figure", "_image_mode": "full", "_message": "", "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "MPLCanvasModel", "_rubberband_height": 0, "_rubberband_width": 0, "_rubberband_x": 0, "_rubberband_y": 0, "_size": [400.0, 400.0], "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "MPLCanvasView", "capture_scroll": false, "footer_visible": false, "header_visible": false, "layout": "IPY_MODEL_ffdbefa90c374f56b582030eeb2e0907", "pan_zoom_throttle": 33.0, "resizable": false, "toolbar": "IPY_MODEL_aeaa449bc1904e2695f0e57fac90928d", "toolbar_position": "left", "toolbar_visible": false}}, "51a97e219a98400baf18cf13fce105f5": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "aeaa449bc1904e2695f0e57fac90928d": {"model_name": "ToolbarModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_current_action": "", "_dom_classes": [], "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "ToolbarModel", "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "ToolbarView", "button_style": "", "collapsed": true, "layout": "IPY_MODEL_51a97e219a98400baf18cf13fce105f5", "orientation": "vertical", "toolitems": [["Home", "Reset original view", "home", "home"], ["Back", "Back to previous view", "arrow-left", "back"], ["Forward", "Forward to next view", "arrow-right", "forward"], ["Pan", "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect", "arrows", "pan"], ["Zoom", "Zoom to rectangle\nx/y fixes axis", "square-o", "zoom"], ["Download", "Download plot", "floppy-o", "save_figure"]]}}, "37def4a1bbfc4479a63333af796c1f8b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "9ddc2a1a78b94b7a9c3adb5bc4bf0faa": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#aaffaa", "font_weight": ""}}, "2aa66c0076324c6383685d3f92a3b754": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["happy"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude42", "disabled": false, "icon": "", "layout": "IPY_MODEL_37def4a1bbfc4479a63333af796c1f8b", "style": "IPY_MODEL_9ddc2a1a78b94b7a9c3adb5bc4bf0faa", "tooltip": "happy"}}, "c6565a839ab549a3ae7fb15253bc533e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "3540cbdcc7804e11a6273fa526fc3366": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#dddd77", "font_weight": ""}}, "96bef24c19a6432b86b0aaf2aff6115b": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["medium"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude10", "disabled": false, "icon": "", "layout": "IPY_MODEL_c6565a839ab549a3ae7fb15253bc533e", "style": "IPY_MODEL_3540cbdcc7804e11a6273fa526fc3366", "tooltip": "medium"}}, "9042749212bd496f9f4e7e02c0b6da4a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "3b2f47bd5f0049edb98b8fca000728c4": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#ffaaaa", "font_weight": ""}}, "13a63ea4ce544aeeae320e75bf6317f0": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["sad"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude41", "disabled": false, "icon": "", "layout": "IPY_MODEL_9042749212bd496f9f4e7e02c0b6da4a", "style": "IPY_MODEL_3b2f47bd5f0049edb98b8fca000728c4", "tooltip": "sad"}}, "7758b0819ca545009c9b6812bd8c5102": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "e8638eca55e54e03ba43f9b96534a7e8": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "2666f386277b4a269fb9f52011061e9f": {"model_name": "TextareaModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TextareaModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TextareaView", "continuous_update": true, "description": "", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_7758b0819ca545009c9b6812bd8c5102", "placeholder": "We want your feedback!", "rows": null, "style": "IPY_MODEL_e8638eca55e54e03ba43f9b96534a7e8", "value": ""}}, "72d1766e6f914ad3885b42c1108a268b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "a7c0e1d6e61b47db87be11ad716d0bc5": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "4fcf9fb1e9bf4cd6b918db0910c1df65": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Submit", "disabled": false, "icon": "", "layout": "IPY_MODEL_72d1766e6f914ad3885b42c1108a268b", "style": "IPY_MODEL_a7c0e1d6e61b47db87be11ad716d0bc5", "tooltip": ""}}, "3d86d728d9f0422eb6c1ed01a8da7878": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1c68dd95f94c4f48a259ba3c184466d7": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_2666f386277b4a269fb9f52011061e9f", "IPY_MODEL_4fcf9fb1e9bf4cd6b918db0910c1df65"], "layout": "IPY_MODEL_3d86d728d9f0422eb6c1ed01a8da7878"}}, "1f21c5f1157a41dcace438a82a458b1c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "21782ef503b64e2d984ddaa110b357ff": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "b737411f1bd74484b38bde7811c68c10": {"model_name": "LabelModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "LabelModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "LabelView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_1f21c5f1157a41dcace438a82a458b1c", "placeholder": "\u200b", "style": "IPY_MODEL_21782ef503b64e2d984ddaa110b357ff", "value": "Thanks for your feedback!"}}, "32d6d360c30c41c6a1b1b5e691a36f6d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "43008818716d49379d81c4094d863bca": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_2aa66c0076324c6383685d3f92a3b754", "IPY_MODEL_96bef24c19a6432b86b0aaf2aff6115b", "IPY_MODEL_13a63ea4ce544aeeae320e75bf6317f0"], "layout": "IPY_MODEL_32d6d360c30c41c6a1b1b5e691a36f6d"}}, "82df4f9ab0064bbfbdf366db9d483a98": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d982fac8fa1f425784d0b7e1fe0b6d68": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_43008818716d49379d81c4094d863bca", "IPY_MODEL_1c68dd95f94c4f48a259ba3c184466d7", "IPY_MODEL_b737411f1bd74484b38bde7811c68c10"], "layout": "IPY_MODEL_82df4f9ab0064bbfbdf366db9d483a98"}}, "4a54d86d2f424f2d92d7e561df25799a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "74824eea8a724b199395e27a816e9e17": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_d982fac8fa1f425784d0b7e1fe0b6d68"], "layout": "IPY_MODEL_4a54d86d2f424f2d92d7e561df25799a"}}, "6f8b52ea6c7a41efbec52b5f744ac495": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "397c1b23845c48568c315c61b6174040": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#aaffaa", "font_weight": ""}}, "e267c9a8f7e24d98b764ff499a71558a": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["happy"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude42", "disabled": false, "icon": "", "layout": "IPY_MODEL_6f8b52ea6c7a41efbec52b5f744ac495", "style": "IPY_MODEL_397c1b23845c48568c315c61b6174040", "tooltip": "happy"}}, "8f985dbe3fc6496698f8f7385f08c62e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "13754f4a1efd4f009eaab0d9c4a9bacc": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#dddd77", "font_weight": ""}}, "69e0d6333f6642b5a53dbdaa8fb704ba": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["medium"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude10", "disabled": false, "icon": "", "layout": "IPY_MODEL_8f985dbe3fc6496698f8f7385f08c62e", "style": "IPY_MODEL_13754f4a1efd4f009eaab0d9c4a9bacc", "tooltip": "medium"}}, "e4a203184be2462597290b613e02a791": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "3bc92ea611fd4b98a4e68a3f1d5546e1": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#ffaaaa", "font_weight": ""}}, "50039a17e1f64c3ea70f5f3a471f3a85": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["sad"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude41", "disabled": false, "icon": "", "layout": "IPY_MODEL_e4a203184be2462597290b613e02a791", "style": "IPY_MODEL_3bc92ea611fd4b98a4e68a3f1d5546e1", "tooltip": "sad"}}, "82fa0b9256f041bb95d05314ad2e1b63": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "cb72d70ed3b54d9189e177056e697620": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "188cae4c534a4df3a6a616150f197946": {"model_name": "TextareaModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TextareaModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TextareaView", "continuous_update": true, "description": "", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_82fa0b9256f041bb95d05314ad2e1b63", "placeholder": "We want your feedback!", "rows": null, "style": "IPY_MODEL_cb72d70ed3b54d9189e177056e697620", "value": ""}}, "de70424f708d4bb2896db3f5e7146265": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "9ed13169bb7f44d590a6c3825d118a78": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "ebbfa4f1a0824f05b882d03d162e38d2": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Submit", "disabled": false, "icon": "", "layout": "IPY_MODEL_de70424f708d4bb2896db3f5e7146265", "style": "IPY_MODEL_9ed13169bb7f44d590a6c3825d118a78", "tooltip": ""}}, "2090f05dea7c4c89a2053b56c9e4ea2d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "53e897f703414d9e9c62504407f245b6": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_188cae4c534a4df3a6a616150f197946", "IPY_MODEL_ebbfa4f1a0824f05b882d03d162e38d2"], "layout": "IPY_MODEL_2090f05dea7c4c89a2053b56c9e4ea2d"}}, "475b6cf66c6346d4bace1cf3c7d22ed5": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9ad0b688fcf34025ab6727ee6f247956": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "8a59dbdb37ec4ac2a67b389cc9bf33fd": {"model_name": "LabelModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "LabelModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "LabelView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_475b6cf66c6346d4bace1cf3c7d22ed5", "placeholder": "\u200b", "style": "IPY_MODEL_9ad0b688fcf34025ab6727ee6f247956", "value": "Thanks for your feedback!"}}, "61286df6f9a64260abe4e849877e6f03": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "138034632d1d41f7af5e68661a642b8b": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e267c9a8f7e24d98b764ff499a71558a", "IPY_MODEL_69e0d6333f6642b5a53dbdaa8fb704ba", "IPY_MODEL_50039a17e1f64c3ea70f5f3a471f3a85"], "layout": "IPY_MODEL_61286df6f9a64260abe4e849877e6f03"}}, "48fda2cc17ad441cace857751d65a947": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1281c0a203ea4222ba02ab360b8f60e0": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_138034632d1d41f7af5e68661a642b8b", "IPY_MODEL_53e897f703414d9e9c62504407f245b6", "IPY_MODEL_8a59dbdb37ec4ac2a67b389cc9bf33fd"], "layout": "IPY_MODEL_48fda2cc17ad441cace857751d65a947"}}, "3cd64f3e9d4b4878aa0e933dedb2741f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "21f70a87638e48d2be56dcbfbfa46519": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_1281c0a203ea4222ba02ab360b8f60e0"], "layout": "IPY_MODEL_3cd64f3e9d4b4878aa0e933dedb2741f"}}, "d0e4d1313ef6433fae073297dcd90d7c": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "ee3c8db6a43f4a4bb900cd75cbf1a3ff": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "35118fe5cace484f961f6fffe905864c": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Lure", "disabled": false, "icon": "", "layout": "IPY_MODEL_ee3c8db6a43f4a4bb900cd75cbf1a3ff", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "74f13fb947df4ca7b781711fc35d776b": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "496f9afccae046b4b61df9e69421e9c4": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Chase", "disabled": false, "icon": "", "layout": "IPY_MODEL_74f13fb947df4ca7b781711fc35d776b", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "ba922d1bd6af452f91d8082db4e48976": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2280641d5e1a4c7a98f1f26881bfdf3f": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Stalk", "disabled": false, "icon": "", "layout": "IPY_MODEL_ba922d1bd6af452f91d8082db4e48976", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "3cce0078e016497092b6d448a2639dab": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3094871247bc4686b17cf66b6079c121": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Snare", "disabled": false, "icon": "", "layout": "IPY_MODEL_3cce0078e016497092b6d448a2639dab", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "a9af2c6834fc491fa63fd652d2c6aea6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "48e4419598f24ee791aee582a7b016a3": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Pounce", "disabled": false, "icon": "", "layout": "IPY_MODEL_a9af2c6834fc491fa63fd652d2c6aea6", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "9d70285960ad45728a9a06c697a4467a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f151daaf8f6d4468adcb8b32aeb36f7e": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Strike", "disabled": false, "icon": "", "layout": "IPY_MODEL_9d70285960ad45728a9a06c697a4467a", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "d65e1870e0e14483877d30c04570dee6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8013036da0e649a991d93f0d2a41db55": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Slash", "disabled": false, "icon": "", "layout": "IPY_MODEL_d65e1870e0e14483877d30c04570dee6", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "c7d06ba43572499fad2b00510faf7a90": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8c39fb9e57cc4f2990d977269bbdc28c": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Crush", "disabled": false, "icon": "", "layout": "IPY_MODEL_c7d06ba43572499fad2b00510faf7a90", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "f08c48b7504845f6b90413dc62d8e660": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "96a03a2e1583475a96178e686f8e3e0c": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Pin", "disabled": false, "icon": "", "layout": "IPY_MODEL_f08c48b7504845f6b90413dc62d8e660", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "1132d5e238354d33a8c21eae4392365f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1fba440cdc6b4c5ba15f584360c99f6f": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Bite", "disabled": false, "icon": "", "layout": "IPY_MODEL_1132d5e238354d33a8c21eae4392365f", "style": "IPY_MODEL_d0e4d1313ef6433fae073297dcd90d7c", "tooltip": ""}}, "e4b071067f394f24abd1d47b9089f446": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "16bf20d965904bd88002a886c727d464": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_e4b071067f394f24abd1d47b9089f446", "msg_id": "", "outputs": []}}, "f946a7e7b9334a8995b5a1f6566f568f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3f20b18a48d141e6bd4c1c243464a024": {"model_name": "OutputModel", "model_module": "@jupyter-widgets/output", "model_module_version": "1.0.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/output", "_model_module_version": "1.0.0", "_model_name": "OutputModel", "_view_count": null, "_view_module": "@jupyter-widgets/output", "_view_module_version": "1.0.0", "_view_name": "OutputView", "layout": "IPY_MODEL_f946a7e7b9334a8995b5a1f6566f568f", "msg_id": "", "outputs": []}}, "b9a1946cce80486f955d810a87df7656": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "34df01a63f4349dc9920347d75892d59": {"model_name": "MPLCanvasModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_cursor": "default", "_data_url": "data:image/png;base64,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", "_dom_classes": [], "_figure_label": "Figure", "_image_mode": "diff", "_message": "", "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "MPLCanvasModel", "_rubberband_height": 0, "_rubberband_width": 0, "_rubberband_x": 0, "_rubberband_y": 0, "_size": [400.0, 400.0], "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "MPLCanvasView", "capture_scroll": false, "footer_visible": false, "header_visible": false, "layout": "IPY_MODEL_b9a1946cce80486f955d810a87df7656", "pan_zoom_throttle": 33.0, "resizable": false, "toolbar": "IPY_MODEL_1e33e72f406a4a5bb392ff6cf4568440", "toolbar_position": "left", "toolbar_visible": false}}, "3fa52b0d14e74c8aa6cc3f00b2e9cb25": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1e33e72f406a4a5bb392ff6cf4568440": {"model_name": "ToolbarModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_current_action": "", "_dom_classes": [], "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "ToolbarModel", "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "ToolbarView", "button_style": "", "collapsed": true, "layout": "IPY_MODEL_3fa52b0d14e74c8aa6cc3f00b2e9cb25", "orientation": "vertical", "toolitems": [["Home", "Reset original view", "home", "home"], ["Back", "Back to previous view", "arrow-left", "back"], ["Forward", "Forward to next view", "arrow-right", "forward"], ["Pan", "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect", "arrows", "pan"], ["Zoom", "Zoom to rectangle\nx/y fixes axis", "square-o", "zoom"], ["Download", "Download plot", "floppy-o", "save_figure"]]}}, "841dee140b464e238e62426a1dd00043": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fe538c0b7f244432ada444db427dcba4": {"model_name": "MPLCanvasModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_cursor": "default", "_data_url": "data:image/png;base64,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", "_dom_classes": [], "_figure_label": "Figure", "_image_mode": "diff", "_message": "", "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "MPLCanvasModel", "_rubberband_height": 0, "_rubberband_width": 0, "_rubberband_x": 0, "_rubberband_y": 0, "_size": [400.0, 400.0], "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "MPLCanvasView", "capture_scroll": false, "footer_visible": false, "header_visible": false, "layout": "IPY_MODEL_841dee140b464e238e62426a1dd00043", "pan_zoom_throttle": 33.0, "resizable": false, "toolbar": "IPY_MODEL_3e5dbd0ab69c490cb79e38cd101d98b2", "toolbar_position": "left", "toolbar_visible": false}}, "6a4fcbc5d2ca4e6b9586efead451be07": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3e5dbd0ab69c490cb79e38cd101d98b2": {"model_name": "ToolbarModel", "model_module": "jupyter-matplotlib", "model_module_version": "^0.11", "state": {"_current_action": "", "_dom_classes": [], "_model_module": "jupyter-matplotlib", "_model_module_version": "^0.11", "_model_name": "ToolbarModel", "_view_count": null, "_view_module": "jupyter-matplotlib", "_view_module_version": "^0.11", "_view_name": "ToolbarView", "button_style": "", "collapsed": true, "layout": "IPY_MODEL_6a4fcbc5d2ca4e6b9586efead451be07", "orientation": "vertical", "toolitems": [["Home", "Reset original view", "home", "home"], ["Back", "Back to previous view", "arrow-left", "back"], ["Forward", "Forward to next view", "arrow-right", "forward"], ["Pan", "Left button pans, Right button zooms\nx/y fixes axis, CTRL fixes aspect", "arrows", "pan"], ["Zoom", "Zoom to rectangle\nx/y fixes axis", "square-o", "zoom"], ["Download", "Download plot", "floppy-o", "save_figure"]]}}, "c87593c9c146477fb988f4b4db1c6b32": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2e2159933a304541bac34f659131d0a5": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_35118fe5cace484f961f6fffe905864c", "IPY_MODEL_496f9afccae046b4b61df9e69421e9c4", "IPY_MODEL_2280641d5e1a4c7a98f1f26881bfdf3f"], "layout": "IPY_MODEL_c87593c9c146477fb988f4b4db1c6b32"}}, "e570bff7c51e4298b628056484fc1d4a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7008c7f72fe74d0dbe5a32eaf87c21a3": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_3094871247bc4686b17cf66b6079c121", "IPY_MODEL_48e4419598f24ee791aee582a7b016a3", "IPY_MODEL_f151daaf8f6d4468adcb8b32aeb36f7e"], "layout": "IPY_MODEL_e570bff7c51e4298b628056484fc1d4a"}}, "fe540983469a478181661d95fafeb8e4": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f08a25f151454b15b4df97a222370fbe": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_8013036da0e649a991d93f0d2a41db55", "IPY_MODEL_8c39fb9e57cc4f2990d977269bbdc28c", "IPY_MODEL_96a03a2e1583475a96178e686f8e3e0c", "IPY_MODEL_1fba440cdc6b4c5ba15f584360c99f6f"], "layout": "IPY_MODEL_fe540983469a478181661d95fafeb8e4"}}, "ef803b434e824a0ab6c30e0c03026f1e": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "6acc9027977b444c8c4db8241c1e1b8d": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_34df01a63f4349dc9920347d75892d59", "IPY_MODEL_2e2159933a304541bac34f659131d0a5", "IPY_MODEL_7008c7f72fe74d0dbe5a32eaf87c21a3", "IPY_MODEL_f08a25f151454b15b4df97a222370fbe"], "layout": "IPY_MODEL_ef803b434e824a0ab6c30e0c03026f1e"}}, "aa5afc50e38244ccbc8c309c60f5e13d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "083e7b34537143ea83a0aa085dd3cae4": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_fe538c0b7f244432ada444db427dcba4", "IPY_MODEL_3f20b18a48d141e6bd4c1c243464a024"], "layout": "IPY_MODEL_aa5afc50e38244ccbc8c309c60f5e13d"}}, "5c4c6cb7746c46fa992c2ba34d78f93c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2c397117c68f4ff894e07a2238d6cc6a": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_083e7b34537143ea83a0aa085dd3cae4", "IPY_MODEL_6acc9027977b444c8c4db8241c1e1b8d", "IPY_MODEL_16bf20d965904bd88002a886c727d464"], "layout": "IPY_MODEL_5c4c6cb7746c46fa992c2ba34d78f93c"}}, "e162a761c17d459588a060d27f751f46": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "0e65096aa78144a5b618ace1b60a8193": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#aaffaa", "font_weight": ""}}, "b3e0dc768cbf4251af0fe39ef001142d": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["happy"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude42", "disabled": false, "icon": "", "layout": "IPY_MODEL_e162a761c17d459588a060d27f751f46", "style": "IPY_MODEL_0e65096aa78144a5b618ace1b60a8193", "tooltip": "happy"}}, "67dcf9839e6d405a97ecd8938163bbde": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "79a5d290ede249919e58e83ee8032149": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#dddd77", "font_weight": ""}}, "95382524797c4fcab151efac92f2f412": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["medium"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude10", "disabled": false, "icon": "", "layout": "IPY_MODEL_67dcf9839e6d405a97ecd8938163bbde", "style": "IPY_MODEL_79a5d290ede249919e58e83ee8032149", "tooltip": "medium"}}, "9bb72e8d30a345049bb422e547075e75": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "8355ca87e1244b5192ff4bf88819627c": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#ffaaaa", "font_weight": ""}}, "6edf43e63f954ef6af5d8a99f0b86ea0": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["sad"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude41", "disabled": false, "icon": "", "layout": "IPY_MODEL_9bb72e8d30a345049bb422e547075e75", "style": "IPY_MODEL_8355ca87e1244b5192ff4bf88819627c", "tooltip": "sad"}}, "85cdb87f4da3477ca38de4eb54dbe472": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "ad75f3e279a742eabaf56c0c56e1a9a4": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "ed718cbb262f42808190e057e49b10a3": {"model_name": "TextareaModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TextareaModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TextareaView", "continuous_update": true, "description": "", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_85cdb87f4da3477ca38de4eb54dbe472", "placeholder": "We want your feedback!", "rows": null, "style": "IPY_MODEL_ad75f3e279a742eabaf56c0c56e1a9a4", "value": ""}}, "8fe517bd942d4deba753042c7b73a7d8": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "fef0087ecb8c488eb309ae4f20e7392b": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "2daa0d90ac1d4c8c99705d7dc6954342": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Submit", "disabled": false, "icon": "", "layout": "IPY_MODEL_8fe517bd942d4deba753042c7b73a7d8", "style": "IPY_MODEL_fef0087ecb8c488eb309ae4f20e7392b", "tooltip": ""}}, "7f5c1dffebb944e78847a56deac0256c": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "47b275301bf046b0a774f40ad8f593ca": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_ed718cbb262f42808190e057e49b10a3", "IPY_MODEL_2daa0d90ac1d4c8c99705d7dc6954342"], "layout": "IPY_MODEL_7f5c1dffebb944e78847a56deac0256c"}}, "0c46214dd07e454fa1b948d22d8566a9": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3d43999e1e934f399d25ca488a7d2c28": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9584089dd4844cb098769ef12711fd2b": {"model_name": "LabelModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "LabelModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "LabelView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_0c46214dd07e454fa1b948d22d8566a9", "placeholder": "\u200b", "style": "IPY_MODEL_3d43999e1e934f399d25ca488a7d2c28", "value": "Thanks for your feedback!"}}, "5a04b95fcf9d4d6eb517cb05c3f5cbc5": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c298d19e65aa4dafbc7631ec9eda9ccc": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_b3e0dc768cbf4251af0fe39ef001142d", "IPY_MODEL_95382524797c4fcab151efac92f2f412", "IPY_MODEL_6edf43e63f954ef6af5d8a99f0b86ea0"], "layout": "IPY_MODEL_5a04b95fcf9d4d6eb517cb05c3f5cbc5"}}, "476dbcea222e4de99264276d2e14f89d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "347dea46fe3b47cf9daf03b4f295e85b": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_c298d19e65aa4dafbc7631ec9eda9ccc", "IPY_MODEL_47b275301bf046b0a774f40ad8f593ca", "IPY_MODEL_9584089dd4844cb098769ef12711fd2b"], "layout": "IPY_MODEL_476dbcea222e4de99264276d2e14f89d"}}, "b695cb497e5844fe83f48c1e24d29f01": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1454a0fc92e04d3d8ee0f0f73c0ce9d1": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_347dea46fe3b47cf9daf03b4f295e85b"], "layout": "IPY_MODEL_b695cb497e5844fe83f48c1e24d29f01"}}, "69538f204e6a4a1fb87ed57c992568a6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "4f44517402074ab7b2e92c6d1b8ae230": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#aaffaa", "font_weight": ""}}, "5cc6b7d5cc8c4294904c3d0f70ede3bf": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["happy"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude42", "disabled": false, "icon": "", "layout": "IPY_MODEL_69538f204e6a4a1fb87ed57c992568a6", "style": "IPY_MODEL_4f44517402074ab7b2e92c6d1b8ae230", "tooltip": "happy"}}, "7abf4e2eb62e4c578771c68d67f633ad": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "0d9fef6acc9e4fd9a7f5492d65b57f20": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#dddd77", "font_weight": ""}}, "22390c1b6ed94ed98a9127fb1326fb3c": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["medium"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude10", "disabled": false, "icon": "", "layout": "IPY_MODEL_7abf4e2eb62e4c578771c68d67f633ad", "style": "IPY_MODEL_0d9fef6acc9e4fd9a7f5492d65b57f20", "tooltip": "medium"}}, "90cba6716d6147069b4a1e3ab8a254df": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": "0.5em", "right": null, "top": null, "visibility": null, "width": "auto"}}, "5a4d504bd6584f508bdfdbbdee132ff5": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": "#ffaaaa", "font_weight": ""}}, "26752949b9ab41fb9c8b57a7974add50": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": ["sad"], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "\ud83d\ude41", "disabled": false, "icon": "", "layout": "IPY_MODEL_90cba6716d6147069b4a1e3ab8a254df", "style": "IPY_MODEL_5a4d504bd6584f508bdfdbbdee132ff5", "tooltip": "sad"}}, "96c7679f136d4c248597562d248c65e2": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "d036aaee243c43c2a39fd05dc61e9184": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "1b35f54c2b464bf49e1abbf369a4b69c": {"model_name": "TextareaModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "TextareaModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "TextareaView", "continuous_update": true, "description": "", "description_tooltip": null, "disabled": false, "layout": "IPY_MODEL_96c7679f136d4c248597562d248c65e2", "placeholder": "We want your feedback!", "rows": null, "style": "IPY_MODEL_d036aaee243c43c2a39fd05dc61e9184", "value": ""}}, "7bb479e6ec9b4ecc84220478e223b8da": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": "auto", "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": "auto"}}, "6d9e4cf9c42d44cc8e31edd48f7e50d2": {"model_name": "ButtonStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "button_color": null, "font_weight": ""}}, "a0f34b430f434b52a2aba978f218bf61": {"model_name": "ButtonModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ButtonModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ButtonView", "button_style": "", "description": "Submit", "disabled": false, "icon": "", "layout": "IPY_MODEL_7bb479e6ec9b4ecc84220478e223b8da", "style": "IPY_MODEL_6d9e4cf9c42d44cc8e31edd48f7e50d2", "tooltip": ""}}, "a9d46aeba3064e0f90a3e6cdd2c45649": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d2147690edce4f9b9c60bf0af872d918": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_1b35f54c2b464bf49e1abbf369a4b69c", "IPY_MODEL_a0f34b430f434b52a2aba978f218bf61"], "layout": "IPY_MODEL_a9d46aeba3064e0f90a3e6cdd2c45649"}}, "f8ff4d7049fa4bfd8470b6a00b71cedf": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": "none", "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b9b720410b434d2488cdaf02296db9aa": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "bf1231dca68847739ee6dc8ee5b6ac9a": {"model_name": "LabelModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "LabelModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "LabelView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f8ff4d7049fa4bfd8470b6a00b71cedf", "placeholder": "\u200b", "style": "IPY_MODEL_b9b720410b434d2488cdaf02296db9aa", "value": "Thanks for your feedback!"}}, "a89f7b013944452faffb37b569778046": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "19f11041c3c046b9ac9d1df548b6e32f": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_5cc6b7d5cc8c4294904c3d0f70ede3bf", "IPY_MODEL_22390c1b6ed94ed98a9127fb1326fb3c", "IPY_MODEL_26752949b9ab41fb9c8b57a7974add50"], "layout": "IPY_MODEL_a89f7b013944452faffb37b569778046"}}, "a31eca0770d34856813493a103601ad1": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7d4c1ad03f9e4f168b9e6d5161acd758": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_19f11041c3c046b9ac9d1df548b6e32f", "IPY_MODEL_d2147690edce4f9b9c60bf0af872d918", "IPY_MODEL_bf1231dca68847739ee6dc8ee5b6ac9a"], "layout": "IPY_MODEL_a31eca0770d34856813493a103601ad1"}}, "edb3b676bc68462fae1032ccac9dad1f": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "928376ac893e4a6c882352c8d65337f6": {"model_name": "VBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "VBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "VBoxView", "box_style": "", "children": ["IPY_MODEL_7d4c1ad03f9e4f168b9e6d5161acd758"], "layout": "IPY_MODEL_edb3b676bc68462fae1032ccac9dad1f"}}}, "version_major": 2, "version_minor": 0}
</script>
<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./sequences/P2C1_Optimization/student"
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>
</div>
</main>
<footer class="footer-article noprint">
<!-- Previous / next buttons -->
<div class="prev-next-area">
<a class="left-prev" href="P2C1_Sequence4.html" id="prev-link" title="previous page">
<i class="fas fa-angle-left"></i>
<div class="prev-next-info">
<p class="prev-next-subtitle">previous</p>
<p class="prev-next-title"><strong>2.1.4: Learning Behaviour as a Form of High Dimensional Optimization</strong></p>
</div>
</a>
</div>
</footer>
</div>
</div>
<div class="footer-content row">
<footer class="col footer"><p>
<div class="extra_footer">
<div>
<a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/88x31.png"/></a>
<a href="https://opensource.org/licenses/BSD-3-Clause"><img src="https://camo.githubusercontent.com/9b9ea65d95c9ef878afa1987df65731d47681336/68747470733a2f2f696d672e736869656c64732e696f2f707970692f6c2f736561626f726e2e737667"/></a>
The contents of this repository are shared under the <a href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Software elements are additionally licensed under the <a href="https://opensource.org/licenses/BSD-3-Clause">BSD (3-Clause) License</a>.
</div>
</div>
</p>
</footer>
</div>
</div>
</div>
</div>
<!-- Scripts loaded after <body> so the DOM is not blocked -->
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
</body>
</html>