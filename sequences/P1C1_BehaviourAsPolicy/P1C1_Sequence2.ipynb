{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C1_BehaviourAsPolicy/P1C1_Sequence2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/P1C1_Sequence2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **Sequence 1.1.2: Thinking of What Receptors and Muscles Do as a Policy**\n",
    "\n",
    "### Objective: In this sequence, we will continue to use the Gridworld environment-organism system to see how behaviour can be abstracted as input --> compute --> output process, and how such processes can be formalized as a **policy** with **parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Picture showing receptor/input, brain/compute, musceles/output embedded in the environment feedback loop. (The following is a place holder for our actual image, but it's close to the right thing.)\n",
    "![Reinforcement_learning_diagram.svg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "\n",
    "!pip install ipympl vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import cdist\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py', 'gw_widgets.py',]\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C1_S2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.2.1: Limited Perception (fog of war)\n",
    "\n",
    "In the previous sequence you and the organism were effectively omniscient, i.e. had both complete and accurate knowledge of the state of the environment. This is also the case in games like go or chess where all relevant information is available. However, these scenarios of total knowledge are unusual, artificial exceptions. In contrast, most living organisms face a situation where the relevant state of the environment is perceived only in part and with uncertainty. To illustrate this, let's introduce a new organism, *Fishy*. Drawing inspiration from weakly electric fish (see for example the work of Malcolm MacIver), *Fishy*, can only perceive the 12 cells of the Gridworld immediately adjacent to it (those cells within a radius of 2 using $L_1$, or 'city block' distance). For now, *Fishy* aimlessly swims around on the grid. Let's see what that looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Random Movement with Limited Perception\n",
    "# @markdown Don't worry about how this code works – just **run this cell** and press the start button to see what *Fishy's* limited field of perception looks like.\n",
    "rng = np.random.default_rng(seed=420)\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30, rng=rng)\n",
    "random_igwg = InteractiveGridworld(gwg, has_fov=True, figsize=(5,4),\n",
    "                                   critter_name='$\\mathit{Fishy}$',\n",
    "                                   player=None)\n",
    "display(random_igwg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that you've seen what this limited field of perception looks like it's time for a micro science question:\n",
    "\n",
    "**Are *Fishy's* perceptions of the environment predictive of whether or not they will eat food?**\n",
    "\n",
    "Make a guess. Think about how you would prove to yourself whether your guess is right. There are lots of ways to validate your guess; let's explore one of them right now. Our first step is to **observe** the behavior and **collect data**! Any way of validating your guess that purports to be science (in the modern sense) will start this way, with data. However, keep in mind, there are other approaches and modes of thinking out there. For example, a theorist or mathematician might bypass observations and data, diving straight into deductions based on abstract assumptions. But, that's not going to be our approach here and now (though we will borrow it occasionally throughout the book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observations of Random Movement with Limited Perception\n",
    "# @markdown Don't worry about how this code works – just **run this cell**, then press the start button and watch the data collection happen.\n",
    "rng = np.random.default_rng(seed=2023)\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30, rng=rng)\n",
    "record_igwg = InteractiveGridworld(gwg, has_fov=True, collect_fov_data=True,\n",
    "                                   player=None, critter_name='$\\mathit{Fishy}$',\n",
    "                                   figsize=(4,4))\n",
    "display(record_igwg.b_fig.canvas)\n",
    "display(record_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(record_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Start by running two or three simulations to collect 'adequate' data for analysis.\n",
    "\n",
    "**Thought/Bonus/Further Reading: How much data is truly 'enough'?:**\n",
    "\n",
    "Entire methodologies have been developed to answer this critical question.  In a hypothesis testing framework, power analysis is used to determine optimal sample size by considering the strength an effect to be detected and the desired statistical power of the test. In an ML framework, notions of Empirical Risk Minimization inform the amount of data needed. In a Bayesian context, assessment of convergence of posterior distributions of parameters indicates whether incoming data is helping (or continues to help) improve those parameter estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have some observations, let's put them in a structure we can easily work with. Here's how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "data = record_igwg.fov_eat_table_data\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When analyzing data we ***always*** plot the data before doing ***anything*** else. Which visualization to start with is a more subjective question, though histograms and scatter plots are often good starting points. In this case we'll go for a [histogram](## \"In a histogram each bin represents a range of values, and the height of the bar shows the frequency (number of data points observed) within that range.\") of the two distributions of number of food items perceived conditional on whether eating happened immediately afterward or not. Before you run the code below to generate the picture, think about what it might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# If data collection step was skipped above use this data instead\n",
    "if np.all(data == np.zeros(data.shape)):\n",
    "  print(\"You haven't collected any data, using canned data instead.\")\n",
    "  data = np.array(\n",
    "    [[10., 22., 11.,  4.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "     [ 0.,  1.,  3.,  1.,  3.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "\n",
    "# Unpack the contingency table into a list of raw observations\n",
    "# A flat array of perception values before 'Not Eating' events\n",
    "not_eating = np.repeat(range(len(data[0])), data[0].astype(int))\n",
    "# A flat array of perception values before 'Eating' events\n",
    "eating = np.repeat(range(len(data[0])), data[1].astype(int))\n",
    "\n",
    "# Create histograms\n",
    "fig_hist, (ax1_hist, ax2_hist) = plt.subplots(2, 1, sharex=True, sharey=True,\n",
    "                                              figsize=(8,6))\n",
    "# Define bin edges\n",
    "nonzero_col_index = np.where(data > 0)[1]\n",
    "bin_edges = np.arange(-0.5, np.max(nonzero_col_index) + 1.5, 1)\n",
    "# Plot Histograms\n",
    "ax2_hist.hist(not_eating, bins=bin_edges, color='blue', alpha=0.7, rwidth=0.9)\n",
    "ax1_hist.hist(eating, bins=bin_edges, color='orange', alpha=0.7, rwidth=0.9)\n",
    "ax1_hist.set_title('Distribution of Food Items in Percept before Eating and Not Eating',\n",
    "                   fontsize=14)\n",
    "ax1_hist.set_ylabel('Before\\nEating Counts', fontsize=12)\n",
    "ax2_hist.set_ylabel('Before\\nNot Eating Counts', fontsize=12)\n",
    "ax2_hist.set_xlabel('Number of Food Items in Perceived', fontsize=12)\n",
    "remove_ip_clutter(fig_hist)\n",
    "fig_hist.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These histograms reveal differences in the distribution of perceived food items immediately prior to 'Eating' and 'Not Eating' events. 'Not Eating' is more frequently observed, especially when fewer food items are perceived beforehand. This should not be surprising, fishy sees 12 spots, a small number of which typically carry food. On the other hand, 'Eating' events, although less common, tend to occur more often when more food items are perceived. These initial observations hint that the perception of food may be predictive of eating.\n",
    "\n",
    "To get at our question — does perceiving food predict eating more directly? — we will next plot the proportion of times eating followed for the different numbers of food items perceived. Again, to visualize what the plot might look like before you run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Replace ... in the line:\n",
    "# prob_eating plot_data[...] / plot_data.sum(axis=...)\n",
    "# Comment out or remove this next lines.\n",
    "raise NotImplementedError(\"Exercise: plot proportion of eating\")\n",
    "################################################################################\n",
    "\n",
    "# Calculate the probability of eating given the amount of food perceived\n",
    "# for food amounts with non-zero observations\n",
    "non_zero_cols = np.where(data.sum(axis=0) > 0)[0]\n",
    "plot_data = data[:, non_zero_cols]\n",
    "prob_eating = plot_data[...] / plot_data.sum(axis=...)\n",
    "# Create a figure and axis for the plot\n",
    "fig_prop, ax_prop = plt.subplots()\n",
    "# Plot the probabilities\n",
    "ax_prop.plot(np.arange(len(data[0]))[non_zero_cols], prob_eating, marker='o')\n",
    "# Set the title and labels\n",
    "ax_prop.set_title('Proportion of Eating Events by\\nAmount of Food Perceived Prior')\n",
    "ax_prop.set_xlabel('Number of Food Items Perceived')\n",
    "ax_prop.set_ylabel('Proportion where Eating Follows')\n",
    "remove_ip_clutter(fig_prop)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "# Calculate the probability of eating given the amount of food perceived\n",
    "# for food amounts with non-zero observations\n",
    "non_zero_cols = np.where(data.sum(axis=0) > 0)[0]\n",
    "plot_data = data[:, non_zero_cols]\n",
    "prob_eating = plot_data[1,:] / plot_data.sum(axis=0)\n",
    "# Create a figure and axis for the plot\n",
    "fig_prop, ax_prop = plt.subplots()\n",
    "# Plot the probabilities\n",
    "ax_prop.plot(np.arange(len(data[0]))[non_zero_cols], prob_eating, marker='o')\n",
    "# Set the title and labels\n",
    "ax_prop.set_title('Proportion of Eating Events by\\nAmount of Food Perceived Prior')\n",
    "ax_prop.set_xlabel('Number of Food Items Perceived')\n",
    "ax_prop.set_ylabel('Proportion where Eating Follows')\n",
    "remove_ip_clutter(fig_prop)\n",
    "fig_prop.tight_layout()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this plot, it appears that the proportion of eating events increases as the amount of food perceived immediately prior increases. This suggests that the amount of food perceived could indeed predict eating behavior. However, this is just a visualization and not a formal test of a causal relationship. Later in the book we will review how to formally assess predictiveness through statistical models, such as logistic regression, which would be suitable for this case of predicting a binary outcome (eating vs. not eating) from a predictor variable (or variables) like the amount of food perceived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Bonus: Think about what makes a good predictor variable:\n",
    "\n",
    "We observed and recorded the total number of food items perceived and used that as our 'predictor' variable in the analysis and visualizations above. Do you think there are other aspects of *Fishy's* perceptions, beyond just the total number of food items perceived that could be used to make better predictions about the amount of food eaten?(Hint: Could the predictor variable consider the relative location of food items within the perceptual field?) [Solution.](## \"In terms of predicting immediate eating, really only the four grid cells immediately adjacent to *fishy* can have any bearing on whether eating will happen in the next round, so the sum of food items just in those four cells would be a much better predictor. Note that we haven't gotten into what 'better' means yet for a predictor so this question is kind of ill formed, but we will eventually and what makes a good predictor is good thing to start thinking about right away.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.2.2: Perception Guiding Actions is a Policy\n",
    "\n",
    "We've seen that even in the case of random drifting, perceptions of the environment can carry information about import things like whether or not eating will happen. Now, we're going to hook this perceptive field up to some highly abstracted 'muscles' by making a simple set of rules that translate *Fishy's* perceptions into a choice of actions (one of up, down, left, right).\n",
    "\n",
    "Before we attempt this we need to understand the form of *Fishy's* perceptions. We recorded some of these in the previous section, what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# 'record_igwg' is an instance of an InteractiveGridworld object\n",
    "# that we ran in the cell above to record data\n",
    "print(f\"Type of 'record_igwg': {type(record_igwg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# 'record_igwg' has a 'percept_eat_records' attribute. This is a list of all\n",
    "# the records from the games played by the InteractiveGridworld object since it\n",
    "# was instantiated.\n",
    "print(f\"Type of 'percept_eat_records': {type(record_igwg.percept_eat_records)}\")\n",
    "# The length of 'percept_eat_records' should be 30 times the number of times you\n",
    "# ran the simulation.\n",
    "print(f\"Length of 'percept_eat_records': {len(record_igwg.percept_eat_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's inspect what a record looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# If we didn't collect any data we'll use some canned data\n",
    "if len(record_igwg.percept_eat_records) < 2:\n",
    "  record_igwg.percept_eat_records.append(\n",
    "    {'perception': np.array(\n",
    "        [-1., -1.,  0., -2., -1.,  0., -2., -2., -1.,  0., -2.,  0.]),\n",
    "     'state': {\n",
    "      'pieces': np.array(\n",
    "          [[[ 0.,  0.,  0., -1., -1.,  0., -1.],\n",
    "            [ 0.,  0., -1.,  0., -1., -1.,  0.],\n",
    "            [ 0.,  0.,  0.,  0., -1.,  0.,  1.],\n",
    "            [ 0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]]),\n",
    "      'scores': np.array([0.]),\n",
    "      'rounds_left': np.array([30.])},\n",
    "     'did_eat': False})\n",
    "  record_igwg.percept_eat_records.append(\n",
    "    {'perception': np.array(\n",
    "        [ 0.,  0.,  0., -2.,  0., -1., -2., -2., -1.,  0., -2.,  0.]),\n",
    "     'state': {\n",
    "        'pieces': np.array(\n",
    "            [[[ 0.,  0.,  0., -1., -1.,  0., -1.],\n",
    "              [ 0.,  0., -1.,  0., -1., -1.,  0.],\n",
    "              [ 0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
    "              [ 0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]]),\n",
    "        'scores': np.array([0.]),\n",
    "        'rounds_left': np.array([29.])},\n",
    "     'did_eat': False})\n",
    "\n",
    "# Let's examine the first two elements of the list.\n",
    "# What type is the record?\n",
    "print(f\"Type of first record: {type(record_igwg.percept_eat_records[0])}\")\n",
    "# What does the first one look like?\n",
    "print('----------------------------------')\n",
    "display(record_igwg.percept_eat_records[0])\n",
    "# What does the second one look like?\n",
    "print('----------------------------------')\n",
    "display(record_igwg.percept_eat_records[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "It looks like the first two elements of the list are dictionaries, and, as it turns out, all elements share this structure. These dictionaries contain relatively self-descriptive keys. Let's walk through the structure of these records element by element.\n",
    "\n",
    "* `perception`: This is a 12-element array with -2's denoting areas where the perceptual field lies outside the boundaries of the Gridworld, 0's indicating empty spaces on the grid, and -1's marking food locations.\n",
    "* `state`: This is another dictionary representing the state of the board at the time of the perception. The `pieces` element of `state` is a 7x7 array depicting the grid positions of the organism (1), the food (-1's), and the empty spaces (0's). The `scores` and `rounds_left` elements describe exactly what their names suggest.\n",
    "* `did_eat`: This is a boolean indicator that is `True` when eating occurred in the round immediately following the perception and `False` when eating did not occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Can you see how the perceptive field, as defined below, is overlaid on the 7x7 array to generate a perceptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# define the L1 ball mask the determines what an organism perceives\n",
    "# Perception radius for an organism\n",
    "radius = 2\n",
    "# Grid size to encompass perception 'circle', include center point\n",
    "diameter = radius*2+1\n",
    "# Empty grid (initially all False)\n",
    "mask = np.zeros((diameter, diameter), dtype=bool)\n",
    "# Relative coordinates for grid points, center at (0,0)\n",
    "mask_coords = np.array([(i-radius, j-radius)\n",
    "  for i in range(diameter)\n",
    "  for j in range(diameter)])\n",
    "# Calculate cityblock distance from each point to center\n",
    "mask_distances = cdist(mask_coords, [[0, 0]], 'cityblock').reshape(mask.shape)\n",
    "# Mark points within perception radius as True\n",
    "mask[mask_distances <= radius] = True\n",
    "# Exclude center point (organism itself)\n",
    "mask[radius,radius] = False\n",
    "print('--------------------Perceptive Field---------------------------')\n",
    "display(mask)\n",
    "print('\\n--------------------Environment-------------------------------')\n",
    "display(record_igwg.percept_eat_records[0]['state']['pieces'])\n",
    "print('\\n--------------------Perception--------------------------------')\n",
    "display(record_igwg.percept_eat_records[0]['perception'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The indices of the perception array correspond to the spaces around the organism as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "numbered_mask = np.ones(mask.shape) * -1\n",
    "numbered_mask[mask] = range(mask.sum())\n",
    "display(numbered_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note that Fishy lacks an orientation in relation to the Gridworld; all directions are defined in absolute or cardinal terms: 'up', 'down', 'left', and 'right'. So another way of thinking about the perception vector is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# in human readable terms the perception is organized as\n",
    "human_readable_percept_structure = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "print(human_readable_percept_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we understand what a perception looks like, let's build a function that accepts a perception as input and outputs one of four possible moves. To start, we'll create a simple function based on the following logic: if any is food immediately adjacent, *Fishy* moves towards it; otherwise *Fishy* moves randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to set up the tests for the code exercise below\n",
    "def test_action_from_perception(test_func):\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "\n",
    "  # Test when there's no food nearby\n",
    "  perception = np.zeros(12)\n",
    "  assert test_func(perception) in ['up', 'down', 'left', 'right']\n",
    "\n",
    "  # Test when there's food to the near up\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near up')] = -1\n",
    "  assert test_func(perception) == 'up'\n",
    "\n",
    "  # Test when there's food to the near down\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near down')] = -1\n",
    "  assert test_func(perception) == 'down'\n",
    "\n",
    "  # Test when there's food to the near left\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near left')] = -1\n",
    "  assert test_func(perception) == 'left'\n",
    "\n",
    "  # Test when there's food to the near right\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near right')] = -1\n",
    "  assert test_func(perception) == 'right'\n",
    "\n",
    "  # Test when there's food in multiple nearby directions\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near up')] = -1\n",
    "  perception[percept_struct.index('near right')] = -1\n",
    "  assert test_func(perception) in ['up', 'right']\n",
    "\n",
    "  # Test when there's food in multiple nearby directions\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near left')] = -1\n",
    "  perception[percept_struct.index('near down')] = -1\n",
    "  assert test_func(perception) in ['down', 'left']\n",
    "\n",
    "  print('Congrats, Tests Passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Complete the lines with ...  to identify directions where\n",
    "# food is nearby and then randomly choose between directions with nearby food\n",
    "# if food is nearby. Then comment out the line below and see if your code\n",
    "# runs and passes the code checks.\n",
    "raise NotImplementedError(\"Exercise: make actions depend on perception\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# to_remove solution\n",
    "def simple_action_from_percept(percept, rng=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # Defines directions corresponding to different perception indices\n",
    "  direction_struct = [\n",
    "    'None', 'None', 'up', 'None',\n",
    "    'None', 'left', 'right', 'None',\n",
    "    'None', 'down', 'None', 'None']\n",
    "  # these are what count as nearby in the percept\n",
    "  nearby_directions = ['near up', 'near down', 'near left', 'near right']\n",
    "  # Get the corresponding indices in the percept array\n",
    "  nearby_indices = [percept_struct.index(dir_) for dir_ in nearby_directions]\n",
    "  # Identify the directions where food is located\n",
    "  food_indices = [index for index in nearby_indices if percept[index] == ...]\n",
    "  food_directions = [direction_struct[index] for index in food_indices]\n",
    "  if len(food_directions) > 0:  # If there is any food nearby\n",
    "    # If there is any food nearby randomly choose a direction with food\n",
    "    return rng.choice(...)  # Move towards a random one\n",
    "  else:\n",
    "    # If there is no food nearby, move randomly\n",
    "    return rng.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "test_action_from_perception(simple_action_from_percept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def simple_action_from_percept(percept, rng=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # Defines directions corresponding to different perception indices\n",
    "  direction_struct = [\n",
    "    'None', 'None', 'up', 'None',\n",
    "    'None', 'left', 'right', 'None',\n",
    "    'None', 'down', 'None', 'None']\n",
    "  # these are what count as nearby in the percept\n",
    "  nearby_directions = ['near up', 'near down', 'near left', 'near right']\n",
    "  # Get the corresponding indices in the percept array\n",
    "  nearby_indices = [percept_struct.index(dir_) for dir_ in nearby_directions]\n",
    "  # Identify the directions where food is located\n",
    "  food_indices = [index for index in nearby_indices if percept[index] == -1]\n",
    "  food_directions = [direction_struct[index] for index in food_indices]\n",
    "  if len(food_directions) > 0:  # If there is any food nearby\n",
    "    # If there is any food nearby randomly choose a direction with food\n",
    "    return rng.choice(food_directions)  # Move towards a random one\n",
    "  else:\n",
    "    # If there is no food nearby, move randomly\n",
    "    return rng.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "test_action_from_perception(simple_action_from_percept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Congratulations, you've just implemented your first policy function. Any  function, like this one, that takes perceptions as inputs and outputs actions, in the context of an environmental feedback loop is called a ***policy***.\n",
    "\n",
    "Also, observe how we use a test here to check if your code does the right things. When writing good, high quality, code, most programmers write such tests for their own code. Writing more lines for the tests than the code being tested is perfectly standard. The number, breadth, and quality of the tests is what ultimately defines how much you can trust your code!\n",
    "\n",
    "Now that we have a policy let's see how it compares to random drifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title A Simple Rule Based Policy Versus Random Drifting\n",
    "# @markdown Don't worry about how this code works – just **run the cell** and press start to compare the policy we defined above with random drifting.\n",
    "\n",
    "\n",
    "def simple_action_from_percept(percept, rng=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # Defines directions corresponding to different perception indices\n",
    "  direction_struct = [\n",
    "    'None', 'None', 'up', 'None',\n",
    "    'None', 'left', 'right', 'None',\n",
    "    'None', 'down', 'None', 'None']\n",
    "  # these are what count as nearby in the percept\n",
    "  nearby_directions = ['near up', 'near down', 'near left', 'near right']\n",
    "  # Get the corresponding indices in the percept array\n",
    "  nearby_indices = [percept_struct.index(dir_) for dir_ in nearby_directions]\n",
    "  # Identify the directions where food is located\n",
    "  food_indices = [index for index in nearby_indices if percept[index] == -1]\n",
    "  food_directions = [direction_struct[index] for index in food_indices]\n",
    "  if len(food_directions) > 0:  # If there is any food nearby\n",
    "    # If there is any food nearby randomly choose a direction with food\n",
    "    return rng.choice(food_directions)  # Move towards a random one\n",
    "  else:\n",
    "    # If there is no food nearby, move randomly\n",
    "    return rng.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleRulePlayer():\n",
    "  \"\"\"\n",
    "  A Player based on the following simple policy:\n",
    "  If there is any food immediately nearby move towards it,\n",
    "  otherwise it move randomly.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, fov_radius=2):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.fov_radius = fov_radius\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "\n",
    "    #get the percept for every board in the batch\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius)\n",
    "    critter_oriented_moves = []\n",
    "    for g in range(batch_size):\n",
    "      critter_oriented_moves.append(\n",
    "          simple_action_from_percept(perceptions[g], self.game.rng))\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, a_1hots\n",
    "\n",
    "\n",
    "# two different ai players\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "srp = SimpleRulePlayer(gwg)\n",
    "h2h_igwg = Head2HeadGridworld(gwg, player0=None, p0_short_name='RANDOM',\n",
    "                              p0_long_name='Random\\nPolicy',\n",
    "                              player1=srp, p1_short_name='EAT_NEAR',\n",
    "                              p1_long_name='Eat Nearby\\nPolicy',\n",
    "                              figsize=(2.4,3.0), has_fov=True, radius=2,\n",
    "                              critter_name='$\\mathit{Fishy}$')\n",
    "display(h2h_igwg.b_fig0.canvas)\n",
    "display(h2h_igwg.b_fig1.canvas)\n",
    "display(h2h_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Okay, hopefully we've implemented our rule correctly and we shouldn't be surprised to see that moving towards food when it is nearby (usually) results in much more eating than just drifting around randomly. In a patchy environment like this - food is in some places but not others - contingent action is a big advantage. However, as you may have noticed, *Fishy* is not taking full advantage of its perceptual field. There is perceived food (not immediately adjacent) that *Fishy* could move closer towards, but right now *Fishy* only acts on perceptions of food immediately adjacent. Next we're going to see if we can make *Fishy* an even more efficient eater by utilizing the whole perceptive field when choosing a movement direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.2.3: Parameterization of a Policy\n",
    "\n",
    "Our first policy function could be neatly summarized as \"If there is food immediately adjacent, move towards it, otherwise move randomly.\" It was not too difficult to translate this simple rule into a concrete function of *Fishy's* perceptions. However, as the number of inputs increase, formulating an efficient rule in natural language and translating that rule into a function becomes increasingly challenging. An effective rule in natural language would need to identify all possible food configurations, group them into cases, and then assign an appropriate direction to each case. It's a feasible approach, but very labour intensive. So, instead we are going to expand our way of thinking about policies to include [parameterization.](## \"In general the distinction between the parameters and the variables of a function is not always clear cut and is often dependent on the context. In this book, in the context of a policy function, variables will primarily be the sensory/environmental inputs to a policy function and parameters will be internal values of an organism which change not as a result of the sensory/environmental context but as a result changes to the organism's internal state as a response to learning and/or evolutionary processes. Even here though the distinction if blurred when considering how memories of recent sensory events are incorporated into a policy function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Another reason to parameterize our policy function, and the motivation for our specific manner parameterization, is to align our policy function implementation slightly towards how neurons in an organism might implement a policy. Specifically, we want to start to think of the strengths of synaptic connections between neurons as a kind of parameter in a neural implementation of a function. For this, let's imagine a simplified 'cartoon' nervous system where each cell in the perceptual field represents a single neuron. These neurons send a signal when food is present in their corresponding section of the perceptual field and remain 'quiet' otherwise.\n",
    "\n",
    "We then imagine that the organism has 4 \"direction neurons\" (up, down, left, and right) and that the relative firing strengths of these direction neurons determine a probability distribution over the organism's movement direction. In this 'cartoon' nervous system, each neuron in the perceptual field connects to the dendrites of the direction neurons via an axon, with the connection strength determined by a parameterized weight. The firing strengths of the direction neurons are simply the sums of the weighted inputs they receive from the perceptual cells.\n",
    "\n",
    "While this 'cartoon' is laughably far removed from how motor control is implemented in most animals, it is still a step closer to real neural computation compared to our initial rule based policy implementation. And, for a few very simple organisms this cartoon isn't so far from reality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In mathematical terms this looks like having $\\mathbf{W} \\in \\mathbb{R}^{4 \\times 12}$ as the weight matrix with $4$ rows and $12$ columns represent the connection strength *weights* between the $12$ perception neurons and the $4$ muscle neurons, and having $\\mathbf{x} \\in \\{0,1\\}^{12}$ as a $12$-dimensional binary column vector represent the organism's perceptive field neuron activity.\n",
    "\n",
    "The *activation* strengths of the muscle neurons, represented as $\\mathbf{a} \\in \\mathbb{R}^4$, are then computed as the matrix-vector product $\\mathbf{Wx}$, that is:\n",
    "\n",
    "$$\\mathbf{a} = \\mathbf{Wx}$$\n",
    "\n",
    "Each element $a_i$ of $\\mathbf{a}$ is computed as:\n",
    "\n",
    "$$a_i = \\sum_{j=1}^{12} W_{ij}x_j$$\n",
    "\n",
    "The matrix-vector product involves taking the dot product of the $i$-th row of $\\mathbf{W}$ with the vector $\\mathbf{x}$ to yield the $i$-th element of $\\mathbf{a}$.\n",
    "\n",
    "We perform this operation for each row $i$ (where $i$ ranges from $1$ to $4$) of the weight matrix $\\mathbf{W}$ to obtain the complete activation vector $\\mathbf{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To help make all this a little more concrete let's see what our first rule based policy would look like if it were structured in this parameterized way. We'll start by building a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# a human interpretable overview of the percept structure\n",
    "percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # a human interpretable overview of the out structure\n",
    "output_struct = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# a more explicit human legible way to set W\n",
    "W1 = np.zeros((len(output_struct), len(percept_struct)))\n",
    "for i, output in enumerate(output_struct):\n",
    "  for j, input in enumerate(percept_struct):\n",
    "    if input == 'near up':\n",
    "      if output == 'up':\n",
    "        W1[i,j] = 1\n",
    "    elif input == 'near left':\n",
    "      if output == 'left':\n",
    "        W1[i,j] = 1\n",
    "    elif input == 'near right':\n",
    "      if output == 'right':\n",
    "        W1[i,j] = 1\n",
    "    elif input == 'near down':\n",
    "      if output == 'down':\n",
    "        W1[i,j] = 1\n",
    "\n",
    "# slick programmer way to set W\n",
    "W2 = np.zeros((len(output_struct), len(percept_struct)))\n",
    "for i, output in enumerate(output_struct):\n",
    "  for j, input in enumerate(percept_struct):\n",
    "    if output in input:\n",
    "      # *in* is a base python function that checks for containment. In this\n",
    "      # case it checks if the output string is contained in the input string.\n",
    "      # If output='up' and input='far up' output in input will evaluate as True\n",
    "      # but if the input were 'near down' it would evaluate as false. Note that\n",
    "      # this approach would fail terribly if our word for up were contained in\n",
    "      # our word for down, i.e. if instead using 'down' we'd used the string\n",
    "      # 'anti-up'. In general string based logic is fragile and bug prone,\n",
    "      # but also can make code very human readable. It's all trade-offs out\n",
    "      # there be careful.\n",
    "      if 'near' in input:\n",
    "        W2[i,j] = 1\n",
    "\n",
    "display(W1)\n",
    "print('--------------------------------------------------------------')\n",
    "display(W2)\n",
    "assert(np.all(W1 == W2))\n",
    "W = W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next we'll use those weights to re-implement our first rule based function, and use our test from before to be sure we've done it right! Tests are ***so*** useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Complete the lines output_activations = ... Then comment\n",
    "# out the line below and see if your code runs and passes the same code checks\n",
    "# we applied to our first rule based policy.\n",
    "raise NotImplementedError(\"Exercise: parameterize a simple policy\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def parameterized_policy(percept, rng=None, W=W, softmax_temp=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "    W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "      between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  if softmax_temp is None:\n",
    "    # very low temp, basically deterministic for this range of values\n",
    "    softmax_temp = 0.01\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # a human interpretable overview of the out structure\n",
    "  output_struct = ['up', 'down', 'left', 'right']\n",
    "  # boolean representation of percept, no edges, just 1's where food is\n",
    "  # zero otherwise\n",
    "  x = np.asarray(percept == -1, int)\n",
    "  # hint: Look at the equations above, the matrix-vector product (and it's\n",
    "  # higher dimension generalizations) are implemented by the @ operator for\n",
    "  # numpy arrays.\n",
    "  output_activations = ...\n",
    "  if np.sum(output_activations > 0):\n",
    "    # softmax shift by max, scale by temp\n",
    "    shift_scale_ex = np.exp((output_activations -\n",
    "                             np.max(output_activations))/softmax_temp)\n",
    "    sm = shift_scale_ex / shift_scale_ex.sum() #normalized\n",
    "    probs_sm = sm / sm.sum(axis=0) #re-normalized again for fp precision issues\n",
    "    # probs below is a naive way to get a discrete probability distribution\n",
    "    # from a real valued vector, why did we use softmax normalization instead?\n",
    "    # probs = output_activations / np.sum(output_activations)\n",
    "    action = rng.choice(output_struct, p=probs_sm)\n",
    "  else:\n",
    "    action = rng.choice(output_struct)\n",
    "  return action\n",
    "\n",
    "test_action_from_perception(parameterized_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "\n",
    "def parameterized_policy(percept, rng=None, W=W, softmax_temp=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "    W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "      between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  if softmax_temp is None:\n",
    "    # very low temp, basically deterministic for this range of values\n",
    "    softmax_temp = 0.01\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # a human interpretable overview of the out structure\n",
    "  output_struct = ['up', 'down', 'left', 'right']\n",
    "  # boolean representation of percept, no edges, just 1's where food is\n",
    "  # zero otherwise\n",
    "  x = np.asarray(percept == -1, int)\n",
    "  # hint: Look at the equations above, the matrix-vector product (and it's\n",
    "  # higher dimension generalizations) are implemented by the @ operator for\n",
    "  # numpy arrays.\n",
    "  output_activations = W @ x\n",
    "  if np.sum(output_activations > 0):\n",
    "    # softmax shift by max, scale by temp\n",
    "    shift_scale_ex = np.exp((output_activations -\n",
    "                             np.max(output_activations))/softmax_temp)\n",
    "    sm = shift_scale_ex / shift_scale_ex.sum() #normalized\n",
    "    probs_sm = sm / sm.sum(axis=0) #re-normalized again for fp precision issues\n",
    "    # probs below is a naive way to get a discrete probability distribution\n",
    "    # from a real valued vector, why did we use softmax normalization instead?\n",
    "    # probs = output_activations / np.sum(output_activations)\n",
    "    action = rng.choice(output_struct, p=probs_sm)\n",
    "  else:\n",
    "    action = rng.choice(output_struct)\n",
    "  return action\n",
    "\n",
    "test_action_from_perception(parameterized_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Okay, now it's your turn to make a really great *Fishy*. You can do this either by manually setting the weights below, or extending the weight setting logic used above to set W programmatically, whichever seems less tedious to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Fill in the values of the weight matrix below to define\n",
    "# a really great parameterized policy for fishy. Then comment out the line\n",
    "# below and see how your parameterized policy compares to the eat when nearby\n",
    "# policy.\n",
    "raise NotImplementedError(\"Exercise: pick good parameter values for W\")\n",
    "################################################################################\n",
    "\n",
    "W_student = np.array(\n",
    "     # input                                             # output\n",
    "     #[fu, lu, nu, ru, fl, nl, nr, fr, ld, nd, rd, fd]   #\n",
    "    [[  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # up\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # down\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # left\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]) # right\n",
    "\n",
    "# comment out these nested for loops if you don't want use them to set the W\n",
    "# W values and you'd rather just enter them manually above\n",
    "for i, output in enumerate(output_struct):\n",
    "  for j, input in enumerate(percept_struct):\n",
    "    if (...):\n",
    "      W_student[i,j] = ...\n",
    "    elif (...):\n",
    "      W_student[i,j] = ...\n",
    "    # more elif logic here maybe\n",
    "\n",
    "display(W_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "W_student = np.array(\n",
    "     # input                                             # output\n",
    "     #[fu, lu, nu, ru, fl, nl, nr, fr, ld, nd, rd, fd]   #\n",
    "    [[  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # up\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # down\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # left\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]) # right\n",
    "\n",
    "# comment out these nested for loops if you don't want use them to set the W\n",
    "# W values and you'd rather just enter them manually above\n",
    "for i, output in enumerate(output_struct):\n",
    "  for j, input in enumerate(percept_struct):\n",
    "    if (input == 'near ' + output):\n",
    "      W_student[i,j] = 4.0\n",
    "    elif (output in input):\n",
    "      W_student[i,j] = 1.0\n",
    "    # more elif logic here maybe\n",
    "\n",
    "display(W_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Your Parameterized Policy Versus the Simple Rule Based Policy\n",
    "# @markdown Don't worry about how this code works – just **run the cell** and press start to compare the parameterized policy you've just defined to our simple rule based policy from before.\n",
    "\n",
    "\n",
    "def parameterized_policy(percept, rng=None, W=W, softmax_temp=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "    W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "      between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  if softmax_temp is None:\n",
    "    # very low temp, basically deterministic for this range of values\n",
    "    softmax_temp = 0.01\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # a human interpretable overview of the out structure\n",
    "  output_struct = ['up', 'down', 'left', 'right']\n",
    "  # boolean representation of percept, no edges, just 1's where food is\n",
    "  # zero otherwise\n",
    "  x = np.asarray(percept == -1, int)\n",
    "  # hint: Look at the equations above, the matrix-vector product (and it's\n",
    "  # higher dimension generalizations) are implemented by the @ operator for\n",
    "  # numpy arrays.\n",
    "  output_activations = W @ x\n",
    "  if np.sum(output_activations > 0):\n",
    "    # softmax shift by max, scale by temp\n",
    "    shift_scale_ex = np.exp((output_activations -\n",
    "                             np.max(output_activations))/softmax_temp)\n",
    "    sm = shift_scale_ex / shift_scale_ex.sum() #normalized\n",
    "    probs_sm = sm / sm.sum(axis=0) #re-normalized again for fp precision issues\n",
    "    # probs below is a naive way to get a discrete probability distribution\n",
    "    # from a real valued vector, why did we use softmax normalization instead?\n",
    "    # probs = output_activations / np.sum(output_activations)\n",
    "    action = rng.choice(output_struct, p=probs_sm)\n",
    "  else:\n",
    "    action = rng.choice(output_struct)\n",
    "  return action\n",
    "\n",
    "if 'W_student' not in globals() or W_student is None:\n",
    "  print('Warning, weights not set, using canned weights instead')\n",
    "  W_student = np.array([\n",
    "      [1, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "      [0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 1],\n",
    "      [0, 1, 0, 0, 1, 4, 0, 0, 1, 0, 0, 0],\n",
    "      [0, 0, 0, 1, 0, 0, 4, 1, 0, 0, 1, 0]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ParamPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a parameterized policy using the weights defined by the\n",
    "  student in W_student.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, weights=None, fov_radius=2, ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    if weights is None:\n",
    "      self.W = np.array(\n",
    "      [[1., 1., 4., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 4., 1., 1.],\n",
    "       [0., 1., 0., 0., 1., 4., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 4., 1., 0., 0., 1., 0.]])\n",
    "    else:\n",
    "      self.W = weights\n",
    "    self.fov_radius = fov_radius\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "  def play(self, board, temp=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "\n",
    "    #get the percept for every board in the batch\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius)\n",
    "    critter_oriented_moves = []\n",
    "    for g in range(batch_size):\n",
    "      critter_oriented_moves.append(\n",
    "          parameterized_policy(perceptions[g], self.game.rng, self.W, temp))\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, a_1hots\n",
    "\n",
    "\n",
    "# two different ai players\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "\n",
    "srp = SimpleRulePlayer(gwg)\n",
    "spp = ParamPlayer(gwg, W_student) #student parameterized player\n",
    "h2h_igwg2 = Head2HeadGridworld(gwg, player0=spp, p0_short_name='PARAMS',\n",
    "                               p0_long_name='Parameterized\\nPolicy',\n",
    "                               player1=srp, p1_short_name='EAT_NEAR',\n",
    "                               p1_long_name='Eat Nearby\\nPolicy',\n",
    "                               figsize=(2.4,3), has_fov=True, radius=2,\n",
    "                               critter_name='$\\mathit{Fishy}$')\n",
    "display(h2h_igwg2.b_fig0.canvas)\n",
    "display(h2h_igwg2.b_fig1.canvas)\n",
    "display(h2h_igwg2.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg2.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully, by taking advantage of the full perceptive field, your parameterized policy able to outperform the simple 'Eat Nearby' policy. How good can a parameterized policy of this form be, and would the best policy be deterministic or probabilistic. We're not going to answer either of those directly now (though we will later). Instead we will explore how a parameter called 'temperature' can be used to control the level of randomness in a policy. As we will see later this randomness level will play a crucial role in answering these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following code snippet (slightly adapted) appeared in the previous coding exercise. Let's take a look at it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define the necessary inputs and context\n",
    "rng = np.random.default_rng()  # Random number generator\n",
    "output_activations = rng.random(4) * 5  # An example output activations array\n",
    "softmax_temp = 1  # An example softmax temperature value\n",
    "output_struct = ['up', 'down', 'left', 'right']  # Output structure\n",
    "\n",
    "#compute softmax probabilities\n",
    "# softmax shift by max, scale by temp\n",
    "shift_scale_ex = np.exp((output_activations - np.max(output_activations)) /\n",
    "                        softmax_temp)\n",
    "sm = shift_scale_ex / shift_scale_ex.sum()  # normalized\n",
    "probs_sm = sm / sm.sum(axis=0)  # re-normalized again for fp precision issues\n",
    "# naive_probs below is a naive way to get a discrete probability distribution\n",
    "# from a real valued vector, why did we use softmax normalization instead?\n",
    "naive_probs = output_activations / np.sum(output_activations)\n",
    "action = rng.choice(output_struct, p=probs_sm)\n",
    "\n",
    "print('----------output activations----------')\n",
    "display(output_activations)\n",
    "print('----------softmax probabilities----------')\n",
    "display(probs_sm)\n",
    "print('----------naive probabilities----------')\n",
    "display(naive_probs)\n",
    "print('----------output structure----------')\n",
    "display(output_struct)\n",
    "print('----------selected action----------')\n",
    "display(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the code snippet above, the 'output_activations' is transformed into a probability distribution, which is then used to select an action. The transformation of the 'output_activations' to probabilities is done using something called a softmax function. Even though this may seem like an unnecessary complication there are many reasons, the most relevant listed here, for using softmax instead another, potentially more simple, normalization:\n",
    "\n",
    "1. **Parameter constraints:** A 'simple' normalization where 'output_activations' is divided (or normalized) its sum would requires that each value in 'output_activations' be non-negative. This places an implicit constraint on $\\mathbf{W}$, since 'output_activations' are a function of $\\mathbf{x}$, i.e. $\\mathbf{a} = \\mathbf{Wx}$. The constraints on $\\mathbf{W}$ therefore depend on the possible values of $\\mathbf{x}$. In general, we want fewer constraints on the parameters of our policy, and we want those constraints we do have to be explicit rather than implicit.\n",
    "\n",
    "2. **Control randomness level:** The softmax function has a 'temperature' parameter, which controls the level of randomness in the policy. It allows the other model parameters $\\mathbf{W}$ to be used for computing an estimate of the relative 'goodness' of the different options, while the temperature parameter controls how these relative differences translate into probabilities via the softmax function.\n",
    "3. **Invariance to constant shifts:** The softmax function produces the same probabilities for any constant shift to its inputs. In other words, it cares about the relative differences between the inputs, not absolute values. If we interpret the softmax inputs as the relative 'goodness' of each action, we are primarily interested in their relative differences. It's these differences alone that should inform our choice of action. The softmax function provides the only consistent way of achieving shift invariance in this context.\n",
    "4. **Mechanistic interpretation:** There is a biological interpretation of the softmax function. If the softmax inputs correspond to log-firing rates of neurons, and if these neurons have exponentially distributed firing times, and if the neuron that fires first, or 'wins the race', determines the chosen direction, then this process corresponds to choosing directions with softmax probabilities. While this might seem contrived, these conditions often hold true, at least approximately, in many real neural systems.\n",
    "\n",
    "More formally, the softmax function is defined as follows:\n",
    "\n",
    "Given a vector of activations $\\mathbf{a} = [a_1, a_2, ..., a_n]$, the softmax function calculates the probability of each activation as:\n",
    "\n",
    "$$\\text{softmax}(a_i) = \\frac{{e^{a_i / T}}}{{\\sum_{j=1}^{n} e^{a_j / T}}}\n",
    "$$\n",
    "\n",
    "where $T$ is the (non-negative) temperature parameter that controls the 'sharpness' of the softmax distribution. Larger $T$ values produce a more uniform, or 'flatter', distribution across actions, minimizing differences in their probabilities. Conversely, smaller $T$ values (approaching 0) create a 'sharper' distribution, amplifying differences and making the action with the highest activation much more likely to be selected.\n",
    "\n",
    "In computer implementations, $\\mathbf{a}$ is shifted down by $\\text{max}(\\mathbf{a})$ to help with numerical stability. This shift doesn't affect the final probability distribution, because the softmax function is invariant to constant shifts. However, in practice, this step helps to avoid overflow errors from exponentiating large numbers and makes the softmax function numerically robust to a broader range of input scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "That was a lot about the softmax function and temperature. Now, let's play around a bit with the temperature parameter of the softmax function to see how it impacts behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Exploring Softmax Temperature\n",
    "# @markdown Don't worry about how this code works – just **run the cell**. Then, select a softmax temperature using the slider and press start to compare the random policy with the parameterized policy using that softmax temperature.\n",
    "\n",
    "\n",
    "\n",
    "# two different ai players\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "spp = ParamPlayer(gwg, W_student) #student parameterized player\n",
    "h2h_igwg3 = Head2HeadGridworld(gwg, player0=spp, p0_short_name='PARAMS',\n",
    "  p0_long_name='Softmax\\nParameterized', player1=None,\n",
    "  p1_short_name='RANDOM', p1_long_name='Random\\nPolicy',\n",
    "  figsize=(2.4,3), has_fov=True, radius=2, critter_name='$\\mathit{Fishy}$',\n",
    "  has_temp_slider=True)\n",
    "display(h2h_igwg3.b_fig0.canvas)\n",
    "display(h2h_igwg3.b_fig1.canvas)\n",
    "display(h2h_igwg3.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg3.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Did a higher or lower temperature make the parameterized policy more like the random policy? [Solution](## \"High temperatures made the parameterized policy basically random, low temperatures made it performs differently from and better than random.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This concludes the sequence on parameterized policies. We have seen that by giving a policy a good number of parameters, we can allow it to do a lot of things. In this case, for example, we can allow it to eat nearby things and prioritize them over further away things. In a way, all of life is about having policies that work in the real world. And much of biology and evolution are about setting up such policies that work in the real world. Now we are curious, how can such policies be chosen better than humans tinkering with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to take the quiz\n",
    "comprehension_quiz = comprehension_quiz = [\n",
    "  {\"question\": \"What is a policy in the context of our Fishy example?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"A set of rules that governs how a user interacts with the system.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The term 'policy' here refers the organism's behavior, not user interaction.\"},\n",
    "      {\"answer\": \"A function or rule that determines how an organism (Fishy!) behaves in response to its perceptions.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. In the context of this example, a policy is a function that determines how the organism (Fishy) behaves in response to its perceptions.\"},\n",
    "      {\"answer\": \"A piece of legislation that regulates how the Gridworld game is played.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. While 'policy' can mean a piece of legislation in other contexts, in this context it refers to the function or rules that regulate how an organism plays the game, i.e. their behavior.\"},\n",
    "      {\"answer\": \"The nutritional habits of the organism in the environment.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. A policy in this context does determine the nutritional habits of the organism, but there is a better answer.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"Based on the observations and data presented in section 1.1.2.1, how does the perception of food appear to impact Fishy's eating behaviour\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "        {\"answer\": \"The total number of food items perceived doesn't seem to impact the likelihood of eating.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. The data suggests that the likelihood of eating appears to increase with the number of food items perceived.\"},\n",
    "        {\"answer\": \"Perceiving a higher number of food items appears to increase the likelihood of eating.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct. The data suggests that as the amount of food perceived increases, the proportion of eating events also seems to increase.\"},\n",
    "        {\"answer\": \"The likelihood of eating seems to be independent of the number of food items perceived, and is solely dependent on the location of food items.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. Although location of food items might also be an important factor, the number of food items perceived appears to also have an impact on the likelihood of eating.\"},\n",
    "        {\"answer\": \"Perception of food doesn't seem to be predictive of eating behavior at all.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. The data suggest a relationship between the number of food items perceived and the likelihood of eating.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Which of the following best describes the structure and content of Fishy's perceptions?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "        {\"answer\": \"Fishy's perceptions consist of a 12-element array with different values representing the state of the environment.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct. This is an accurate description of the structure of Fishy's perceptions and their relationship with actions.\"},\n",
    "        {\"answer\": \"Fishy's perceptions only consist of a boolean indicator 'did_eat' that shows if eating occurred.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. While 'did_eat' is a part of the data we recorded, it's not strictly part of Fishy's perceptions in this scenario\"},\n",
    "        {\"answer\": \"Fishy's perceptions only consider the four grid spaces immediately adjacent to it.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. Fishy's perception includes a wider field than just the immediately adjacent spaces.\"},\n",
    "        {\"answer\": \"Fishy's perceptions are a complex data structure that have no clear relation with Fishy's environment.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. Fishy's perceptions in this example are a relatively simple and reflect the state of the surrounding environment.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"What is the role of parameterization in defining policies?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"Parameterization simplifies the policy by removing unnecessary variables.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Parameterization doesn't necessarily simplify the policy. Instead, it provides a systematic and numeric approach to describing and adjusting a policy.\"},\n",
    "      {\"answer\": \"Parameterization allows aspects of a policy to be numerically defined and adjusted.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. By parameterizing a policy, the parameterized aspects of the policy can be numerically and systematically described and adjusted.\"},\n",
    "      {\"answer\": \"Parameterization is used to fix the outcome of a policy.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Parameterization doesn't fix the outcome of a policy.\"},\n",
    "      {\"answer\": \"Parameterization refers to the evolution of a species over time.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Parameterization will help us to think about evolutionary and learning processes, but it is not evolution itself.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"In our 'cartoon' model of a neural system, how is the probability of the organism's movement direction determined?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\"answer\": \"The movement probability is proportional to the firing strengths/rates of the direction neurons.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. In the model described, the direction neurons with higher firing strengths are more likely to determine the movement direction of the organism.\"},\n",
    "      {\"answer\": \"By the neuron in the perceptual field that signals the presence of food.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. While the presence of food does trigger activity in the perceptual neurons, whether or not this determines movement depends on how the perceptual field neurons are connected to the direction neurons.\"},\n",
    "      {\"answer\": \"By averaging the signals from all four direction neurons.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The movement direction is not determined by averaging the signals.\"},\n",
    "      {\"answer\": \"By the strength of the predator avoidance instinct in the organism.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Any instincts the organism has are encoded in the weights between its perception neurons and direction neurons.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"What does the weight matrix W represent in the context of the neural model discussed?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"The strength of synaptic connections between perception neurons and direction neurons.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. In the model discussed, the weight matrix W represents the strength of synaptic connections between the perception neurons and the direction neurons.\"},\n",
    "      {\"answer\": \"The activation strengths of the muscle neurons.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. While the muscle/direction neuron activations are calculated based on the weight matrix and the perception neuron activity, the weight matrix itself does not directly represent these activations.\"},\n",
    "      {\"answer\": \"The current state of the organism's perceptive field.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The weight matrix does not represent the organism's perceptual state. Rather, it reflects the strength of the synaptic connections between the perceptive field and the direction neurons.\"},\n",
    "      {\"answer\": \"The physical weight of the organism in the environment.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"In the softmax function, what role does the temperature parameter play?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"It determines the highest possible activation strength.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The temperature parameter doesn't set an upper limit on activation strengths.\"},\n",
    "      {\"answer\": \"It controls the level of randomness in a distribution derived from activation strengths, influencing the 'sharpness' of the softmax distribution.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. The temperature parameter influences the 'sharpness' of the softmax distribution, controlling the level of randomness.\"},\n",
    "      {\"answer\": \"It adjusts the weight matrix to avoid overflow errors.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The temperature parameter is not related to adjusting the weight matrix to avoid overflow errors.\"},\n",
    "      {\"answer\": \"It represents the physical temperature of the environment the organism is in.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The temperature parameter in the softmax function is unrelated to the physical temperature of the environment.\"}\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "display_quiz(comprehension_quiz)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "P1C1_Sequence2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
