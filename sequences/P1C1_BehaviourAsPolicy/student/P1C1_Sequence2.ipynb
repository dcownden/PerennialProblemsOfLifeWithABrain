{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C1_BehaviourAsPolicy/student/P1C1_Sequence2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/student/P1C1_Sequence2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is a second test for an upcoming text book on computational neuroscience from an optimization/learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as a learning algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do.\n",
    "\n",
    "# **Part 1 Behaviour, Environments and Optimization: Evolution and Learning**\n",
    "\n",
    "### **Animals are adapted to their specific environments; their behaviour is best understood within the context of their evolutionary environment.**\n",
    "\n",
    "### Objective: Part 1 of the book aims to introduce the fundamental concepts of\n",
    "* **environment**, the (statistical) properties of where an organism lives\n",
    "* **behaviour**, the statistics of what the organism does\n",
    "* **optimization**, how learning and evolution shape an organism's behaviour to make it better suited to its environment\n",
    "\n",
    "This very much is the core of why we are writing this book: we can view pretty much anything happening in the brain (and biology) as being part of a process that brings about improvement in this sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# Chapter 1.1 Behaviour as a Policy in an Environmental Context\n",
    "\n",
    "### Objective: This chapter develops examples of how behaviour is described and evaluated in relation to its [goodness](## \"This is a very loaded term, to be unpacked carefully later\") within a specific environmental niche.\n",
    "\n",
    "You will learn:\n",
    "*   What is a policy? A policy is a formalization of behaviour as a function that takes an organism's experiences of their environment as an input and outputs the organism's actions.\n",
    "*   What is a good policy? The rewards and other environmental signals resulting from the organism's actions in the environment are integrated into a Loss/Objective function to evaluatate, and potentially improve, a policy.\n",
    "*   What is stochasticity? Both the environment and an organism's behavior can contain random elements. This randomness can pose challenges when evaluating policies as it becomes difficult to determine whether poor outcomes are due the policy itself or simply bad luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **Sequence 1.1.2: Thinking of What Receptors and Muscles Do as a Policy**\n",
    "\n",
    "### Objective: In this sequence, we will continue to use the Gridworld environment-organism system to see how behaviour can be abstracted as input --> compute --> output process, and how such processes can be formalized as a **policy** with **parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Picture showing receptor/input, brain/compute, musceles/output embedded in the environment feedback loop. (The following is a place holder for our actual image, but it's close to the right thing.)\n",
    "![Reinforcement_learning_diagram.svg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "\n",
    "!pip install ipympl vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "from scipy.spatial.distance import cdist\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/every_util.py\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Make sure we got a valid response\n",
    "if response.status_code == 200:\n",
    "  with open(\"every_util.py\", 'w') as f:\n",
    "    f.write(response.content.decode())\n",
    "else:\n",
    "  print(f'Failed to download {url}')\n",
    "\n",
    "#%run every_util.py\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C1_S2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "# define some util functions again locally for prototyping,\n",
    "# push into utils later\n",
    "\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for futher use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Labels for major ticks\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "\n",
    "  # Minor ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "\n",
    "  ax.xaxis.tick_top()\n",
    "\n",
    "  # Gridlines based on minor ticks\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "\n",
    "  # Remove minor ticks\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  ax.invert_yaxis()\n",
    "  if title is not None:\n",
    "    fig.suptitle(title)\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=150, marker='o', color='red', label='Food')\n",
    "  rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "  #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc,\n",
    "                 critter=None, critter_name='Critter'):\n",
    "  \"\"\"\n",
    "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h',\n",
    "                         color='blue', label=critter_name)\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov, fov=None):\n",
    "  \"\"\"\n",
    "  Plots a mask on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter: ndarry(int) (row,col) of the critter\n",
    "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib Image object of mask, either new if no handle\n",
    "    was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
    "  mask_array = np.ones((n_rows, n_cols, 4))\n",
    "  mask_array[:, :, :3] = 0.5  # light grey color\n",
    "  if has_fov == True:\n",
    "    mask_array[:, :, 3] = 0.5  # 50% opacity\n",
    "    # Create arrays representing the row and column indices\n",
    "    rows = np.arange(n_rows)[:, np.newaxis]\n",
    "    cols = np.arange(n_cols)[np.newaxis, :]\n",
    "    # Iterate over each critter location\n",
    "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
    "    # Set the region within the specified radius around the critter to transparent\n",
    "    mask_array[dist <= radius, 3] = 0\n",
    "  else:\n",
    "    mask_array[:, :, 3] = 0\n",
    "\n",
    "  if fov is None:\n",
    "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
    "  else:\n",
    "    fov.set_data(mask_array)\n",
    "\n",
    "  return fov\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld Board Object\n",
    "# define some util functions again locally for prototyping,\n",
    "# push into utils later\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game board that\n",
    "  define the logic of the game.\n",
    "  Board state is tracked as a triple (pieces, scores, rounds_left)\n",
    "  pieces: batch_size x n_rows x n_cols np.array\n",
    "  scores: batch_size np.array\n",
    "  rounds_left: batch_size np.array\n",
    "\n",
    "  Pieces are interpreted as:\n",
    "  1=critter, -1=food, 0=empty\n",
    "\n",
    "  First dim is batch, second dim row , third is col, so pieces[0][1][7]\n",
    "  is the square in row 2, in column 8 of the first board in the batch of boards\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_food=10, lifetime=30,\n",
    "               rng = None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement\n",
    "\n",
    "    Args:\n",
    "      n_rows: int\n",
    "      n_cols: int\n",
    "      num:    int, number of samples to generate, should\n",
    "              throw an error ifnum <= n_rows x n_cols\n",
    "\n",
    "    Optional Keyword Args\n",
    "      rng:  instance of numpy.random's default random number generator\n",
    "            (to enable reproducibility)\n",
    "\n",
    "    Returns:\n",
    "      int_loc:  ndarray(int) of flat indices for the grid\n",
    "      rc_index: (ndarray(int), ndarray(int)) a pair of arrays the first\n",
    "        giving the row indices, the second giving the col indices, useful\n",
    "        for indexing an n_rows by n_cols numpy array\n",
    "      rc_plotting: ndarray(int) num x 2, same rc coordinates but structured\n",
    "        in a way that matplotlib likes\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"Set up starting board using game parameters\"\"\"\n",
    "    #set rounds_left and score\n",
    "    self.rounds_left = np.ones(self.batch_size) * self.lifetime\n",
    "    self.scores = np.zeros(self.batch_size)\n",
    "    # create an empty board array.\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols))\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+1 because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(\n",
    "        self.n_rows, self.n_cols, self.num_food+1)\n",
    "      # critter random start location\n",
    "      self.pieces[(ii, rc_idx[0][0], rc_idx[1][0])] = 1\n",
    "      # food random start locations\n",
    "      self.pieces[(ii, rc_idx[0][1:], rc_idx[1][1:])] = -1\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\" board is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    self.pieces = board['pieces'].copy()\n",
    "    self.scores = board['scores'].copy()\n",
    "    self.rounds_left = board['rounds_left'].copy()\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state, which is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves):\n",
    "    \"\"\"\n",
    "    Updates the state of the board given the moves made.\n",
    "\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord.\n",
    "\n",
    "    Note:\n",
    "      Assumes that there is exactly one valid move for each board in the\n",
    "      batch of boards. i.e. it does't check for bounce/reflection on edges,\n",
    "      or for multiple move made on the same board. It only checks for eating\n",
    "      food and adds new food when appropriate. Invalid moves could lead to\n",
    "      illegal teleporting behavior, critter dublication, or index out of range\n",
    "      errors.\n",
    "    This assumes the move is valid, i.e. doesn't check for\n",
    "    bounce/reflection on edges, it only checks eating and adds new food,\n",
    "    so invalid moves could lead to illegal teleporting behaviour or index out\n",
    "    of range errors\n",
    "    \"\"\"\n",
    "    #critters leave their spots\n",
    "    self.pieces[self.pieces==1] = 0\n",
    "    #which critters have food in their new spots\n",
    "    eats_food = self.pieces[moves] == -1\n",
    "    # some critters eat and their scores go up\n",
    "    self.scores = self.scores + eats_food\n",
    "\n",
    "    num_empty_after_eat = self.n_rows*self.n_cols - self.num_food\n",
    "    # -1 for the critter +1 for food eaten\n",
    "    # which boards in the batch had eating happen\n",
    "    g_eating = np.where(eats_food)[0]\n",
    "    if np.any(eats_food):\n",
    "      # add random food to replace what is eaten\n",
    "      possible_new_locs = np.where(np.logical_and(\n",
    "          self.pieces == 0, #the spot is empty\n",
    "          eats_food.reshape(self.batch_size, 1, 1))) #food eaten on that board\n",
    "      food_sample_ = self.rng.choice(num_empty_after_eat,\n",
    "                                     size=np.sum(eats_food))\n",
    "      food_sample = food_sample_ + np.arange(len(g_eating))*num_empty_after_eat\n",
    "      assert np.all(self.pieces[(possible_new_locs[0][food_sample],\n",
    "                                 possible_new_locs[1][food_sample],\n",
    "                                 possible_new_locs[2][food_sample])] == 0)\n",
    "      #put new food on the board\n",
    "      self.pieces[(possible_new_locs[0][food_sample],\n",
    "                   possible_new_locs[1][food_sample],\n",
    "                   possible_new_locs[2][food_sample])] = -1\n",
    "    # put critters in new positions\n",
    "    self.pieces[moves] = 1.0\n",
    "    self.rounds_left = self.rounds_left - 1\n",
    "    assert np.all(self.pieces.sum(axis=(1,2)) == ((self.num_food * -1) + 1))\n",
    "\n",
    "\n",
    "  def get_legal_moves(self):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter, taking into acount\n",
    "    bouncing/reflection at edges,\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offstet on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    #apply all possible offsets to each game\n",
    "    moves = np.stack([\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0,  1, 0])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, -1, 0])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, 0,  1])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, 0, -1])]*self.batch_size).T]).swapaxes(0,2)\n",
    "\n",
    "    #check bounces at boundaries\n",
    "    moves[:,1,:] = np.where(moves[:,1,:] >=\n",
    "                            self.n_rows, self.n_rows-2, moves[:,1,:])\n",
    "    moves[:,2,:] = np.where(moves[:,2,:] >=\n",
    "                            self.n_cols, self.n_cols-2, moves[:,2,:])\n",
    "    moves[:,1,:] = np.where(moves[:,1,:] < 0, 1, moves[:,1,:])\n",
    "    moves[:,2,:] = np.where(moves[:,2,:] < 0, 1, moves[:,2,:])\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def get_perceptions(self, radius):\n",
    "    \"\"\"\n",
    "    Generates a vector representation of of the critter perceptions, oriented\n",
    "    around the critter. get_precept_filter is used to get a canonical version\n",
    "    of the board with unknonw positions ocluded\n",
    "\n",
    "    Args:\n",
    "      radius: int, how many grid squared the critter can see around it\n",
    "        using L1  (Manhattan/cityblock) distance\n",
    "\n",
    "    Returns:\n",
    "      A batch_size x 2*radius*(radius+1) + 1, giving the values\n",
    "      of the percept reading left to right, top to bottom over the board,\n",
    "      for each board in the batch\n",
    "    \"\"\"\n",
    "    # define the L1 ball mask\n",
    "    diameter = radius*2+1\n",
    "    mask = np.zeros((diameter, diameter), dtype=bool)\n",
    "    mask_coords = np.array([(i-radius, j-radius)\n",
    "      for i in range(diameter)\n",
    "        for j in range(diameter)])\n",
    "    mask_distances = cdist(mask_coords, [[0, 0]],\n",
    "                           'cityblock').reshape(mask.shape)\n",
    "    mask[mask_distances <= radius] = True\n",
    "    mask[radius,radius] = False  # exclude the center\n",
    "\n",
    "    # pad the array\n",
    "    padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "     (radius, radius)), constant_values=-2)\n",
    "\n",
    "    # get locations of critters\n",
    "    critter_locs = np.argwhere(padded_arr == 1)\n",
    "\n",
    "    percepts = []\n",
    "    for critter_loc in critter_locs:\n",
    "      b, r, c = critter_loc\n",
    "      surrounding = padded_arr[b, r-radius:r+radius+1, c-radius:c+radius+1]\n",
    "      percept = surrounding[mask]\n",
    "      percepts.append(percept)\n",
    "    return(np.array(percepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld Game Object\n",
    "# define some util functions again locally for prototyping,\n",
    "# push into utils later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "  square_content = {\n",
    "      -1: \"X\", #Food\n",
    "      +0: \"-\", #Nothing\n",
    "      +1: \"O\"  #Critter\n",
    "      }\n",
    "\n",
    "\n",
    "  def get_square_piece(self, piece):\n",
    "    return GridworldGame.square_content[piece]\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size, n_rows, n_cols,\n",
    "               num_food, lifetime, rng=None):\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(self.batch_size, self.n_rows, self.n_cols,\n",
    "                       self.num_food, self.lifetime, rng=self.rng)\n",
    "    return b.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_size(self):\n",
    "    \"\"\"Shape of a sinlge board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,x,y coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 2-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to x,y indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return (board['pieces'][g].tobytes() + board['scores'][g].tobytes() +\n",
    "            board['rounds_left'][g].tobytes())\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.square_content[square] for row in board['pieces'][g]\n",
    "                       for square in row])\n",
    "    board_s = board_s + '_' + str(board['scores'][g])\n",
    "    board_s = board_s + '_' + str(board['rounds_left'][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board['rounds_left'].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g):\n",
    "    \"\"\"Dispalys the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, \"|\", end=\"\")    # Print the row\n",
    "      for r_ in range(self.n_rows):\n",
    "        piece = board['pieces'][g,c_,r_]    # Get the piece to print\n",
    "        print(GridworldGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board['rounds_left'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g):\n",
    "    return np.squeeze(np.array(np.where(board['pieces'][g]==1)))\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g,\n",
    "                 fig=None, ax=None, critter=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war\n",
    "                 radius=2,\n",
    "                 figsize=(6,5),\n",
    "                 critter_name='Critter',\n",
    "                 title=None):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "    rc_critter = self.get_critter_rc(board, g)\n",
    "\n",
    "    if critter is None:\n",
    "      critter = plot_critter(fig, ax, rc_critter,\n",
    "                             critter_name=critter_name)\n",
    "    else:\n",
    "      critter = plot_critter(fig, ax, rc_critter,\n",
    "                             critter, critter_name=critter_name)\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] == -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "\n",
    "    if fov is None:\n",
    "      fov = plot_fov(fig, ax, rc_critter, n_rows, n_cols,\n",
    "                     radius, has_fov)\n",
    "    else:\n",
    "      fov = plot_fov(fig, ax, rc_critter, n_rows, n_cols,\n",
    "                     radius, has_fov, fov)\n",
    "\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\")\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center')\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter, food, fov\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                       self.num_food, self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves()\n",
    "    valids = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for ii, g in enumerate(legal_moves[:,1:,:]):\n",
    "      for x,y in zip(g[0],g[1]):\n",
    "        valids[ii, x*self.n_cols+y] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, g):\n",
    "    \"\"\"Dispaly possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    A=np.reshape(self.get_valid_actions(board)[g], (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, \"|\", end=\"\")    # Print the row\n",
    "      for row in range(self.n_rows):\n",
    "        piece = A[col][row]    # Get the piece to print\n",
    "        print(GridworldGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                       self.num_food, self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    return(b.get_perceptions(radius))\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: list of integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board tiple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parellel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if board['rounds_left'][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      moves = self.actions_to_moves(actions)\n",
    "      b = GridworldBoard(len(actions), n_rows, n_cols,\n",
    "                         self.num_food, self.lifetime,\n",
    "                         rng=self.rng)\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        b.set_state(board)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        newPieces = np.array([board['pieces'][ai].copy() for ai in a_indx])\n",
    "        newScores = np.array([board['scores'][ai].copy() for ai in a_indx])\n",
    "        newrounds_left = np.array([board['rounds_left'][ai].copy() for ai in a_indx])\n",
    "        b.set_state((newPieces, newScores, newrounds_left))\n",
    "      b.execute_moves(moves)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                       self.num_food, self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.critter_direction_to_move(board, offsets)\n",
    "    b.execute_moves(moves)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1)}\n",
    "    critter_locs = np.where(board['pieces'] == 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right'\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    critter_locs = np.where(board['pieces'] == 1)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      batch size np.array of -1 if not ended, and scores for\n",
    "      each game in the batch if it is ended\n",
    "    \"\"\"\n",
    "    rounds_left = board['rounds_left']\n",
    "    scores = board['scores']\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Interactive Gridworld Game Widgets\n",
    "# define some util functions again locally for prototyping,\n",
    "# push into utils later\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game):\n",
    "    self.game = game\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates random game play\n",
    "    Args:\n",
    "      a board state (pieces, scores, rounds_left)\n",
    "    Returns:\n",
    "      a: [int] a batch_size array randomly chosen actions\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board)\n",
    "    action_size = self.game.get_action_size()\n",
    "    # Compute the probability of each move being played (random player means\n",
    "    # this should be uniform for valid moves, 0 for others)\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "    # Pick a random action based on the probabilities\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii]) for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None, has_fov=False,\n",
    "               radius=2, collect_fov_data=False,\n",
    "               figsize=(6,5), critter_name='Critter', player='human'):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_name: a string that determines what the critter is called in the\n",
    "       plot legend\n",
    "      player: either 'human', None, or a player object with a play method\n",
    "       if 'human' use buttons, if None make a RandomPlayer object to play\n",
    "       the game, otherwise use the player object with start button.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.figsize = figsize\n",
    "    self.critter_name = critter_name\n",
    "    if player is None:\n",
    "      self.player = RandomPlayer(self.gwg)\n",
    "    else:\n",
    "      self.player = player\n",
    "    self.final_scores = []\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    if self.collect_fov_data is True:\n",
    "      # keep raw records of percept and eating for manipulation later\n",
    "      self.percept_eat_records = []\n",
    "      # keep data in contingency table of how many food items were in\n",
    "      # the percept, and whether or not food was eaten\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "        width = '230px', min_width='230px', max_width='250px',\n",
    "        min_height='160px', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "        min_width='200px', max_width='210px',\n",
    "        min_height='100px', overflow='auto'))\n",
    "    self.fov_eat_table_display = widgets.Output(layout=widgets.Layout(\n",
    "        min_width='400px', min_height='300px', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\")\n",
    "    self.down_button = widgets.Button(description=\"Down\")\n",
    "    self.left_button = widgets.Button(description=\"Left\")\n",
    "    self.right_button = widgets.Button(description=\"Right\")\n",
    "    self.start_button = widgets.Button(description=\"Start\")\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if self.collect_fov_data == True and self.player != 'human':\n",
    "      self.legend_type = None\n",
    "      # do legend seperately if showing observations and no human player\n",
    "      (self.b_fig, self.b_ax, self.b_critter, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, 0, legend_type='separate', figsize=self.figsize,\n",
    "          has_fov=self.has_fov, radius=self.radius,\n",
    "          critter_name=self.critter_name)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_critter, self.b_food, self.b_fov\n",
    "        ) = self.gwg.plot_board(self.board_state, 0, has_fov=self.has_fov,\n",
    "                                radius=self.radius, figsize=self.figsize,\n",
    "                                critter_name=self.critter_name)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.player == 'human':\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "      self.output_and_score = widgets.HBox([self.scoreboard, self.output])\n",
    "      self.no_table_final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                                  self.output_and_score])\n",
    "      if self.collect_fov_data == True:\n",
    "        # human player collecting data\n",
    "        self.final_display = widgets.HBox([self.no_table_final_display,\n",
    "                                           self.fov_eat_table_display])\n",
    "      else: # self.collect_fov_data == False:\n",
    "        # human player not collecting data\n",
    "        self.final_display = self.no_table_final_display\n",
    "    else: # player is some kind of ai\n",
    "      if self.collect_fov_data == True:\n",
    "        # an ai player with recording\n",
    "        # in this case legend is seperate\n",
    "        self.V_score_start_output_legend = widgets.VBox([self.scoreboard,\n",
    "          self.start_button,  self.output, self.b_fig_legend.canvas])\n",
    "        self.V_board_table = widgets.VBox([self.b_fig.canvas,\n",
    "                                           self.fov_eat_table_display])\n",
    "        self.final_display = widgets.HBox([self.V_board_table,\n",
    "                                           self.V_score_start_output_legend])\n",
    "      else:\n",
    "        # an ai player without recording\n",
    "        self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "        self.final_display = widgets.VBox([\n",
    "            self.b_fig.canvas, self.H_score_output_start])\n",
    "\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['High Score:', '--'],\n",
    "               ['Last Score:', '--'],\n",
    "               ['Average Score:', '--']]\n",
    "      print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.player == 'human':\n",
    "        print('Click a button to start playing')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.fov_eat_table_display:\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    old_score = old_board['scores'][0]\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                         self.gwg.num_food, self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "      b.set_state(old_board)\n",
    "      percept = b.get_perceptions(self.radius)[0]\n",
    "\n",
    "    if (self.player == 'human'):\n",
    "      direction = which_button\n",
    "    else:\n",
    "      a_player, _, _ = self.player.play(old_board)\n",
    "      a_player = self.gwg.action_to_critter_direction(old_board, a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = a_player[0]\n",
    "\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "          self.board_state, [direction])\n",
    "    new_score = self.board_state['scores'][0]\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = self.gwg.lifetime - rounds_left\n",
    "    if new_score > old_score: #eating happened\n",
    "      eating_string = \"They ate the food there!\"\n",
    "      did_eat = 1\n",
    "    else: #eating didn't happen\n",
    "      eating_string = \"There's no food there.\"\n",
    "      did_eat = 0\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0)\n",
    "    (self.b_fig, self.b_ax, self.b_critter, self.b_food, self.b_fov\n",
    "     ) = self.gwg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                             self.b_critter, self.b_food, self.b_fov,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=self.legend_type)\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = {'perception': percept.copy(),\n",
    "                  'state': old_board,\n",
    "                  'did_eat': bool(did_eat)}\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1) # number of food items in FoV\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      print(\"The critter (tried) to move \" + direction +\n",
    "            \" and is now at ({}, {}).\".format(row,col))\n",
    "      print(eating_string)\n",
    "      print(\"Rounds Left: {}\\nFood Eaten: {}\\nFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_score, new_score / num_moves))\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_score)\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_score))\n",
    "        print('Resetting the board for another game')\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax, self.b_critter, self.b_food, self.b_fov\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                               self.b_critter, self.b_food, self.b_fov,\n",
    "                               has_fov=self.has_fov, radius=self.radius,\n",
    "                               legend_type=self.legend_type)\n",
    "    with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = [\n",
    "            ['High Score:', str(np.max(np.array(self.final_scores)))],\n",
    "            ['Last Score:', str(self.final_scores[-1])],\n",
    "            ['Average Score',\n",
    "             '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "        else:\n",
    "          table = [['High Score:', '--'],\n",
    "                   ['Last Score:', '--'],\n",
    "                   ['Average Score:', '--']]\n",
    "        print(tabulate(table))\n",
    "    with self.fov_eat_table_display:\n",
    "      clear_output()\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.button_output_update('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.button_output_update('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.button_output_update('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.button_output_update('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    for ii in range(self.gwg.lifetime):\n",
    "      self.button_output_update('tbd')\n",
    "    self.start_button.disabled = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Head2HeadGridworld():\n",
    "  \"\"\"\n",
    "  A widget for interacting with a gridworld game while an artificial player\n",
    "  plays on an identical board or watching two artificial players play, again\n",
    "  with identical starting positions (though RNG not synched between the two\n",
    "  boards, so not like duplicate bridge)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None,\n",
    "               player0='human', p0_short_name='YOU', p0_long_name=None,\n",
    "               player1=None, p1_short_name='THEM', p1_long_name=None,\n",
    "               has_fov=False, radius=2, collect_fov_data=False,\n",
    "               critter_name='Critter', figsize=(5,4.5),\n",
    "               has_temp_slider=False):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batch_size of 2\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      player0: object with a play method that takes a board state\n",
    "        as an argument and returns a move. If none will use a random player\n",
    "        if the special string 'human' is passed make arrow keys for that player\n",
    "      player1: same deal as player0, never more than 1 human player\n",
    "      has_fov: bool, whether or not to display field of view around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "    \"\"\"\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.final_scores = []\n",
    "    self.player0 = player0\n",
    "    self.p0_short_name = p0_short_name\n",
    "    self.p0_long_name = p0_long_name\n",
    "    self.player1 = player1\n",
    "    self.p1_short_name = p1_short_name\n",
    "    self.p1_long_name = p1_long_name\n",
    "    self.no_human = True\n",
    "    if self.player0 == 'human':\n",
    "      assert self.player1 != 'human'\n",
    "      self.no_human = False\n",
    "    if self.player1 == 'human':\n",
    "      assert self.player0 != 'human'\n",
    "      self.no_human = False\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.critter_name = critter_name\n",
    "    self.figsize = figsize\n",
    "    if player0 is None:\n",
    "      self.player0 = RandomPlayer(self.gwg)\n",
    "    else:\n",
    "      self.player0 = player0\n",
    "    if player1 is None:\n",
    "      self.player1 = RandomPlayer(self.gwg)\n",
    "    else:\n",
    "      self.player1 = player1\n",
    "    self.has_temp_slider = has_temp_slider\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      self.percept_eat_records = []\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    #print(self.board_state)\n",
    "\n",
    "    # both players have same starting board\n",
    "    self.board_state['pieces'][1] = self.board_state['pieces'][0].copy()\n",
    "\n",
    "    # Initialize widgets and buttons\n",
    "    if self.has_temp_slider:\n",
    "      self.sft_slider_label = widgets.Label(value='Softmax Temperature')\n",
    "      self.sft_slider = widgets.FloatSlider(value=1.0, min=0.05,\n",
    "                                            max=5.0, step=0.05)\n",
    "      self.softmax_temp_slider = widgets.VBox([self.sft_slider_label,\n",
    "                                               self.sft_slider])\n",
    "    self.output0 = widgets.Output(layout=widgets.Layout(\n",
    "        width='250px', height='160px'))\n",
    "    self.output1 = widgets.Output(layout=widgets.Layout(\n",
    "        width='250px', height='160px'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "        min_width='300px', max_width='310px',\n",
    "        min_height='100px', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\")\n",
    "    self.down_button = widgets.Button(description=\"Down\")\n",
    "    self.left_button = widgets.Button(description=\"Left\")\n",
    "    self.right_button = widgets.Button(description=\"Right\")\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(margin='10px 0 0 0'))  # 10px top margin cludge\n",
    "\n",
    "    #set up buttons and outputs and layouts\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    plt.ioff()\n",
    "    (self.b_fig0, self.b_ax0, self.b_critter0, self.b_food0, self.b_fov0,\n",
    "     self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "        self.board_state, 0, legend_type='separate', figsize=self.figsize,\n",
    "        has_fov=self.has_fov, radius=self.radius,\n",
    "        critter_name=self.critter_name, title=self.p0_long_name)\n",
    "    (self.b_fig1, self.b_ax1, self.b_critter1, self.b_food1, self.b_fov1\n",
    "     ) = self.gwg.plot_board(self.board_state, 1, legend_type=None,\n",
    "                             figsize=self.figsize, has_fov=self.has_fov,\n",
    "                             radius=self.radius, title=self.p1_long_name)\n",
    "    # player 0 is human\n",
    "    self.board_buttons_and_output0 = widgets.VBox(\n",
    "      [self.b_fig0.canvas, self.buttons, self.output0])\n",
    "    # player 1 is human\n",
    "    self.board_buttons_and_output1 = widgets.VBox(\n",
    "      [self.b_fig1.canvas, self.buttons, self.output1])\n",
    "    # non human players\n",
    "    self.board_and_output0 = widgets.VBox([self.b_fig0.canvas, self.output0])\n",
    "    self.board_and_output1 = widgets.VBox([self.b_fig1.canvas, self.output1])\n",
    "\n",
    "    self.legend_and_scores = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                           self.scoreboard])\n",
    "    if self.has_temp_slider:\n",
    "      self.legend_scores_start = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                               self.scoreboard,\n",
    "                                               self.softmax_temp_slider,\n",
    "                                               self.start_button])\n",
    "    else:\n",
    "      self.legend_scores_start = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                               self.scoreboard,\n",
    "                                               self.start_button])\n",
    "    if self.player0 == 'human':\n",
    "      self.final_display = widgets.HBox([self.board_buttons_and_output0,\n",
    "                                         self.legend_and_scores,\n",
    "                                         self.board_and_output1])\n",
    "    elif self.player1 == 'human':\n",
    "      self.final_display = widgets.HBox([self.board_and_output0,\n",
    "                                         self.legend_and_scores,\n",
    "                                         self.board_buttons_and_output1])\n",
    "    else: # no human player\n",
    "      self.final_display = widgets.HBox([self.board_and_output0,\n",
    "                                          self.legend_scores_start,\n",
    "                                          self.board_and_output1])\n",
    "    # initial text outputs\n",
    "    # if there's a temp slider check who, if anyone uses it\n",
    "    self.p0_uses_temp = False\n",
    "    self.p1_uses_temp = False\n",
    "    if self.has_temp_slider:\n",
    "      if self.player0=='human':\n",
    "        pass\n",
    "      else:\n",
    "        try:\n",
    "          _ = self.player0.play(self.board_state, temp=1.0)\n",
    "          self.p0_uses_temp = True\n",
    "        except TypeError: pass\n",
    "      if self.player1 == 'human':\n",
    "        pass\n",
    "      else:\n",
    "        try:\n",
    "          _ = self.player1.play(self.board_state, temp=1.0)\n",
    "          self.p1_uses_temp = True\n",
    "        except TypeError: pass\n",
    "      if not self.p0_uses_temp and not self.p1_uses_temp:\n",
    "        with self.output0:\n",
    "          print(\"Warning: neither player supports temperature adjustment. \"\n",
    "                \"The slider will have no effect.\")\n",
    "    with self.output0:\n",
    "      if self.no_human == False:\n",
    "        print('Click a button to start.')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.scoreboard:\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Avg. Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    old_scores = old_board['scores']\n",
    "    if self.player0 == 'human':\n",
    "      a_player0 = which_button\n",
    "    else:\n",
    "      if self.p0_uses_temp:\n",
    "        a_player0_, _, _ = self.player0.play(old_board,\n",
    "                                             temp=self.sft_slider.value)\n",
    "      else:\n",
    "        a_player0_, _, _ = self.player0.play(old_board)\n",
    "      a_player0_ = self.gwg.action_to_critter_direction(old_board, a_player0_)\n",
    "      a_player0 = a_player0_[0]\n",
    "    if self.player1 == 'human':\n",
    "      a_player1 = which_button\n",
    "    else:\n",
    "      if self.p1_uses_temp:\n",
    "        a_player1_, _, _ = self.player1.play(old_board,\n",
    "                                             temp=self.sft_slider.value)\n",
    "      else:\n",
    "        a_player1_, _, _ = self.player1.play(old_board)\n",
    "      a_player1_ = self.gwg.action_to_critter_direction(old_board, a_player1_)\n",
    "      a_player1 = a_player1_[1]\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                         self.gwg.num_food, self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "      b.set_state(old_board)\n",
    "      percept = b.get_perceptions(self.radius)\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "        self.board_state, [a_player0, a_player1])\n",
    "    new_scores = self.board_state['scores']\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = self.gwg.lifetime - rounds_left\n",
    "\n",
    "    if new_scores[0] > old_scores[0]:\n",
    "      eating_string0 = \"They ate the food there!\"\n",
    "    else:\n",
    "      eating_string0 = \"There's no food there.\"\n",
    "    if new_scores[1] > old_scores[1]:\n",
    "      eating_string1 = \"They ate the food there!\"\n",
    "    else:\n",
    "      eating_string1 = \"There's no food there.\"\n",
    "    did_eat = int(new_scores[0] > old_scores[0])\n",
    "\n",
    "    row0, col0 = self.gwg.get_critter_rc(self.board_state, 0)\n",
    "    (self.b_fig0, self.b_ax0, self.b_critter0, self.b_food0, self.b_fov0\n",
    "     ) = self.gwg.plot_board(self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "                             self.b_critter0, self.b_food0, self.b_fov0,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "    row1, col1 = self.gwg.get_critter_rc(self.board_state, 1)\n",
    "    (self.b_fig1, self.b_ax1, self.b_critter1, self.b_food1, self.b_fov1\n",
    "     ) = self.gwg.plot_board(self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "                             self.b_critter1, self.b_food1, self.b_fov1,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "\n",
    "    with self.output0:\n",
    "      clear_output()\n",
    "      if self.player0 == 'human':\n",
    "        print(\"You clicked the \" + which_button +\n",
    "              \" button and your critter is now at ({}, {}).\".format(row0,col0))\n",
    "      else:\n",
    "        print(\"This player (tried) to move \" + a_player0 +\n",
    "              \" and is now at ({}, {}).\".format(row0,col0))\n",
    "      print(eating_string0)\n",
    "      print(\"Rounds Left: {} \\nFood Eaten: {} \\nFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_scores[0], new_scores[0] / num_moves))\n",
    "    with self.output1:\n",
    "      clear_output()\n",
    "      if self.player1 == 'human':\n",
    "        print(\"You clicked the \" + which_button +\n",
    "              \" button and your critter is now at ({}, {}).\".format(row1,col1))\n",
    "      else:\n",
    "        print(\"This player (tried) to move \" + a_player1 +\n",
    "              \" and is now at ({}, {}).\".format(row1,col1))\n",
    "      print(eating_string1)\n",
    "      print(\"Rounds Left: {} \\nFood Eaten: {} \\nFood Per Move: {:.2f}\".format(\n",
    "        rounds_left, new_scores[1], new_scores[1] / num_moves))\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = (percept.copy(), did_eat, old_board)\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1, axis=1)\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_scores)\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "      self.board_state['pieces'][1] = self.board_state['pieces'][0].copy()\n",
    "      (self.b_fig0, self.b_ax0, self.b_critter0, self.b_food0, self.b_fov0\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "                             self.b_critter0, self.b_food0, self.b_fov0,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "      (self.b_fig1, self.b_ax1, self.b_critter1, self.b_food1, self.b_fov1\n",
    "       ) = self.gwg.plot_board(self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "                             self.b_critter1, self.b_food1, self.b_fov1,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "      with self.output0:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores[0]))\n",
    "        print('Resetting the board for another game')\n",
    "      with self.output1:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores[1]))\n",
    "        print('Resetting the board for another game')\n",
    "    with self.scoreboard:\n",
    "      clear_output()\n",
    "      self.b_fig_legend.canvas.draw()\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      if len(self.final_scores) > 0:\n",
    "        table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', str(np.max(np.array(self.final_scores)[:,0])),\n",
    "                          str(np.max(np.array(self.final_scores)[:,1]))],\n",
    "          ['Last Score:', str(self.final_scores[-1][0]),\n",
    "                          str(self.final_scores[-1][1])],\n",
    "          ['Average Score',\n",
    "            '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,0])),\n",
    "            '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,1]))]]\n",
    "      else:\n",
    "        table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Average Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.button_output_update('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.button_output_update('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.button_output_update('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.button_output_update('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    if self.has_temp_slider:\n",
    "      self.softmax_temp_slider.disabled = True\n",
    "    for ii in range(self.gwg.lifetime):\n",
    "      self.button_output_update('tbd')\n",
    "    self.start_button.disabled = False\n",
    "    if self.has_temp_slider:\n",
    "      self.softmax_temp_slider.disabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.2.1: Limited Perception (fog of war)\n",
    "\n",
    "In the previous sequence you and the organism were effectively omniscient, i.e. had both complete and accurate knowledge of the state of the environment. This is also the case in games like go or chess where all relevant information is available. However, these scenarios of total knowledge are unusual, artificial exceptions. In contrast, most living organisms face a situation where the relevant state of the environment is perceived only in part and with uncertainty. To illustrate this, let's introduce a new organism, *Fishy*. Drawing inspiration from weakly electric fish (see for example the work of Malcolm MacIver), *Fishy*, can only perceive the 12 cells of the Gridworld immediately adjacent to it (those cells within a radius of 2 using $L_1$, or 'city block' distance). For now, *Fishy* aimlessly swims around on the grid. Let's see what that looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Random Movement with Limited Perception\n",
    "# @markdown Don't worry about how this code works – just **run this cell** and press the start button to see what *Fishy's* limited field of perception looks like.\n",
    "rng = np.random.default_rng(seed=420)\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30, rng=rng)\n",
    "random_igwg = InteractiveGridworld(gwg, has_fov=True, figsize=(5,4),\n",
    "                                   critter_name='$\\mathit{Fishy}$',\n",
    "                                   player=None)\n",
    "display(random_igwg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that you've seen what this limited field of perception looks like it's time for a micro-science question:\n",
    "\n",
    "**Are *Fishy's* perceptions of the environment predictive of whether or not they will eat food?**\n",
    "\n",
    "Make a guess. Think about how you would prove to yourself whether your guess is right. There are lots of ways to validate your guess; let's explore one of them right now. Our first step is to **observe** the behavior and **collect data**! Any way of validating your guess that purports to be science (in the modern sense) will start this way, with data. However, keep in mind, there are other approaches and modes of thinking out there. For example, a theorist or mathematician might bypass observations and data, diving straight into deductions based on abstract assumptions. But, that's not going to be our approach here and now (though we will borrow it occasionally throughout the book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Observations of Random Movement with Limited Perception\n",
    "# @markdown Don't worry about how this code works – just **run this cell**, then press the start button and watch the data collection happen.\n",
    "rng = np.random.default_rng(seed=2023)\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30, rng=rng)\n",
    "record_igwg = InteractiveGridworld(gwg, has_fov=True, collect_fov_data=True,\n",
    "                                   player=None, critter_name='$\\mathit{Fishy}$',\n",
    "                                   figsize=(4,4))\n",
    "display(record_igwg.b_fig.canvas)\n",
    "display(record_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(record_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Start by running two or three simulations to collect 'adequate' data for analysis.\n",
    "\n",
    "**Thought/Bonus/Further Reading: How much data is truly 'enough'?:**\n",
    "\n",
    "Entire methodologies have been developed to answer this critical question.  In a hypothesis testing framework, power analysis is used to determine optimal sample size by considering the strength an effect to be detected and the desired statistical power of the test. In an ML framework, notions of Empirical Risk Minimization inform the amount of data needed. In a Bayesian context, assessment of convergence of posterior distributions of parameters indicates whether incoming data is helping (or continues to help) improve those parameter estimates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have some observations, let's put them in a structure we can easily work with. Here's how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "data = record_igwg.fov_eat_table_data\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When analyzing data we ***always*** plot the data before doing ***anything*** else. Which visualization to start with is a more subjective question, though histograms and scatter plots are often good starting points. In this case we'll go for a [histogram](## \"In a histogram each bin represents a range of values, and the height of the bar shows the frequency (number of data points observed) within that range.\") of the two distributions of number of food items perceived conditional on whether eating happened immediately afterward or not. Before you run the code below to generate the picture, think about what it might look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# If data collection step was skipped above use this data instead\n",
    "if np.all(data == np.zeros(data.shape)):\n",
    "  print(\"You haven't collected any data, using canned data instead.\")\n",
    "  data = np.array(\n",
    "    [[10., 22., 11.,  4.,  4.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "     [ 0.,  1.,  3.,  1.,  3.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "\n",
    "# Unpack the contingency table into a list of raw observations\n",
    "# A flat array of perception values before 'Not Eating' events\n",
    "not_eating = np.repeat(range(len(data[0])), data[0].astype(int))\n",
    "# A flat array of perception values before 'Eating' events\n",
    "eating = np.repeat(range(len(data[0])), data[1].astype(int))\n",
    "\n",
    "# Create histograms\n",
    "fig_hist, (ax1_hist, ax2_hist) = plt.subplots(2, 1, sharex=True, sharey=True,\n",
    "                                              figsize=(8,6))\n",
    "# Define bin edges\n",
    "nonzero_col_index = np.where(data > 0)[1]\n",
    "bin_edges = np.arange(-0.5, np.max(nonzero_col_index) + 1.5, 1)\n",
    "# Plot Histograms\n",
    "ax2_hist.hist(not_eating, bins=bin_edges, color='blue', alpha=0.7, rwidth=0.9)\n",
    "ax1_hist.hist(eating, bins=bin_edges, color='orange', alpha=0.7, rwidth=0.9)\n",
    "ax1_hist.set_title('Distribution of Food Items in Percept before Eating and Not Eating',\n",
    "                   fontsize=14)\n",
    "ax1_hist.set_ylabel('Before\\nEating Counts', fontsize=12)\n",
    "ax2_hist.set_ylabel('Before\\nNot Eating Counts', fontsize=12)\n",
    "ax2_hist.set_xlabel('Number of Food Items in Perceived', fontsize=12)\n",
    "remove_ip_clutter(fig_hist)\n",
    "fig_hist.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These histograms reveal differences in the distribution of perceived food items immediately prior to 'Eating' and 'Not Eating' events. 'Not Eating' is more frequently observed, especially when fewer food items are perceived beforehand. This should not be surprising, fishy sees 12 spots, a small number of which typically carry food. On the other hand, 'Eating' events, although less common, tend to occur more often when more food items are perceived. These initial observations hint that the perception of food may be predictive of eating.\n",
    "\n",
    "To get at our question — does perceiving food predict eating more directly? — we will next plot the proportion of times eating followed for the different numbers of food items perceived. Again, to visualize what the plot might look like before you run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Replace ... in the line:\n",
    "# prob_eating plot_data[...] / plot_data.sum(axis=...)\n",
    "# Comment out or remove this next lines.\n",
    "raise NotImplementedError(\"Exercise: plot proportion of eating\")\n",
    "################################################################################\n",
    "\n",
    "# Calculate the probability of eating given the amount of food perceived\n",
    "# for food amounts with non-zero observations\n",
    "non_zero_cols = np.where(data.sum(axis=0) > 0)[0]\n",
    "plot_data = data[:, non_zero_cols]\n",
    "prob_eating = plot_data[...] / plot_data.sum(axis=...)\n",
    "# Create a figure and axis for the plot\n",
    "fig_prop, ax_prop = plt.subplots()\n",
    "# Plot the probabilities\n",
    "ax_prop.plot(np.arange(len(data[0]))[non_zero_cols], prob_eating, marker='o')\n",
    "# Set the title and labels\n",
    "ax_prop.set_title('Proportion of Eating Events by\\nAmount of Food Perceived Prior')\n",
    "ax_prop.set_xlabel('Number of Food Items Perceived')\n",
    "ax_prop.set_ylabel('Proportion where Eating Follows')\n",
    "remove_ip_clutter(fig_prop)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/dcownden/PerennialProblemsOfLifeWithABrain/tree/main//sequences/P1C1_BehaviourAsPolicy/solutions/P1C1_Sequence2_Solution_526d0ba6.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=800.0 height=600.0 src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/static/P1C1_Sequence2_Solution_526d0ba6_0.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this plot, it appears that the proportion of eating events increases as the amount of food perceived immediately prior increases. This suggests that the amount of food perceived could indeed predict eating behavior. However, this is just a visualization and not a formal test of a causal relationship. Later in the book we will review how to formally assess predictiveness through statistical models, such as logistic regression, which would be suitable for this case of predicting a binary outcome (eating vs. not eating) from a predictor variable (or variables) like the amount of food perceived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Bonus: Think about what makes a good predictor variable:\n",
    "\n",
    "We observed and recorded the total number of food items percieved and used that as our 'predictor' variable in the analysis and visualizations above. Do you think there are other aspects of *Fishy's* perceptions, beyond just the total number of food items perceived that could be used to make better predictions about the amount of food eaten?(Hint: Could the predictor variable consider the relative location of food items within the perceptual field?) [Solution.](## \"In terms of predicting immediate eating, really only the four grid cells immediately adjacent to *fishy* can have any bearing on whether eating will happen in the next round, so the sum of food items just in those four cells would be a much better predictor. Note that we haven't gotten into what 'better' means yet for a predictor so this question is kind of ill formed, but we will eventually and what makes a good predictor is good thing to start thinking about right away.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.2.2: Perception Guiding Actions is a Policy\n",
    "\n",
    "We've seen that even in the case of random drifting, perceptions of the environment can carry information about import things like whether or not eating will happen. Now, we're going to hook this perceptive field up to some highly abstracted 'muscles' by making a simple set of rules that translate *Fishy's* perceptions into a choice of actions (one of up, down, left, right).\n",
    "\n",
    "Before we attempt this we need to understand the form of *Fishy's* perceptions. We recorded some of these in the previous section, what do they look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# 'record_igwg' is an instance of an InteractiveGridworld object\n",
    "# that we ran in the cell above to record data\n",
    "print(f\"Type of 'record_igwg': {type(record_igwg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# 'record_igwg' has a 'percept_eat_records' attribute. This is a list of all\n",
    "# the records from the games played by the InteractiveGridworld object since it\n",
    "# was instantiated.\n",
    "print(f\"Type of 'percept_eat_records': {type(record_igwg.percept_eat_records)}\")\n",
    "# The length of 'percept_eat_records' should be 30 times the number of times you\n",
    "# ran the simulation.\n",
    "print(f\"Length of 'percept_eat_records': {len(record_igwg.percept_eat_records)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, let's inspect what a record looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# If we didn't collect any data we'll use some canned data\n",
    "if len(record_igwg.percept_eat_records) < 2:\n",
    "  record_igwg.percept_eat_records.append(\n",
    "    {'perception': np.array(\n",
    "        [-1., -1.,  0., -2., -1.,  0., -2., -2., -1.,  0., -2.,  0.]),\n",
    "     'state': {\n",
    "      'pieces': np.array(\n",
    "          [[[ 0.,  0.,  0., -1., -1.,  0., -1.],\n",
    "            [ 0.,  0., -1.,  0., -1., -1.,  0.],\n",
    "            [ 0.,  0.,  0.,  0., -1.,  0.,  1.],\n",
    "            [ 0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "            [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]]),\n",
    "      'scores': np.array([0.]),\n",
    "      'rounds_left': np.array([30.])},\n",
    "     'did_eat': False})\n",
    "  record_igwg.percept_eat_records.append(\n",
    "    {'perception': np.array(\n",
    "        [ 0.,  0.,  0., -2.,  0., -1., -2., -2., -1.,  0., -2.,  0.]),\n",
    "     'state': {\n",
    "        'pieces': np.array(\n",
    "            [[[ 0.,  0.,  0., -1., -1.,  0., -1.],\n",
    "              [ 0.,  0., -1.,  0., -1., -1.,  0.],\n",
    "              [ 0.,  0.,  0.,  0., -1.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0., -1.,  1.],\n",
    "              [ 0.,  0.,  0.,  0.,  0., -1.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
    "              [ 0.,  0.,  0.,  0.,  0.,  0., -1.]]]),\n",
    "        'scores': np.array([0.]),\n",
    "        'rounds_left': np.array([29.])},\n",
    "     'did_eat': False})\n",
    "\n",
    "# Let's examine the first two elements of the list.\n",
    "# What type is the record?\n",
    "print(f\"Type of first record: {type(record_igwg.percept_eat_records[0])}\")\n",
    "# What does the first one look like?\n",
    "print('----------------------------------')\n",
    "display(record_igwg.percept_eat_records[0])\n",
    "# What does the second one look like?\n",
    "print('----------------------------------')\n",
    "display(record_igwg.percept_eat_records[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "It looks like the first two elements of the list are dictionaries, and, as it turns out, all elements share this structure. These dictionaries contain relatively self-descriptive keys. Let's walk through the structure of these records element by element.\n",
    "\n",
    "* `perception`: This is a 12-element array with -2's denoting areas where the perceptual field lies outside the boundaries of the Gridworld, 0's indicating empty spaces on the grid, and -1's marking food locations.\n",
    "* `state`: This is another dictionary representing the state of the board at the time of the perception. The `pieces` element of `state` is a 7x7 array depicting the grid positions of the organism (1), the food (-1's), and the empty spaces (0's). The `scores` and `rounds_left` elements describe exactly what their names suggest.\n",
    "* `did_eat`: This is a boolean indicator that is `True` when eating occurred in the round immediately following the perception and `False` when eating did not occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Can you see how the perceptive field, as defined below, is overlaid on the 7x7 array to generate a perceptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# define the L1 ball mask the determines what an organism perceives\n",
    "# Perception radius for an organism\n",
    "radius = 2\n",
    "# Grid size to encompass perception 'circle', include center point\n",
    "diameter = radius*2+1\n",
    "# Empty grid (initially all False)\n",
    "mask = np.zeros((diameter, diameter), dtype=bool)\n",
    "# Relative coordinates for grid points, center at (0,0)\n",
    "mask_coords = np.array([(i-radius, j-radius)\n",
    "  for i in range(diameter)\n",
    "  for j in range(diameter)])\n",
    "# Calculate cityblock distance from each point to center\n",
    "mask_distances = cdist(mask_coords, [[0, 0]], 'cityblock').reshape(mask.shape)\n",
    "# Mark points within perception radius as True\n",
    "mask[mask_distances <= radius] = True\n",
    "# Exclude center point (organism itself)\n",
    "mask[radius,radius] = False\n",
    "print('--------------------Perceptive Field---------------------------')\n",
    "display(mask)\n",
    "print('\\n--------------------Environment-------------------------------')\n",
    "display(record_igwg.percept_eat_records[0]['state']['pieces'])\n",
    "print('\\n--------------------Perception--------------------------------')\n",
    "display(record_igwg.percept_eat_records[0]['perception'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The indices of the perception array correspond to the spaces around the organism as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "numbered_mask = np.ones(mask.shape) * -1\n",
    "numbered_mask[mask] = range(mask.sum())\n",
    "display(numbered_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note that Fishy lacks an orientation in relation to the Gridworld; all directions are defined in absolute or cardinal terms: 'up', 'down', 'left', and 'right'. So another way of thinking about the perception vector is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# in human readable terms the perception is organized as\n",
    "human_readable_percept_structure = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "print(human_readable_percept_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we understand what a perception looks like, let's build a function that accepts a perception as input and outputs one of four possible moves. To start, we'll create a simple function based on the following logic: if any is food immediately adjecent, *Fishy* moves towards it; otherwise *Fishy* moves randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to set up the tests for the code exercise below\n",
    "def test_action_from_perception(test_func):\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "\n",
    "  # Test when there's no food nearby\n",
    "  perception = np.zeros(12)\n",
    "  assert test_func(perception) in ['up', 'down', 'left', 'right']\n",
    "\n",
    "  # Test when there's food to the near up\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near up')] = -1\n",
    "  assert test_func(perception) == 'up'\n",
    "\n",
    "  # Test when there's food to the near down\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near down')] = -1\n",
    "  assert test_func(perception) == 'down'\n",
    "\n",
    "  # Test when there's food to the near left\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near left')] = -1\n",
    "  assert test_func(perception) == 'left'\n",
    "\n",
    "  # Test when there's food to the near right\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near right')] = -1\n",
    "  assert test_func(perception) == 'right'\n",
    "\n",
    "  # Test when there's food in multiple nearby directions\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near up')] = -1\n",
    "  perception[percept_struct.index('near right')] = -1\n",
    "  assert test_func(perception) in ['up', 'right']\n",
    "\n",
    "  # Test when there's food in multiple nearby directions\n",
    "  perception = np.zeros(12)\n",
    "  perception[percept_struct.index('near left')] = -1\n",
    "  perception[percept_struct.index('near down')] = -1\n",
    "  assert test_func(perception) in ['down', 'left']\n",
    "\n",
    "  print('Congrats, Tests Passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Complete the lines with ...  to identify directions where\n",
    "# food is nearby and then randomly choose between directions with nearby food\n",
    "# if food is nearby. Then comment out the line below and see if your code\n",
    "# runs and passes the code checks.\n",
    "raise NotImplementedError(\"Exercise: make actions depend on perception\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# to_remove solution\n",
    "def simple_action_from_percept(percept, rng=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # Defines directions corresponding to different perception indices\n",
    "  direction_struct = [\n",
    "    'None', 'None', 'up', 'None',\n",
    "    'None', 'left', 'right', 'None',\n",
    "    'None', 'down', 'None', 'None']\n",
    "  # these are what count as nearby in the percpt\n",
    "  nearby_directions = ['near up', 'near down', 'near left', 'near right']\n",
    "  # Get the corresponding indices in the percept array\n",
    "  nearby_indices = [percept_struct.index(dir_) for dir_ in nearby_directions]\n",
    "  # Identify the directions where food is located\n",
    "  food_indices = [index for index in nearby_indices if percept[index] == ...]\n",
    "  food_directions = [direction_struct[index] for index in food_indices]\n",
    "  if len(food_directions) > 0:  # If there is any food nearby\n",
    "    # If there is any food nearby randomly choose a direction with food\n",
    "    return rng.choice(...)  # Move towards a random one\n",
    "  else:\n",
    "    # If there is no food nearby, move randomly\n",
    "    return rng.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "test_action_from_perception(simple_action_from_percept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/dcownden/PerennialProblemsOfLifeWithABrain/tree/main//sequences/P1C1_BehaviourAsPolicy/solutions/P1C1_Sequence2_Solution_1b1b57ed.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Congratulations, you've just implemented your first policy function. Any  function, like this one, that takes perceptions as inputs and outputs actions, in the context of an environmental feedback loop is called a ***policy***.\n",
    "\n",
    "Also, observe how we use a test here to check if your code does the right things. When writing good, high quality, code, most programmers write such tests for their own code. Writing more lines for the tests than the code being tested is perfectly standard. The number, breadth, and quality of the tests is what ultimately defines how much you can trust your code!\n",
    "\n",
    "Now that we have a policy let's see how it compares to random drifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title A Simple Rule Based Policy Versus Random Drifting\n",
    "# @markdown Don't worry about how this code works – just **run the cell** and press start to compare the policy we defined above with random drifting.\n",
    "\n",
    "class SimpleRulePlayer():\n",
    "  \"\"\"\n",
    "  A Player based on the following simple policy:\n",
    "  If there is any food immediately nearby move towards it,\n",
    "  otherwise it move randomly.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, fov_radius=2):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.fov_radius = fov_radius\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indeces of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "\n",
    "    #get the percept for every board in the batch\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius)\n",
    "    critter_oriented_moves = []\n",
    "    for g in range(batch_size):\n",
    "      critter_oriented_moves.append(\n",
    "          simple_action_from_percept(perceptions[g], self.game.rng))\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, a_1hots\n",
    "\n",
    "\n",
    "# two different ai players\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "srp = SimpleRulePlayer(gwg)\n",
    "h2h_igwg = Head2HeadGridworld(gwg, player0=None, p0_short_name='RANDOM',\n",
    "                              p0_long_name='Random\\nPolicy',\n",
    "                              player1=srp, p1_short_name='EAT_NEAR',\n",
    "                              p1_long_name='Eat Nearby\\nPolicy',\n",
    "                              figsize=(2.4,3.0), has_fov=True, radius=2,\n",
    "                              critter_name='$\\mathit{Fishy}$')\n",
    "display(h2h_igwg.b_fig0.canvas)\n",
    "display(h2h_igwg.b_fig1.canvas)\n",
    "display(h2h_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Okay, hopefully we've implemented our rule correctly and we shouldn't be surprised to see that moving towards food when it is nearby (usually) results in much more eating than just drifting around randomly. In a patchy environment like this - food is in some places but not others - contingent action is a big advantage. However, as you may have noticed, *Fishy* is not taking full advantage of its perceptual field. There is perceived food (not immediately adjacent) that *Fishy* could move closer towards, but right now *Fishy* only acts on perceptions of food immediately adjacent. Next we're going to see if we can make *Fishy* an even more efficient eater by utilizing the whole perceptive field when choosing a movement direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.2.3: Parameterization of a Policy\n",
    "\n",
    "Our first policy function could be neatly summarized as \"If there is food immediately adjecent, move towards it, otherwise move randomly.\" It was not too difficult to translate this simple rule into a concrete function of *Fishy's* perceptions. However, as the number of inputs increase, formulating an efficient rule in natural language and translating that rule into a function becomes increasingly challenging. An effective rule in natural language would need to identify all possible food configurations, group them into cases, and then assign an appropriate direction to each case. It's a feasible approach, but very labour intensive. So, instead we are going to expand our way of thinking about policies to include [parameterization.](## \"In general the distinction between the parameters and the variables of a function is not always clear cut and is often dependent on the context. In this book, in the context of a policy function, variables will primarily be the sensory/environmental inputs to a policy funtion and parameters will be internal values of an organism which change not as a result of the sensory/environmental context but as a result changes to the organism's internal state as a response to learning and/or evolutionary processes. Even here though the distinction if blurred when considering how memories of recent sensory events are incorporated into a policy function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Another reason to parameterize our policy function, and the motivation for our specific manner parameterization, is to align our policy function implementation slightly towards how neurons in an organism might implement a policy. Specifically, we want to start to think of the strengths of synaptic connections between neurons as a kind of parameter in a neural implementation of a function. For this, let's imagine a simplified 'cartoon' nervous system where each cell in the perceptual field represents a single neuron. These neurons send a signal when food is present in their corresponding section of the perceptual field and remain 'quiet' otherwise.\n",
    "\n",
    "We then imagine that the organism has 4 \"direction neurons\" (up, down, left, and right) and that the relative firing strenghts of these direction neurons determine a probability distribution over the organism's movement direction. In this 'cartoon' nervous system, each neuron in the perceptual field connects to the dendrites of the direction neurons via an axon, with the connection strength determined by a parameterized weight. The firing strengths of the direction neurons are simply the sums of the weighted inputs they receive from the perceptual cells.\n",
    "\n",
    "While this 'cartoon' is laughably far removed from how motor control is implemented in most animals, it is still a step closer to real neural computation compared to our initial rule based policy implementation. And, for a few very simple organisms this cartoon isn't so far from reality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In mathematical terms this looks like having $\\mathbf{W} \\in \\mathbb{R}^{4 \\times 12}$ as the weight matrix with $4$ rows and $12$ columns represent the connection strength *weights* between the $12$ perception neurons and the $4$ muscle neurons, and having $\\mathbf{x} \\in \\{0,1\\}^{12}$ as a $12$-dimensional binary column vector represent the organism's perceptive field neuron activity.\n",
    "\n",
    "The *activation* strengths of the muscle neurons, represented as $\\mathbf{a} \\in \\mathbb{R}^4$, are then computed as the matrix-vector product $\\mathbf{Wx}$, that is:\n",
    "\n",
    "$$\\mathbf{a} = \\mathbf{Wx}$$\n",
    "\n",
    "Each element $a_i$ of $\\mathbf{a}$ is computed as:\n",
    "\n",
    "$$a_i = \\sum_{j=1}^{12} W_{ij}x_j$$\n",
    "\n",
    "The matrix-vector product involves taking the dot product of the $i$-th row of $\\mathbf{W}$ with the vector $\\mathbf{x}$ to yield the $i$-th element of $\\mathbf{a}$.\n",
    "\n",
    "We perform this operation for each row $i$ (where $i$ ranges from $1$ to $4$) of the weight matrix $\\mathbf{W}$ to obtain the complete activation vector $\\mathbf{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To help make all this a little more concrete let's see what our first rule based policy would look like if it were structured in this parameterized way. We'll start by building a weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# a human interpretable overview of the percept structure\n",
    "percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # a human iterpretable overview of the out structure\n",
    "output_struct = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# a more explicit human legible way to set W\n",
    "W1 = np.zeros((len(output_struct), len(percept_struct)))\n",
    "for i, output in enumerate(output_struct):\n",
    "  for j, input in enumerate(percept_struct):\n",
    "    if input == 'near up':\n",
    "      if output == 'up':\n",
    "        W1[i,j] = 1\n",
    "    elif input == 'near left':\n",
    "      if output == 'left':\n",
    "        W1[i,j] = 1\n",
    "    elif input == 'near right':\n",
    "      if output == 'right':\n",
    "        W1[i,j] = 1\n",
    "    elif input == 'near down':\n",
    "      if output == 'down':\n",
    "        W1[i,j] = 1\n",
    "\n",
    "# slick programmer way to set W\n",
    "W2 = np.zeros((len(output_struct), len(percept_struct)))\n",
    "for i, output in enumerate(output_struct):\n",
    "  for j, input in enumerate(percept_struct):\n",
    "    if output in input:\n",
    "      # *in* is a base python function that checks for containment. In this\n",
    "      # case it checks if the ouput string is contained in the input string.\n",
    "      # If output='up' and input='far up' output in input will evaluate as True\n",
    "      # but if the input were 'near down' it would evaluate as false. Note that\n",
    "      # this approach would fail terribly if our word for up were contained in\n",
    "      # our word for down, i.e. if instead using 'down' we'd used the string\n",
    "      # 'anti-up'. In general string based logic is fragile and bug prone,\n",
    "      # but also can make code very human readabile. It's all trade-offs out\n",
    "      # there be careful.\n",
    "      if 'near' in input:\n",
    "        W2[i,j] = 1\n",
    "\n",
    "display(W1)\n",
    "print('--------------------------------------------------------------')\n",
    "display(W2)\n",
    "assert(np.all(W1 == W2))\n",
    "W = W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next we'll use those weights to re-implement our first rule based function, and use our test from before to be sure we've done it right! Tests are ***so*** useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Complete the lines output_activations = ... Then comment\n",
    "# out the line below and see if your code runs and passes the same code checks\n",
    "# we applied to our first rule based policy.\n",
    "raise NotImplementedError(\"Exercise: parameterize a simple policy\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def parameterized_policy(percept, rng=None, W=W, softmax_temp=None):\n",
    "  \"\"\"\n",
    "  Determine an action based on perception.\n",
    "\n",
    "  Args:\n",
    "    percept: A 1D len 12 array representing the perception of the organism.\n",
    "      Indices correspond to spaces around the organism. The values in the array\n",
    "      can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "    W: a 4 x 12 weight matrix parameter representing the connection strenghts\n",
    "      between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "  Returns:\n",
    "    action: a str, one of 'up', 'down', 'left', 'right'. If food in one or more\n",
    "    of the spaces immediately beside the organism, the function will return a\n",
    "    random choice among these directions. If there is no food nearby, the\n",
    "    function will return a random direction.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  if softmax_temp is None:\n",
    "    # very low temp, basically deterministic for this range of values\n",
    "    softmax_temp = 0.01\n",
    "  # a human interpretable overview of the percept structure\n",
    "  percept_struct = [\n",
    "  'far up', 'left up', 'near up', 'right up',\n",
    "  'far left', 'near left', 'near right', 'far right',\n",
    "  'left down', 'near down', 'right down', 'far down']\n",
    "  # a human iterpretable overview of the out structure\n",
    "  output_struct = ['up', 'down', 'left', 'right']\n",
    "  # boolean representation of percept, no edges, just 1's where food is\n",
    "  # zero otherwise\n",
    "  x = np.asarray(percept == -1, int)\n",
    "  # hint: Look at the equations above, the matrix-vector product (and it's\n",
    "  # higher dimension generalizations) are implemented by the @ operator for\n",
    "  # numpy arrays.\n",
    "  output_activations = ...\n",
    "  if np.sum(output_activations > 0):\n",
    "    # softmax shift by max, scale by temp\n",
    "    shift_scale_ex = np.exp((output_activations -\n",
    "                             np.max(output_activations))/softmax_temp)\n",
    "    sm = shift_scale_ex / shift_scale_ex.sum() #normalized\n",
    "    probs_sm = sm / sm.sum(axis=0) #re-normalized again for fp precision issues\n",
    "    # probs below is a naive way to get a discrete probability distribution\n",
    "    # from a real valued vector, why did we use softmax normalization instead?\n",
    "    # probs = output_activations / np.sum(output_activations)\n",
    "    action = rng.choice(output_struct, p=probs_sm)\n",
    "  else:\n",
    "    action = rng.choice(output_struct)\n",
    "  return action\n",
    "\n",
    "test_action_from_perception(parameterized_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/dcownden/PerennialProblemsOfLifeWithABrain/tree/main//sequences/P1C1_BehaviourAsPolicy/solutions/P1C1_Sequence2_Solution_38397e00.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Okay, now it's your turn to make a really great *Fishy*. You can do this either by manually setting the weights below, or extending the weight setting logic used above to set W programatically, whichever seems less tedious to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Fill in the values of the weight matrix below to define\n",
    "# a really great parameterized policy for fishy. Then comment out the line\n",
    "# below and see how your parameterized policy compares to the eat when nearby\n",
    "# policy.\n",
    "raise NotImplementedError(\"Exercise: pick good parameter values for W\")\n",
    "################################################################################\n",
    "\n",
    "W_student = np.array(\n",
    "     # input                                             # output\n",
    "     #[fu, lu, nu, ru, fl, nl, nr, fr, ld, nd, rd, fd]   #\n",
    "    [[  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # up\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # down\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],  # left\n",
    "     [  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]) # right\n",
    "\n",
    "# comment out these nested for loops if you don't want use them to set the W\n",
    "# W values and you'd rather just enter them manually above\n",
    "for i, output in enumerate(output_struct):\n",
    "  for j, input in enumerate(percept_struct):\n",
    "    ... # your logic here\n",
    "\n",
    "display(W_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/dcownden/PerennialProblemsOfLifeWithABrain/tree/main//sequences/P1C1_BehaviourAsPolicy/solutions/P1C1_Sequence2_Solution_c5d51290.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Your Parameterized Policy Versus the Simple Rule Based Policy\n",
    "# @markdown Don't worry about how this code works – just **run the cell** and press start to compare the parameterized policy you've just defined to our simple rule based policy from before.\n",
    "\n",
    "class ParamPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a parameterized policy using the weights defined by the\n",
    "  student in W_student.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, weights=None, fov_radius=2, ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    if weights is None:\n",
    "      self.W = np.array(\n",
    "      [[1., 1., 4., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 4., 1., 1.],\n",
    "       [0., 1., 0., 0., 1., 4., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 4., 1., 0., 0., 1., 0.]])\n",
    "    else:\n",
    "      self.W = weights\n",
    "    self.fov_radius = fov_radius\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "  def play(self, board, temp=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indeces of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "\n",
    "    #get the percept for every board in the batch\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius)\n",
    "    critter_oriented_moves = []\n",
    "    for g in range(batch_size):\n",
    "      critter_oriented_moves.append(\n",
    "          parameterized_policy(perceptions[g], self.game.rng, self.W, temp))\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, a_1hots\n",
    "\n",
    "\n",
    "# two different ai players\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "\n",
    "srp = SimpleRulePlayer(gwg)\n",
    "spp = ParamPlayer(gwg, W_student) #student parameterized player\n",
    "h2h_igwg2 = Head2HeadGridworld(gwg, player0=spp, p0_short_name='PARAMS',\n",
    "                               p0_long_name='Parameterized\\nPolicy',\n",
    "                               player1=srp, p1_short_name='EAT_NEAR',\n",
    "                               p1_long_name='Eat Nearby\\nPolicy',\n",
    "                               figsize=(2.4,3), has_fov=True, radius=2,\n",
    "                               critter_name='$\\mathit{Fishy}$')\n",
    "display(h2h_igwg2.b_fig0.canvas)\n",
    "display(h2h_igwg2.b_fig1.canvas)\n",
    "display(h2h_igwg2.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg2.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully, by taking advantage of the full perceptive field, your parameterized policy able to outperform the simple 'Eat Nearby' policy. How good can a parameterized policy of this form be, and would the best policy be deterministic or probabalistic. We're not going to answer either of those directly now (though we will later). Instead we will explore how a parameter called 'temperature' can be used to control the level of randomness in a policy. As we will see later this randomness level will play a crucial role in answering these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following code snippet (slightly adapted) appeared in the previous coding exercise. Let's take a look at it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define the necessary inputs and context\n",
    "rng = np.random.default_rng()  # Random number generator\n",
    "output_activations = rng.random(4) * 5  # An example output activations array\n",
    "softmax_temp = 1  # An example softmax temperature value\n",
    "output_struct = ['up', 'down', 'left', 'right']  # Output structure\n",
    "\n",
    "#compute softmax probabilities\n",
    "# softmax shift by max, scale by temp\n",
    "shift_scale_ex = np.exp((output_activations - np.max(output_activations)) /\n",
    "                        softmax_temp)\n",
    "sm = shift_scale_ex / shift_scale_ex.sum()  # normalized\n",
    "probs_sm = sm / sm.sum(axis=0)  # re-normalized again for fp precision issues\n",
    "# naive_probs below is a naive way to get a discrete probability distribution\n",
    "# from a real valued vector, why did we use softmax normalization instead?\n",
    "naive_probs = output_activations / np.sum(output_activations)\n",
    "action = rng.choice(output_struct, p=probs_sm)\n",
    "\n",
    "print('----------output activations----------')\n",
    "display(output_activations)\n",
    "print('----------softmax probabilities----------')\n",
    "display(probs_sm)\n",
    "print('----------naive probabilities----------')\n",
    "display(naive_probs)\n",
    "print('----------output structure----------')\n",
    "display(output_struct)\n",
    "print('----------selected action----------')\n",
    "display(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the code snippet above, the 'output_activations' is transformed into a probability distribution, which is then used to select an action. The transformation of the 'output_activations' to probabilities is done using something called a softmax function. Even though this may seem like an unnecessary complication there are many reasons, the most relevant listed here, for using softmax instead another, potentially more simple, normalization:\n",
    "\n",
    "1. **Parameter constraints:** A 'simple' normalization where 'output_activations' is divided (or normalized) its sum would requires that each value in 'output_activations' be non-negative. This places an implicit constraint on $\\mathbf{W}$, since 'output_activations' are a function of $\\mathbf{x}$, i.e. $\\mathbf{a} = \\mathbf{Wx}$. The constraints on $\\mathbf{W}$ therefore depend on the possible values of $\\mathbf{x}$. In general, we want fewer constraints on the parameters of our policy, and we want those constraints we do have to be explicit rather than implicit.\n",
    "\n",
    "2. **Control randomness level:** The softmax function has a 'temperature' parameter, which controls the level of randomness in the policy. It allows the other model parameters $\\mathbf{W}$ to be used for computing an estimate of the relative 'goodness' of the different options, while the temperature parameter controls how these relative differences translate into probabilities via the softmax function.\n",
    "3. **Invariance to constant shifts:** The softmax function produces the same probabilities for any constant shift to its inputs. In other words, it cares about the relative differences between the inputs, not absolute values. If we interpret the softmax inputs as the relative 'goodness' of each action, we are primarily interested in their relative differences. It's these differences alone that should inform our choice of action. The softmax function provides the only consistent way of achieving shift invariance in this context.\n",
    "4. **Mechanistic interpretation:** There is a biological interpretation of the softmax function. If the softmax inputs correspond to log-firing rates of neurons, and if these neurons have exponentially distributed firing times, and if the neuron that fires first, or 'wins the race', determines the chosen direction, then this process corresponds to choosing directions with softmax probabilities. While this might seem contrived, these conditions often hold true, at least approximately, in many real neural systems.\n",
    "\n",
    "More formally, the softmax function is defined as follows:\n",
    "\n",
    "Given a vector of activations $\\mathbf{a} = [a_1, a_2, ..., a_n]$, the softmax function calculates the probability of each activation as:\n",
    "\n",
    "$$\\text{softmax}(a_i) = \\frac{{e^{a_i / T}}}{{\\sum_{j=1}^{n} e^{a_j / T}}}\n",
    "$$\n",
    "\n",
    "where $T$ is the (non-negative) temperature parameter that controls the 'sharpness' of the softmax distribution. Larger $T$ values produce a more uniform, or 'flatter', distribution across actions, minimizing differences in their probabilities. Conversely, smaller $T$ values (approaching 0) create a 'sharper' distribution, amplifying differences and making the action with the highest activation much more likely to be selected.\n",
    "\n",
    "In computer implementations, $\\mathbf{a}$ is shifted down by $\\text{max}(\\mathbf{a})$ to help with numerical stability. This shift doesn't affect the final probability distribution, because the softmax function is invariant to constant shifts. However, in practice, this step helps to avoid overflow errors from exponentiating large numbers and makes the softmax function numerically robust to a broader range of input scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "That was a lot about the softmax function and temperature. Now, let's play around a bit with the temperature parameter of the softmax function to see how it impacts behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Exploring Softmax Temperature\n",
    "# @markdown Don't worry about how this code works – just **run the cell**. Then, select a softmax temperature using the slider and press start to compare the random policy with the parameterized policy using that softmax temperature.\n",
    "\n",
    "\n",
    "\n",
    "# two different ai players\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "spp = ParamPlayer(gwg, W_student) #student parameterized player\n",
    "h2h_igwg3 = Head2HeadGridworld(gwg, player0=spp, p0_short_name='PARAMS',\n",
    "  p0_long_name='Softmax\\nParameterized', player1=None,\n",
    "  p1_short_name='RANDOM', p1_long_name='Random',\n",
    "  figsize=(2.4,3), has_fov=True, radius=2, critter_name='$\\mathit{Fishy}$',\n",
    "  has_temp_slider=True)\n",
    "display(h2h_igwg3.b_fig0.canvas)\n",
    "display(h2h_igwg3.b_fig1.canvas)\n",
    "display(h2h_igwg3.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg3.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Did a higher or lower temperature make the parameterized policy more like the random policy? [Solution](## \"High temperatures made the parameterized policy basically random, low temperatures made it performs differently from and better than random.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This concludes the sequence on parameterized policies. We have seen that by giving a policy a good number of parameters, we can allow it to do a lot of things. In this case, for example, we can allow it to eat nearby things and prioritize them over further away things. In a way, all of life is about having policies that work in the real world. And much of biology and evolution are about setting up such policies that work in the real world. Now we are curious, how can such policies be chosen better than humans tinkering with weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to take the quiz\n",
    "comprehension_quiz = comprehension_quiz = [\n",
    "  {\"question\": \"What is a policy in the context of our Fishy example?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"A set of rules that governs how a user interacts with the system.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The term 'policy' here refers the organism's behavior, not user interaction.\"},\n",
    "      {\"answer\": \"A function or rule that determines how an organism (Fishy!) behaves in response to its perceptions.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. In the context of this example, a policy is a function that determines how the organism (Fishy) behaves in response to its perceptions.\"},\n",
    "      {\"answer\": \"A piece of legislation that regulates how the Gridworld game is played.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. While 'policy' can mean a piece of legislation in other contexts, in this context it refers to the function or rules that regulate how an organism plays the game, i.e. their behavior.\"},\n",
    "      {\"answer\": \"The nutritional habits of the organism in the environment.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. A policy in this context does determine the nutritional habits of the organism, but there is a better answer.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"Based on the observations and data presented in section 1.1.2.1, how does the perception of food appear to impact Fishy's eating behaviour\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "        {\"answer\": \"The total number of food items perceived doesn't seem to impact the likelihood of eating.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. The data suggests that the likelihood of eating appears to increase with the number of food items perceived.\"},\n",
    "        {\"answer\": \"Perceiving a higher number of food items appears to increase the likelihood of eating.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct. The data suggests that as the amount of food perceived increases, the proportion of eating events also seems to increase.\"},\n",
    "        {\"answer\": \"The likelihood of eating seems to be independent of the number of food items perceived, and is solely dependent on the location of food items.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. Although location of food items might also be an important factor, the number of food items perceived appears to also have an impact on the likelihood of eating.\"},\n",
    "        {\"answer\": \"Perception of food doesn't seem to be predictive of eating behavior at all.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. The data suggest a relationship between the number of food items perceived and the likelihood of eating.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Which of the following best describes the structure and content of Fishy's perceptions?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "        {\"answer\": \"Fishy's perceptions consist of a 12-element array with different values representing the state of the environment.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct. This is an accurate description of the structure of Fishy's perceptions and their relationship with actions.\"},\n",
    "        {\"answer\": \"Fishy's perceptions only consist of a boolean indicator 'did_eat' that shows if eating occurred.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. While 'did_eat' is a part of the data we recorded, it's not strictly part of Fishy's perceptions in this scenario\"},\n",
    "        {\"answer\": \"Fishy's perceptions only consider the four grid spaces immediately adjacent to it.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. Fishy's perception includes a wider field than just the immediately adjacent spaces.\"},\n",
    "        {\"answer\": \"Fishy's perceptions are a complex data structure that have no clear relation with Fishy's environment.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"False. Fishy's perceptions in this example are a relatively simple and reflect the state of the surrounding environment.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"What is the role of parameterization in defining policies?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"Parameterization simplifies the policy by removing unnecessary variables.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Parameterization doesn't necessarily simplify the policy. Instead, it provides a systematic and numeric approach to describing and adjusting a policy.\"},\n",
    "      {\"answer\": \"Parameterization allows aspects of a policy to be numerically defined and adjusted.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. By parameterizing a policy, the parameterized aspects of the policy can be numerically and systematically described and adjusted.\"},\n",
    "      {\"answer\": \"Parameterization is used to fix the outcome of a policy.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Parameterization doesn't fix the outcome of a policy.\"},\n",
    "      {\"answer\": \"Parameterization refers to the evolution of a species over time.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Parameterization will help us to think about evolutionary and learning processes, but it is not evolution itself.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"In our 'cartoon' model of a neural system, how is the probability of the organism's movement direction determined?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\"answer\": \"The movement probability is proportional to the firing strengths/rates of the direction neurons.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. In the model described, the direction neurons with higher firing strengths are more likely to determine the movement direction of the organism.\"},\n",
    "      {\"answer\": \"By the neuron in the perceptual field that signals the presence of food.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. While the presence of food does trigger activity in the perceptual neurons, whether or not this determines movement depends on how the perceptual field neurons are connected to the direction neurons.\"},\n",
    "      {\"answer\": \"By averaging the signals from all four direction neurons.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The movement direction is not determined by averaging the signals.\"},\n",
    "      {\"answer\": \"By the strength of the predator avoidance instinct in the organism.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Any instincts the organism has are encoded in the weights between its perception neurons and direction neurons.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"What does the weight matrix W represent in the context of the neural model discussed?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"The strength of synaptic connections between perception neurons and direction neurons.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. In the model discussed, the weight matrix W represents the strength of synaptic connections between the perception neurons and the direction neurons.\"},\n",
    "      {\"answer\": \"The activation strengths of the muscle neurons.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. While the muscle/direction neuron activations are calculated based on the weight matrix and the perception neuron activity, the weight matrix itself does not directly represent these activations.\"},\n",
    "      {\"answer\": \"The current state of the organism's perceptive field.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The weight matrix does not represent the organism's perceptual state. Rather, it reflects the strength of the synaptic connections between the perceptive field and the direction neurons.\"},\n",
    "      {\"answer\": \"The physical weight of the organism in the environment.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"In the softmax function, what role does the temperature parameter play?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"It determines the highest possible activation strength.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The temperature parameter doesn't set an upper limit on activation strengths.\"},\n",
    "      {\"answer\": \"It controls the level of randomness in a distribution derived from activation strengths, influencing the 'sharpness' of the softmax distribution.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. The temperature parameter influences the 'sharpness' of the softmax distribution, controlling the level of randomness.\"},\n",
    "      {\"answer\": \"It adjusts the weight matrix to avoid overflow errors.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The temperature parameter is not related to adjusting the weight matrix to avoid overflow errors.\"},\n",
    "      {\"answer\": \"It represents the physical temperature of the environment the organism is in.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The temperature parameter in the softmax function is unrelated to the physical temperature of the environment.\"}\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "display_quiz(comprehension_quiz)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "P1C1_Sequence2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
