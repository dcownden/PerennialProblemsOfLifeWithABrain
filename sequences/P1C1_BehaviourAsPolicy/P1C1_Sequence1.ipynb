{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C1_BehaviourAsPolicy/P1C1_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/P1C1_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is a first test for an upcoming text book on computational neuroscience from an optimization perspective. The book will start with evolution. We are sharing it now to get feedback on what works and what does not and the developments we should do.\n",
    "\n",
    "# **Part 1 Behaviour, Environments and Optimization: Evolution and Learning**\n",
    "\n",
    "### **Animals are adapted to their specific environments; their behaviour is best understood within the context of their evolutionary environment.**\n",
    "\n",
    "### Objective: Part 1 of the book aims to introduce the fundamental concepts of\n",
    "* ### **environment**, the (statistical) properties of where an organism lives\n",
    "* ### **behaviour**, the statistics of what the organism does\n",
    "* ### **optimization**, how learning and evolution shape an organism's behaviour to make it better suited to its environment\n",
    "\n",
    "This very much is the core of why we are writing this book: we can view pretty much anything happening in the brain (and biology) as being part of a process that brings about improvement in this sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# Chapter 1.1 Behaviour as a Policy in an Environmental Context\n",
    "\n",
    "### Objective: This chapter develops examples of how behaviour is described and evaluated in relation to its [goodness](## \"This is a very loaded term, to be unpacked carefully later\") within a specific environmental niche.\n",
    "\n",
    "You will learn:\n",
    "*   What is a policy? A policy is a formalization of behaviour as a function that takes an organism's experiences of their environment as an input and outputs the organism's actions.\n",
    "*   What is a good policy? The rewards and other environmental signals resulting from the organism's actions in the environment are integrated into a Loss/Objective function to evaluatate, and potentially improve, a policy.\n",
    "*   What is stochasticity? Both the environment and an organism's behavior can contain random elements. This randomness can pose challenges when evaluating policies as it becomes difficult to determine whether poor outcomes are due the policy itself or simply bad luck.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **Sequence 1.1.1: Gridworld Introduction**\n",
    "\n",
    "### Objective: In this sequence, we will create a simple environment-organism system to demonstrate how an organism's **behaviour**, within an **environment**, can be evaluated using **rewards**. We will also see how intelligent behaviour can lead to better outcomes and how **randomness** can make evaluation of behaviour more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "\n",
    "\n",
    "# pip install dependencies\n",
    "!pip install ipympl vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "\n",
    "# import modules\n",
    "import ipywidgets as widgets\n",
    "import functools\n",
    "import threading\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#import jax\n",
    "import collections\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "from jupyterquiz import display_quiz\n",
    "from tqdm.notebook import tqdm\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "plt.ioff()\n",
    "\n",
    "\n",
    "# Plotting Functions and Setup\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(5,4)):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for futher use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Labels for major ticks\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "\n",
    "  # Minor ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "\n",
    "  ax.xaxis.tick_top()\n",
    "\n",
    "  # Gridlines based on minor ticks\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "\n",
    "  # Remove minor ticks\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  ax.invert_yaxis()\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=150, marker='o', color='red', label='Food')\n",
    "  rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "  #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc, critter=None):\n",
    "  \"\"\"\n",
    "  Plots \"critters\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h', color='blue', label='Critter')\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "# Logic for Gridworld environment\n",
    "\n",
    "\n",
    "def init_loc(n_rows, n_cols, num, rng=None):\n",
    "  \"\"\"\n",
    "  Samples random 2d grid locations without replacement\n",
    "\n",
    "  Args:\n",
    "    n_rows: int\n",
    "    n_cols: int\n",
    "    num:    int, wnumber of samples to generate, should\n",
    "            throw an error ifnum <= n_rows x n_cols\n",
    "\n",
    "  Optional Keyword Args\n",
    "    rng:    instance of numpy.random's default random number generator\n",
    "            (to enable reproducability)\n",
    "\n",
    "  Returns:\n",
    "    int_loc:  ndarray(int) of flat indices for the grid\n",
    "    rc_index: (ndarray(int), ndarray(int)) a pair of arrays the first\n",
    "      giving the row indices, the second giving the col indices, useful\n",
    "      for indexing an n_rows by n_cols numpy array\n",
    "    rc_plotting: ndarray(int) num x 2, same rc coordinates but structured\n",
    "      in a way that matplotlib likes\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "  int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "  rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "  rc_plotting = np.array(rc_index).T\n",
    "  return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game board that\n",
    "  define the logic of the game.\n",
    "  Board state is tracked as a triple (pieces, scores, rounds_left)\n",
    "  pieces: batch_size x n_rows x n_cols np.array\n",
    "  scores: batch_size np.array\n",
    "  rounds_left: batch_size np.array\n",
    "\n",
    "  Pieces are interpreted as:\n",
    "  1=critter, -1=food, 0=empty\n",
    "\n",
    "  First dim is batch, second dim row , third is col, so pieces[0][1][7]\n",
    "  is the square in row 2, in column 8 of the first board in the batch of boards\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_food=10, lifetime=30,\n",
    "               rng = None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"Set up starting board using game parameters\"\"\"\n",
    "    #set rounds_left and score\n",
    "    self.rounds_left = np.ones(self.batch_size) * self.lifetime\n",
    "    self.scores = np.zeros(self.batch_size)\n",
    "    # create an empty board array.\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols))\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+1 because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = init_loc(self.n_rows,\n",
    "                                          self.n_cols,\n",
    "                                          self.num_food+1,\n",
    "                                          rng=self.rng)\n",
    "      # critter random start location\n",
    "      self.pieces[(ii, rc_idx[0][0], rc_idx[1][0])] = 1\n",
    "      # food random start locations\n",
    "      self.pieces[(ii, rc_idx[0][1:], rc_idx[1][1:])] = -1\n",
    "    return(self.pieces.copy(), self.scores.copy(), self.rounds_left.copy())\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\" board is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    self.pieces = board[0].copy()\n",
    "    self.scores = board[1].copy()\n",
    "    self.rounds_left = board[2].copy()\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state, which is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    return(self.pieces.copy(), self.scores.copy(), self.rounds_left.copy())\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, rng=None):\n",
    "    \"\"\"\n",
    "    Updates the state of the board given the moves made.\n",
    "\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new x coord for each critter\n",
    "        on each board and the third gives the new y coord.\n",
    "\n",
    "    Note:\n",
    "      Assumes that there is exactly one valid move for each board in the\n",
    "      batch of boards. i.e. it does't check for bounce/reflection on edges,\n",
    "      or for multiple move made on the same board. It only checks for eating\n",
    "      food and adds new food when appropriate. Invalid moves could lead to\n",
    "      illegal teleporting behavior, critter dublication, or index out of range\n",
    "      errors.\n",
    "    This assumes the move is valid, i.e. doesn't check for\n",
    "    bounce/reflection on edges, it only checks eating and adds new food,\n",
    "    so invalid moves could lead to illegal teleporting behaviour or index out\n",
    "    of range errors\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    #critters leave their spots\n",
    "    self.pieces[self.pieces==1] = 0\n",
    "    #which critters have food in their new spots\n",
    "    eats_food = self.pieces[moves] == -1\n",
    "    # some critters eat and their scores go up\n",
    "    self.scores = self.scores + eats_food\n",
    "\n",
    "    num_empty_after_eat = self.n_rows*self.n_cols - self.num_food\n",
    "    # -1 for the critter +1 for food eaten\n",
    "    # which boards in the batch had eating happen\n",
    "    g_eating = np.where(eats_food)[0]\n",
    "    if np.any(eats_food):\n",
    "      # add random food to replace what is eaten\n",
    "      possible_new_locs = np.where(np.logical_and(\n",
    "          self.pieces == 0, #the spot is empty\n",
    "          eats_food.reshape(self.batch_size, 1, 1))) #food eaten on that board\n",
    "      food_sample_ = rng.choice(num_empty_after_eat, size=np.sum(eats_food))\n",
    "      food_sample = food_sample_ + np.arange(len(g_eating))*num_empty_after_eat\n",
    "      assert np.all(self.pieces[(possible_new_locs[0][food_sample],\n",
    "                                 possible_new_locs[1][food_sample],\n",
    "                                 possible_new_locs[2][food_sample])] == 0)\n",
    "      #put new food on the board\n",
    "      self.pieces[(possible_new_locs[0][food_sample],\n",
    "                   possible_new_locs[1][food_sample],\n",
    "                   possible_new_locs[2][food_sample])] = -1\n",
    "    # put critters in new positions\n",
    "    self.pieces[moves] = 1.0\n",
    "    self.rounds_left = self.rounds_left - 1\n",
    "    assert np.all(self.pieces.sum(axis=(1,2)) == ((self.num_food * -1) + 1))\n",
    "\n",
    "\n",
    "  def get_legal_moves(self):\n",
    "    \"\"\"Identifies all legal moves for the critter, taking into acount\n",
    "    bouncing/reflection at edges,\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offstet on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    #apply all possible offsets to each game\n",
    "    moves = np.stack([\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0,  1, 0])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, -1, 0])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, 0,  1])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, 0, -1])]*self.batch_size).T]).swapaxes(0,2)\n",
    "\n",
    "    #check bounces at boundaries\n",
    "    moves[:,1,:] = np.where(moves[:,1,:] >=\n",
    "                            self.n_rows, self.n_rows-2, moves[:,1,:])\n",
    "    moves[:,2,:] = np.where(moves[:,2,:] >=\n",
    "                            self.n_cols, self.n_cols-2, moves[:,2,:])\n",
    "    moves[:,1,:] = np.where(moves[:,1,:] < 0, 1, moves[:,1,:])\n",
    "    moves[:,2,:] = np.where(moves[:,2,:] < 0, 1, moves[:,2,:])\n",
    "    return moves\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "  square_content = {\n",
    "      -1: \"X\", #Food\n",
    "      +0: \"-\", #Nothing\n",
    "      +1: \"O\"  #Critter\n",
    "      }\n",
    "\n",
    "\n",
    "  def get_square_piece(self, piece):\n",
    "    return GridworldGame.square_content[piece]\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size, n_rows, n_cols,\n",
    "               num_food, lifetime, rng=None):\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(self.batch_size, self.n_rows, self.n_cols,\n",
    "                       self.num_food, self.lifetime, rng=self.rng)\n",
    "    return b.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_size(self):\n",
    "    \"\"\"Shape of a sinlge board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,x,y coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 2-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to x,y indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return board[0][g].tobytes() + board[1][g].tobytes() + board[2][g].tobytes()\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.square_content[square] for row in board[0][g]\n",
    "                       for square in row])\n",
    "    board_s = board_s + '_' + str(board[1][g])\n",
    "    board_s = board_s + '_' + str(board[2][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board[1].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board[2].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g):\n",
    "    \"\"\"Dispalys the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, \"|\", end=\"\")    # Print the row\n",
    "      for r_ in range(self.n_rows):\n",
    "        piece = board[0][g,c_,r_]    # Get the piece to print\n",
    "        print(GridworldGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board[2][g]))\n",
    "    print(\"Score: \" + str(board[1][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g):\n",
    "    return np.squeeze(np.array(np.where(board[0][g]==1)))\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g,\n",
    "                 fig=None, ax=None, critter=None, food=None,\n",
    "                 legend_type='included',\n",
    "                 figsize=(5,4)):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize)\n",
    "    rc_critter = self.get_critter_rc(board, g)\n",
    "    if critter is None:\n",
    "      critter = plot_critter(fig, ax, rc_critter)\n",
    "    else:\n",
    "      critter = plot_critter(fig, ax, rc_critter, critter)\n",
    "    rc_food_index = np.array(np.where(board[0][g] == -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\")\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter, food\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(2,2), layout='constrained')\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center')\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter, food, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter, food\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                       self.num_food, self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves()\n",
    "    valids = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for ii, g in enumerate(legal_moves[:,1:,:]):\n",
    "      for x,y in zip(g[0],g[1]):\n",
    "        valids[ii, x*self.n_cols+y] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, g):\n",
    "    \"\"\"Dispaly possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    A=np.reshape(self.get_valid_actions(board)[g], (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for y in range(self.n_cols):\n",
    "      print(y, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for y in range(self.n_cols):\n",
    "      print(y, \"|\", end=\"\")    # Print the row\n",
    "      for x in range(self.n_rows):\n",
    "        piece = A[y][x]    # Get the piece to print\n",
    "        print(GridworldGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: list of integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board tiple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parellel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    if board[2][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "      b = GridworldBoard(len(actions), n_rows, n_cols,\n",
    "                         self.num_food, self.lifetime,\n",
    "                         rng=self.rng)\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        b.set_state(board)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        newPieces = np.array([board[0][ai].copy() for ai in a_indx])\n",
    "        newScores = np.array([board[1][ai].copy() for ai in a_indx])\n",
    "        newrounds_left = np.array([board[2][ai].copy() for ai in a_indx])\n",
    "        b.set_state((newPieces, newScores, newrounds_left))\n",
    "      b.execute_moves(moves)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board[0].shape[0]\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                       self.num_food, self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1)}\n",
    "    critter_locs = np.where(board[0] == 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    b.execute_moves(moves)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right'\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    critter_locs = np.where(board[0] == 1)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      batch size np.array of -1 if not ended, and scores for\n",
    "      each game in the batch if it is ended\n",
    "    \"\"\"\n",
    "    rounds_left = board[2]\n",
    "    scores = board[1]\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Timer:\n",
    "  def __init__(self, timeout, callback):\n",
    "    self._timeout = timeout\n",
    "    self._callback = callback\n",
    "  async def _job(self):\n",
    "    await asyncio.sleep(self._timeout)\n",
    "    self._callback()\n",
    "  def start(self):\n",
    "    self._task = asyncio.ensure_future(self._job())\n",
    "  def cancel(self):\n",
    "    self._task.cancel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None, random_seed=None):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "    \"\"\"\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "    self.final_scores = []\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output()\n",
    "    self.scoreboard = widgets.Output()\n",
    "    self.up_button = widgets.Button(description=\"Up\")\n",
    "    self.down_button = widgets.Button(description=\"Down\")\n",
    "    self.left_button = widgets.Button(description=\"Left\")\n",
    "    self.right_button = widgets.Button(description=\"Right\")\n",
    "    self.random_movement = widgets.Checkbox( value=False,\n",
    "                                            description='Move Randomly',\n",
    "                                             disabled=False,\n",
    "                                             indent=False)\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    plt.ioff()\n",
    "    (self.b_fig, self.b_ax,\n",
    "     self.b_critter, self.b_food) = self.gwg.plot_board(self.board_state, 0)\n",
    "    self.output_and_score = widgets.HBox([self.scoreboard, self.output])\n",
    "    self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                            self.buttons])\n",
    "    self.board_output_and_score = widgets.VBox(\n",
    "        [self.b_fig.canvas, self.output_and_score],\n",
    "        layout=widgets.Layout(justify_content='flex-start'))\n",
    "    self.board_buttons_and_score = widgets.VBox(\n",
    "        [self.board_and_buttons, self.output_and_score],\n",
    "        layout=widgets.Layout(justify_content='flex-start'))\n",
    "\n",
    "    # Sometimes use timer\n",
    "    # self.click_timer = Timer(5.0, self.random_click)\n",
    "\n",
    "    # initial text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['High Score:', '--'],\n",
    "               ['Last Score:', '--'],\n",
    "               ['Average Score:', '--']]\n",
    "      print(tabulate(table))\n",
    "    with self.output:\n",
    "      print('\\tRun the cell below to start the simulation')\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = tuple([self.board_state[ii].copy() for ii in range(3)])\n",
    "    old_score = old_board[1][0]\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "        self.board_state, [which_button])\n",
    "    new_score = self.board_state[1][0]\n",
    "    rounds_left = self.board_state[2][0]\n",
    "    num_moves = self.gwg.lifetime - rounds_left\n",
    "    if new_score > old_score:\n",
    "      eating_string = \"The critter ate the food there!\"\n",
    "    else:\n",
    "      eating_string = \"There's no food there.\"\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0)\n",
    "    (self.b_fig, self.b_ax,\n",
    "     self.b_critter, self.b_food,\n",
    "     ) = self.gwg.plot_board(\n",
    "        self.board_state, 0, self.b_fig, self.b_ax, self.b_critter, self.b_food)\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      print(\"\\tThe critter (tried) to move \" + which_button +\n",
    "            \" and is now at ({}, {}).\".format(row,col))\n",
    "      print(\"\\t\" + eating_string)\n",
    "      print(\"\\tRounds Left: {}\\n\\tFood Eaten: {}\\n\\tFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_score, new_score / num_moves))\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_score)\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print('\\tGame Over. Final Score {}'.format(new_score))\n",
    "        print('\\tResetting the board for another game')\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax,\n",
    "       self.b_critter, self.b_food) = self.gwg.plot_board(\n",
    "        self.board_state, 0, self.b_fig, self.b_ax, self.b_critter, self.b_food)\n",
    "    with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = [\n",
    "            ['High Score:', str(np.max(np.array(self.final_scores)))],\n",
    "            ['Last Score:', str(self.final_scores[-1])],\n",
    "            ['Average Score',\n",
    "             '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "        else:\n",
    "          table = [['High Score:', '--'],\n",
    "                   ['Last Score:', '--'],\n",
    "                   ['Average Score:', '--']]\n",
    "        print(tabulate(table))\n",
    "\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.button_output_update('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.button_output_update('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.button_output_update('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.button_output_update('right')\n",
    "\n",
    "\n",
    "  def random_click(self):\n",
    "    move = self.rng.integers(0,4)\n",
    "    if move == 0:\n",
    "      self.up_button.click()\n",
    "    elif move == 1:\n",
    "      self.down_button.click()\n",
    "    elif move == 2:\n",
    "      self.left_button.click()\n",
    "    elif move == 3:\n",
    "      self.right_button.click()\n",
    "    else:\n",
    "      print('should not happen')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Head2HeadGridworld():\n",
    "  \"\"\"\n",
    "  A widget for interacting with a gridworld game while\n",
    "  an artificial player plays an identical board\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game,\n",
    "               init_board=None,\n",
    "               player=None,\n",
    "               random_seed=None):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batch_size of 2\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      player: object with a play method that takes a board state\n",
    "        as an argument and returns a move if left out will use a random player\n",
    "    \"\"\"\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "    self.final_scores = []\n",
    "    self.player = player\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "\n",
    "    # both players play on the same board\n",
    "    self.board_state[0][1] = self.board_state[0][0].copy()\n",
    "\n",
    "    # Initialize widgets and buttons\n",
    "    self.output0 = widgets.Output()\n",
    "    self.output1 = widgets.Output()\n",
    "    self.scoreboard = widgets.Output()\n",
    "    self.up_button = widgets.Button(description=\"Up\")\n",
    "    self.down_button = widgets.Button(description=\"Down\")\n",
    "    self.left_button = widgets.Button(description=\"Left\")\n",
    "    self.right_button = widgets.Button(description=\"Right\")\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    plt.ioff()\n",
    "    (self.b_fig0, self.b_ax0,\n",
    "     self.b_critter0, self.b_food0,\n",
    "     self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "        self.board_state, 0, legend_type='separate', figsize=(5,4))\n",
    "    (self.b_fig1, self.b_ax1,\n",
    "     self.b_critter1, self.b_food1) = self.gwg.plot_board(self.board_state, 1,\n",
    "                                                          legend_type=None,\n",
    "                                                          figsize=(5,4))\n",
    "    self.board_buttons_and_output0 = widgets.VBox([self.b_fig0.canvas,\n",
    "                                                   self.buttons,\n",
    "                                                   self.output0])\n",
    "    self.board_and_output1 = widgets.VBox([self.b_fig1.canvas, self.output1])\n",
    "    self.legend_and_scores = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                           self.scoreboard])\n",
    "    self.boards_and_scores = widgets.HBox([self.board_buttons_and_output0,\n",
    "                                           self.legend_and_scores,\n",
    "                                           self.board_and_output1])\n",
    "    # Sometimes use timer\n",
    "    self.click_timer = Timer(5.0, self.random_click)\n",
    "\n",
    "    # initial text outputs\n",
    "    with self.output0:\n",
    "      print('\\tClick a button to start.')\n",
    "    with self.scoreboard:\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      table = [['', 'YOU', 'THEM'],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Average Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = tuple([self.board_state[ii].copy() for ii in range(3)])\n",
    "    old_scores = old_board[1]\n",
    "    if self.player is None:\n",
    "      move = self.rng.integers(0,4)\n",
    "      if move == 0: a_player = 'up'\n",
    "      elif move == 1: a_player = 'down'\n",
    "      elif move == 2: a_player = 'right'\n",
    "      elif move == 3: a_player = 'left'\n",
    "    else:\n",
    "      a_player, _, _ = self.player.play(old_board)\n",
    "      a_player = self.gwg.action_to_critter_direction(old_board, a_player)\n",
    "      a_player = a_player[1]\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "        self.board_state, [which_button, a_player])\n",
    "    new_scores = self.board_state[1]\n",
    "    rounds_left = self.board_state[2][0]\n",
    "    num_moves = self.gwg.lifetime - rounds_left\n",
    "\n",
    "    if new_scores[0] > old_scores[0]:\n",
    "      eating_string0 = \"The critter ate the food there!\"\n",
    "    else:\n",
    "      eating_string0 = \"There's no food there.\"\n",
    "    if new_scores[1] > old_scores[1]:\n",
    "      eating_string1 = \"The critter ate the food there!\"\n",
    "    else:\n",
    "      eating_string1 = \"There's no food there.\"\n",
    "\n",
    "    row0, col0 = self.gwg.get_critter_rc(self.board_state, 0)\n",
    "    (self.b_fig0, self.b_ax0,\n",
    "     self.b_critter0, self.b_food0) = self.gwg.plot_board(\n",
    "        self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "        self.b_critter0, self.b_food0, legend_type=None)\n",
    "    with self.output0:\n",
    "      clear_output()\n",
    "      print(\"\\tYou clicked the \" + which_button +\n",
    "            \" button and your critter is now at ({}, {}).\".format(row0,col0))\n",
    "      print(\"\\t\"+eating_string0)\n",
    "      print(\"\\tRounds Left: {} \\tFood Eaten: {} \\tFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_scores[0], new_scores[0] / num_moves))\n",
    "    row1, col1 = self.gwg.get_critter_rc(self.board_state, 1)\n",
    "    (self.b_fig1, self.b_ax1,\n",
    "     self.b_critter1, self.b_food1) = self.gwg.plot_board(\n",
    "        self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "        self.b_critter1, self.b_food1, legend_type=None)\n",
    "    with self.output1:\n",
    "      clear_output()\n",
    "      print(\"\\tThe other player (tried) to move \" + a_player +\n",
    "            \" and is now at ({}, {}).\".format(row1,col1))\n",
    "      print(\"\\t\"+eating_string1)\n",
    "      print(\"\\tRounds Left: {} \\tFood Eaten: {} \\tFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_scores[1], new_scores[1] / num_moves))\n",
    "\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_scores)\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "      self.board_state[0][1] = self.board_state[0][0].copy()\n",
    "      with self.output0:\n",
    "        clear_output\n",
    "        print('\\tGame Over. Final Score {}'.format(new_scores[0]))\n",
    "        print('\\tResetting the board for another game')\n",
    "        (self.b_fig0, self.b_ax0,\n",
    "         self.b_critter0, self.b_food0) = self.gwg.plot_board(\n",
    "           self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "           self.b_critter0, self.b_food0,\n",
    "           legend_type=None)\n",
    "      with self.output1:\n",
    "        clear_output\n",
    "        print('\\tGame Over. Final Score {}'.format(new_scores[1]))\n",
    "        print('\\tResetting the board for another game')\n",
    "        (self.b_fig1, self.b_ax1,\n",
    "         self.b_critter1, self.b_food1) = self.gwg.plot_board(\n",
    "           self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "           self.b_critter1, self.b_food1,\n",
    "           legend_type=None)\n",
    "\n",
    "    with self.scoreboard:\n",
    "      clear_output()\n",
    "      self.b_fig_legend.canvas.draw()\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      if len(self.final_scores) > 0:\n",
    "        table = [['', 'YOU', 'THEM '],\n",
    "          ['High Score:', str(np.max(np.array(self.final_scores)[:,0])),\n",
    "                          str(np.max(np.array(self.final_scores)[:,1]))],\n",
    "          ['Last Score:', str(self.final_scores[-1][0]),\n",
    "                          str(self.final_scores[-1][1])],\n",
    "          ['Average Score',\n",
    "           '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,0])),\n",
    "           '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,1]))]]\n",
    "      else:\n",
    "        table = [['', 'YOU', 'THEM'],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Average Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.button_output_update('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.button_output_update('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.button_output_update('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.button_output_update('right')\n",
    "\n",
    "\n",
    "  def random_click(self):\n",
    "    move = self.rng.integers(0,4)\n",
    "    if move == 0:\n",
    "      self.up_button.click()\n",
    "    elif move == 1:\n",
    "      self.down_button.click()\n",
    "    elif move == 2:\n",
    "      self.left_button.click()\n",
    "    elif move == 3:\n",
    "      self.right_button.click()\n",
    "    else:\n",
    "      print('should not happen')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "  def __init__(self, game, random_seed=None):\n",
    "    self.game = game\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates random game play\n",
    "    Args:\n",
    "      a board state (pieces, scores, rounds_left)\n",
    "    Returns:\n",
    "      a: [int] a batch_size array randomly chosen actions\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board[0].shape\n",
    "    valids = self.game.get_valid_actions(board)\n",
    "    action_size = self.game.get_action_size()\n",
    "    # Compute the probability of each move being played (random player means this should\n",
    "    # be uniform for valid moves, 0 for others)\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "    # Pick a random action based on the probabilities\n",
    "    a = [self.rng.choice(action_size, p=probs[ii]) for ii in range(batch_size)]\n",
    "    a_1Hots = np.zeros((batch_size, action_size))\n",
    "    a_1Hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1Hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Nueral Network and Monte Carlo Setup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "  def __getattr__(self, name):\n",
    "    return self[name]\n",
    "\n",
    "\n",
    "args = dotdict({\n",
    "  'numIters': 1,            # In training, number of iterations = 1000 and num of episodes = 100\n",
    "  'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n",
    "  'tempThreshold': 15,      # To control exploration and exploitation\n",
    "  'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "  'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n",
    "  'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n",
    "  'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n",
    "  'cpuct': 1,\n",
    "  'maxDepth':5,             # Maximum number of rollouts\n",
    "  'numMCsims': 5,           # Number of monte carlo simulations\n",
    "  'mc_topk': 3,             # Top k actions for monte carlo rollout\n",
    "\n",
    "  'checkpoint': './temp/',\n",
    "  'load_model': False,\n",
    "  'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "  'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "  # Define neural network arguments\n",
    "  'lr': 0.001,               # lr: Learning Rate\n",
    "  'dropout': 0.3,\n",
    "  'epochs': 10,\n",
    "  'batch_size': 64,\n",
    "  'device': DEVICE,\n",
    "  'num_channels': 512,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridWorldNNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Instantiate GridWorld Neural Net with following configuration\n",
    "  nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 1\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 2\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 3\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 4\n",
    "  nn.BatchNorm2d(args.num_channels) X 4\n",
    "  nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024) # Fully-connected Layer 1\n",
    "  nn.Linear(1024, 512) # Fully-connected Layer 2\n",
    "  nn.Linear(512, self.action_size) # Fully-connected Layer 3\n",
    "  nn.Linear(512, 1) # Fully-connected Layer 4\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, args):\n",
    "    \"\"\"\n",
    "    Initialise game parameters\n",
    "\n",
    "    Args:\n",
    "      game: GridWorld Game instance\n",
    "        Instance of the GridWorldGame class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.board_x, self.board_y = game.get_board_size()\n",
    "    self.action_size = game.get_action_size()\n",
    "    self.args = args\n",
    "\n",
    "    super(GridWorldNNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1,\n",
    "                           padding=1)\n",
    "    self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "    self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn3 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn4 = nn.BatchNorm2d(args.num_channels)\n",
    "\n",
    "    self.fc1 = nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024)\n",
    "    self.fc_bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    #figure out how to connect score and rounds left in here somewhere\n",
    "\n",
    "    self.fc2 = nn.Linear(1024, 512)\n",
    "    self.fc_bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "    self.fc3 = nn.Linear(512, self.action_size)\n",
    "\n",
    "    self.fc4 = nn.Linear(512, 1)\n",
    "\n",
    "\n",
    "  def forward(self, s, currentScore, rounds_left):\n",
    "    \"\"\"\n",
    "    Controls forward pass of GridWorldNNet\n",
    "\n",
    "    Args:\n",
    "      s: np.ndarray\n",
    "        Array of size (batch_size x board_x x board_y)\n",
    "      scoreRoundsContext: np.ndarray\n",
    "        Array of size (batch_size x 2)\n",
    "    Returns:\n",
    "      Probability distribution over actions at the current state and the value of the current state.\n",
    "    \"\"\"\n",
    "    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "    rounds_left = rounds_left.view(-1, 1)                          # batch_siez x 1\n",
    "    currentScore = currentScore.view(-1, 1)                          # batch_siez x 1\n",
    "\n",
    "    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n",
    "\n",
    "    #need figure out how to put currentScore and rounds_left into the network here instead of and/or in addition to\n",
    "    #finessing the value function at the end\n",
    "\n",
    "    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
    "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "    pi = self.fc3(s)  # batch_size x action_size\n",
    "    v = self.fc4(s)   # batch_size x 1 # the way this is structured now this is\n",
    "                      # the average return per move, takes no account of rounds\n",
    "                      # left though so kind of rough, but let's start with\n",
    "                      # this for now\n",
    "    #softmax_pi = F.softmax(pi, dim=1) # batch_size x action_size\n",
    "    #v_pi_approx = torch.tensordot(softmax_pi, pi, dims=1) # batch_size x 1\n",
    "    log_softmax_pi = F.log_softmax(pi, dim=1) # batch_size x action_size\n",
    "    #corrected_pi = pi + (v - v_q_approx) # batch_size x num\n",
    "    #scaled_pi = ...\n",
    "    scaled_v = torch.add(torch.multiply(torch.add(torch.tanh(v), 1),\n",
    "                                        rounds_left), currentScore)\n",
    "    # Returns probability distribution over actions at the current state\n",
    "    # and the value of the current state.\n",
    "    return log_softmax_pi, scaled_v#, scaled_q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PolicyValueNetwork():\n",
    "  \"\"\"\n",
    "  Initiates the Policy-Value Network\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialise network parameters\n",
    "\n",
    "    Args:\n",
    "      game: GridWorld Game instance\n",
    "        Instance of the GridWorldGame class above;\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.nnet = GridWorldNNet(game, args)\n",
    "    self.board_x, self.board_y = game.get_board_size()\n",
    "    self.action_size = game.get_action_size()\n",
    "    self.nnet.to(args.device)\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def train(self, games, targetType='total',\n",
    "            verbose=True, num_epochs=args.epochs):\n",
    "    \"\"\"\n",
    "    Function to train network using just Value prediction loss\n",
    "\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples with each example is of form (board, pi, v)\n",
    "      targetType = 'total', 'value', 'policy'\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "    print('training on a set of examples')\n",
    "    for examples in games:\n",
    "      for epoch in range(num_epochs):\n",
    "        if verbose:\n",
    "          print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        v_losses = []   # To store the value losses per epoch\n",
    "        pi_losses = []  # To store the policy losses per epoch\n",
    "        t_losses = [] # To store the total losses per epoch\n",
    "        batch_count = int(len(examples) / args.batch_size)  # e.g. len(examples)=200, batch_size=64, batch_count=3\n",
    "        if verbose:\n",
    "          t = tqdm(range(batch_count), desc='Training Value Network')\n",
    "        else:\n",
    "          t = range(batch_count)\n",
    "        for _ in t:\n",
    "          sample_ids = self.rng.integers(len(examples), size=args.batch_size)  # Read the ground truth information from MCTS examples\n",
    "          boards, currentScores, rounds_lefts, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # Length of boards, pis, vis = 64\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          currentScores = torch.FloatTensor(np.array(currentScores).astype(np.float64))\n",
    "          rounds_lefts = torch.FloatTensor(np.array(rounds_lefts).astype(np.float64))\n",
    "          target_pis = torch.FloatTensor(np.array(pis).astype(np.float64))\n",
    "          target_vs = torch.FloatTensor(np.array(vs).reshape((-1, 1)).astype(np.float64)) # reshape to batch_size x 1 (not just batch_size) so can be treated the same as target pis\n",
    "\n",
    "          # Predict\n",
    "          # To run on GPU if available\n",
    "          boards = boards.contiguous().to(args.device)\n",
    "          currentScores = currentScores.contiguous().to(args.device)\n",
    "          rounds_lefts = rounds_lefts.contiguous().to(args.device)\n",
    "          target_pis = target_pis.contiguous().to(args.device)\n",
    "          target_vs = target_vs.contiguous().to(args.device)\n",
    "\n",
    "          # Compute output\n",
    "          out_pi, out_v = self.nnet(boards, currentScores, rounds_lefts)\n",
    "          #print(out_v.shape)\n",
    "          #print(target_vs.shape)\n",
    "          #print(out_pi.shape)\n",
    "          #print(target_pis.shape)\n",
    "\n",
    "          l_pi = self.loss_pi(target_pis, out_pi) # policy loss\n",
    "          l_v = self.loss_v(target_vs, out_v)    # value loss\n",
    "          l_total = torch.add(l_pi, l_v)        # total loss (no regularization term?!? or is that built in somewhere)\n",
    "\n",
    "          # Record loss\n",
    "          pi_losses.append(l_pi.item())\n",
    "          v_losses.append(l_v.item())\n",
    "          t_losses.append(l_total.item())\n",
    "          if verbose:\n",
    "            t.set_postfix(Loss_v=l_v.item(), Loss_pi=l_pi.item(), Loss_total=l_total.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          if targetType == 'total':\n",
    "            l_total.backward()\n",
    "          elif targetType == 'value':\n",
    "            l_v.backward()\n",
    "          elif targetType == 'policy':\n",
    "            l_pi.backward()\n",
    "          else:\n",
    "            print('Invalid trainType chosen')\n",
    "          optimizer.step()\n",
    "        if verbose:\n",
    "          print('v loss: ' + str(np.mean(v_losses)) +\n",
    "                ' ::: pi loss: ' + str(np.mean(pi_losses)) +\n",
    "                ' ::: total loss: ' + str(np.mean(t_losses)))\n",
    "        else:\n",
    "          if (epoch + 1) == args.epochs:\n",
    "            print('Last Epoch Losses:')\n",
    "            print('v loss: ' + str(np.mean(v_losses)) +\n",
    "                  ' ::: pi loss: ' + str(np.mean(pi_losses)) +\n",
    "                  ' ::: total loss: ' + str(np.mean(t_losses)))\n",
    "\n",
    "\n",
    "  def predict(self, board, score, rounds_left):\n",
    "    \"\"\"\n",
    "    Function to perform prediction of both policy and value, note\n",
    "    policy is exponentiated on the way out so these should be directly\n",
    "    interpretable as probabilities\n",
    "\n",
    "    Args:\n",
    "      board: batch x 7 x 7 np.ndarray giving board positions\n",
    "      score: batch np.ndarray the current scores\n",
    "      rounds_left: batch np.ndarray of the turns left\n",
    "\n",
    "    Returns:\n",
    "      pi: probabilities over actions\n",
    "      v: predicted score at game end;\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    # start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(-1, self.board_x, self.board_y)\n",
    "\n",
    "    score = torch.FloatTensor(np.array(score, dtype=np.float64))\n",
    "    score = score.contiguous().to(args.device)\n",
    "    score = score.view(-1, 1)\n",
    "\n",
    "    rounds_left = torch.FloatTensor(np.array(rounds_left, dtype=np.float64))\n",
    "    rounds_left = rounds_left.contiguous().to(args.device)\n",
    "    rounds_left = rounds_left.view(-1, 1)\n",
    "\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "        pi, v = self.nnet(board, score, rounds_left)\n",
    "    return torch.exp(pi).data.cpu().numpy(), v.data.cpu().numpy().flatten()\n",
    "\n",
    "  def loss_v(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Mean squared error\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth end game scores corresponding to input board state\n",
    "      outputs: np.ndarray\n",
    "        value prediction of network as raw score\n",
    "\n",
    "    Returns:\n",
    "      MSE Loss calculated as: square of the difference between model predictions\n",
    "      and the ground truth and averaged across the whole batch\n",
    "    \"\"\"\n",
    "    # Mean squared error (MSE)\n",
    "    return torch.sum((targets - outputs)**2) / targets.size()[0]\n",
    "\n",
    "  def loss_pi(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Negative Log Likelihood(NLL) of Targets\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth action played during recording of \"expert\" player\n",
    "      outputs: np.ndarray\n",
    "        log-softmax action probability predictions of network\n",
    "\n",
    "    Returns:\n",
    "      Negative Log Likelihood calculated as:\n",
    "    \"\"\"\n",
    "    return -torch.sum(targets * outputs) / targets.size()[0]\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Code Checkpointing\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(folder):\n",
    "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "      os.mkdir(folder)\n",
    "    else:\n",
    "      print(\"Checkpoint Directory exists! \")\n",
    "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
    "    print(\"Model saved! \")\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Load code checkpoint\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "      raise (\"No model in path {}\".format(filepath))\n",
    "\n",
    "    checkpoint = torch.load(filepath, map_location=args.device)\n",
    "    self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarlo():\n",
    "  \"\"\"\n",
    "  Implementation of Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet, default_depth=5, random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "  # Call this rollout\n",
    "  def simulate(self, board, actions, action_indexes, depth=None):\n",
    "    \"\"\"\n",
    "    Helper function to simulate one Monte Carlo rollout\n",
    "\n",
    "    Args:\n",
    "      board: triple (batch_size x x_size x y_size np.array of board position,\n",
    "                     scalar of current score,\n",
    "                     scalar of rounds left\n",
    "      actions: batch size list/array of integer idexes for moves on each board\n",
    "      these are assumed to be legal, no check for validity of moves\n",
    "    Returns:\n",
    "      temp_v:\n",
    "        Terminal State\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board[0].shape\n",
    "    next_board = self.game.get_next_state(board, actions, action_indexes)\n",
    "    if depth is None:\n",
    "      depth = self.default_depth\n",
    "    # potentially expand the game tree here,\n",
    "    # but just do straigt rollouts after this\n",
    "    # doesn't expand to deal with all random food generation possibilities\n",
    "    # just expands based on the actions given\n",
    "    expand_bs, _, _ = next_board[0].shape\n",
    "\n",
    "    for i in range(depth):  # maxDepth\n",
    "      if next_board[2][0] <= 0:\n",
    "        # check that game isn't over\n",
    "        # assumes all boards have the same rounds left\n",
    "        # no rounds left return scores as true values\n",
    "        terminal_vs = next_board[1].copy()\n",
    "        return terminal_vs\n",
    "      else:\n",
    "        pis, vs = self.nnet.predict(next_board[0], next_board[1], next_board[2])\n",
    "        valids = self.game.get_valid_actions(next_board)\n",
    "        masked_pis = pis * valids\n",
    "        sum_pis = np.sum(masked_pis, axis=1)\n",
    "        probs = np.array(\n",
    "            [masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "             else valid / valid.sum()\n",
    "             for valid, masked_pi in zip(valids, masked_pis)])\n",
    "        samp = self.rng.uniform(size = expand_bs).reshape((expand_bs,1))\n",
    "        sampled_actions = np.argmax(probs.cumsum(axis=1) > samp, axis=1)\n",
    "      next_board = self.game.get_next_state(next_board, sampled_actions)\n",
    "\n",
    "    pis, vs = self.nnet.predict(next_board[0], next_board[1], next_board[2])\n",
    "    return vs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarloBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Player based on Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet,\n",
    "               default_depth=1,\n",
    "               default_rollouts=1,\n",
    "               default_K=4,\n",
    "               default_temp=1.0,\n",
    "               random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.default_rollouts = default_rollouts\n",
    "    self.mc = MonteCarlo(self.game, self.nnet, self.default_depth)\n",
    "    self.default_K = default_K\n",
    "    self.default_temp = default_temp\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def play(self, board,\n",
    "           num_rollouts=None,\n",
    "           rollout_depth=None,\n",
    "           K=None,\n",
    "           softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: triple (batch x num_rows x num_cols np.ndarray of board position,\n",
    "                     batch x a of current score,\n",
    "                     batch x 1 of rounds left\n",
    "\n",
    "    Returns:\n",
    "      best_action: tuple\n",
    "        (avg_value, action) i.e., Average value associated with corresponding action\n",
    "        i.e., Action with the highest topK probability\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board[0].shape\n",
    "    if num_rollouts is None:\n",
    "      num_rollouts = self.default_rollouts\n",
    "    if rollout_depth is None:\n",
    "      rollout_depth = self.default_depth\n",
    "    if K is None:\n",
    "      K = self.default_K\n",
    "    if softmax_temp is None:\n",
    "      softmax_temp = self.default_temp\n",
    "\n",
    "    # figure out top k actions acording to normalize action probability\n",
    "    # given by our policy network prediction\n",
    "    pis, vs = self.nnet.predict(board[0], board[1], board[2])\n",
    "    valids = self.game.get_valid_actions(board)\n",
    "    masked_pis = pis * valids  # Masking invalid moves\n",
    "    sum_pis = np.sum(masked_pis, axis=1)\n",
    "    num_valid_actions = np.sum(valids, axis=1)\n",
    "    effective_topk = np.array(np.minimum(num_valid_actions, K), dtype= int)\n",
    "    probs = np.array([masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "                      else valid / valid.sum()\n",
    "                      for valid, masked_pi in zip(valids, masked_pis)])\n",
    "    partioned = np.argpartition(probs,-effective_topk)\n",
    "    topk_actions = [partioned[g,-(ii+1)]\n",
    "                      for g in range(batch_size)\n",
    "                        for ii in range(effective_topk[g])]\n",
    "    topk_actions_index = [ii\n",
    "                            for ii, etk in enumerate(effective_topk)\n",
    "                              for _ in range(etk)]\n",
    "    values = np.zeros(len(topk_actions))\n",
    "    # Do some rollouts\n",
    "    for _ in range(num_rollouts):\n",
    "      values = values + self.mc.simulate(board, topk_actions,\n",
    "                                         topk_actions_index,\n",
    "                                         depth=rollout_depth)\n",
    "    values = values / num_rollouts\n",
    "\n",
    "    value_expand = np.zeros((batch_size, x_size*y_size))\n",
    "    value_expand[(topk_actions_index, topk_actions)] = values\n",
    "    #softmax_normalize those values into a selection prob\n",
    "    v_probs = np.exp(value_expand/softmax_temp) / np.sum(\n",
    "        np.exp(value_expand/softmax_temp), axis=1, keepdims=True)\n",
    "    v_probs = v_probs * valids\n",
    "    v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "    samp = self.rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "    #print(samp)\n",
    "    #print(v_probs.cumsum(axis=1))\n",
    "    sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "    a_1Hots = np.zeros((batch_size, x_size*y_size))\n",
    "    a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    #print(topk_actions)\n",
    "    #print(topk_actions_index)\n",
    "    #print(values)\n",
    "    #no baseline probs here so just return 1hots twice\n",
    "    return sampled_actions, a_1Hots, v_probs\n",
    "\n",
    "\n",
    "\n",
    "# Content review setup\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.1.1: Initializing Gridworld\n",
    "\n",
    "Before we introduce an organism with **behaviour** we're going to build an **environment** for them to behave in. To start, this world will consist of a 7 x 7 grid. Let's make a picture of that and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## TODO for students: Replace ... with the correct arguments(inputs) in the\n",
    "## make_grid function below to make our grid the right size and shape (7x7).\n",
    "## You can use the tool tip by hovering over the word make_grid, when this is\n",
    "## the active cell, to find out how to use the make_grid function. You can\n",
    "## also use the tool tip to view the source code. How does it work?\n",
    "# Comment out or remove these next two lines.\n",
    "raise NotImplementedError(\n",
    "  \"Exercise: make a grid using the make_grid function\")\n",
    "############################################################################\n",
    "\n",
    "fig, ax = make_grid(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "with plt.xkcd():\n",
    "  fig, ax = make_grid(7, 7)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Bonus: see where the actual code is:*\n",
    "\n",
    "Find and tweak the make_grid function in the setup cell above to make the grid lines green. Where is the make_grid function defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Wow, what a boring environment. Let's add an organism and something for that organism to interact with. We'll start with 10 food items scattered randomly throughout the grid, never more than one food item per grid cell. To plot these food items we need their locations. We will set these by randomly sampling grid coordinates [without replacement](## \"never picking the same (row,col) coordinate pair twice\"). We'll place the organism in the same way and not on a food item to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Replace ... in init_loc(...) to initialize the right\n",
    "# number of food item locations and critter locations in coordinates that make\n",
    "# sense for our grid environment. Then replace the ... in rc_plotting[...] to\n",
    "# index the plotting coordinates for the food locations.\n",
    "# Hint: The syntax for indexing elements of numpy arrays using [] can be\n",
    "# confusing at first. If you're lost read the docs,\n",
    "# https://numpy.org/doc/stable/user/basics.indexing.html and add some code\n",
    "# cells below to play around with indexing and displaying different sub-arrays\n",
    "# of the rc_plotting array.\n",
    "# Comment out or remove this next lines.\n",
    "raise NotImplementedError(\"Exercise: initialize food and critter locations\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def init_loc(n_rows, n_cols, num, rng=None):\n",
    "  \"\"\"\n",
    "  Samples random 2d grid locations without replacement\n",
    "\n",
    "  Args:\n",
    "    n_rows: int\n",
    "    n_cols: int\n",
    "    num:    int, wnumber of samples to generate, should\n",
    "            throw an error ifnum <= n_rows x n_cols\n",
    "\n",
    "  Optional Keyword Args\n",
    "    rng:    instance of numpy.random's default random number generator\n",
    "            (to enable reproducability)\n",
    "\n",
    "  Returns:\n",
    "    int_loc:  ndarray(int) of flat indices for the grid\n",
    "    rc_index: (ndarray(int), ndarray(int)) a pair of arrays the first\n",
    "      giving the row indices, the second giving the col indices, useful\n",
    "      for indexing an n_rows by n_cols numpy array\n",
    "    rc_plotting: ndarray(int) num x 2, same rc coordinates but structured\n",
    "      in a way that matplotlib likes\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "  int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "  rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "  rc_plotting = np.array(rc_index).T\n",
    "  return int_loc, rc_index, rc_plotting\n",
    "\n",
    "fig, ax = make_grid(7, 7)\n",
    "int_locs, rc_index, rc_plotting = init_loc(..., ..., ...)\n",
    "\n",
    "rc_critter = (rc_plotting[0])\n",
    "plot_critter(fig, ax, rc_critter)\n",
    "\n",
    "rc_food = rc_plotting[...]\n",
    "plot_food(fig, ax, rc_food)\n",
    "\n",
    "fig.legend(loc='outside right upper')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "\n",
    "def init_loc(n_rows, n_cols, num, rng=None):\n",
    "  \"\"\"\n",
    "  Samples random 2d grid locations without replacement\n",
    "\n",
    "  Args:\n",
    "    n_rows: int\n",
    "    n_cols: int\n",
    "    num:    int, wnumber of samples to generate, should\n",
    "            throw an error ifnum <= n_rows x n_cols\n",
    "  Optional Keyword Args\n",
    "    rng:    instance of numpy.random's default rng (reproducability)\n",
    "\n",
    "  Returns:\n",
    "    int_loc: ndarray(int) of flat indices for a grid\n",
    "    rc_index: (ndarray(int), ndarray(int)) a pair of arrays the first\n",
    "      giving the row indices, the second giving the col indices, useful\n",
    "      for indexing an n_rows by n_cols numpy array\n",
    "    rc_plotting: ndarray(int) num x 2, same rc coordinates but structured\n",
    "      in the way that matplotlib likes\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(seed=SEED)\n",
    "  int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "  rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "  rc_plotting = np.array(rc_index).T\n",
    "  return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "with plt.xkcd():\n",
    "  fig, ax = make_grid(7, 7)\n",
    "  int_locs, rc_index, rc_plotting = init_loc(7, 7, 11)\n",
    "\n",
    "  rc_critter = (rc_plotting[0])\n",
    "  plot_critter(fig, ax, rc_critter)\n",
    "\n",
    "  rc_food = rc_plotting[1:]\n",
    "  plot_food(fig, ax, rc_food)\n",
    "\n",
    "  fig.legend(loc='outside right upper')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "content_review(\"Sequence 1.1.1 Micro 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# 1.1.1.2: Random Eating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have an environment scattered with food and an organism, let's introduce some behaviour. The organism drifts around the environment randomly and eats the food it happens to stumble upon. (Can you think of any organisms that employ this strategy? [hint](## \"think about the way very very small biological things move around\")). When food is eaten, the organism gets a **reward**, in this case a *Food Eaten* point, and a new food item appears randomly somewhere else in the environment (that doesn't already have food). Run the code cell below to see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Random Movement\n",
    "# @markdown Don't worry about how this code works â€“ just **run this cell** and the one below for now and watch what happens\n",
    "\n",
    "rng = np.random.default_rng(seed=420)\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30, rng=rng)\n",
    "random_igwg = InteractiveGridworld(gwg)\n",
    "display(random_igwg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to watch the random organism play\n",
    "for ii in range(30):\n",
    "  time.sleep(0.1)\n",
    "  random_igwg.random_click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Question:* When the organism is just drifting around randomly how good is it at eating lots of food, what is its efficiency in terms of food per movement? Now run the cell above a few more times. Does the organism always eat the same amount of food or does it change between simulation runs? [explanation](## \"The amount of food eaten varies from simulation run to simulation run,usually the organism manages to eat one or two pieces of food, sometimes more\n",
    "sometimes less.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Bonus: see how the effectiveness of a strategy  depend on the environment:*\n",
    "\n",
    "Before we move on it's important to test that our simulation is running as we expect. Randomness can make testing hard, but can be overcome in part by setting up the environment in such a way that the outcome becomes deterministic. In the code cells bellow change how the Gridworld is initialized. By altering the size, shape and number of food items available create a scenario where the organism will always achieve perfect efficiency and a scenario where the organism will fail completely.\n",
    "\n",
    "We will do this here by either providing food everywhere or nowhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TODO for students: Replace the ...'s in GridworldGame(...) to initialize a\n",
    "# Gridworld where the organism is always 100% efficient. Food. Everywhere.\n",
    "raise NotImplementedError(\"Exercise: make random movement 100% efficient\")\n",
    "################################################################################\n",
    "\n",
    "gwg = GridworldGame(1, ..., ..., ..., 30)\n",
    "random_igwg_100 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_100.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_100.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "gwg = GridworldGame(1, 2, 2, 3 , 30)\n",
    "random_igwg_100 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_100.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_100.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to watch the random organism play in the environment above\n",
    "for ii in range(30):\n",
    "  time.sleep(0.05)\n",
    "  random_igwg_100.random_click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Ok. We have just seen a super successful (albeit completely dumb) organism. Lets see if we can have one where any organism would fail (maybe surprisingly intelligence can not make food out of nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TODO for students: Replace the ...'s in GridworldGame(...) to initialize a\n",
    "# Gridworld where the organism is always 0% efficient.\n",
    "raise NotImplementedError(\"Exercise: make random movement 0% efficient\")\n",
    "################################################################################\n",
    "\n",
    "gwg = GridworldGame(1, ..., ..., ..., 30)\n",
    "random_igwg_0 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_0.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_0.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "gwg = GridworldGame(1, 2, 2, 0, 30)\n",
    "random_igwg_0 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_0.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_0.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to watch the random organism play in the environment above\n",
    "for ii in range(30):\n",
    "  random_igwg_0.random_click()\n",
    "  time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "content_review(\"Sequence 1.1.1 Micro 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# 1.1.1.3: Better Than Random Eating\n",
    "Now it's your turn to actually control the organism with some level of intelligence (give it your all). Run the next cell and see how much more efficient than random drifting your control of the organism is in terms of food per movement. Does intelligence help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Controlled Movement\n",
    "# @markdown Don't worry about how this code works â€“ just **run the cell** and then use the buttons to guide the organism\n",
    "\n",
    "# user in control\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "h2h_igwg = Head2HeadGridworld(gwg)\n",
    "display(h2h_igwg.b_fig0.canvas)\n",
    "display(h2h_igwg.b_fig1.canvas)\n",
    "display(h2h_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg.boards_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully your performance was more successful than random flailing (if not, reset to the safe point). Even in this relatively simple and contrived foraging scenario intelligence can help a lot. What kinds of strategies and heuristics did you use to guide your choice of direction? A fundamental purpose of nervous systems and brains is to solve problems of this kindâ€”chosing which actions to take based on environmental inputs to maximize rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "content_review(\"Sequence 1.1.1 Micro 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "#1.1.1.4: Optimized Eating\n",
    "\n",
    "\n",
    "Let's welcome a special guest, GW7x7-10-30, from the final chapter of this book. Utilizing a blend of deep reinforcement learning and Monte-Carlo search based on the AlphaZero optimization algorithm, GW7x7-10-30 has achieved mastery of the 7x7 Gridworld environment, with 10 food items and a game duration of 30 rounds.\n",
    "\n",
    "The AlphaZero optimization algorithm finds inspiration from our understanding of the brain, and also draws upon various concepts from machine learning. As such, our specific computer implementation of the algorithm is unlikely to mirror the learning algorithms used by the brain in any particular detail.\n",
    "\n",
    "Despite this lack of immediate correspondence, we can still gain significant insight by identifying the generalized form of learning problems that the brain solves together with the classes of optimization algorithms capable of feasibly solving these learning problems subject to biological constraints. These constraints derive from a multitude of factors based on the evolution, ecology, physiology, development, etc, of the organism. These insights will enable us to deduce the most probable types of learning algorithms employed by brains.\n",
    "\n",
    "Subsequently, these deductions can guide us in seeking out the specific mechanisms and intricate details of the learning algorithms found in brains. Throughout this book, we will focus on introducing the general learning problems encountered by living organisms. We will identify different machine learning techniques that can presently solve these problems under various conditions. Furthermore, we will link the feasible machine learning solutions of these broader learning problems to our current empirical understanding of how a brain might implement similar solutions.\n",
    "\n",
    "Our aim with this approach is to foster a principled, systematic, and integrative groundwork for neuroscience research. Now, let's run the next cells to see who is more efficient â€“ you or GW7x7-10-30. Reading this book will empower you to design the next generation of GW7x7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Optimized Movement\n",
    "# @markdown Don't worry about how this code works â€“ **run this cell** to set up the superorganism and an environment for it and you.\n",
    "\n",
    "# initialize the game, network, and MonteCarlo player\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30)\n",
    "pvnetMC = PolicyValueNetwork(gwg)\n",
    "mcp = MonteCarloBasedPlayer(gwg, pvnetMC, default_depth=2,\n",
    "                            default_rollouts=8, default_temp=0.1)\n",
    "\n",
    "\n",
    "#grab the saved model from the repo or where it ends up being hosted\n",
    "url = \"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/data/pvnetMC.pth.tar\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "  filename = os.path.basename(url)\n",
    "  # Write the contents to a file in the current working directory\n",
    "  with open(filename, 'wb') as file:\n",
    "    file.write(r.content)\n",
    "    #print(f'{filename} downloaded successfully.')\n",
    "else:\n",
    "  print('Error occurred while downloading the file.')\n",
    "\n",
    "# load the saved model\n",
    "pvnetMC.load_checkpoint(folder=os.getcwd(), filename='pvnetMC.pth.tar')\n",
    "\n",
    "# user in control versus mc player\n",
    "gwg = GridworldGame(2, 7, 7, 10, 30,\n",
    "                    rng=np.random.default_rng(seed=2023))\n",
    "h2h_igwg = Head2HeadGridworld(gwg, player=mcp)\n",
    "display(h2h_igwg.b_fig0.canvas)\n",
    "display(h2h_igwg.b_fig1.canvas)\n",
    "display(h2h_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg.boards_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Who was more efficient in this environment you or gw7x7-10-30? If gw7x7-10-30 was better, you really have read this book ðŸ˜‰ (If you can't beat the AIs, at least learn how to program them.) Even if you were about as good as gw7x7-10-30 you still might want to read this book. A deep understanding of the optimization processes that shape behaviour in simple organism-environment systems like this one will allow for generalizition to more intricate systems, specifically, a rich understanding of how brains generate adaptive behaviour as a result of optimzation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown\n",
    "content_review(\"Sequence 1.1.1 Micro 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Comprehension Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Quiz\n",
    "# @markdown **Run this cell** to take the quiz\n",
    "comprehension_quiz = [\n",
    "{\n",
    "  \"question\": \"What does a policy represent in the context of behaviour?\",\n",
    "  \"type\": \"multiple_choice\",\n",
    "  \"answers\": [\n",
    "  {\n",
    "    \"answer\": \"The evolutionary history of an organism\",\n",
    "    \"correct\": False,\n",
    "    \"feedback\": \"This is true in a broad and abstract sense, but there is a more precise answer here.\"\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"The environment in which an organism lives\",\n",
    "    \"correct\": False,\n",
    "    \"feedback\": \"There is a sense in which a policy shaped by evolution can reflect aspects of an organism's environment, but there is a more precise answer here.\"\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"The formal description of behaviour as a function that maps experiences to actions\",\n",
    "    \"correct\": True,\n",
    "    \"feedback\": \"Correct.\"\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"The randomness present in an organism's behavior\",\n",
    "    \"correct\": False,\n",
    "    \"feedback\": \"The policy might have randomness in it, but that's not what it is.\"\n",
    "  }]\n",
    "},\n",
    "{\n",
    "  \"question\": \"How is a policy evaluated in terms of its goodness?\",\n",
    "  \"type\": \"multiple_choice\",\n",
    "  \"answers\": [\n",
    "  {\n",
    "    \"answer\": \"By integrating rewards and environmental signals into a loss/objective function\",\n",
    "    \"correct\": True,\n",
    "    \"feedback\": \"Corret, 'goodness' needs to be formalized in a loss/objective funtion\"\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"By measuring the organism's fitness in the environment\",\n",
    "    \"correct\": True,\n",
    "    \"feedback\": \"This is one important way of evaluating a policy, but there is a more generally correct answer here.\"\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"By determining the amount of randomness present in the policy\",\n",
    "    \"correct\": False,\n",
    "    \"feedback\": \"Incorrect.\"\n",
    "  },\n",
    "  {\n",
    "    \"answer\": \"By analyzing the organism's evolutionary adaptations\",\n",
    "    \"correct\": False,\n",
    "    \"feedback\": \"Incorrect.\"\n",
    "  }]\n",
    "},\n",
    "{\n",
    "  \"question\": \"What is stochasticity in the context of behaviour?\",\n",
    "  \"type\": \"multiple_choice\",\n",
    "  \"answers\": [\n",
    "    {\n",
    "      \"answer\": \"The specific niche an organism occupies within its environment\",\n",
    "      \"correct\": False,\n",
    "      \"feedback\": \"Incorrect.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"The ability of an organism to adapt to changing environmental conditions\",\n",
    "      \"correct\": False,\n",
    "      \"feedback\": \"Incorrect.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"The random elements present in both the environment and an organism's behavior\",\n",
    "      \"correct\": True,\n",
    "      \"feedback\": \"Correct.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"The process of optimizing a policy to achieve better outcomes\",\n",
    "      \"correct\": False,\n",
    "      \"feedback\": \"Incorrect.\"\n",
    "    }]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the main difference between random eating and controlled movement in the environment?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "    {\n",
    "      \"answer\": \"Random eating involves unpredictable movements, while controlled movement can be planned and strategic.\",\n",
    "      \"correct\": True,\n",
    "      \"feedback\": \"Correct.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"Random eating leads to higher efficiency, while controlled movement leads to lower efficiency.\",\n",
    "      \"correct\": False,\n",
    "      \"feedback\": \"Incorrect.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"Random eating relies on external cues, while controlled movement relies on internal motivations.\",\n",
    "      \"correct\": False,\n",
    "      \"feedback\": \"Incorrect.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"Random eating results in adaptive behavior, while controlled movement leads to stagnation.\",\n",
    "      \"correct\": False,\n",
    "      \"feedback\": \"Incorrect.\"\n",
    "    }]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What is the significance of GW7x7-10-30 in the context of optimized eating?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "    {\n",
    "      \"answer\": \"It represents a time-traveling superorganism with advanced cognitive abilities.\",\n",
    "      \"correct\": False,\n",
    "      \"feedback\": \"Incorrect.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"It demonstrates the limitations of optimized behavior in a simple environment.\",\n",
    "      \"correct\": True,\n",
    "      \"feedback\": \"It could serve this purpose, though that wasn't the main reason we introduced it here.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"It showcases the potential efficiency achievable through optimized behavior.\",\n",
    "      \"correct\": True,\n",
    "      \"feedback\": \"Correct.\"\n",
    "    },\n",
    "    {\n",
    "      \"answer\": \"It serves as a benchmark for comparing different organisms' performance.\",\n",
    "      \"correct\": True,\n",
    "      \"feedback\": \"It could serve this purpose, though that wasn't the main reason we introduced it here.\"\n",
    "    }]\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "display_quiz(comprehension_quiz)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "e97kdmn_my25"
   ],
   "include_colab_link": true,
   "name": "P1C1_Sequence1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
