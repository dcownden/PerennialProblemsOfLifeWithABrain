{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C1_BehaviourAsPolicy/P1C1_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/P1C1_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is a third test for an upcoming text book on computational neuroscience from an optimization/learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as a learning algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do.\n",
    "\n",
    "# **Part 1 Behaviour, Environments and Optimization: Evolution and Learning**\n",
    "\n",
    "### **Animals are adapted to their specific environments; their behaviour is best understood within the context of their evolutionary environment.**\n",
    "\n",
    "### Objective: Part 1 of the book aims to introduce the fundamental concepts of\n",
    "* **environment**, the (statistical) properties of where an organism lives\n",
    "* **behaviour**, the statistics of what the organism does\n",
    "* **optimization**, how learning and evolution shape an organism's behaviour to make it better suited to its environment\n",
    "\n",
    "This very much is the core of why we are writing this book: we can view pretty much anything happening in the brain (and biology) as being part of a process that brings about improvement in this sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# Chapter 1.1 Behaviour as a Policy in an Environmental Context\n",
    "\n",
    "### Objective: This chapter develops examples of how behaviour is described and evaluated in relation to its [goodness](## \"This is a very loaded term, to be unpacked carefully later\") within a specific environmental niche. Please note, we've used mouseover text for hints, solutions, and footnotes (like with 'goodness' here). These appear as blue, underlined text (for now) but are not clickable links; instead, hovering over them will display additional information.\n",
    "\n",
    "You will learn:\n",
    "*   What is a policy? A policy is a formalization of behaviour as a function that takes an organism's experiences of their environment as an input and outputs the organism's actions.\n",
    "*   What is a good policy? The rewards and other environmental signals resulting from the organism's actions in the environment are integrated into a Loss/Objective function to evaluate, and potentially improve, a policy.\n",
    "*   What is stochasticity? Both the environment and an organism's behavior can contain random elements. This randomness can pose challenges when evaluating policies as it becomes difficult to determine whether poor outcomes are due the policy itself or simply bad luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **Sequence 1.1.3: Abstraction and Reasoning with Policies**\n",
    "\n",
    "### Objective: In this sequence, we will reason about and compare different policies using the Gridworld environment-organism system from 1.1.1 and our understanding of the input --> compute --> output process as a policy from 1.1.2. In this sequence you will:\n",
    "* Visualize and compare different policies.\n",
    "* Incorporate a simple form of memory into a policy and expand our sense of what a policy as an abstraction of sensation --> neural computation --> behaviour can represent\n",
    "* See how interaction in a shared environment can impact policy performance.\n",
    "\n",
    "This will take us further along in answering the question 'What is a good policy?', specifically starting to appreciate how open ended and context dependent this question is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Picture showing receptor/input, brain/compute, musceles/output embedded in the environment feedback loop, this time extended to allow the agent to have some internal state that it propigates into the next time step (some very limited version of 'memory'). (The following is a place holder for our actual image, same as last time, but it's close to the right thing, though not as close as in 1.1.2.)\n",
    "![Reinforcement_learning_diagram.svg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "\n",
    "!pip install ipympl vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import copy\n",
    "from scipy.spatial.distance import cdist\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
    "             'gw_widgets.py', 'gw_NN_RL.py']\n",
    "#filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C1_S3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "# More plotting functions\n",
    "#################################################\n",
    "\n",
    "\n",
    "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
    "                    name=None):\n",
    "  \"\"\"\n",
    "  Plot vector field indicating critter direction probabilities.\n",
    "\n",
    "  Args:\n",
    "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
    "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
    "      and values as lists of direction probabilities corresponding to the\n",
    "      directions ['right', 'down', 'left', 'up'].\n",
    "    critter (int): Identifier for which critter directions are associated with.\n",
    "    deterministic (bool, optional): If True, the probabilities array is\n",
    "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
    "      and are larger. Defaults to False.\n",
    "  \"\"\"\n",
    "\n",
    "  #looks like direction ignores inverted axis\n",
    "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
    "                       'left': (-1, 0), 'up': (0, 1)}\n",
    "  # but offsets need to be aware of inverted\n",
    "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
    "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
    "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
    "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
    "  # same logic for colors\n",
    "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
    "  # Get the offset and color for this critter\n",
    "  critter_offset = critter_offsets[critter]\n",
    "  critter_color = critter_colors[critter]\n",
    "\n",
    "  # Add legend only if critter is not 0\n",
    "  custom_leg_handles = []\n",
    "  if critter != 0:\n",
    "    if name is None:\n",
    "      name = f'Critter {critter}'\n",
    "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
    "    # Add the legend for this critter\n",
    "    custom_leg_handles.append(legend_patch)\n",
    "\n",
    "  C, R, U, V, A = [], [], [], [], []\n",
    "\n",
    "  for loc in loc_prob_dict.keys():\n",
    "    row, col = loc\n",
    "    probs = loc_prob_dict[loc]\n",
    "    for dir_key, prob in probs.items():\n",
    "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
    "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
    "      U.append(direction_vectors[dir_key][0])\n",
    "      V.append(direction_vectors[dir_key][1])\n",
    "\n",
    "      if deterministic:\n",
    "        A.append(1 if prob == max(probs.values()) else 0)\n",
    "      else:\n",
    "        A.append(prob)\n",
    "\n",
    "  linewidth = 1.5 if deterministic else 0.5\n",
    "  scale = 15 if deterministic else 30\n",
    "\n",
    "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
    "            scale=scale, linewidth=linewidth)\n",
    "  return fig, ax, custom_leg_handles\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for further use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  # Create a new figure and axes with given figsize\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  # Set width and height padding, remove horizontal and vertical spacing\n",
    "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
    "  # Show right and top borders (spines) of the plot\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  # Set major ticks (where grid lines will be) on x and y axes\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Set labels for major ticks with font size of 8\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "  # Set minor ticks (no grid lines here) to be between major ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "  # Move x-axis ticks to the top of the plot\n",
    "  ax.xaxis.tick_top()\n",
    "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "  # Remove minor ticks (not the grid lines)\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "  # Set limits of x and y axes\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  # Invert y axis direction\n",
    "  ax.invert_yaxis()\n",
    "  # If title is provided, set it as the figure title\n",
    "  if title is not None:\n",
    "    fig.suptitle(title)\n",
    "  # Hide header and footer, disable toolbar and resizing of the figure\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  # Redraw the figure with these settings\n",
    "  fig.canvas.draw()\n",
    "  # Return figure and axes handles for further customization\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PathCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=150, marker='o', color='red', label='Food')\n",
    "  rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "  #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]]) -> List[Dict[str, object]]:\n",
    "  \"\"\"\n",
    "  Plots multiple types of \"critters\" on a grid implied by the given\n",
    "  fig, ax arguments.\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects.\n",
    "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
    "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
    "    critter.\n",
    "\n",
    "  Returns:\n",
    "    Updated critter_specs with handles.\n",
    "  \"\"\"\n",
    "  for spec in critter_specs:\n",
    "    # Ensure required keys are present\n",
    "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
    "      if key not in spec:\n",
    "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
    "    handle_ = spec.get('handle')\n",
    "    if handle_ is None:\n",
    "      handle_ = ax.scatter([], [], s=250, marker=spec['marker'],\n",
    "                           color=spec['color'], label=spec['name'])\n",
    "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
    "    spec.update({'handle': handle_})\n",
    "  return critter_specs\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc,\n",
    "                 critter=None, critter_name='Critter'):\n",
    "  \"\"\"\n",
    "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PathCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h',\n",
    "                         color='blue', label=critter_name)\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov, fov=None):\n",
    "  \"\"\"\n",
    "  Plots a mask on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter: ndarry(int) (row,col) of the critter\n",
    "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib Image object of mask, either new if no handle\n",
    "    was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
    "  mask_array = np.ones((n_rows, n_cols, 4))\n",
    "  mask_array[:, :, :3] = 0.5  # light grey color\n",
    "  if has_fov == True:\n",
    "    mask_array[:, :, 3] = 0.5  # 50% opacity\n",
    "    # Create arrays representing the row and column indices\n",
    "    rows = np.arange(n_rows)[:, np.newaxis]\n",
    "    cols = np.arange(n_cols)[np.newaxis, :]\n",
    "    # Iterate over each critter location\n",
    "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
    "    # Set the region within the specified radius around the critter to transparent\n",
    "    mask_array[dist <= radius, 3] = 0\n",
    "  else:\n",
    "    mask_array[:, :, 3] = 0\n",
    "\n",
    "  if fov is None:\n",
    "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
    "  else:\n",
    "    fov.set_data(mask_array)\n",
    "\n",
    "  return fov\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# new definition gridworld board to support multiple agents\n",
    "###########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game board that\n",
    "  define the logic of the game, and allows for multiple critters on the same\n",
    "  board\n",
    "\n",
    "  board state is represented by primarily by pieces, score, and rounds left\n",
    "  pieces is a batch x n_rows x n_cols numpy array positive integers are critter\n",
    "  locations 0's are empty space and -1's are food.\n",
    "\n",
    "  For pieces first dim is batch, second dim row , third is col,\n",
    "  so pieces[0][1][7] is the square in row 2, in column 8 of the first board in\n",
    "  the batch of boards.\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game.\n",
    "\n",
    "  active_player keeps track of which players turn it is\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_critters=2, num_food=10,\n",
    "               lifetime=30, rng = None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_critters = num_critters\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"Set up starting board using game parameters\"\"\"\n",
    "    #set rounds_left and score\n",
    "    self.rounds_left = (np.ones(self.batch_size) *\n",
    "                        self.lifetime * self.num_critters)\n",
    "    # each players move counts down the clock so making this a multiple of the\n",
    "    # number of critters ensures every player gets an equal number of turns\n",
    "    self.scores = np.zeros((self.batch_size, self.num_critters))\n",
    "    # create an empty board array.\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols))\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+num_critter because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(\n",
    "        self.n_rows, self.n_cols, self.num_food+self.num_critters)\n",
    "      # critter random start locations\n",
    "      for c_ in np.arange(self.num_critters):\n",
    "        self.pieces[(ii, rc_idx[0][c_], rc_idx[1][c_])] = c_ + 1\n",
    "      # food random start locations\n",
    "      self.pieces[(ii, rc_idx[0][self.num_critters:],\n",
    "                   rc_idx[1][self.num_critters:])] = -1\n",
    "    self.active_player = 0\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy(),\n",
    "             'active_player': copy(self.active_player)}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\" board is dictionary giving game state a triple of np arrays\n",
    "      pieces:        numpy array (batch_size x n_rows x n_cols),\n",
    "      scores:        numpy array (batch_size x num_critters)\n",
    "      rounds_left:   numpy array (batch_size)\n",
    "      active_player: int\n",
    "    \"\"\"\n",
    "    self.pieces = board['pieces'].copy()\n",
    "    self.scores = board['scores'].copy()\n",
    "    self.rounds_left = board['rounds_left'].copy()\n",
    "    self.active_player = copy(board['active_player'])\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state, which is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy(),\n",
    "             'active_player': copy(self.active_player)}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, critter):\n",
    "    \"\"\"\n",
    "    Updates the state of the board given the moves made.\n",
    "\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord.\n",
    "\n",
    "    Notes:\n",
    "      Assumes that there is exactly one valid move for each board in the\n",
    "      batch of boards for the critter type given. i.e. it does't check for\n",
    "      bounce/reflection on edges or with other critters, or for multiple move\n",
    "      made on the same board. It only checks for eating food and adds new food\n",
    "      when appropriate. Invalid moves could lead to illegal teleporting\n",
    "      behavior, critter duplication, or index out of range errors.\n",
    "\n",
    "      Currently just prints a message if critter making the move is not the\n",
    "      active player, could enforce this more strictly if needed.\n",
    "    \"\"\"\n",
    "    if critter-1 != self.active_player:\n",
    "      # note critter is [1 to num_critter] inclusive so that it can be used\n",
    "      # directly in where statements on pieces but self.active_player is\n",
    "      # [0 to numcritter-1] inclusive so that it can be used directly in\n",
    "      # indexing player lists\n",
    "      raise ValueError(\"Warning! The critter moving is not the expected active player\")\n",
    "    #critters leave their spots\n",
    "    self.pieces[self.pieces==critter] = 0\n",
    "    #which critters have food in their new spots\n",
    "    eats_food = self.pieces[moves] == -1\n",
    "    # some critters eat and their scores go up\n",
    "    # note critter is +int so need to -1 for indexing\n",
    "    self.scores[:,critter-1] = self.scores[:,critter-1] + eats_food\n",
    "\n",
    "    num_empty_after_eat = (self.n_rows*self.n_cols - self.num_food -\n",
    "                           self.num_critters + 1) # +1 for the food just eaten\n",
    "    # which boards in the batch had eating happen\n",
    "    g_eating = np.where(eats_food)[0]\n",
    "    # put critters in new positions\n",
    "    self.pieces[moves] = critter\n",
    "    if np.any(eats_food):\n",
    "      # add random food to replace what is eaten\n",
    "      possible_new_locs = np.where(np.logical_and(\n",
    "          self.pieces == 0, #the spot is empty\n",
    "          eats_food.reshape(self.batch_size, 1, 1))) #food eaten on that board\n",
    "      food_sample_ = self.rng.choice(num_empty_after_eat,\n",
    "                                     size=np.sum(eats_food))\n",
    "      food_sample = food_sample_ + np.arange(len(g_eating))*num_empty_after_eat\n",
    "      assert np.all(self.pieces[(possible_new_locs[0][food_sample],\n",
    "                                 possible_new_locs[1][food_sample],\n",
    "                                 possible_new_locs[2][food_sample])] == 0)\n",
    "      #put new food on the board\n",
    "      self.pieces[(possible_new_locs[0][food_sample],\n",
    "                   possible_new_locs[1][food_sample],\n",
    "                   possible_new_locs[2][food_sample])] = -1\n",
    "    self.rounds_left = self.rounds_left - 1\n",
    "    if not np.all(self.pieces.sum(axis=(1,2)) ==\n",
    "                  ((self.num_food * -1) + np.sum(np.arange(self.num_critters)+1))):\n",
    "      print(self.pieces.sum(axis=(1,2)))\n",
    "      print(((self.num_food * -1) + np.sum(np.arange(self.num_critters)+1)))\n",
    "    assert np.all(self.pieces.sum(axis=(1,2)) ==\n",
    "                  ((self.num_food * -1) + np.sum(np.arange(self.num_critters)+1)))\n",
    "    # next player's turn\n",
    "    self.active_player = (self.active_player + 1) % (self.num_critters)\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, critter):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter, taking into account\n",
    "    bouncing/reflection at edges,\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "    # can only move one cell down, up, right, and left from current location\n",
    "    critter_locs = np.array(np.where(self.pieces == critter))\n",
    "    legal_offsets = np.stack([\n",
    "      critter_locs + np.array([np.array([0,  1, 0])]*self.batch_size).T,\n",
    "      critter_locs + np.array([np.array([0, -1, 0])]*self.batch_size).T,\n",
    "      critter_locs + np.array([np.array([0, 0,  1])]*self.batch_size).T,\n",
    "      critter_locs + np.array([np.array([0, 0, -1])]*self.batch_size).T])\n",
    "    legal_offsets = np.vstack(np.transpose(legal_offsets, (0, 2, 1)))\n",
    "    legal_offsets = set([tuple(m_) for m_ in legal_offsets])\n",
    "    # must land on the board and not on another critter\n",
    "    legal_destinations = np.where(self.pieces <= 0)\n",
    "    legal_destinations = set([(g, r, c) for\n",
    "                              g, r, c in zip(*legal_destinations)])\n",
    "    # legal moves satisfy both these conditions\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_perceptions(self, radius, critter):\n",
    "    \"\"\"\n",
    "    Generates a vector representation of the critter perceptions, oriented\n",
    "    around the critter.\n",
    "\n",
    "    Args:\n",
    "      radius: int, how many grid squared the critter can see around it\n",
    "        using L1  (Manhattan/cityblock) distance\n",
    "\n",
    "    Returns:\n",
    "      A batch_size x 2*radius*(radius+1) + 1, giving the values\n",
    "      of the percept reading left to right, top to bottom over the board,\n",
    "      for each board in the batch\n",
    "    \"\"\"\n",
    "    # define the L1 ball mask\n",
    "    diameter = radius*2+1\n",
    "    mask = np.zeros((diameter, diameter), dtype=bool)\n",
    "    mask_coords = np.array([(i-radius, j-radius)\n",
    "      for i in range(diameter)\n",
    "        for j in range(diameter)])\n",
    "    mask_distances = cdist(mask_coords, [[0, 0]],\n",
    "                           'cityblock').reshape(mask.shape)\n",
    "    mask[mask_distances <= radius] = True\n",
    "    mask[radius,radius] = False  # exclude the center\n",
    "\n",
    "    # pad the array\n",
    "    padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "     (radius, radius)), constant_values=-2)\n",
    "\n",
    "    # get locations of critters\n",
    "    critter_locs = np.argwhere(padded_arr == critter)\n",
    "\n",
    "    percepts = []\n",
    "    for critter_loc in critter_locs:\n",
    "      b, r, c = critter_loc\n",
    "      surrounding = padded_arr[b, r-radius:r+radius+1, c-radius:c+radius+1]\n",
    "      percept = surrounding[mask]\n",
    "      percepts.append(percept)\n",
    "    return(np.array(percepts))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# refactor Monte Carlo for boards that support multiple critters\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarlo():\n",
    "  \"\"\"\n",
    "  Implementation of Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, nnet, default_depth=5, random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def pis_vs_from_board(self, board, critter):\n",
    "    #helper function, to put board in canonical form that nn was trained on\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    co_pieces = board['pieces'].copy()\n",
    "    this_critter_locs = np.where(co_pieces == critter)\n",
    "    all_critter_locs = np.where(co_pieces >= 1)\n",
    "    # other critters are invisible to this player\n",
    "    co_pieces[all_critter_locs] = 0\n",
    "    # nnet trained to see self as 1\n",
    "    co_pieces[this_critter_locs] = 1\n",
    "    scalar_rounds_left = board['rounds_left'][0]\n",
    "    co_rounds_left = scalar_rounds_left // self.game.num_critters\n",
    "    if critter-1 < scalar_rounds_left % self.game.num_critters:\n",
    "       # add an extra if we haven't had this players turn yet in the round cycle\n",
    "       co_rounds_left = co_rounds_left + 1\n",
    "    co_rounds_left = np.array([co_rounds_left]*batch_size)\n",
    "    pis, vs = self.nnet.predict(co_pieces,\n",
    "                                board['scores'][:,critter-1],\n",
    "                                co_rounds_left)\n",
    "    return pis, vs\n",
    "\n",
    "\n",
    "  def simulate(self, board, actions, action_indexes, critter=1, depth=None):\n",
    "    \"\"\"\n",
    "    Helper function to simulate one Monte Carlo rollout\n",
    "\n",
    "    Args:\n",
    "      board: triple (batch_size x x_size x y_size np.array of board position,\n",
    "                     scalar of current score,\n",
    "                     scalar of rounds left\n",
    "      actions: batch size list/array of integer indexes for moves on each board\n",
    "      these are assumed to be legal, no check for validity of moves\n",
    "    Returns:\n",
    "      temp_v:\n",
    "        Terminal State\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    next_board = self.game.get_next_state(board, critter,\n",
    "                                          actions, action_indexes)\n",
    "    # in this version of the mc player, the existence of other players is\n",
    "    # ignored, in another version of mc other players moves might be simulated\n",
    "    next_board['active_player'] = critter-1\n",
    "\n",
    "    if depth is None:\n",
    "      depth = self.default_depth\n",
    "    # potentially expand the game tree here,\n",
    "    # but just do straight rollouts after this\n",
    "    # doesn't expand to deal with all random food generation possibilities\n",
    "    # just expands based on the actions given\n",
    "    expand_bs, _, _ = next_board['pieces'].shape\n",
    "\n",
    "    for i in range(depth):  # maxDepth\n",
    "      if next_board['rounds_left'][0] <= 0:\n",
    "        # check that game isn't over\n",
    "        # assumes all boards have the same rounds left\n",
    "        # no rounds left return scores as true values\n",
    "        terminal_vs = next_board['scores'][:,critter-1].copy()\n",
    "        return terminal_vs\n",
    "      else:\n",
    "        #pis, vs = self.nnet.predict(next_board['pieces'], next_board['scores'], next_board['rounds_left'])\n",
    "        pis, vs = self.pis_vs_from_board(next_board, critter)\n",
    "        valids = self.game.get_valid_actions(next_board, critter)\n",
    "        masked_pis = pis * valids\n",
    "        sum_pis = np.sum(masked_pis, axis=1)\n",
    "        probs = np.array(\n",
    "            [masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "             else valid / valid.sum()\n",
    "             for valid, masked_pi in zip(valids, masked_pis)])\n",
    "        samp = self.rng.uniform(size = expand_bs).reshape((expand_bs,1))\n",
    "        sampled_actions = np.argmax(probs.cumsum(axis=1) > samp, axis=1)\n",
    "      next_board = self.game.get_next_state(next_board, critter,\n",
    "                                            sampled_actions)\n",
    "      # in this version of the mc player, existence of other players is ignored\n",
    "      # in another better version other players moves might be simulated, either\n",
    "      # as copies of self, or as distinct environmental dynamics\n",
    "      next_board['active_player'] = critter-1\n",
    "\n",
    "\n",
    "    pis, vs = self.pis_vs_from_board(next_board, critter)\n",
    "    #pis, vs = self.nnet.predict(next_board['pieces'], next_board['scores'],\n",
    "    #                            next_board['rounds_left'])\n",
    "    #print(vs.shape)\n",
    "    return vs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################################\n",
    "# make a separate player zoo\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomValidPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    action_size = self.game.get_action_size()\n",
    "\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomDirectionPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function assigns a uniform probability to going up down left or right\n",
    "    independent of whether it is at an edge or corner or not. Then because of\n",
    "    bouncing off edges it will have a higher probability of moving away from\n",
    "    edges as opposed to along them than the random valid move player.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    action_probs = {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
    "\n",
    "    critter_oriented_moves = self.game.rng.choice(list(action_probs.keys()),\n",
    "                                                  size=(batch_size))\n",
    "    direction_probs = [action_probs] * batch_size\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarloBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Player based on Monte Carlo Algorithm\n",
    "\n",
    "  Note: Has dependencies in the gw_NN_RL.py util, namely a policy/value\n",
    "  network and the Monte Carlo class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet,\n",
    "               critter_index=1,\n",
    "               default_depth=1,\n",
    "               default_rollouts=1,\n",
    "               default_K=4,\n",
    "               default_temp=1.0,\n",
    "               random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.default_rollouts = default_rollouts\n",
    "    self.mc = MonteCarlo(self.game, self.nnet, self.default_depth)\n",
    "    self.default_K = default_K\n",
    "    self.default_temp = default_temp\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def play(self, board,\n",
    "           num_rollouts=None,\n",
    "           rollout_depth=None,\n",
    "           K=None,\n",
    "           softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Simulates a batch Monte Carlo based plays on the given board state.\n",
    "\n",
    "    Computes the probability of each valid move being played using a softmax\n",
    "    activation on the Monte Carlo based value (Q) of each action then selects a\n",
    "    move randomly for each game in the batch based on those probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if num_rollouts is None:\n",
    "      num_rollouts = self.default_rollouts\n",
    "    if rollout_depth is None:\n",
    "      rollout_depth = self.default_depth\n",
    "    if K is None:\n",
    "      K = self.default_K\n",
    "    if softmax_temp is None:\n",
    "      softmax_temp = self.default_temp\n",
    "\n",
    "    # figure out top k actions according to normalize action probability\n",
    "    # given by our policy network prediction\n",
    "    #co_pieces = board['pieces'].copy()\n",
    "    #this_critter_locs = np.where(co_pieces == self.critter_index+1)\n",
    "    #all_critter_locs = np.where(co_pieces >= 1)\n",
    "    # other critters are invisible to this player\n",
    "    #co_pieces[all_critter_locs] = 0\n",
    "    # nnet trained to see self as 1\n",
    "    #co_pieces[this_critter_locs] = 1\n",
    "    #scalar_rounds_left = board['rounds_left'][0]\n",
    "    #co_rounds_left = scalar_rounds_left // self.game.num_critters\n",
    "    #if self.critter_index-1 < scalar_rounds_left % self.game.num_critters:\n",
    "       # add an extra if we haven't had this players turn yet in the round cycle\n",
    "    #   co_rounds_left = co_rounds_left + 1\n",
    "    #co_rounds_left = np.array([co_rounds_left]*batch_size)\n",
    "    #pis, vs = self.nnet.predict(co_pieces,\n",
    "    #                            board['scores'][:,self.critter_index-1],\n",
    "    #                            co_rounds_left)\n",
    "    pis, vs = self.mc.pis_vs_from_board(board, self.critter_index)\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    masked_pis = pis * valids  # Masking invalid moves\n",
    "    sum_pis = np.sum(masked_pis, axis=1)\n",
    "    num_valid_actions = np.sum(valids, axis=1)\n",
    "    effective_topk = np.array(np.minimum(num_valid_actions, K), dtype= int)\n",
    "    probs = np.array([masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "                      else valid / valid.sum()\n",
    "                      for valid, masked_pi in zip(valids, masked_pis)])\n",
    "    partioned = np.argpartition(probs,-effective_topk)\n",
    "    topk_actions = [partioned[g,-(ii+1)]\n",
    "                      for g in range(batch_size)\n",
    "                        for ii in range(effective_topk[g])]\n",
    "    topk_actions_index = [ii\n",
    "                            for ii, etk in enumerate(effective_topk)\n",
    "                              for _ in range(etk)]\n",
    "    values = np.zeros(len(topk_actions))\n",
    "    # Do some rollouts\n",
    "    for _ in range(num_rollouts):\n",
    "      values = values + self.mc.simulate(board, topk_actions,\n",
    "                                         topk_actions_index,\n",
    "                                         critter=self.critter_index,\n",
    "                                         depth=rollout_depth)\n",
    "    values = values / num_rollouts\n",
    "\n",
    "    value_expand = np.zeros((batch_size, n_rows*n_cols))\n",
    "    value_expand[(topk_actions_index, topk_actions)] = values\n",
    "    value_expand_shift = value_expand - np.max(value_expand, axis=1, keepdims=True)\n",
    "    value_expand_scale = value_expand_shift/softmax_temp\n",
    "    v_probs = np.exp(value_expand_scale) / np.sum(\n",
    "        np.exp(value_expand_scale), axis=1, keepdims=True)\n",
    "    v_probs = v_probs * valids\n",
    "    v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "    samp = self.rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "    sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "    a_1Hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    return sampled_actions, a_1Hots, v_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleRulePlayer():\n",
    "  \"\"\"\n",
    "  A Player based on the following simple policy:\n",
    "  If there is any food immediately nearby move towards it,\n",
    "  otherwise it move randomly.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, fov_radius=2, critter_index=1):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.fov_radius = fov_radius\n",
    "\n",
    "\n",
    "  def simple_action_from_percept(self, percept):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A 1D array (len 12 if fov_radius = 2)representing the perception\n",
    "        of the organism. Indices correspond to spaces around the organism. The\n",
    "        values in the array can be -2 (out-of-bounds), 0 (empty space), or\n",
    "        -1 (food).\n",
    "\n",
    "    Returns:\n",
    "      action: a str, one of 'up', 'down', 'left', 'right'. If food in one or\n",
    "        more of the spaces immediately beside the organism, the function will\n",
    "        return a random choice among these directions. If there is no food\n",
    "        nearby, the function will return a random direction.\n",
    "    \"\"\"\n",
    "    # a human interpretable overview of the percept structure\n",
    "    percept_struct = [\n",
    "      'far up', 'left up', 'near up', 'right up',\n",
    "      'far left', 'near left', 'near right', 'far right',\n",
    "      'left down', 'near down', 'right down', 'far down']\n",
    "    # Defines directions corresponding to different perception indices\n",
    "    direction_struct = [\n",
    "      'None', 'None', 'up', 'None',\n",
    "      'None', 'left', 'right', 'None',\n",
    "      'None', 'down', 'None', 'None']\n",
    "    # these are what count as nearby in the percept\n",
    "    nearby_directions = ['near up', 'near left', 'near right', 'near down']\n",
    "    # Get the corresponding indices in the percept array\n",
    "    nearby_indices = [percept_struct.index(dir_) for dir_ in nearby_directions]\n",
    "    # Identify the directions where food is located\n",
    "    food_indices = [index for index in nearby_indices if percept[index] == -1]\n",
    "    food_directions = [direction_struct[index] for index in food_indices]\n",
    "\n",
    "    action_probs = {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
    "    if len(food_directions) > 0:  # If there is any food nearby\n",
    "      # If there is any food nearby randomly choose a direction with food\n",
    "      action = self.game.rng.choice(food_directions)  # Move towards a random one\n",
    "      for direction in food_directions:\n",
    "        action_probs[direction] = 1.0 /len(food_directions)\n",
    "    else:\n",
    "      # If there is no food nearby, move randomly\n",
    "      action = self.game.rng.choice(['up', 'down', 'left', 'right'])\n",
    "      for direction in ['up', 'down', 'left', 'right']:\n",
    "        action_probs[direction] = 0.25\n",
    "\n",
    "    return action, action_probs\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius,\n",
    "                                            self.critter_index)\n",
    "\n",
    "    critter_oriented_moves = []\n",
    "    direction_probs = []\n",
    "    for g in range(batch_size):\n",
    "      action, action_probs = self.simple_action_from_percept(perceptions[g])\n",
    "      critter_oriented_moves.append(action)\n",
    "      direction_probs.append(action_probs)\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                direction_probs,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PerceptParamPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a parameterized policy defined by the given weights\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, weights=None, fov_radius=2, critter_index=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      weights: 4 x 12 numpy array (assumes fov_radius = 2), that gives the\n",
    "        connection strengths between the 'perception' neurons and the direction\n",
    "        'neurons'\n",
    "      fov_radius: int how far around itself the critter perceives, weights is\n",
    "        expecting fov_radius = 2\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    if weights is None:\n",
    "      self.W = np.array(\n",
    "      [[1., 1., 4., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 4., 1., 1.],\n",
    "       [0., 1., 0., 0., 1., 4., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 4., 1., 0., 0., 1., 0.]])\n",
    "    else:\n",
    "      self.W = weights\n",
    "    self.fov_radius = fov_radius\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "\n",
    "  def param_action_from_percept(self, percept, valid_directions, W,\n",
    "                                softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A 1D len 12 array representing the perception of the organism.\n",
    "        Indices correspond to spaces around the organism. The values in the\n",
    "        array can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "      W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "        between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "    Returns:\n",
    "      direction: a str, one of 'up', 'down', 'left', 'right'. If food in one or\n",
    "        more of the spaces immediately beside the organism, the function will\n",
    "        return a random choice among these directions. If there is no food\n",
    "        nearby, the function will return a random direction.\n",
    "      direction_probs: dictionary with probabilities of taking each action.\n",
    "    \"\"\"\n",
    "    if len(valid_directions) == 0:\n",
    "      # if there is no where legit to move, stay put\n",
    "      return 'still', {direction: 0 for direction in output_struct}\n",
    "\n",
    "    if softmax_temp is None:\n",
    "      # very low temp, basically deterministic for this range of values\n",
    "      softmax_temp = self.default_softmax_temp\n",
    "    # a human interpretable overview of the percept structure\n",
    "    percept_struct = [\n",
    "      'far up', 'left up', 'near up', 'right up',\n",
    "      'far left', 'near left', 'near right', 'far right',\n",
    "      'left down', 'near down', 'right down', 'far down']\n",
    "    # a human interpretable overview of the out structure\n",
    "    output_struct = ['up', 'down', 'left', 'right']\n",
    "    # boolean representation of percept, no edges, just 1's where food is,\n",
    "    # zero otherwise, also means other organisms are invisible\n",
    "    x = np.asarray(percept == -1, int)\n",
    "    output_activations = W @ x\n",
    "\n",
    "    # softmax shift by max, scale by temp\n",
    "    shift_scale_ex = np.exp((output_activations -\n",
    "                             np.max(output_activations))/softmax_temp)\n",
    "    # set invalid direction activations to zero\n",
    "    invalid_directions = [direction for direction in output_struct\n",
    "                           if direction not in valid_directions]\n",
    "    invalid_indices = [output_struct.index(direction)\n",
    "                        for direction in valid_directions]\n",
    "    sm = shift_scale_ex / shift_scale_ex.sum() #normalized\n",
    "    # set invalid direction probabilities to zero\n",
    "    invalid_directions = [direction for direction in output_struct\n",
    "                           if direction not in valid_directions]\n",
    "    invalid_indices = [output_struct.index(direction)\n",
    "                        for direction in invalid_directions]\n",
    "    sm[invalid_indices] = 0\n",
    "    probs_sm = sm / sm.sum(axis=0) #re-normalized again for fp issues\n",
    "    direction = self.game.rng.choice(output_struct, p=probs_sm)\n",
    "    direction_probs = {direction: prob\n",
    "                        for direction, prob in zip(output_struct, probs_sm)}\n",
    "    return direction, direction_probs\n",
    "\n",
    "\n",
    "  def play(self, board, temp=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius,\n",
    "                                            self.critter_index)\n",
    "    critter_oriented_moves = []\n",
    "    direction_probs = []\n",
    "\n",
    "    # Get valid actions for each game in the batch\n",
    "    valid_directions = self.game.get_valid_directions(board, self.critter_index)\n",
    "    for g in range(batch_size):\n",
    "      direction, batch_direction_probs = self.param_action_from_percept(\n",
    "        perceptions[g], valid_directions[g], self.W, softmax_temp=temp)\n",
    "      critter_oriented_moves.append(direction)\n",
    "      direction_probs.append(batch_direction_probs)\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# extend GridworldGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1, n_rows=7, n_cols=7,\n",
    "               num_critters=2, num_food=10,\n",
    "               lifetime=30, rng=None):\n",
    "    \"\"\"\n",
    "    Initializes an instance of the class with the specified parameters.\n",
    "\n",
    "    Args:\n",
    "      batch_size (int, optional): Number of instances in a batch. Default is 1.\n",
    "      n_rows (int, optional): Number of rows in the grid. Default is 7.\n",
    "      n_cols (int, optional): Number of columns in the grid. Default is 7.\n",
    "      num_critters (int, optional): Number of different agents running around\n",
    "        on each board in the batch. Default is 2.\n",
    "      num_food (int, optional): Number of food items. Default is 10.\n",
    "      lifetime (int, optional): Time before critter's life ends, in terms of\n",
    "        time steps. Default is 30.\n",
    "      rng (numpy random number generator, optional): Random number generator\n",
    "        for reproducibility. If None, uses default RNG with a preset seed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for positive integer inputs\n",
    "    assert all(isinstance(i, int) and i >= 0\n",
    "               for i in [batch_size, n_rows, n_cols, num_critters, num_food,\n",
    "                         lifetime]), \"All inputs must be non-negative integers.\"\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_critters = num_critters\n",
    "    # Check for num_food exceeding maximum possible value\n",
    "    max_food = n_rows * n_cols - num_critters\n",
    "    if num_food > max_food:\n",
    "      print(f'num_food is too large, setting it to maximum possible value: {max_food}')\n",
    "      num_food = max_food\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    # Set up random number generator\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                       n_cols=self.n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    return b.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_size(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,r,c coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 0-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to r,c indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g=0):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return (board['pieces'][g].tobytes() + board['scores'][g].tobytes() +\n",
    "            board['rounds_left'][g].tobytes())\n",
    "\n",
    "\n",
    "  def get_square_symbol(self, piece):\n",
    "    \"\"\" Translate integer piece value to symbol for display\"\"\"\n",
    "    if piece == -1:\n",
    "      return \"X\"\n",
    "    elif piece == 0:\n",
    "      return \"-\"\n",
    "    elif piece >= 1:\n",
    "      return \"0\"\n",
    "    else:\n",
    "      return \"???????????????????????????\"\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g=0):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.get_square_symbol(square)\n",
    "                        for row in board['pieces'][g]\n",
    "                          for square in row])\n",
    "    board_s = board_s + '_' + str(board['scores'][g])\n",
    "    board_s = board_s + '_' + str(board['rounds_left'][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board['rounds_left'].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, \"|\", end=\"\")    # Print the row\n",
    "      for r_ in range(self.n_rows):\n",
    "        piece = board['pieces'][g,c_,r_]    # Get the piece to print\n",
    "        #print(piece)\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board['rounds_left'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g, critter_index):\n",
    "    return np.squeeze(np.array(np.where(board['pieces'][g]==critter_index)))\n",
    "\n",
    "\n",
    "  def plot_moves(self, board, player0, g=0, player1=None,\n",
    "                 fig=None, ax=None, p0_name='Player 0', p1_name='Player 1',\n",
    "                 figsize=(6,5), critter_name='Critter', title=None,\n",
    "                 deterministic=False):\n",
    "    \"\"\"\n",
    "    Uses plotting functions to make picture of the current board state, and what\n",
    "    a critter would do at each non-food location in the current board state\n",
    "    \"\"\"\n",
    "    def make_prob_dict(critter_locs, play):\n",
    "      offset_dict = {(0, 1): 'right',\n",
    "                     (0,-1): 'left',\n",
    "                     ( 1, 0): 'down',\n",
    "                     (-1, 0): 'up'}\n",
    "      index_probs = play[2].copy()\n",
    "      loc_prob_dict = {}\n",
    "      # for each non food locations\n",
    "      for g, loc_ in enumerate(critter_locs):\n",
    "        # this is the location as an r, c tuple\n",
    "        rc_tup = tuple((loc_[1], loc_[2]))\n",
    "        # the relevant probabilities\n",
    "        raw_probs = index_probs[g]\n",
    "        probs = raw_probs[raw_probs > 0]\n",
    "        indexes = np.argwhere(raw_probs > 0)\n",
    "        # turn the probability indexes into r, c coords\n",
    "        rows = np.floor_divide(indexes, gwg.n_cols)\n",
    "        cols = np.remainder(indexes, gwg.n_cols)\n",
    "        moves = np.squeeze(np.array([z for z in zip(rows, cols)]), axis=2)\n",
    "        #compute the offsets and turn them to strings\n",
    "        offsets = moves - loc_[1:]\n",
    "        str_offsets = np.array(list(map(offset_dict.get, map(tuple, offsets))))\n",
    "        # update the loc_prob_dict for plotting\n",
    "        prob_dict = dict(zip(str_offsets, probs))\n",
    "        loc_prob_dict.update({rc_tup: prob_dict})\n",
    "      return loc_prob_dict\n",
    "\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] == -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    food = plot_food(fig, ax, rc_food_plotting)\n",
    "\n",
    "    expanded_board = self.critter_everywhere_state_expansion(\n",
    "      board, player0.critter_index, to_expand=g)\n",
    "    critter_locs = np.argwhere(expanded_board['pieces']==player0.critter_index)\n",
    "    #play the expanded state\n",
    "    p0_play = player0.play(expanded_board)\n",
    "    #get the prob dict\n",
    "    p0_loc_prob_dict = make_prob_dict(critter_locs, p0_play)\n",
    "    # same for player1 if there is one\n",
    "    if player1 is not None:\n",
    "      p1_play = player1.play(expanded_board)\n",
    "      p1_loc_prob_dict = make_prob_dict(critter_locs, p1_play)\n",
    "\n",
    "    existing_handels, _ = ax.get_legend_handles_labels()\n",
    "    if player1 is None:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=0, deterministic=deterministic)\n",
    "      leg_handles = existing_handels\n",
    "    else:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=1, deterministic=deterministic, name=p0_name)\n",
    "      fig, ax, leg_handles_1 = plot_directions(fig, ax, p1_loc_prob_dict,\n",
    "        critter=2, deterministic=deterministic, name=p1_name)\n",
    "      leg_handles = existing_handels + leg_handles_0 + leg_handles_1\n",
    "\n",
    "    fig.legend(handles=leg_handles, loc=\"outside right upper\")\n",
    "    fig.canvas.draw()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war feild_of_view\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter'):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(self.num_critters):\n",
    "        critter_name = name if self.num_critters == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == self.num_critters, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(np.where(board['pieces'][g] == spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] == -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # will need to think about how to do this for multiple\n",
    "      # critters, currently just use rc of first critter in the spec list\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[0]['rc_loc'], n_rows, n_cols,\n",
    "                       radius, has_fov)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[0]['rc_loc'], n_rows, n_cols,\n",
    "                       radius, has_fov, fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board, critter):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=batch_size, n_rows=n_rows,\n",
    "                       n_cols=n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves(critter)\n",
    "    valids = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for g, r, c in legal_moves:\n",
    "      valids[g, r * n_cols + c] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, critter=1, g=0):\n",
    "    \"\"\"Displays possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    A=np.reshape(self.get_valid_actions(board, critter)[g],\n",
    "                 (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, \"|\", end=\"\")    # Print the row\n",
    "      for row in range(self.n_rows):\n",
    "        piece = A[col][row]    # Get the piece to print\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, critter):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=batch_size, n_rows=n_rows,\n",
    "                       n_cols=n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    return(b.get_perceptions(radius, critter))\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, critter, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if board['rounds_left'][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      moves = self.actions_to_moves(actions)\n",
    "      b = GridworldBoard(batch_size=len(actions), n_rows=n_rows,\n",
    "                         n_cols=n_cols, num_critters=self.num_critters,\n",
    "                         num_food=self.num_food, lifetime=self.lifetime,\n",
    "                         rng=self.rng)\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        b.set_state(board)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        new_pieces = np.array([board['pieces'][ai].copy() for ai in a_indx])\n",
    "        new_scores = np.array([board['scores'][ai].copy() for ai in a_indx])\n",
    "        new_rounds_left = np.array([board['rounds_left'][ai].copy() for ai in a_indx])\n",
    "        new_active_player = copy(board['active_player'])\n",
    "        new_state = {'pieces': new_pieces,\n",
    "                     'scores': new_scores,\n",
    "                     'rounds_left': new_rounds_left,\n",
    "                     'active_player': new_active_player}\n",
    "        b.set_state(new_state)\n",
    "      b.execute_moves(moves, critter)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=batch_size, n_rows=n_rows,\n",
    "                       n_cols=n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.critter_direction_to_move(board, offsets, critter)\n",
    "    b.execute_moves(moves, critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right'\n",
    "      critter: integer index for the critter we want moves for\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = np.where(board['pieces'] == critter)\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def direction_probs_to_flat_probs(self, board, direction_probs):\n",
    "    \"\"\"\n",
    "    Converts direction probabilities in reference to the critter's location into\n",
    "    probability arrays on the flattened board.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      direction_probs: batch length list of dictionaries with keys\n",
    "        ['up', 'down', 'left', 'right'] and corresponding probabilities.\n",
    "\n",
    "    Returns:\n",
    "      probs_arrays: list of arrays, where each array is of length n_rows*n_cols\n",
    "                    and represents the flattened probability distribution for\n",
    "                    board in the batch.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {\n",
    "        'up': (0, -1, 0),\n",
    "        'down': (0, 1, 0),\n",
    "        'left': (0, 0, -1),\n",
    "        'right': (0, 0, 1)\n",
    "    }\n",
    "    critter_locs = np.where(board['pieces'] == 1)\n",
    "    probs_arrays = []\n",
    "    for batch_index in range(batch_size):\n",
    "      prob_array = np.zeros(n_rows * n_cols)\n",
    "      for direction, prob in direction_probs[batch_index].items():\n",
    "          offset = np.array(offset_dict[direction])\n",
    "          new_loc = np.array(critter_locs)[:, batch_index] + offset\n",
    "          # Check bounces at boundaries\n",
    "          new_loc[1] = np.where(new_loc[1] >= n_rows, n_rows-2, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] >= n_cols, n_cols-2, new_loc[2])\n",
    "          new_loc[1] = np.where(new_loc[1] < 0, 1, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] < 0, 1, new_loc[2])\n",
    "          # Convert 2D location to flattened index\n",
    "          flattened_index = new_loc[1] * n_cols + new_loc[2]\n",
    "          prob_array[flattened_index] += prob\n",
    "      probs_arrays.append(prob_array)\n",
    "    return probs_arrays\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, critter, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right'\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    valid_directions = []\n",
    "    for g, batch_valids in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valids==1)[0]\n",
    "      critter_loc = np.squeeze(np.array(np.where(board['pieces'][g] == critter)))\n",
    "      batch_valid_directions = []\n",
    "      for a in valid_int_indices:\n",
    "        move = np.array([np.floor_divide(a, n_cols), np.remainder(a, n_cols)])\n",
    "        offset = move - critter_loc\n",
    "        batch_valid_directions.append(offset_dict[tuple(offset)])\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "    Returns a batch size np.array of -1 if not ended, and scores for each game\n",
    "    in the batch if it is ended, note only returns scores if all games in the\n",
    "    batch have ended\n",
    "    \"\"\"\n",
    "    rounds_left = board['rounds_left']\n",
    "    scores = board['scores']\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores\n",
    "\n",
    "\n",
    "  def critter_everywhere_state_expansion(self, board_state,\n",
    "                                         critter=1, to_expand=0):\n",
    "    \"\"\"\n",
    "    Expand a given board state by placing a critter at each non-food location.\n",
    "\n",
    "    The function takes a game state and returns an expanded version of it. For\n",
    "    each board in the state, it creates a new version of the board for every\n",
    "    non-food location, placing a critter at that location. The scores and\n",
    "    remaining rounds are copied for each new board. The result is a new game state\n",
    "    with a larger number of boards, each representing a possible configuration\n",
    "    with a critter at a different location.\n",
    "\n",
    "    Args:\n",
    "      board_state (dict): A dictionary containing the current game state.\n",
    "      It should have the following keys:\n",
    "        - 'pieces': a 3D numpy array (batch x n_col x n_row) representing the game\n",
    "          board. -1 -> food, 0 -> empty cell, and 1 -> critter.\n",
    "        - 'scores': 1D numpyp array of the score for each board in the batch.\n",
    "        - 'rounds_left': a 1D numpy array of the rounds left for\n",
    "          each board in the batch.\n",
    "      critter: integer index to place on the expanded board state\n",
    "      to_expand (list (int)): list of batch indices to have state expanded\n",
    "\n",
    "    Returns:\n",
    "      dict: A dictionary containing the expanded game state with the same keys\n",
    "        as the input. The number of boards will be larger than the input state.\n",
    "    \"\"\"\n",
    "    pieces = board_state['pieces'].copy()\n",
    "    scores = board_state['scores'].copy()\n",
    "    rounds_left = board_state['rounds_left'].copy()\n",
    "    active_player = copy(board_state['active_player'])\n",
    "    # Determine non-food locations\n",
    "    non_food_locs = np.argwhere(pieces[to_expand] != -1)\n",
    "    #scrub all existing critter locations,\n",
    "    # maybe later only scrub specific critter type\n",
    "    pieces[pieces >= 1] = 0\n",
    "    # lists to store expanded states\n",
    "    expanded_pieces = []\n",
    "    expanded_scores = []\n",
    "    expanded_rounds_left = []\n",
    "    # Iterate over each non-food location\n",
    "    for i in range(non_food_locs.shape[0]):\n",
    "      # Create a copy of the board\n",
    "      expanded_board = np.copy(pieces[to_expand])\n",
    "      # Place the critter at the non-food location\n",
    "      # later consider only placing at non-food,\n",
    "      # non-other critter locs\n",
    "      expanded_board[tuple(non_food_locs[i])] = critter\n",
    "      # Add the expanded board to the list along score and rounds_left\n",
    "      expanded_pieces.append(expanded_board)\n",
    "      expanded_scores.append(scores[to_expand])\n",
    "      expanded_rounds_left.append(rounds_left[to_expand])\n",
    "    # Convert to arrays and create expanded board state\n",
    "    expanded_state = {'pieces': np.stack(expanded_pieces),\n",
    "                      'scores': np.array(expanded_scores),\n",
    "                      'rounds_left': np.array(expanded_rounds_left),\n",
    "                      'active_player': active_player}\n",
    "    return expanded_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "# widgets refactor for multi-critter\n",
    "#########################################\n",
    "# Interactive Gridworld Game Widgets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None, has_fov=False,\n",
    "               radius=2, collect_fov_data=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human']):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the gridworld_game.num_critters\n",
    "        attribute. Order should align with critter_name.\n",
    "\n",
    "      Note: fov is going to look pretty janky with more than one player, maybe\n",
    "      we get fov to only turn on for the 'active' player?\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.gwg.num_critters):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] # implicit check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    if self.collect_fov_data is True:\n",
    "      # keep raw records of percept and eating for manipulation later\n",
    "      self.percept_eat_records = []\n",
    "      # keep data in contingency table of how many food items were in\n",
    "      # the percept, and whether or not food was eaten\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.fov_eat_table_display = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='25.0em', min_height='18.8em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if self.collect_fov_data and self.any_human_players:\n",
    "      self.legend_type = None # don't keep regenerating the legend\n",
    "      # do legend separately if showing observations and no human player\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          legend_type='separate', figsize=self.figsize, has_fov=self.has_fov,\n",
    "          radius=self.radius)\n",
    "    elif len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, figsize=self.figsize)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                radius=self.radius, figsize=self.figsize)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "\n",
    "      if len(self.players) == 1:\n",
    "        self.output_and_score = widgets.HBox([self.scoreboard, self.output])\n",
    "        self.no_table_final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                                  self.output_and_score])\n",
    "        if self.collect_fov_data == True:\n",
    "          # a single human player collecting data\n",
    "          self.final_display = widgets.HBox([self.no_table_final_display,\n",
    "                                           self.fov_eat_table_display])\n",
    "        else: # self.collect_fov_data == False:\n",
    "          # a single human player not collecting data\n",
    "          self.final_display = self.no_table_final_display\n",
    "      else:\n",
    "        # more than one player, one of them human\n",
    "        self.V_board_outbput = widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        self.final_display = widgets.HBox([self.V_board_outbput,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # player is some kind of ai\n",
    "      if self.collect_fov_data == True:\n",
    "        # an ai player with recording\n",
    "        # in this case legend is separate\n",
    "        self.V_score_start_output_legend = widgets.VBox([self.scoreboard,\n",
    "          self.start_button,  self.output, self.b_fig_legend.canvas])\n",
    "        self.V_board_table = widgets.VBox([self.b_fig.canvas,\n",
    "                                           self.fov_eat_table_display])\n",
    "        self.final_display = widgets.HBox([self.V_board_table,\n",
    "                                           self.V_score_start_output_legend])\n",
    "      else:\n",
    "        if len(self.players) == 1:\n",
    "          # an ai player without recording\n",
    "          self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "          self.final_display = widgets.VBox([\n",
    "            self.b_fig.canvas, self.H_score_output_start])\n",
    "        else:\n",
    "          # more than one ai player\n",
    "          self.V_board_outbput = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "          self.V_scoreboard_start_legend = widgets.VBox([\n",
    "              self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "          self.final_display = widgets.HBox([self.V_board_outbput,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.fov_eat_table_display:\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    # index is 0 through num_critter-1, critter value in pieces is\n",
    "    # index + 1\n",
    "    active_player_index = old_board['active_player']\n",
    "    old_scores = old_board['scores'][0]\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                         self.gwg.num_food, self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "      b.set_state(old_board)\n",
    "      percept = b.get_perceptions(self.radius)[0]\n",
    "\n",
    "    #print(self.players[active_player_index])\n",
    "    #print(active_player_index)\n",
    "    #if not self.players[active_player_index] == 'human':\n",
    "    #  print(self.players[active_player_index].critter_index)\n",
    "    #else:\n",
    "    #  print('human has no critter_index attribute')\n",
    "\n",
    "    if (isinstance(self.players[active_player_index], str) and\n",
    "        'human' in self.players[active_player_index]):\n",
    "      direction = which_button\n",
    "    else:\n",
    "      a_player, _, _ = self.players[active_player_index].play(old_board)\n",
    "      a_player = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                      active_player_index+1,\n",
    "                                                      a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = a_player[0]\n",
    "\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "          self.board_state, active_player_index+1, [direction])\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = np.floor(self.gwg.lifetime -\n",
    "                         rounds_left / self.gwg.num_critters)\n",
    "    if new_scores[active_player_index] > old_scores[active_player_index]:\n",
    "      #eating happened\n",
    "      eating_string = \"They ate the food there!\"\n",
    "      did_eat = 1\n",
    "    else: #eating didn't happen\n",
    "      eating_string = \"There's no food there.\"\n",
    "      did_eat = 0\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0,\n",
    "                                       active_player_index+1)\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             radius=self.radius, legend_type=self.legend_type)\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = {'perception': percept.copy(),\n",
    "                  'state': old_board,\n",
    "                  'did_eat': bool(did_eat)}\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1) # number of food items in FoV\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        print(\"The critter (tried) to move \" + direction +\n",
    "              \" and is now at ({}, {}).\".format(row,col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\\nFood Per Move: {:.2f}\".format(\n",
    "            rounds_left, new_scores[active_player_index],\n",
    "            new_scores[active_player_index] / num_moves))\n",
    "      else: # more than one players\n",
    "        print(\"Critter {} (tried) to move \".format(active_player_index+1) +\n",
    "              direction +\n",
    "              \" and is now at ({}, {}).\".format(row, col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\".format(\n",
    "            rounds_left, new_scores))\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_scores)\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores))\n",
    "        print('Resetting the board for another game')\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                               self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                               has_fov=self.has_fov, radius=self.radius,\n",
    "                               legend_type=self.legend_type)\n",
    "    with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.players) == 1:\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = [\n",
    "              ['High Score:', str(np.max(np.array(self.final_scores)))],\n",
    "              ['Last Score:', str(self.final_scores[-1])],\n",
    "              ['Average Score',\n",
    "              '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "          else:\n",
    "            table = [['High Score:', '--'],\n",
    "                     ['Last Score:', '--'],\n",
    "                     ['Average Score:', '--']]\n",
    "          print(tabulate(table))\n",
    "        else: # len(self.players) > 1\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = []\n",
    "            # Assuming the batch size is 1 for now\n",
    "            current_scores = self.final_scores[-1]\n",
    "            max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "            average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "            table.append(['High Scores:'] +\n",
    "              [str(score) for score in max_scores])\n",
    "            table.append(['Last Scores:'] +\n",
    "              [str(score) for score in current_scores])\n",
    "            table.append(['Average Scores:'] +\n",
    "              ['{:.2f}'.format(score) for score in average_scores])\n",
    "          else:\n",
    "            table = [\n",
    "              ['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "          print(tabulate(table, headers=headers))\n",
    "\n",
    "    with self.fov_eat_table_display:\n",
    "      clear_output()\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "\n",
    "  def human_ai_player_loop(self, direction):\n",
    "    self.disable_direction_buttons()\n",
    "    for player in self.players:\n",
    "      if player == 'human':\n",
    "        self.button_output_update(direction)\n",
    "      else:\n",
    "        self.button_output_update('tbd')\n",
    "    self.enable_direction_buttons()\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    for ii in range(self.gwg.lifetime*self.gwg.num_critters):\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.2)\n",
    "    self.start_button.disabled = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Head2HeadGridworld():\n",
    "  \"\"\"\n",
    "  A widget for interacting with a gridworld game while an artificial player\n",
    "  plays on an identical board or watching two artificial players play, again\n",
    "  with identical starting positions (though RNG not synched between the two\n",
    "  boards, so not like duplicate bridge). We are not going to worry about having\n",
    "  more than 1 critter type playing in head to head, (maybe we will to talk\n",
    "  about cooperation... maybe).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None,\n",
    "               player0='human', p0_short_name='YOU', p0_long_name=None,\n",
    "               player1=None, p1_short_name='THEM', p1_long_name=None,\n",
    "               has_fov=False, radius=2, collect_fov_data=False,\n",
    "               critter_name='Critter', figsize=(5,4.5),\n",
    "               has_temp_slider=False):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batch_size of 2\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      player0: object with a play method that takes a board state\n",
    "        as an argument and returns a move. If none will use a random player\n",
    "        if the special string 'human' is passed make arrow keys for that player\n",
    "      player1: same deal as player0, never more than 1 human player\n",
    "      has_fov: bool, whether or not to display field of view around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "    \"\"\"\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.final_scores = []\n",
    "    self.player0 = player0\n",
    "    self.p0_short_name = p0_short_name\n",
    "    self.p0_long_name = p0_long_name\n",
    "    self.player1 = player1\n",
    "    self.p1_short_name = p1_short_name\n",
    "    self.p1_long_name = p1_long_name\n",
    "    self.no_human = True\n",
    "    if self.player0 == 'human':\n",
    "      assert self.player1 != 'human'\n",
    "      self.no_human = False\n",
    "    if self.player1 == 'human':\n",
    "      assert self.player0 != 'human'\n",
    "      self.no_human = False\n",
    "    self.p0_next_move = None\n",
    "    self.p1_next_move = None\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.critter_name = critter_name\n",
    "    self.figsize = figsize\n",
    "    if player0 is None:\n",
    "      self.player0 = RandomValidPlayer(self.gwg)\n",
    "    else:\n",
    "      self.player0 = player0\n",
    "    if player1 is None:\n",
    "      self.player1 = RandomValidPlayer(self.gwg)\n",
    "    else:\n",
    "      self.player1 = player1\n",
    "    self.has_temp_slider = has_temp_slider\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      self.percept_eat_records = []\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    #print(self.board_state)\n",
    "\n",
    "    # both players have same starting board\n",
    "    self.board_state['pieces'][1] = self.board_state['pieces'][0].copy()\n",
    "\n",
    "    # Initialize widgets and buttons\n",
    "    if self.has_temp_slider:\n",
    "      self.sft_slider_label = widgets.Label(value='Softmax Temperature')\n",
    "      self.sft_slider = widgets.FloatSlider(value=1.0, min=0.05,\n",
    "                                            max=5.0, step=0.05)\n",
    "      self.softmax_temp_slider = widgets.VBox([self.sft_slider_label,\n",
    "                                               self.sft_slider])\n",
    "    self.output0 = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.output1 = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='20em', max_width='21em', min_height='6.3em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "                                    layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "                                      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "                                      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "                                       layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em', margin='0.6em 0 0 0'))  # 0.6em top margin\n",
    "\n",
    "    #set up buttons and outputs and layouts\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    plt.ioff()\n",
    "    (self.b_fig0, self.b_ax0, self.b_crit_specs0, self.b_food0, self.b_fov0,\n",
    "     self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "        self.board_state, g=0, legend_type='separate', figsize=self.figsize,\n",
    "        has_fov=self.has_fov, radius=self.radius,\n",
    "        name=self.critter_name, title=self.p0_long_name)\n",
    "    (self.b_fig1, self.b_ax1, self.b_crit_specs1, self.b_food1, self.b_fov1\n",
    "     ) = self.gwg.plot_board(self.board_state, g=1, legend_type=None,\n",
    "                             figsize=self.figsize, has_fov=self.has_fov,\n",
    "                             radius=self.radius, title=self.p1_long_name)\n",
    "    # player 0 is human\n",
    "    self.board_buttons_and_output0 = widgets.VBox(\n",
    "      [self.b_fig0.canvas, self.buttons, self.output0])\n",
    "    # player 1 is human\n",
    "    self.board_buttons_and_output1 = widgets.VBox(\n",
    "      [self.b_fig1.canvas, self.buttons, self.output1])\n",
    "    # non human players\n",
    "    self.board_and_output0 = widgets.VBox([self.b_fig0.canvas, self.output0])\n",
    "    self.board_and_output1 = widgets.VBox([self.b_fig1.canvas, self.output1])\n",
    "\n",
    "    self.legend_and_scores = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                           self.scoreboard])\n",
    "    if self.has_temp_slider:\n",
    "      self.legend_scores_start = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                               self.scoreboard,\n",
    "                                               self.softmax_temp_slider,\n",
    "                                               self.start_button])\n",
    "    else:\n",
    "      self.legend_scores_start = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                               self.scoreboard,\n",
    "                                               self.start_button])\n",
    "    if self.player0 == 'human':\n",
    "      self.final_display = widgets.HBox([self.board_buttons_and_output0,\n",
    "                                         self.legend_and_scores,\n",
    "                                         self.board_and_output1])\n",
    "    elif self.player1 == 'human':\n",
    "      self.final_display = widgets.HBox([self.board_and_output0,\n",
    "                                         self.legend_and_scores,\n",
    "                                         self.board_buttons_and_output1])\n",
    "    else: # no human player\n",
    "      self.final_display = widgets.HBox([self.board_and_output0,\n",
    "                                          self.legend_scores_start,\n",
    "                                          self.board_and_output1])\n",
    "    # initial text outputs\n",
    "    # if there's a temp slider check who, if anyone uses it\n",
    "    self.p0_uses_temp = False\n",
    "    self.p1_uses_temp = False\n",
    "    if self.has_temp_slider:\n",
    "      if self.player0=='human':\n",
    "        pass\n",
    "      else:\n",
    "        try:\n",
    "          _ = self.player0.play(self.board_state, temp=1.0)\n",
    "          self.p0_uses_temp = True\n",
    "        except TypeError: pass\n",
    "      if self.player1 == 'human':\n",
    "        pass\n",
    "      else:\n",
    "        try:\n",
    "          _ = self.player1.play(self.board_state, temp=1.0)\n",
    "          self.p1_uses_temp = True\n",
    "        except TypeError: pass\n",
    "      if not self.p0_uses_temp and not self.p1_uses_temp:\n",
    "        with self.output0:\n",
    "          print(\"Warning: neither player supports temperature adjustment. \"\n",
    "                \"The slider will have no effect.\")\n",
    "    with self.output0:\n",
    "      if self.no_human == False:\n",
    "        print('Click a button to start.')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.scoreboard:\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Avg. Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    old_scores = old_board['scores'][:,0] #both batches only one critter type\n",
    "    active_player_index = old_board['active_player']\n",
    "    self.disable_buttons()\n",
    "    if self.player0 == 'human':\n",
    "      a_player0 = which_button\n",
    "    else:\n",
    "      if self.p0_next_move is not None:\n",
    "        a_player0_ = self.p0_next_move\n",
    "        self.p0_next_move = None\n",
    "      else:\n",
    "        with self.output0:\n",
    "          print(\"AI is thinking...\")\n",
    "        if self.p0_uses_temp:\n",
    "          a_player0_, _, _ = self.player0.play(old_board,\n",
    "                                               temp=self.sft_slider.value)\n",
    "        else:\n",
    "          a_player0_, _, _ = self.player0.play(old_board)\n",
    "      a_player0_ = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                        active_player_index+1,\n",
    "                                                        a_player0_)\n",
    "      a_player0 = a_player0_[0]\n",
    "    if self.player1 == 'human':\n",
    "      a_player1 = which_button\n",
    "    else:\n",
    "      if self.p1_next_move is not None:\n",
    "        a_player1_ = self.p1_next_move\n",
    "        self.p1_next_move = None\n",
    "      else:\n",
    "        with self.output1:\n",
    "          print(\"AI is thinking...\")\n",
    "        if self.p1_uses_temp:\n",
    "          a_player1_, _, _ = self.player1.play(old_board,\n",
    "                                               temp=self.sft_slider.value)\n",
    "        else:\n",
    "          a_player1_, _, _ = self.player1.play(old_board)\n",
    "      a_player1_ = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                        active_player_index+1,\n",
    "                                                        a_player1_)\n",
    "      a_player1 = a_player1_[1]\n",
    "    self.enable_buttons()\n",
    "\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "        self.board_state, active_player_index+1, [a_player0, a_player1])\n",
    "\n",
    "    # Try to precompute next AI player move(s) if there are any rounds left\n",
    "    if self.board_state['rounds_left'][0] > 0:\n",
    "      if self.player0 != 'human':\n",
    "        if self.p0_uses_temp:\n",
    "          self.p0_next_move, _, _ = self.player0.play(\n",
    "            self.board_state, temp=self.sft_slider.value)\n",
    "        else:\n",
    "          self.p0_next_move, _, _ = self.player0.play(self.board_state)\n",
    "      if self.player1 != 'human':\n",
    "        if self.p1_uses_temp:\n",
    "          self.p1_next_move, _, _ = self.player1.play(\n",
    "            self.board_state, temp=self.sft_slider.value)\n",
    "        else:\n",
    "          self.p1_next_move, _, _ = self.player1.play(self.board_state)\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                         self.gwg.num_food, self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "      b.set_state(old_board)\n",
    "      percept = b.get_perceptions(self.radius)\n",
    "\n",
    "    new_scores = self.board_state['scores'][:,0] #both batches one critter type\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = self.gwg.lifetime - rounds_left\n",
    "\n",
    "    if new_scores[0] > old_scores[0]:\n",
    "      eating_string0 = \"They ate the food there!\"\n",
    "    else:\n",
    "      eating_string0 = \"There's no food there.\"\n",
    "    if new_scores[1] > old_scores[1]:\n",
    "      eating_string1 = \"They ate the food there!\"\n",
    "    else:\n",
    "      eating_string1 = \"There's no food there.\"\n",
    "    did_eat = int(new_scores[0] > old_scores[0])\n",
    "\n",
    "    row0, col0 = self.gwg.get_critter_rc(self.board_state, 0, 1)\n",
    "    (self.b_fig0, self.b_ax0, self.b_crit_specs0, self.b_food0, self.b_fov0\n",
    "     ) = self.gwg.plot_board(self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "                             self.b_crit_specs0, self.b_food0, self.b_fov0,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "    row1, col1 = self.gwg.get_critter_rc(self.board_state, 1, 1)\n",
    "    (self.b_fig1, self.b_ax1, self.b_crit_specs1, self.b_food1, self.b_fov1\n",
    "     ) = self.gwg.plot_board(self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "                             self.b_crit_specs1, self.b_food1, self.b_fov1,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "\n",
    "    with self.output0:\n",
    "      clear_output()\n",
    "      if self.player0 == 'human':\n",
    "        print(\"You clicked the \" + which_button +\n",
    "              \" button and your critter is now at ({}, {}).\".format(row0,col0))\n",
    "      else:\n",
    "        print(\"This player (tried) to move \" + a_player0 +\n",
    "              \" and is now at ({}, {}).\".format(row0,col0))\n",
    "      print(eating_string0)\n",
    "      print(\"Rounds Left: {} \\nFood Eaten: {} \\nFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_scores[0], new_scores[0] / num_moves))\n",
    "    with self.output1:\n",
    "      clear_output()\n",
    "      if self.player1 == 'human':\n",
    "        print(\"You clicked the \" + which_button +\n",
    "              \" button and your critter is now at ({}, {}).\".format(row1,col1))\n",
    "      else:\n",
    "        print(\"This player (tried) to move \" + a_player1 +\n",
    "              \" and is now at ({}, {}).\".format(row1,col1))\n",
    "      print(eating_string1)\n",
    "      print(\"Rounds Left: {} \\nFood Eaten: {} \\nFood Per Move: {:.2f}\".format(\n",
    "        rounds_left, new_scores[1], new_scores[1] / num_moves))\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = (percept.copy(), did_eat, old_board)\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1, axis=1)\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_scores)\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "      self.board_state['pieces'][1] = self.board_state['pieces'][0].copy()\n",
    "      (self.b_fig0, self.b_ax0, self.b_crit_specs0, self.b_food0, self.b_fov0\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "                             self.b_crit_specs0, self.b_food0, self.b_fov0,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "      (self.b_fig1, self.b_ax1, self.b_crit_specs1, self.b_food1, self.b_fov1\n",
    "       ) = self.gwg.plot_board(self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "                             self.b_crit_specs1, self.b_food1, self.b_fov1,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             legend_type=None)\n",
    "      with self.output0:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores[0]))\n",
    "        print('Resetting the board for another game')\n",
    "      with self.output1:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores[1]))\n",
    "        print('Resetting the board for another game')\n",
    "    with self.scoreboard:\n",
    "      clear_output()\n",
    "      self.b_fig_legend.canvas.draw()\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      if len(self.final_scores) > 0:\n",
    "        table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', str(np.max(np.array(self.final_scores)[:,0])),\n",
    "                          str(np.max(np.array(self.final_scores)[:,1]))],\n",
    "          ['Last Score:', str(self.final_scores[-1][0]),\n",
    "                          str(self.final_scores[-1][1])],\n",
    "          ['Average Score',\n",
    "            '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,0])),\n",
    "            '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,1]))]]\n",
    "      else:\n",
    "        table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Average Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.button_output_update('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.button_output_update('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.button_output_update('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.button_output_update('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    if self.has_temp_slider:\n",
    "      self.softmax_temp_slider.disabled = True\n",
    "    for ii in range(self.gwg.lifetime):\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.2)\n",
    "    self.start_button.disabled = False\n",
    "    if self.has_temp_slider:\n",
    "      self.softmax_temp_slider.disabled = False\n",
    "\n",
    "  def disable_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "\n",
    "  def enable_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.3.1: Comparing Policies\n",
    "\n",
    "In the previous sequences when comparing policies, we primarily looked at how efficient different policies were at eating food. However, overall performance is not the only way policies can be compared. A policy function can be used to create visualizations that allow for more direct and detailed comparisons between policies. As an example, here we will look at specific environmental contexts (sensory inputs) and see in which contexts policies differ and where they are the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Instead of watching a simulation play out in time, we will plot a given food layout and then ask which direction (or direction probabilities) a given policy would select at any of the non-food positions on the board. This gives a static snapshot of what a given policy will do across a broad range of different situations. First, we will use this to compare two very similar 'Random' policies. The 'Random Valid' policy is equally likely to choose any valid direction, while the 'Random Direction' policy tries to move up, down, left or right with uniform probability, but then sometimes 'bounces' off the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to see the movement direction probabilities for the 'Random Valid' policy. The darkness of each arrow, *alpha level* in plotting lingo, is proportional to the probility of moving in that direction.\n",
    "gwg = GridworldGame(batch_size=2, n_cols=7, n_rows=7, num_critters=1)\n",
    "init_state = gwg.get_init_board()\n",
    "rvp = RandomValidPlayer(gwg, 1)\n",
    "gwg.plot_moves(init_state, rvp, title=\"'Random Valid'\\nDirection Probabilities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to see the effective movement probabilities of the 'Random Direction' policy.\n",
    "rdp = RandomDirectionPlayer(gwg)\n",
    "gwg.plot_moves(init_state, rdp, title=\"'Random Direction'\\nDirection Probabilities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Question:** Although the 'Random Valid' and 'Random Direction' policies are very similar, there are some slight differences. Can you pinpoint these differences and the specific contexts in which they occur?\n",
    "[Answer](## \"Along the board's edges, the 'Random Direction' policy moves away from the edge with probability 1/2. In contrast, the 'Random Valid' policy moves away from the edge with a probability of 1/3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next let's compare the heavily optimized and omniscient GW7x7-10-30 policy (a combination of Deep Reinforcement Learning and Monte Carlo search), with the 'Parameterized Weights' policy (linear weights from a limited perceptual field determine the probability of movement direction.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to see the direction probabilities for GW7x7-10-30's.\n",
    "\n",
    "pvnetMC = PolicyValueNetwork(gwg)\n",
    "mcp = MonteCarloBasedPlayer(gwg, pvnetMC, default_depth=3,\n",
    "                            default_rollouts=80, default_temp=0.02)\n",
    "\n",
    "\n",
    "#grab the saved model from the repo or where it ends up being hosted\n",
    "url = \"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/data/pvnetMC.pth.tar\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "  filename = os.path.basename(url)\n",
    "  # Write the contents to a file in the current working directory\n",
    "  with open(filename, 'wb') as file:\n",
    "    file.write(r.content)\n",
    "    #print(f'{filename} downloaded successfully.')\n",
    "else:\n",
    "  print('Error occurred while downloading the file.')\n",
    "\n",
    "# load the saved model\n",
    "pvnetMC.load_checkpoint(folder=os.getcwd(), filename='pvnetMC.pth.tar')\n",
    "\n",
    "gwg.plot_moves(init_state, mcp, title=\"GW7x7-10-30\\nDirection Probabilities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to see the direction probabilities of the 'Parameterized Weights' policy.\n",
    "\n",
    "ppp = PerceptParamPlayer(gwg)\n",
    "gwg.plot_moves(init_state, ppp, title=\"'Parameterized Weights'\\nDirection Probabilities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "It looks like the 'Parameterized Weights' policy and GW7x7-10-30 are somewhat similar. Instead of having to scroll back and forth to compare these policies we can plot the two policies side by side on the same board and see where they are the same, and where they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to compare 'Parameterized Weights' and 'GW7x7-10-30' side by side.\n",
    "gwg.plot_moves(init_state, player0=ppp, g=0, player1=mcp, p0_name='Parameterized', p1_name='GW7x7-10-30', figsize=(7,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now let's compare these two policies.\n",
    "Questions:\n",
    "1.   Looking at grid cells (5,3) and (4,4), the two policies are very similar. Why might this be?\n",
    "2.   Looking at grid cell (1,3) the policies differ, how and why?\n",
    "3. Looking at grid cells (4,3) and (4,5) the policies differ again but for different reasons. What are these differences and their reasons?\n",
    "\n",
    "[Answer](## \"In some cells, like (5,3) and (4,4) the two policies behave identically, efficiently moving to nearby food. In cases like this the parameterized policy acts on nearby relevant info in the same way as the more complicated policy. In other cells like (1,3) the two polices are very different and this can be chalked up to the fact that the parameterized policy has limited perception and so cannot navigate efficiently to distant food. In this cell (1,3) its perceptual field is empty, in contrast GW7x7-10-30 'sees' the whole board and so knows how to move towards distant food. However, in yet other cells, like (4,5), more interesting differences appear. The parameterized policy is inherently a short term maximizer, and so it moves immediately towards the food at (6,5). In contrast GW7x7-10-30 has learned to sometimes forgo immediate reward to 'set itself up' for an efficient series of moves, and so moves to (5,6) after which it will be able to make (at least) four eating moves in a row. (Note that this holds only for the board configuration when random seed set as SEED = 2021).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "While there are sets of parameters that will allow the parameterized player to behave like GW7x7-10-30 in some circumstances, the basic architecture of the 'Parameterized Weights' policy is not sufficiently rich to match GW7x7-10-30 in all circumstances.\n",
    "\n",
    "The 'Parameterized Weights' policy chooses a direction based on a linear combination of its perceptual inputs. This enables it to move towards the centers of high-density patches of food. In contrast, the non-linear architecture of GW7x7-10-30 allows it to both move towards the edges of high-density patches of food *and then* move efficiently through the patch, effectively implementing a more subtle logic that takes into account future moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.3.2: Memory\n",
    "\n",
    "Up to this point, all the policies we've examined have based their actions on immediate perceptions of the environment. However, in many cases taking into account what has happened in the past can drastically improve the efficiency of a policy. While GW7x7x-10-30's omniscience negates the potential benefits of integrating memory into its decision-making, other policies, particularly the simple 'Random Valid' policy, can benefit from even very simple incorporation of memory.\n",
    "\n",
    "To see this, we will now make a modified version of the random player that remembers which grid cell it was in last round and uses this to avoid immediate backtracking. It is worth noting that \"memory\", especially in straightforward cases like this, need not be solely dependent on an internal neural state. Instead, memory might also be fully or partially embedded in an organism's movement mechanics. For instance, \"memory\" could be an orientation in relation to the direction of motion, with new decisions based on this relative orientation. Regardless of the mechanism, some ability to integrate the past can be beneficial.\n",
    "\n",
    "Let's implement this 'Memory Improved Random Valid' policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to define the tests for the coding exercise below.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def test_memory_reset(game):\n",
    "  player = RandomValidMemoryPlayer(game)\n",
    "  board = game.get_init_board()\n",
    "  board['rounds_left'] = np.zeros(board['rounds_left'].shape)\n",
    "  player.last_locs = [0]\n",
    "  player.play(board)\n",
    "  assert player.last_locs is None, \"Memory was not reset after the last round!\"\n",
    "  return \"Memory reset test passed!\"\n",
    "\n",
    "def test_memory_update(game):\n",
    "  player = RandomValidMemoryPlayer(game)\n",
    "  board = game.get_init_board()\n",
    "  expected_last_loc = game.moves_to_actions(np.where(board['pieces'] == player.critter_index))\n",
    "  player.play(board)\n",
    "  assert np.array_equal(player.last_locs, expected_last_loc), \"Memory was not updated correctly!\"\n",
    "  return \"Memory update test passed!\"\n",
    "\n",
    "def test_invalidating_old_moves(game):\n",
    "  player = RandomValidMemoryPlayer(game)\n",
    "  # Define an initial board state\n",
    "  board = {\n",
    "  'pieces': np.zeros((1, 7, 7)),\n",
    "  'scores': np.array([0]),\n",
    "  'rounds_left': np.array([2]),\n",
    "  'active_player': 0\n",
    "  }\n",
    "  # Set a starting position for the player (for example, center of the board)\n",
    "  board['pieces'][0, 3, 3] = 1\n",
    "  #play the board\n",
    "  _, _, probs = player.play(board)\n",
    "  # Manually set this starting position to last_locs\n",
    "  player.last_locs = [3 * 7 + 3]  # This translates (3,3) to a flattened index\n",
    "  # Ensure the player doesn't move back to the previous position\n",
    "  assert probs[0][player.last_locs[0]] == 0, \"Backtracking was not invalidated!\"\n",
    "  return \"Invalidating old moves test passed!\"\n",
    "\n",
    "def test_probabilities(game):\n",
    "    player = RandomValidMemoryPlayer(game)\n",
    "    board = game.get_init_board()\n",
    "    _, _, probs = player.play(board)\n",
    "    assert np.isclose(np.sum(probs), 1), \"Probabilities don't sum up to 1!\"\n",
    "    return \"Probabilities test passed!\"\n",
    "\n",
    "def test_valid_actions(game):\n",
    "    player = RandomValidMemoryPlayer(game, critter_index=1)\n",
    "    board = game.get_init_board()\n",
    "    actions, _, _ = player.play(board)\n",
    "    valids = game.get_valid_actions(board, player.critter_index)\n",
    "    for action in actions:\n",
    "        assert valids[0][action] == 1, f\"Invalid action: {action}\"\n",
    "    return \"Valid actions test passed!\"\n",
    "\n",
    "def run_all_tests():\n",
    "    test_game = GridworldGame(batch_size=2,\n",
    "                              n_rows=7, n_cols=7, num_critters=1,\n",
    "                              num_food=10, lifetime=30)\n",
    "    tests = [test_memory_reset, test_memory_update, test_invalidating_old_moves,\n",
    "             test_probabilities, test_valid_actions]\n",
    "    for test_func in tests:\n",
    "        result = test_func(test_game)\n",
    "        print(result)\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "  test_game = GridworldGame(batch_size=1, n_rows=7, n_cols=7, num_critters=1,\n",
    "                            num_food=10, lifetime=30)\n",
    "  tests = [test_memory_reset, test_memory_update, test_invalidating_old_moves,\n",
    "           test_probabilities, test_valid_actions]\n",
    "  for test_func in tests:\n",
    "    result = test_func(test_game)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO for students: Complete the lines with ...  to stop the critter from\n",
    "# moving to cells it was in immediately before, and to update the memory of\n",
    "# where the critter was last round. Then comment out the line below and see if\n",
    "# your code runs and passes the code checks.\n",
    "raise NotImplementedError(\"Exercise: limit backtracking with memory\")\n",
    "################################################################################\n",
    "\n",
    "class RandomValidMemoryPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player that uses memory for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.last_locs = None\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    #invalidate old moves using memory of where critter last was\n",
    "    if self.last_locs is not None:\n",
    "      valids[(np.arange(batch_size), self.last_locs)] = ...\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "    action_size = n_rows * n_cols\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    # update memory\n",
    "    rounds_left = board['rounds_left'][0]\n",
    "    if rounds_left > 1:\n",
    "      self.last_locs = self.game.moves_to_actions(\n",
    "          np.where(board['pieces'] == ... ))\n",
    "    else:\n",
    "      # last move of the game reset memory for next game\n",
    "      self.last_locs = None\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "class RandomValidMemoryPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player that uses memory for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.last_locs = None\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    #invalidate old moves using memory of where critter last was\n",
    "    if self.last_locs is not None:\n",
    "      valids[(np.arange(batch_size), self.last_locs)] = 0\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "    action_size = n_rows * n_cols\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    # update memory\n",
    "    rounds_left = board['rounds_left'][0]\n",
    "    if rounds_left > 1:\n",
    "      self.last_locs = self.game.moves_to_actions(\n",
    "          np.where(board['pieces'] == self.critter_index ))\n",
    "    else:\n",
    "      # last move of the game reset memory for next game\n",
    "      self.last_locs = None\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# To figure out this exercise you might want/need to play around with and think\n",
    "# about what self.last_locs should look like, and what valids does look like.\n",
    "# To do that run this cell and then hack around a bit.\n",
    "gwg_explore = GridworldGame(batch_size=1, n_rows=7, n_cols=7, num_critters=1,\n",
    "                            num_food=10, lifetime=30,\n",
    "                            rng=np.random.default_rng(seed=5))\n",
    "init_board = gwg_explore.get_init_board()\n",
    "print('Initial board state')\n",
    "print(init_board)\n",
    "valids = gwg_explore.get_valid_actions(init_board, critter=1)\n",
    "\n",
    "print('\\n-----------------------------------------------------')\n",
    "print('Human friendly (text) display of the board:')\n",
    "gwg_explore.display(init_board, g=0)\n",
    "\n",
    "print('\\n-----------------------------------------------------')\n",
    "print('np.where critter location format:')\n",
    "display(np.where(init_board['pieces']==1))\n",
    "\n",
    "\n",
    "print('\\n-----------------------------------------------------')\n",
    "print('Flattened index representation of critter location')\n",
    "display(gwg_explore.moves_to_actions(\n",
    "          np.where(init_board['pieces']==1)))\n",
    "\n",
    "\n",
    "print('\\n-----------------------------------------------------')\n",
    "print('Human friendly (text) display of the valid moves')\n",
    "gwg_explore.display_moves(init_board, g=0)\n",
    "\n",
    "print('\\n-----------------------------------------------------')\n",
    "print('Array representation of Valids')\n",
    "print('Shape of valids:' +str(valids.shape))\n",
    "print(valids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Why does `pieces` represent the board as a batch of 7x7 arrays, but `valids` give a boolean (or mask-based) index for a flattened 7x7 array? How does the boolean index provided by `valids` relate to directions relative to the organism's location? Why is the organism's location represented both as a triple of arrays by the output of `np.where` but also as a flattened index of the 7x7 array?\n",
    "\n",
    "The reason for this diversity of representations of the same fundamental underlying state is that different types of computation and reasoning are much more efficient using different representations. This efficiency can and often does compensate for the additional effort of maintaining and translating between a variety of representations. (The transformation of inputs into internal representations well suited to specific computations is a critical aspect of neural processing that we will explore later in this book.)\n",
    "\n",
    "Up to this point, we've abstracted many details of data representation to streamline the material. While this approach generally simplifies the learning process, it is also essential to be aware of these behind-the-scenes complexities. Wholly ignoring these details would do you a disservice, especially given that the often tedious bookkeeping involved in translating between data representations is integral to the implementation of effective learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we've got our memory-enhanced random drifter, let's see how it compares to memoryless random drifting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Random Drifting with and without memory\n",
    "# @markdown **Run the cell** and press start to compare the random + memory policy we defined above with memoryless random drifting.\n",
    "\n",
    "class RandomValidMemoryPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player that uses memory for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.last_locs = None\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    #invalidate old moves using memory of where critter last was\n",
    "    if self.last_locs is not None:\n",
    "      valids[(np.arange(batch_size), self.last_locs)] = 0\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "    action_size = n_rows * n_cols\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    # update memory\n",
    "    rounds_left = board['rounds_left'][0]\n",
    "    if rounds_left > 1:\n",
    "      self.last_locs = self.game.moves_to_actions(\n",
    "          np.where(board['pieces']==self.critter_index))\n",
    "    else:\n",
    "      # last move of the game reset memory for next game\n",
    "      self.last_locs = None\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "\n",
    "gwg = GridworldGame(batch_size=2,\n",
    "                    n_rows=7, n_cols=7,\n",
    "                    num_critters=1,\n",
    "                    num_food=10,\n",
    "                    lifetime=30,\n",
    "                    rng=np.random.default_rng(seed=9))\n",
    "rvp = RandomValidPlayer(gwg)\n",
    "rvmp = RandomValidMemoryPlayer(gwg)\n",
    "h2h_igwg = Head2HeadGridworld(gwg, player0=rvp, p0_short_name='RANDOM',\n",
    "                              p0_long_name='Random Valid\\nPolicy',\n",
    "                              player1=rvmp, p1_short_name='MEMORY',\n",
    "                              p1_long_name='Random + Memory\\nPolicy',\n",
    "                              figsize=(2.4,3.0), has_fov=False, radius=2,\n",
    "                              critter_name='$\\mathit{Critter}$')\n",
    "display(h2h_igwg.b_fig0.canvas)\n",
    "display(h2h_igwg.b_fig1.canvas)\n",
    "display(h2h_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(h2h_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "How much more efficient was drifting with memory? Why? Determining exactly how much more efficient the elimination of backtracking makes random drifting is challenging. However, we can reason through a rough upper bound on the improvement as follows:\n",
    "\n",
    "1. The gains in efficiency largely come from the avoidance of backtracking.\n",
    "2. A random mover spends on average more than 1/4 of its time backtracking. To figure out precisely how much backtracking occurs, we'd need to know the amount of time spent in corners and on the edges, where the probability of backtracking is greater relative to the middle. However, more than 1/4 is a good enough estimate for now.\n",
    "3. Assume that when not backtracking, the 'Random Valid' and 'Random Valid + Memory' policies are equally likely to consume food. This assumption probably isn't exactly true, but it's close enough for a rough answer.\n",
    "4. If the 'Random Valid' policy is throwing away at least 1/4 of moves due to direct backtracking, then the average score of 'Random Valid + Memory' should be at least 1.33 times greater.\n",
    "\n",
    "Run the above simulation a few more times, until the average scores appear to have stabilized. How much better is 'Random Valid + Memory' compared to 'Random Valid'? Is the improvement more, less, or about 1.33 times better?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Reasoning exercises like this can serve as both a sanity check of intuitions against data and of data (including collection and display) against intuitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.1.3.3 Interacting Policies\n",
    "So far, when we have compared policies, each organism has been contentedly pursuing its objectives in its own personal private Gridworld universe, and we have assessed the efficiency of each organism acting in isolation. However, for many organisms, a key aspect of life is competitionâ€”specifically, competition for limited resources critical to survival and replication, such as food, viable nesting sites, and mating opportunities. Competition can occur between conspecifics or organisms of different species. For example, individuals of a prey species may compete among themselves to avoid being eaten by a predator while also competing directly with individuals of the predator species over who gets eaten and who goes hungry.\n",
    "\n",
    "To explore how competition might affect performance, we now modify Gridworld to include two different organisms that take turns moving (and eating if they land on food, they do not eat each other yet). Food is immediately replenished after being consumed; strictly speaking, it is not more scarce. However, the presence of two organisms pursuing the same goal does create a more complex environment with new dynamics. In this multi-organism Gridworld, organisms are not allowed to move onto a space already occupied by another organism, much like they can't move off the board.\n",
    "\n",
    "Let's observe what happens when GW7x7-10-30 and the 'Parameterized Weights' policy compete directly in this shared environment. For now, neither policy perceives the other organism. Specifically, each organism perceives the grid cell occupied by the other organism as empty when initially determining action probabilities; that is they employ the same policies as they did in the single-organism version of Gridworld. However, these initial action probabilities are now re-normalized over valid movement options to avoid moving onto the grid cell occupied by the other organism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title 'Parameterized Weights' and GW7x7-10-30 in a Shared Environment\n",
    "# @markdown Don't worry about how this code works â€“ just **run this cell** and press the start button to see the 'Parameterized Weights' policy and GW7x7-10-30 in a shared environment.\n",
    "rng = np.random.default_rng(seed=420)\n",
    "gwg = GridworldGame(batch_size=1, n_rows=7, n_cols=7,\n",
    "                    num_critters=2, num_food=10,\n",
    "                    lifetime=30, rng=rng)\n",
    "init_board = gwg.get_init_board()\n",
    "pvnetMC = PolicyValueNetwork(gwg)\n",
    "#grab the saved model from the repo or where it ends up being hosted\n",
    "url = \"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/data/pvnetMC.pth.tar\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "  filename = os.path.basename(url)\n",
    "  # Write the contents to a file in the current working directory\n",
    "  with open(filename, 'wb') as file:\n",
    "    file.write(r.content)\n",
    "    #print(f'{filename} downloaded successfully.')\n",
    "else:\n",
    "  print('Error occurred while downloading the file.')\n",
    "\n",
    "# load the saved model\n",
    "pvnetMC.load_checkpoint(folder=os.getcwd(), filename='pvnetMC.pth.tar')\n",
    "\n",
    "ppp = PerceptParamPlayer(gwg, critter_index=1)\n",
    "mcp = MonteCarloBasedPlayer(gwg, pvnetMC, critter_index=2, default_depth=4,\n",
    "                            default_rollouts=120, default_temp=0.01)\n",
    "compete1_igwg = InteractiveGridworld(gwg, init_board=init_board, figsize=(4,4),\n",
    "                                     critter_names=['Params', 'GW7x7'],\n",
    "                                     players=[ppp, mcp])\n",
    "display(compete1_igwg.b_fig.canvas)\n",
    "display(compete1_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(compete1_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Just watching a few simulations it's difficult to pin down whether sharing the environment is a net positive or negative for these policies. Sometimes a shared environment can make the policies more efficient, like when one organism eating food makes new food appear closer to the other organism. In other instances though, the two organisms, being unaware of each other, pursue the same food and end up getting in each other's way, depleting the immediately available food more quickly. Specifically, this version of GW7x7-10-30 does not take into account another organism. So, sometimes when GW7x7-10-30 forgoes immediate reward to 'set up' a series of efficient eating moves, the other organism swoops in, eats the food, and wrecks the plan.\n",
    "\n",
    "Later, we will examine such mechanisms statistically. For now, let's see if you can use these insights when playing with or against the 'Parameterized Weights' policy (solo version) in a shared environment. First, try to maximize your score without worrying about what the other organism's score is. Then, try to maximize the difference between your score and the other organism without worrying about how high your score is over all. Lastly, try to maximize the other organism's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title You and the 'Parameterized Weights' Policy in a Shared Environment\n",
    "# @markdown Don't worry about how this code works â€“ just **run this cell** and then press a direction button to play with (against?) the 'Parameterized Weights' policy.\n",
    "rng = np.random.default_rng(seed=420)\n",
    "gwg = GridworldGame(batch_size=1, n_rows=7, n_cols=7,\n",
    "                    num_critters=2, num_food=10,\n",
    "                    lifetime=30, rng=rng)\n",
    "init_board = gwg.get_init_board()\n",
    "ppp = PerceptParamPlayer(gwg, critter_index=2)\n",
    "compete2_igwg = InteractiveGridworld(gwg, init_board=init_board, figsize=(4,4),\n",
    "                                     critter_names=['You', 'Params'],\n",
    "                                     players=['human', ppp])\n",
    "display(compete2_igwg.b_fig.canvas)\n",
    "display(compete2_igwg.b_fig_legend.canvas)\n",
    "clear_output()\n",
    "display(compete2_igwg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "How did you do? How did you adjust your strategy depending on what your goal was?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This concludes the sequence on abstraction and reasoning about policies. By thinking of policies as functions that turn environmental inputs into behavioural outputs, we directly compared policies to see in which contexts they were the same and in which contexts they differed. As another form of more direct comparison, we looked at two organisms sharing, and to some extent competing, in the same environment. We saw that policies that had worked well in isolation did not always work quite so well in this new shared environment; a good policy for one environment might not be so great in another. The environment isn't just a static thing, but potentially a rich dynamical system dependent on the variety of organisms that live there. So, this grand challenge of biology and evolutionâ€”setting up policies that work in the real worldâ€”is even trickier than it might seem at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to take the quiz\n",
    "comprehension_quiz = [\n",
    "  {\"question\": \"In this sequence, what did we focus on when comparing policies?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"Analyzing the computing power each policy requires.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The focus was not on computational requirements.\"},\n",
    "      {\"answer\": \"Examining how each policy behaves in different environmental contexts.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. The main focus was on behavior in different contexts.\"},\n",
    "      {\"answer\": \"Determining which policy is the most efficient forager.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"This was a secondary focus in this sequences.\"},\n",
    "      {\"answer\": \"Studying the ability of each policy to adapt over time.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Adaptation over time was not the focus of the comparison.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"What was the primary reason for introducing memory into policies in this sequence?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"To increase the computational complexity of a policy.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The aim was not to make the policy more complex.\"},\n",
    "      {\"answer\": \"To decrease the computational complexity of a policy.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Simplification was not the primary goal.\"},\n",
    "      {\"answer\": \"To demonstrate the potential performance benefits of memory.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. The primary reason was to show how memory could improve performance.\"},\n",
    "      {\"answer\": \"To make movement more random by eliminating backtracking.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. The aim was not to randomize movements.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"What is the purpose of having different types of representations for different aspects of the same underlying state like 'pieces' and 'valids'?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"To confuse the reader.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Confusion is not the intent.\"},\n",
    "      {\"answer\": \"To enable efficient computation and reasoning.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. Different representations facilitate different types of computation.\"},\n",
    "      {\"answer\": \"To make the source code easier to read.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Readability is important and related but not the primary reason.\"},\n",
    "      {\"answer\": \"To increase the time it takes for the agent to make a decision.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Increasing decision time is not the goal.\"}\n",
    "    ]\n",
    "  },\n",
    "  {\"question\": \"How does sharing an environment with another organism generally affect the foraging efficiency of policies?\",\n",
    "   \"type\": \"multiple_choice\",\n",
    "   \"answers\": [\n",
    "      {\"answer\": \"Efficiency is reduced due to competition for scarce resources.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Competition can reduce foraging efficiency, but there is a more complete answer.\"},\n",
    "      {\"answer\": \"Efficiency is improved because the consumption of food by one organism generates new food in locations that might be more accessible to the other organism.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"In some cases yes, sharing can lead to synergistic benefits, but there is a more complete answer\"},\n",
    "      {\"answer\": \"Efficiency can vary; it may improve due to synergistic food sharing dynamics or decrease due to competitive obstruction between organisms.\",\n",
    "       \"correct\": True,\n",
    "       \"feedback\": \"Correct. The effect on efficiency can be variable, depending on many factors.\"},\n",
    "      {\"answer\": \"Efficiency is largely unaffected as both policies operate independently and food is instantly replenished.\",\n",
    "       \"correct\": False,\n",
    "       \"feedback\": \"False. Independent policies and instant replenishment of food does not ensure that efficiency will be unaffected.\"}\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "display_quiz(comprehension_quiz)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P1C1_Sequence3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
