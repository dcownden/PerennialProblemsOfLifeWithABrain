{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C1_BehaviourAsPolicy/instructor/P1C1_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/instructor/P1C1_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# **Part 1 Behaviour, Environments and Optimization: Evolution and Learning**\n",
    "\n",
    "### **Animals are adapted to their specific environments; their behaviour is best understood within the context of their evolutionary environment.**\n",
    "\n",
    "### Objective: Part 1 of the book aims to introduce the fundamental concepts of\n",
    "* ### **environment**, where an organism lives;\n",
    "* ### **behaviour**, what the organism does there;\n",
    "* ### **optimization**, how learning and evolution shape an organism's behaviour to make it better suited to its environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# Chapter 1.1 Behaviour as a Policy in an Environmental Context\n",
    "\n",
    "### Objective: This chapter develops examples of how behaviour is described and evaluated in relation to its [goodness](## \"This is a very loaded term, to be unpacked carefully later\") within a specific environmental niche.\n",
    "\n",
    "You will learn:\n",
    "*   What is a policy? A policy is a formalization of behaviour as a function that takes an organism's experiences of their environment as an input and outputs the organism's actions.\n",
    "*   What is a good policy? The rewards and other environmental signals, resulting from the organism's actions in the environment, are integrated into a Loss/Objective function to evaluatate, and potentially improve, a policy.\n",
    "*   What is stochasticity? Both the environment and an organism's behavior can contain random elements. This randomness can pose challenges when evaluating policies, as it becomes difficult to determine whether poor outcomes are due the policy itself or simply bad luck.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **Sequence 1.1.1: Gridworld Introduction**\n",
    "\n",
    "### Objective: In this sequence, we will create a simple environment-organism system to demonstrate how an organism's **behaviour**, within an **environment**, can be evaluated using **rewards**. We will also explore how intelligent behaviour can lead to better outcomes and how **randomness** can make evaluation of behaviour difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "You don't need to worry about how the code in the following cells works, but you do need to run these cells, in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependency Downloads and Installs\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "!pip install ipympl vibecheck datatops > /dev/null 2> /dev/null #google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import Modules\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "import ipywidgets as widgets\n",
    "import functools\n",
    "import threading\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#import jax\n",
    "import collections\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "\n",
    "IN_COLAB = False\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  print('Not running in colab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "np_rng = np.random.default_rng(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting Setup and Functions\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "def make_grid(num_rows, num_cols):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for futher use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots(figsize = (7,6), layout='constrained')\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Labels for major ticks\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "\n",
    "  # Minor ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "\n",
    "  ax.xaxis.tick_top()\n",
    "\n",
    "  # Gridlines based on minor ticks\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "\n",
    "  # Remove minor ticks\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  ax.invert_yaxis()\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "  return fig, ax\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=150, marker='o', color='red', label='Food')\n",
    "  rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "  #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc, critter=None):\n",
    "  \"\"\"\n",
    "  Plots \"critters\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h', color='blue', label='Critter')\n",
    "  #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "from ast import AsyncFunctionDef\n",
    "# @title Simulation Functions, Variables and widgets\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "\n",
    "\n",
    "def init_loc(n_rows, n_cols, num):\n",
    "  \"\"\"\n",
    "  Samples random 2d grid locations, without replacement,\n",
    "\n",
    "  Returns:\n",
    "    int_loc: ndarray(int) of flat indices for a grid\n",
    "    rc_index: (ndarray(int), ndarray(int)) a pair of arrays the first\n",
    "      giving the row indices, the second giving the col indices, useful\n",
    "      for indexing an n_rows by n_cols numpy array\n",
    "    rc_plotting: ndarray(int) num x 2, same rc coordinates but structured\n",
    "      in the way that matplotlib likes\n",
    "  \"\"\"\n",
    "  int_loc = np.random.choice(n_rows * n_cols, num, replace=False)\n",
    "  rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "  rc_plotting = np.fliplr(np.array(rc_index).T)\n",
    "  return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game board that\n",
    "  define the logic of the game.\n",
    "  Board state is tracked as a triple (pieces, scores, rounds_left)\n",
    "  pieces: batch_size x n_rows x n_cols np.array\n",
    "  scores: batch_size np.array\n",
    "  rounds_left: batch_size np.array\n",
    "\n",
    "  Pieces are interpreted as:\n",
    "  1=critter, -1=food, 0=empty\n",
    "\n",
    "  First dim is batch, second dim row , third is col, so pieces[0][1][7]\n",
    "  is the square in row 2, in column 8 of the first board in the batch of boards\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_food=10, lifetime=30):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"Set up starting board using game parameters\"\"\"\n",
    "    #set rounds_left and score\n",
    "    self.rounds_left = np.ones(self.batch_size) * self.lifetime\n",
    "    self.scores = np.zeros(self.batch_size)\n",
    "    # create an empty board array.\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols))\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+1 because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = init_loc(self.n_rows,\n",
    "                                          self.n_cols,\n",
    "                                          self.num_food+1)\n",
    "      # critter random start location\n",
    "      self.pieces[(ii, rc_idx[0][0], rc_idx[1][0])] = 1\n",
    "      # food random start locations\n",
    "      self.pieces[(ii, rc_idx[0][1:], rc_idx[1][1:])] = -1\n",
    "    return(self.pieces.copy(), self.scores.copy(), self.rounds_left.copy())\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\" board is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    self.pieces = board[0].copy()\n",
    "    self.scores = board[1].copy()\n",
    "    self.rounds_left = board[2].copy()\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state, which is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    return(self.pieces.copy(), self.scores.copy(), self.rounds_left.copy())\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves):\n",
    "    \"\"\"\n",
    "    Updates the state of the board given the moves made.\n",
    "\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new x coord for each critter\n",
    "        on each board and the third gives the new y coord.\n",
    "\n",
    "    Note:\n",
    "      Assumes that there is exactly one valid move for each board in the\n",
    "      batch of boards. i.e. it does't check for bounce/reflection on edges,\n",
    "      or for multiple move made on the same board. It only checks for eating\n",
    "      food and adds new food when appropriate. Invalid moves could lead to\n",
    "      illegal teleporting behavior, critter dublication, or index out of range\n",
    "      errors.\n",
    "    This assumes the move is valid, i.e. doesn't check for\n",
    "    bounce/reflection on edges, it only checks eating and adds new food,\n",
    "    so invalid moves could lead to illegal teleporting behaviour or index out\n",
    "    of range errors\n",
    "    \"\"\"\n",
    "    assert len(moves[0]) == self.pieces.shape[0]\n",
    "    #critters leave their spots\n",
    "    self.pieces[self.pieces==1] = 0\n",
    "    #which critters have food in their new spots\n",
    "    eats_food = self.pieces[moves] == -1\n",
    "    # some critters eat and their scores go up\n",
    "    self.scores = self.scores + eats_food\n",
    "\n",
    "    num_empty_after_eat = self.n_rows*self.n_cols - self.num_food\n",
    "    # -1 for the critter +1 for food eaten\n",
    "    # which boards in the batch had eating happen\n",
    "    g_eating = np.where(eats_food)[0]\n",
    "    new_food_sample = np_rng.choice(num_empty_after_eat, size=np.sum(eats_food))\n",
    "    # add random food to replace what is eaten\n",
    "    if np.any(eats_food):\n",
    "      possible_new_locs = np.where(np.logical_and(\n",
    "          self.pieces == 0, #the spot is empty\n",
    "          eats_food.reshape(self.batch_size, 1, 1))) #food eaten on that board\n",
    "      food_sample_ = np_rng.choice(num_empty_after_eat, size=np.sum(eats_food))\n",
    "      food_sample = food_sample_ + np.arange(len(g_eating))*num_empty_after_eat\n",
    "      assert np.all(self.pieces[(possible_new_locs[0][food_sample],\n",
    "                                 possible_new_locs[1][food_sample],\n",
    "                                 possible_new_locs[2][food_sample])] == 0)\n",
    "      #put new food on the board\n",
    "      self.pieces[(possible_new_locs[0][food_sample],\n",
    "                   possible_new_locs[1][food_sample],\n",
    "                   possible_new_locs[2][food_sample])] = -1\n",
    "    # put critters in new positions\n",
    "    self.pieces[moves] = 1.0\n",
    "    self.rounds_left = self.rounds_left - 1\n",
    "    assert np.all(self.pieces.sum(axis=(1,2)) == ((self.num_food * -1) + 1))\n",
    "\n",
    "\n",
    "  def get_legal_moves(self):\n",
    "    \"\"\"Identifies all legal moves for the critter, taking into acount\n",
    "    bouncing/reflection at edges,\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offstet on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    #apply all possible offsets to each game\n",
    "    moves = np.stack([\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0,  1, 0])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, -1, 0])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, 0,  1])]*self.batch_size).T,\n",
    "      np.array(np.where(self.pieces == 1)) +\n",
    "        np.array([np.array([0, 0, -1])]*self.batch_size).T]).swapaxes(0,2)\n",
    "\n",
    "    #check bounces at boundaries\n",
    "    moves[:,1,:] = np.where(moves[:,1,:] >=\n",
    "                            self.n_rows, self.n_rows-2, moves[:,1,:])\n",
    "    moves[:,2,:] = np.where(moves[:,2,:] >=\n",
    "                            self.n_cols, self.n_cols-2, moves[:,2,:])\n",
    "    moves[:,1,:] = np.where(moves[:,1,:] < 0, 1, moves[:,1,:])\n",
    "    moves[:,2,:] = np.where(moves[:,2,:] < 0, 1, moves[:,2,:])\n",
    "    return moves\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "  square_content = {\n",
    "      -1: \"X\", #Food\n",
    "      +0: \"-\", #Nothing\n",
    "      +1: \"O\"  #Critter\n",
    "      }\n",
    "\n",
    "\n",
    "  def get_square_piece(self, piece):\n",
    "    return GridworldGame.square_content[piece]\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size, n_rows, n_cols, num_food, lifetime):\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(self.batch_size, self.n_rows, self.n_cols,\n",
    "                       self.num_food, self.lifetime)\n",
    "    return b.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_size(self):\n",
    "    \"\"\"Shape of a sinlge board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,x,y coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 2-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to x,y indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return board[0][g].tobytes() + board[1][g].tobytes() + board[2][g].tobytes()\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.square_content[square] for row in board[0][g]\n",
    "                       for square in row])\n",
    "    board_s = board_s + '_' + str(board[1][g])\n",
    "    board_s = board_s + '_' + str(board[2][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board[1].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board[2].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g):\n",
    "    \"\"\"Dispalys the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, \"|\", end=\"\")    # Print the row\n",
    "      for r_ in range(self.n_rows):\n",
    "        piece = board[0][g,c_,r_]    # Get the piece to print\n",
    "        print(GridworldGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board[2][g]))\n",
    "    print(\"Score: \" + str(board[1][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g):\n",
    "    return np.squeeze(np.array(np.where(board[0][g]==1)))\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g,\n",
    "                 fig=None, ax=None, critter=None, food=None):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols)\n",
    "    rc_critter = self.get_critter_rc(board, g)\n",
    "    if critter is None:\n",
    "      critter = plot_critter(fig, ax, rc_critter)\n",
    "    else:\n",
    "      critter = plot_critter(fig, ax, rc_critter, critter)\n",
    "    rc_food_index = np.array(np.where(board[0][g] == -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "    fig.legend(loc = \"outside right upper\")\n",
    "    fig.canvas.draw()\n",
    "    return fig, ax, critter, food\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                       self.num_food, self.lifetime)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves()\n",
    "    valids = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for ii, g in enumerate(legal_moves[:,1:,:]):\n",
    "      for x,y in zip(g[0],g[1]):\n",
    "        valids[ii, x*self.n_cols+y] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, g):\n",
    "    \"\"\"Dispaly possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    A=np.reshape(self.get_valid_actions(board)[g], (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for y in range(self.n_cols):\n",
    "      print(y, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for y in range(self.n_cols):\n",
    "      print(y, \"|\", end=\"\")    # Print the row\n",
    "      for x in range(self.n_rows):\n",
    "        piece = A[y][x]    # Get the piece to print\n",
    "        print(GridworldGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: list of integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board tiple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parellel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    if board[2][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "      b = GridworldBoard(len(actions), n_rows, n_cols,\n",
    "                         self.num_food, self.lifetime)\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        b.set_state(board)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        newPieces = np.array([board[0][ai].copy() for ai in a_indx])\n",
    "        newScores = np.array([board[1][ai].copy() for ai in a_indx])\n",
    "        newrounds_left = np.array([board[2][ai].copy() for ai in a_indx])\n",
    "        b.set_state((newPieces, newScores, newrounds_left))\n",
    "      b.execute_moves(moves)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board[0].shape[0]\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                       self.num_food, self.lifetime)\n",
    "    b.set_state(board)\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1)}\n",
    "    critter_locs = np.where(board[0] == 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    b.execute_moves(moves)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right'\n",
    "\n",
    "\n",
    "    Returns:\n",
    "      direction: a string giving the direction relative to the critter of\n",
    "        the action\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board[0].shape\n",
    "    critter_locs = np.where(board[0] == 1)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      batch size np.array of -1 if not ended, and scores for\n",
    "      each game in the batch if it is ended\n",
    "    \"\"\"\n",
    "    rounds_left = board[2]\n",
    "    scores = board[1]\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Timer:\n",
    "  def __init__(self, timeout, callback):\n",
    "    self._timeout = timeout\n",
    "    self._callback = callback\n",
    "  async def _job(self):\n",
    "    await asyncio.sleep(self._timeout)\n",
    "    self._callback()\n",
    "  def start(self):\n",
    "    self._task = asyncio.ensure_future(self._job())\n",
    "  def cancel(self):\n",
    "    self._task.cancel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based tool for interating with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "    \"\"\"\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.final_scores = []\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output()\n",
    "    self.scoreboard = widgets.Output()\n",
    "    self.up_button = widgets.Button(description=\"Up\")\n",
    "    self.down_button = widgets.Button(description=\"Down\")\n",
    "    self.left_button = widgets.Button(description=\"Left\")\n",
    "    self.right_button = widgets.Button(description=\"Right\")\n",
    "    self.random_movement = widgets.Checkbox( value=False,\n",
    "                                            description='Move Randomly',\n",
    "                                             disabled=False,\n",
    "                                             indent=False)\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    plt.ioff()\n",
    "    (self.b_fig, self.b_ax,\n",
    "     self.b_critter, self.b_food) = self.gwg.plot_board(self.board_state, 0)\n",
    "    self.board_and_output = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "    self.board_and_buttons = widgets.VBox([self.board_and_output,\n",
    "                                            self.buttons])\n",
    "    self.board_output_and_score = widgets.HBox([self.board_and_output,\n",
    "                                                self.scoreboard])\n",
    "    self.boards_buttons_and_score = widgets.HBox([self.board_and_buttons,\n",
    "                                                  self.scoreboard])\n",
    "\n",
    "    # Sometimes use timer\n",
    "    self.click_timer = Timer(5.0, self.random_click)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = tuple([self.board_state[ii].copy() for ii in range(3)])\n",
    "    old_score = old_board[1][0]\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "        self.board_state, [which_button])\n",
    "    new_score = self.board_state[1][0]\n",
    "    rounds_left = self.board_state[2][0]\n",
    "    num_moves = self.gwg.lifetime - rounds_left\n",
    "    if new_score > old_score:\n",
    "      eating_string = \"The critter ate the food there!\"\n",
    "    else:\n",
    "      eating_string = \"There's no food there.\"\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0)\n",
    "    (self.b_fig, self.b_ax,\n",
    "     self.b_critter, self.b_food) = self.gwg.plot_board(\n",
    "        self.board_state, 0, self.b_fig, self.b_ax, self.b_critter, self.b_food)\n",
    "    with self.output:\n",
    "      #display(self.b_fig.canvas)\n",
    "      clear_output()\n",
    "      print(\"Critter moved \" + which_button +\n",
    "            \" and is now at ({}, {}).\".format(row,col))\n",
    "      print(eating_string)\n",
    "      print(\"Rounds Left: {} \\tFood Eaten: {} \\tFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_score, new_score / num_moves))\n",
    "      #if self.random_movement.value == True:\n",
    "      #  self.click_timer.start()\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_score)\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_score))\n",
    "        print('Resetting the board for another game')\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax,\n",
    "       self.b_critter, self.b_food) = self.gwg.plot_board(\n",
    "        self.board_state, 0, self.b_fig, self.b_ax, self.b_critter, self.b_food)\n",
    "    with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.final_scores) > 0:\n",
    "          print('High Score: ' + str(np.max(np.array(self.final_scores))))\n",
    "          print('Last Score: ' + str(self.final_scores[-1]))\n",
    "          print('Average Score: {:.2f}'.format(np.mean(self.final_scores)))\n",
    "        else:\n",
    "          print('High Score: --')\n",
    "          print('Last Score: --')\n",
    "          print('Average Score: --')\n",
    "\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.button_output_update('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.button_output_update('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.button_output_update('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.button_output_update('right')\n",
    "\n",
    "\n",
    "  def random_click(self):\n",
    "    move = np.random.randint(0,4)\n",
    "    if move == 0:\n",
    "      self.up_button.click()\n",
    "    elif move == 1:\n",
    "      self.down_button.click()\n",
    "    elif move == 2:\n",
    "      self.left_button.click()\n",
    "    elif move == 3:\n",
    "      self.right_button.click()\n",
    "    else:\n",
    "      print('should not happen')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title Define the Random Organism\n",
    "class RandomPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "  def __init__(self, game):\n",
    "    self.game = game\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates random game play\n",
    "    Args:\n",
    "      a board state (pieces, scores, rounds_left)\n",
    "    Returns:\n",
    "      a: [int] a batch_size array randomly chosen actions\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board[0].shape\n",
    "    valids = self.game.get_valid_actions(board)\n",
    "    action_size = self.game.get_action_size()\n",
    "    # Compute the probability of each move being played (random player means this should\n",
    "    # be uniform for valid moves, 0 for others)\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "    # Pick a random action based on the probabilities\n",
    "    a = [np.random.choice(action_size, p=probs[ii]) for ii in range(batch_size)]\n",
    "    a_1Hots = np.zeros((batch_size, action_size))\n",
    "    a_1Hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1Hots, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Other Helper Functions\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "  def __getattr__(self, name):\n",
    "    return self[name]\n",
    "\n",
    "\n",
    "args = dotdict({\n",
    "  'numIters': 1,            # In training, number of iterations = 1000 and num of episodes = 100\n",
    "  'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n",
    "  'tempThreshold': 15,      # To control exploration and exploitation\n",
    "  'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "  'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n",
    "  'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n",
    "  'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n",
    "  'cpuct': 1,\n",
    "  'maxDepth':5,             # Maximum number of rollouts\n",
    "  'numMCsims': 5,           # Number of monte carlo simulations\n",
    "  'mc_topk': 3,             # Top k actions for monte carlo rollout\n",
    "\n",
    "  'checkpoint': './temp/',\n",
    "  'load_model': False,\n",
    "  'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "  'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "  # Define neural network arguments\n",
    "  'lr': 0.001,               # lr: Learning Rate\n",
    "  'dropout': 0.3,\n",
    "  'epochs': 10,\n",
    "  'batch_size': 64,\n",
    "  'device': DEVICE,\n",
    "  'num_channels': 512,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridWorldNNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Instantiate GridWorld Neural Net with following configuration\n",
    "  nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 1\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1, padding=1) # Convolutional Layer 2\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 3\n",
    "  nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1) # Convolutional Layer 4\n",
    "  nn.BatchNorm2d(args.num_channels) X 4\n",
    "  nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024) # Fully-connected Layer 1\n",
    "  nn.Linear(1024, 512) # Fully-connected Layer 2\n",
    "  nn.Linear(512, self.action_size) # Fully-connected Layer 3\n",
    "  nn.Linear(512, 1) # Fully-connected Layer 4\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, args):\n",
    "    \"\"\"\n",
    "    Initialise game parameters\n",
    "\n",
    "    Args:\n",
    "      game: GridWorld Game instance\n",
    "        Instance of the GridWorldGame class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.board_x, self.board_y = game.get_board_size()\n",
    "    self.action_size = game.get_action_size()\n",
    "    self.args = args\n",
    "\n",
    "    super(GridWorldNNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1,\n",
    "                           padding=1)\n",
    "    self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "    self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn3 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn4 = nn.BatchNorm2d(args.num_channels)\n",
    "\n",
    "    self.fc1 = nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024)\n",
    "    self.fc_bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    #figure out how to connect score and rounds left in here somewhere\n",
    "\n",
    "    self.fc2 = nn.Linear(1024, 512)\n",
    "    self.fc_bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "    self.fc3 = nn.Linear(512, self.action_size)\n",
    "\n",
    "    self.fc4 = nn.Linear(512, 1)\n",
    "\n",
    "\n",
    "  def forward(self, s, currentScore, rounds_left):\n",
    "    \"\"\"\n",
    "    Controls forward pass of GridWorldNNet\n",
    "\n",
    "    Args:\n",
    "      s: np.ndarray\n",
    "        Array of size (batch_size x board_x x board_y)\n",
    "      scoreRoundsContext: np.ndarray\n",
    "        Array of size (batch_size x 2)\n",
    "    Returns:\n",
    "      Probability distribution over actions at the current state and the value of the current state.\n",
    "    \"\"\"\n",
    "    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "    rounds_left = rounds_left.view(-1, 1)                          # batch_siez x 1\n",
    "    currentScore = currentScore.view(-1, 1)                          # batch_siez x 1\n",
    "\n",
    "    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n",
    "\n",
    "    #need figure out how to put currentScore and rounds_left into the network here instead of and/or in addition to\n",
    "    #finessing the value function at the end\n",
    "\n",
    "    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
    "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "    pi = self.fc3(s)  # batch_size x action_size\n",
    "    v = self.fc4(s)   # batch_size x 1 # the way this is structured now this is\n",
    "                      # the average return per move, takes no account of rounds\n",
    "                      # left though so kind of rough, but let's start with\n",
    "                      # this for now\n",
    "    #softmax_pi = F.softmax(pi, dim=1) # batch_size x action_size\n",
    "    #v_pi_approx = torch.tensordot(softmax_pi, pi, dims=1) # batch_size x 1\n",
    "    log_softmax_pi = F.log_softmax(pi, dim=1) # batch_size x action_size\n",
    "    #corrected_pi = pi + (v - v_q_approx) # batch_size x num\n",
    "    #scaled_pi = ...\n",
    "    scaled_v = torch.add(torch.multiply(torch.add(torch.tanh(v), 1),\n",
    "                                        rounds_left), currentScore)\n",
    "    # Returns probability distribution over actions at the current state\n",
    "    # and the value of the current state.\n",
    "    return log_softmax_pi, scaled_v#, scaled_q\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PolicyValueNetwork():\n",
    "  \"\"\"\n",
    "  Initiates the Policy-Value Network\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game):\n",
    "    \"\"\"\n",
    "    Initialise network parameters\n",
    "\n",
    "    Args:\n",
    "      game: GridWorld Game instance\n",
    "        Instance of the GridWorldGame class above;\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.nnet = GridWorldNNet(game, args)\n",
    "    self.board_x, self.board_y = game.get_board_size()\n",
    "    self.action_size = game.get_action_size()\n",
    "    self.nnet.to(args.device)\n",
    "\n",
    "\n",
    "  def train(self, games, targetType='total',\n",
    "            verbose=True, num_epochs=args.epochs):\n",
    "    \"\"\"\n",
    "    Function to train network using just Value prediction loss\n",
    "\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples with each example is of form (board, pi, v)\n",
    "      targetType = 'total', 'value', 'policy'\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "    print('training on a set of examples')\n",
    "    for examples in games:\n",
    "      for epoch in range(num_epochs):\n",
    "        if verbose:\n",
    "          print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        v_losses = []   # To store the value losses per epoch\n",
    "        pi_losses = []  # To store the policy losses per epoch\n",
    "        t_losses = [] # To store the total losses per epoch\n",
    "        batch_count = int(len(examples) / args.batch_size)  # e.g. len(examples)=200, batch_size=64, batch_count=3\n",
    "        if verbose:\n",
    "          t = tqdm(range(batch_count), desc='Training Value Network')\n",
    "        else:\n",
    "          t = range(batch_count)\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # Read the ground truth information from MCTS examples\n",
    "          boards, currentScores, rounds_lefts, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # Length of boards, pis, vis = 64\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          currentScores = torch.FloatTensor(np.array(currentScores).astype(np.float64))\n",
    "          rounds_lefts = torch.FloatTensor(np.array(rounds_lefts).astype(np.float64))\n",
    "          target_pis = torch.FloatTensor(np.array(pis).astype(np.float64))\n",
    "          target_vs = torch.FloatTensor(np.array(vs).reshape((-1, 1)).astype(np.float64)) # reshape to batch_size x 1 (not just batch_size) so can be treated the same as target pis\n",
    "\n",
    "          # Predict\n",
    "          # To run on GPU if available\n",
    "          boards = boards.contiguous().to(args.device)\n",
    "          currentScores = currentScores.contiguous().to(args.device)\n",
    "          rounds_lefts = rounds_lefts.contiguous().to(args.device)\n",
    "          target_pis = target_pis.contiguous().to(args.device)\n",
    "          target_vs = target_vs.contiguous().to(args.device)\n",
    "\n",
    "          # Compute output\n",
    "          out_pi, out_v = self.nnet(boards, currentScores, rounds_lefts)\n",
    "          #print(out_v.shape)\n",
    "          #print(target_vs.shape)\n",
    "          #print(out_pi.shape)\n",
    "          #print(target_pis.shape)\n",
    "\n",
    "          l_pi = self.loss_pi(target_pis, out_pi) # policy loss\n",
    "          l_v = self.loss_v(target_vs, out_v)    # value loss\n",
    "          l_total = torch.add(l_pi, l_v)        # total loss (no regularization term?!? or is that built in somewhere)\n",
    "\n",
    "          # Record loss\n",
    "          pi_losses.append(l_pi.item())\n",
    "          v_losses.append(l_v.item())\n",
    "          t_losses.append(l_total.item())\n",
    "          if verbose:\n",
    "            t.set_postfix(Loss_v=l_v.item(), Loss_pi=l_pi.item(), Loss_total=l_total.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          if targetType == 'total':\n",
    "            l_total.backward()\n",
    "          elif targetType == 'value':\n",
    "            l_v.backward()\n",
    "          elif targetType == 'policy':\n",
    "            l_pi.backward()\n",
    "          else:\n",
    "            print('Invalid trainType chosen')\n",
    "          optimizer.step()\n",
    "        if verbose:\n",
    "          print('v loss: ' + str(np.mean(v_losses)) +\n",
    "                ' ::: pi loss: ' + str(np.mean(pi_losses)) +\n",
    "                ' ::: total loss: ' + str(np.mean(t_losses)))\n",
    "        else:\n",
    "          if (epoch + 1) == args.epochs:\n",
    "            print('Last Epoch Losses:')\n",
    "            print('v loss: ' + str(np.mean(v_losses)) +\n",
    "                  ' ::: pi loss: ' + str(np.mean(pi_losses)) +\n",
    "                  ' ::: total loss: ' + str(np.mean(t_losses)))\n",
    "\n",
    "\n",
    "  def predict(self, board, score, rounds_left):\n",
    "    \"\"\"\n",
    "    Function to perform prediction of both policy and value, note\n",
    "    policy is exponentiated on the way out so these should be directly\n",
    "    interpretable as probabilities\n",
    "\n",
    "    Args:\n",
    "      board: batch x 7 x 7 np.ndarray giving board positions\n",
    "      score: batch np.ndarray the current scores\n",
    "      rounds_left: batch np.ndarray of the turns left\n",
    "\n",
    "    Returns:\n",
    "      pi: probabilities over actions\n",
    "      v: predicted score at game end;\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    # start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(-1, self.board_x, self.board_y)\n",
    "\n",
    "    score = torch.FloatTensor(np.array(score, dtype=np.float64))\n",
    "    score = score.contiguous().to(args.device)\n",
    "    score = score.view(-1, 1)\n",
    "\n",
    "    rounds_left = torch.FloatTensor(np.array(rounds_left, dtype=np.float64))\n",
    "    rounds_left = rounds_left.contiguous().to(args.device)\n",
    "    rounds_left = rounds_left.view(-1, 1)\n",
    "\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "        pi, v = self.nnet(board, score, rounds_left)\n",
    "    return torch.exp(pi).data.cpu().numpy(), v.data.cpu().numpy().flatten()\n",
    "\n",
    "  def loss_v(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Mean squared error\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth end game scores corresponding to input board state\n",
    "      outputs: np.ndarray\n",
    "        value prediction of network as raw score\n",
    "\n",
    "    Returns:\n",
    "      MSE Loss calculated as: square of the difference between model predictions\n",
    "      and the ground truth and averaged across the whole batch\n",
    "    \"\"\"\n",
    "    # Mean squared error (MSE)\n",
    "    return torch.sum((targets - outputs)**2) / targets.size()[0]\n",
    "\n",
    "  def loss_pi(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Negative Log Likelihood(NLL) of Targets\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth action played during recording of \"expert\" player\n",
    "      outputs: np.ndarray\n",
    "        log-softmax action probability predictions of network\n",
    "\n",
    "    Returns:\n",
    "      Negative Log Likelihood calculated as:\n",
    "    \"\"\"\n",
    "    return -torch.sum(targets * outputs) / targets.size()[0]\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Code Checkpointing\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(folder):\n",
    "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "      os.mkdir(folder)\n",
    "    else:\n",
    "      print(\"Checkpoint Directory exists! \")\n",
    "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
    "    print(\"Model saved! \")\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Load code checkpoint\n",
    "    Args:\n",
    "      folder: string\n",
    "        Path specifying training examples\n",
    "      filename: string\n",
    "        File name of training examples\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "      raise (\"No model in path {}\".format(filepath))\n",
    "\n",
    "    checkpoint = torch.load(filepath, map_location=args.device)\n",
    "    self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarlo():\n",
    "  \"\"\"\n",
    "  Implementation of Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet, default_depth=5):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "\n",
    "  # Call this rollout\n",
    "  def simulate(self, board, actions, action_indexes, depth=None):\n",
    "    \"\"\"\n",
    "    Helper function to simulate one Monte Carlo rollout\n",
    "\n",
    "    Args:\n",
    "      board: triple (batch_size x x_size x y_size np.array of board position,\n",
    "                     scalar of current score,\n",
    "                     scalar of rounds left\n",
    "      actions: batch size list/array of integer idexes for moves on each board\n",
    "      these are assumed to be legal, no check for validity of moves\n",
    "    Returns:\n",
    "      temp_v:\n",
    "        Terminal State\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board[0].shape\n",
    "    next_board = self.game.get_next_state(board, actions, action_indexes)\n",
    "    if depth is None:\n",
    "      depth = self.default_depth\n",
    "    # potentially expand the game tree here,\n",
    "    # but just do straigt rollouts after this\n",
    "    # doesn't expand to deal with all random food generation possibilities\n",
    "    # just expands based on the actions given\n",
    "    expand_bs, _, _ = next_board[0].shape\n",
    "\n",
    "    for i in range(depth):  # maxDepth\n",
    "      if next_board[2][0] <= 0:\n",
    "        # check that game isn't over\n",
    "        # assumes all boards have the same rounds left\n",
    "        # no rounds left return scores as true values\n",
    "        terminal_vs = next_board[1].copy()\n",
    "        return terminal_vs\n",
    "      else:\n",
    "        pis, vs = self.nnet.predict(next_board[0], next_board[1], next_board[2])\n",
    "        valids = self.game.get_valid_actions(next_board)\n",
    "        masked_pis = pis * valids\n",
    "        sum_pis = np.sum(masked_pis, axis=1)\n",
    "        probs = np.array(\n",
    "            [masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "             else valid / valid.sum()\n",
    "             for valid, masked_pi in zip(valids, masked_pis)])\n",
    "        samp = np_rng.uniform(size = expand_bs).reshape((expand_bs,1))\n",
    "        sampled_actions = np.argmax(probs.cumsum(axis=1) > samp, axis=1)\n",
    "      next_board = self.game.get_next_state(next_board, sampled_actions)\n",
    "\n",
    "    pis, vs = self.nnet.predict(next_board[0], next_board[1], next_board[2])\n",
    "    return vs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarloBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Player based on Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet,\n",
    "               default_depth = 1,\n",
    "               default_rollouts = 1,\n",
    "               default_K = 4,\n",
    "               default_temp = 1.0):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.default_rollouts = default_rollouts\n",
    "    self.mc = MonteCarlo(self.game, self.nnet, self.default_depth)\n",
    "    self.default_K = default_K\n",
    "    self.default_temp = default_temp\n",
    "\n",
    "\n",
    "  def play(self, board,\n",
    "           num_rollouts=None,\n",
    "           rollout_depth=None,\n",
    "           K=None,\n",
    "           softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: triple (batch x num_rows x num_cols np.ndarray of board position,\n",
    "                     batch x a of current score,\n",
    "                     batch x 1 of rounds left\n",
    "\n",
    "    Returns:\n",
    "      best_action: tuple\n",
    "        (avg_value, action) i.e., Average value associated with corresponding action\n",
    "        i.e., Action with the highest topK probability\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board[0].shape\n",
    "    if num_rollouts is None:\n",
    "      num_rollouts = self.default_rollouts\n",
    "    if rollout_depth is None:\n",
    "      rollout_depth = self.default_depth\n",
    "    if K is None:\n",
    "      K = self.default_K\n",
    "    if softmax_temp is None:\n",
    "      softmax_temp = self.default_temp\n",
    "\n",
    "    # figure out top k actions acording to normalize action probability\n",
    "    # given by our policy network prediction\n",
    "    pis, vs = self.nnet.predict(board[0], board[1], board[2])\n",
    "    valids = self.game.get_valid_actions(board)\n",
    "    masked_pis = pis * valids  # Masking invalid moves\n",
    "    sum_pis = np.sum(masked_pis, axis=1)\n",
    "    num_valid_actions = np.sum(valids, axis=1)\n",
    "    effective_topk = np.array(np.minimum(num_valid_actions, K), dtype= int)\n",
    "    probs = np.array([masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "                      else valid / valid.sum()\n",
    "                      for valid, masked_pi in zip(valids, masked_pis)])\n",
    "    partioned = np.argpartition(probs,-effective_topk)\n",
    "    topk_actions = [partioned[g,-(ii+1)]\n",
    "                      for g in range(batch_size)\n",
    "                        for ii in range(effective_topk[g])]\n",
    "    topk_actions_index = [ii\n",
    "                            for ii, etk in enumerate(effective_topk)\n",
    "                              for _ in range(etk)]\n",
    "    values = np.zeros(len(topk_actions))\n",
    "    # Do some rollouts\n",
    "    for _ in range(num_rollouts):\n",
    "      values = values + self.mc.simulate(board, topk_actions,\n",
    "                                         topk_actions_index,\n",
    "                                         depth=rollout_depth)\n",
    "    values = values / num_rollouts\n",
    "\n",
    "    value_expand = np.zeros((batch_size, x_size*y_size))\n",
    "    value_expand[(topk_actions_index, topk_actions)] = values\n",
    "    #softmax_normalize those values into a selection prob\n",
    "    v_probs = np.exp(value_expand/softmax_temp) / np.sum(\n",
    "        np.exp(value_expand/softmax_temp), axis=1, keepdims=True)\n",
    "    v_probs = v_probs * valids\n",
    "    v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "    samp = np_rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "    #print(samp)\n",
    "    #print(v_probs.cumsum(axis=1))\n",
    "    sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "    a_1Hots = np.zeros((batch_size, x_size*y_size))\n",
    "    a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    #rollout_pieces = np.repeat(board[0], repeats=num_rollouts, axis=0)\n",
    "    #rollout_scores = np.repeat(board[1], repeats=num_rollouts, axis=0)\n",
    "    #rollout_rounds_left = np.repeat(board[2], repeats=num_rollouts,axis=0)\n",
    "    #rollout_board = (rollout_pieces, rollout_scores, rollout_rounds_left)\n",
    "    #rollout_topk_actions = list(np.repeat(np.array(topk_actions),\n",
    "    #                                      repeats=num_rollouts, axis=0))\n",
    "    #rollout_topk_actions_index = list(np.repeat(np.array(topk_actions_index),\n",
    "    #                                            repeats=num_rollouts, axis=0))\n",
    "    #rollout_values = self.mc.simulate(rollout_board, rollout_topk_actions,\n",
    "    #                                  rollout_topk_actions_index,\n",
    "    #                                  depth=rollout_depth)\n",
    "    # Taking advantage implicit structure from repeats to collapse each\n",
    "    # rollout back to its game and action using a list comprehension.\n",
    "    # Be careful when using implicit order based information!\n",
    "    #values = [np.mean(rollout_values[i*num_rollouts:(i+1)*num_rollouts])\n",
    "    #          for i in range(len(topk_actions))]\n",
    "    #values = np.array(values)\n",
    "\n",
    "    #best_actions = np.zeros(batch_size, dtype= int)\n",
    "    #best_a_1hots = np.zeros((batch_size, self.game.get_action_size()))\n",
    "    #for g in range(batch_size):\n",
    "    #  g_index = np.array(topk_actions_index, dtype=int) == g\n",
    "    #  qsa_g = values[g_index].copy()\n",
    "    #  g_actions = np.array(topk_actions)[g_index].copy()\n",
    "    #  best_actions[g] = g_actions[np.argmax(qsa_g)]\n",
    "    #  best_a_1hots[g][best_actions[g]] = 1\n",
    "    #print(topk_actions)\n",
    "    #print(topk_actions_index)\n",
    "    #print(values)\n",
    "    #no baseline probs here so just return 1hots twice\n",
    "    return sampled_actions, a_1Hots, v_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Micro 1.1.1.1: Initializing Gridworld\n",
    "\n",
    "Before we introduce an organism with **behaviour** we're going to build an **environment** for them to behave in. To start, this world will consist of a 7 x 7 grid of cells. Let's make a picture of that and see what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "############################################################################\n",
    "## TO DO for students: replace ... with the correct arguments(inputs) is the\n",
    "## make_grid function below to a grid the right size and shape. You can use the\n",
    "## tool tip by hovering over the word make_grid to find out how to use it. You\n",
    "## can also use the tool tip to view the source code. How does it work?\n",
    "raise NotImplementedError(\"Student exercise: make grid using the make_grid function\")\n",
    "############################################################################\n",
    "\n",
    "fig, ax = make_grid(...)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "with plt.xkcd():\n",
    "  fig, ax = make_grid(7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Bonus*: Tweak the make_grid function in the Plotting Functions cell above to make the grid lines green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Wow, what a boring environment. Let's add an organism and something for that organism to interact with. We'll start with 10 food items scattered randomly throughout the grid, never more than one food item per cell. To plot these food items we need their locations. We will set these by randomly sampling grid coordinates [without replacement](## \"never picking the same (row,col) coordinate pair twice\"). We'll place the organism in the same way and not on a food item to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "################################################################################\n",
    "# TODO for students: replace ... in init_loc(...) to initialize the right\n",
    "# number of food item locations and critter locations in coordinates that make\n",
    "# sense for our grid environment. Then replace the ... in rc_plotting[...] to\n",
    "# index the plotting coordinates for the food locations.\n",
    "# then comment out or remove the next line.\n",
    "raise NotImplementedError(\"Exercise: initialize food and critter locations\")\n",
    "################################################################################\n",
    "def init_loc(n_rows, n_cols, num):\n",
    "  \"\"\"\n",
    "  Samples random 2d grid locations without replacement\n",
    "\n",
    "  Args:\n",
    "    n_rows: int\n",
    "    n_cols: int\n",
    "    num:    int, wnumber of samples to generate, should\n",
    "            throw an error ifnum <= n_rows x n_cols\n",
    "\n",
    "  Returns:\n",
    "    int_loc: ndarray(int) of flat indices for a grid\n",
    "    rc_index: (ndarray(int), ndarray(int)) a pair of arrays the first\n",
    "      giving the row indices, the second giving the col indices, useful\n",
    "      for indexing an n_rows by n_cols numpy array\n",
    "    rc_plotting: ndarray(int) num x 2, same rc coordinates but structured\n",
    "      in the way that matplotlib likes\n",
    "  \"\"\"\n",
    "  int_loc = np.random.choice(n_rows * n_cols, num, replace=False)\n",
    "  rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "  rc_plotting = np.array(rc_index).T\n",
    "  return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "with plt.xkcd():\n",
    "  fig, ax = make_grid(7, 7)\n",
    "  int_locs, rc_index, rc_plotting = init_loc(...)\n",
    "\n",
    "  rc_critter = (rc_plotting[0])\n",
    "  plot_critter(fig, ax, rc_critter)\n",
    "\n",
    "  rc_food = rc_plotting[...]\n",
    "  plot_food(fig, ax, rc_food)\n",
    "\n",
    "  fig.legend(loc='outside right upper')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "\n",
    "\n",
    "def init_loc(n_rows, n_cols, num):\n",
    "  \"\"\"\n",
    "  Samples random 2d grid locations without replacement\n",
    "\n",
    "  Args:\n",
    "    n_rows: int\n",
    "    n_cols: int\n",
    "    num:    int, wnumber of samples to generate, should\n",
    "            throw an error ifnum <= n_rows x n_cols\n",
    "\n",
    "  Returns:\n",
    "    int_loc: ndarray(int) of flat indices for a grid\n",
    "    rc_index: (ndarray(int), ndarray(int)) a pair of arrays the first\n",
    "      giving the row indices, the second giving the col indices, useful\n",
    "      for indexing an n_rows by n_cols numpy array\n",
    "    rc_plotting: ndarray(int) num x 2, same rc coordinates but structured\n",
    "      in the way that matplotlib likes\n",
    "  \"\"\"\n",
    "  int_loc = np.random.choice(n_rows * n_cols, num, replace=False)\n",
    "  rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "  rc_plotting = np.array(rc_index).T\n",
    "  return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "with plt.xkcd():\n",
    "  fig, ax = make_grid(7, 7)\n",
    "  int_locs, rc_index, rc_plotting = init_loc(7, 7, 11)\n",
    "\n",
    "  rc_critter = (rc_plotting[0])\n",
    "  plot_critter(fig, ax, rc_critter)\n",
    "\n",
    "  rc_food = rc_plotting[1:]\n",
    "  plot_food(fig, ax, rc_food)\n",
    "\n",
    "  fig.legend(loc='outside right upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Vibe Check\n",
    "content_review(\"Sequence 1.1.1 Micro 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Micro 1.1.1.2: Random Eating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have an environment scattered with food and an organism, let's introduce some behaviour. The organism drifts around the environment randomly and eats the food it happens to stumble upon. (Can you think of any organisms that employ this strategy?). When food is eaten, the organism gets a **reward**, in this case a *Food Eaten* point, and a new food item appears randomly somewhere else in the environment (that doesn't already have food). Run the code cell below to see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Random Movement\n",
    "# @markdown You don't need to worry about how this code works, yet – just **run this cell** and the one below for now and watch what happens\n",
    "\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30)\n",
    "random_igwg = InteractiveGridworld(gwg)\n",
    "display(random_igwg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell again to watch the random organism play\n",
    "for ii in range(30):\n",
    "  time.sleep(0.1)\n",
    "  random_igwg.random_click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "*Question:* When the organism is just drifting around randomly how good is it at eating lots of food, what is its efficiency in terms of food per movement? Now run the cell above a few more times. Does the organism always eat the same amount of food or does it change between simulation runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "The amount of food eaten varies a lot from simulation run to simulation run,\n",
    "usually it manages to eat one or two pieces of food, sometimes more\n",
    "sometimes less.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Before we move on it's important to test that our simulation is running as we expect. Randomness can make testing hard, but can overcome in part by setting up the environment in such a way that the outcome becomes deterministic. In the two cells bellow change how the the Gridworld is initialized. By altering the size, shape and number of food items available create a scenario where the organism will always achieve perfect efficiency and a scenario where the organism will fail completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "###############################################################################\n",
    "# TODO for students: replace the ...'s in GridworldGame(...) to initialize a\n",
    "# grid world where the organism is always 100% efficient.\n",
    "raise NotImplementedError(\"Exercise: make random movement 100% efficient\")\n",
    "################################################################################\n",
    "\n",
    "gwg = GridworldGame(1, ..., ..., ..., 30)\n",
    "random_igwg_100 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_100.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_100.board_output_and_score)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "gwg = GridworldGame(1, 2, 2, 3, 30)\n",
    "random_igwg_100 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_100.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_100.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "for ii in range(30):\n",
    "  time.sleep(0.1)\n",
    "  random_igwg_100.random_click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "###############################################################################\n",
    "# TODO for students: replace the ...'s in GridworldGame(...) to initialize a\n",
    "# grid world where the organism is always 0% efficient.\n",
    "raise NotImplementedError(\"Exercise: make random movement 0% efficient\")\n",
    "################################################################################\n",
    "\n",
    "gwg = GridworldGame(1, ..., ..., ..., 30)\n",
    "random_igwg_0 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_0.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_0.board_output_and_score)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "#to_remove solution\n",
    "gwg = GridworldGame(1, 2, 2, 0, 30)\n",
    "random_igwg_0 = InteractiveGridworld(gwg)\n",
    "display(random_igwg_0.b_fig.canvas)\n",
    "clear_output()\n",
    "display(random_igwg_0.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "for ii in range(30):\n",
    "  random_igwg_0.random_click()\n",
    "  time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Vibe Check\n",
    "content_review(\"Sequence 1.1.1 Micro 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Micro 1.1.1.3: Better Than Random Eating\n",
    "Now it's your turn to control the organism. Run the next cell and see how much more efficient than random drifting your control of the organism is in terms of food per movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Controlled Movement\n",
    "# @markdown You don't need to worry about how this code works – just **run the cell** and then use the buttons to guide the organism\n",
    "\n",
    "# user in control\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30)\n",
    "user_control_igwg = InteractiveGridworld(gwg)\n",
    "display(user_control_igwg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(user_control_igwg.boards_buttons_and_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully your performance was more successful than random flailing. Even in this relatively simple and contrived foraging scenario intelligence can help a lot. What kinds of strategies and heuristics did you use to guide your choice of direction? Essentially, the fundamental purpose of a nervous system and a brain is to solve problems of this kind—deciding which actions to take based on environmental inputs to maximize rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Vibe Check\n",
    "content_review(\"Sequence 1.1.1 Micro 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Micro 1.1.1.4: Optimized Eating\n",
    "Finally we'd like to introduce a time traveling super organism, GW7x7-10-30, from the last chapter of this book. GW7x7-10-30 has mastered 7x7 Gridworld, with 10 food items and a 30 round duration. Run the next two cells to see how efficient an optimized Gridworld organism can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Optimal Movement\n",
    "# @markdown You don't need to worry about how this code works – **run this cell** to set up the superorganism and an environment for it.\n",
    "\n",
    "# initialize the game, network, and MonteCarlo player\n",
    "gwg = GridworldGame(1, 7, 7, 10, 30)\n",
    "pvnetMC = PolicyValueNetwork(gwg)\n",
    "mcp = MonteCarloBasedPlayer(gwg, pvnetMC, default_depth=2,\n",
    "                            default_rollouts=8, default_temp=0.1)\n",
    "\n",
    "\n",
    "#grab the saved model from the repo or where it ends up being hosted\n",
    "url = \"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C1_BehaviourAsPolicy/data/pvnetMC.pth.tar\"\n",
    "r = requests.get(url)\n",
    "\n",
    "if r.status_code == 200:\n",
    "  filename = os.path.basename(url)\n",
    "  # Write the contents to a file in the current working directory\n",
    "  with open(filename, 'wb') as file:\n",
    "    file.write(r.content)\n",
    "    #print(f'{filename} downloaded successfully.')\n",
    "else:\n",
    "  print('Error occurred while downloading the file.')\n",
    "\n",
    "# load the saved model\n",
    "pvnetMC.load_checkpoint(folder=os.getcwd(), filename='pvnetMC.pth.tar')\n",
    "\n",
    "# set up the widget for display/interaction\n",
    "optimal_igwg = InteractiveGridworld(gwg)\n",
    "display(optimal_igwg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(optimal_igwg.board_output_and_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to watch the superorganism behave in this environment.\n",
    "\n",
    "for ii in range(30):\n",
    "  actions, _, _ = mcp.play(optimal_igwg.board_state)\n",
    "  directions = gwg.action_to_critter_direction(optimal_igwg.board_state,\n",
    "                                               actions)\n",
    "  if directions[0] == 'up':\n",
    "    optimal_igwg.up_button.click()\n",
    "  elif directions[0] == 'down':\n",
    "    optimal_igwg.down_button.click()\n",
    "  elif directions[0] == 'left':\n",
    "    optimal_igwg.left_button.click()\n",
    "  elif directions[0] == 'right':\n",
    "    optimal_igwg.right_button.click()\n",
    "  else:\n",
    "    print('should not happen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Who was more efficient in this environment you or gw7x7-10-30? If gw7x7-10-30 was better, you really have read this book 😉 (If you can't beat the AIs, at least learn how to program them.) Even if you were about as good as gw7x7-10-30 you still might want to read this book. A deep understanding of the optimization processes that shape behaviour in simple organism-environment systems like this paves the way for generalizition to more intricate systems, specifically, understanding how brains generate adaptive behaviour as a result of optimzation processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Vibe Check\n",
    "content_review(\"Sequence 1.1.1 Micro 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Graveyard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Several options on how to host/load the model\n",
    "\n",
    "1) In the RL NMA tutorial, looks like they used to clone a github repo and pull files directly from there but now looks like they are downloading a snapshot of the repo hosted on open science foundation\n",
    "\n",
    "```\n",
    "#!git clone git://github.com/raymondchua/nma_rl_games.git --quiet\n",
    "REPO_PATH = 'nma_rl_games'\n",
    "\n",
    "if not os.path.exists(REPO_PATH):\n",
    "  download_string = \"Downloading\"\n",
    "  zipurl = 'https://osf.io/kf4p9/download'\n",
    "  print(f\"{download_string} and unzipping the file... Please wait.\")\n",
    "  with urlopen(zipurl) as zipresp:\n",
    "    with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n",
    "      zfile.extractall()\n",
    "  print(\"Download completed.\")\n",
    "\n",
    "# add the repo in the path\n",
    "sys.path.append('nma_rl_games/alpha-zero')\n",
    "print(f\"Added the {REPO_PATH} in the path and imported the modules.\")\n",
    "\n",
    "```\n",
    "\n",
    "But looks like NMA is moving to hosting on open science foundation\n",
    "\n",
    "```\n",
    "import io\n",
    "import requests\n",
    "r = requests.get('https://osf.io/sy5xt/download')\n",
    "if r.status_code != 200:\n",
    "  print('Failed to download data')\n",
    "else:\n",
    "  spike_times = np.load(io.BytesIO(r.content), allow_pickle=True)['spike_times']\n",
    "```\n",
    "\n",
    "I think eventually we want to do something like hosting more directly on the open science foundation, but for now, using small file sizes we will download files directly from the repo using requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Useful scavenging from github and SO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc, patches\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xticks(np.arange(0, 11, 1))\n",
    "ax.set_yticks(np.arange(0, 11, 1))\n",
    "\n",
    "# Labels for major ticks\n",
    "ax.set_xticklabels(np.arange(0, 11, 1))\n",
    "ax.set_yticklabels(np.arange(0, 11, 1))\n",
    "\n",
    "# Minor ticks\n",
    "ax.set_xticks(np.arange(1.5, 10.5, 1), minor=True)\n",
    "ax.set_yticks(np.arange(1.5, 10.5, 1), minor=True)\n",
    "\n",
    "# Gridlines based on minor ticks\n",
    "ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "\n",
    "# Remove minor ticks\n",
    "ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "ax.set_xlim(( 0.5, 10.5))\n",
    "ax.set_ylim(( 0.5, 10.5))\n",
    "\n",
    "#ax.axis(\"equal\")\n",
    "scatter = ax.scatter([], [])\n",
    "#critterCircle = plt.Circle((5,5), 0.4)\n",
    "#dot = ax.add_patch(critterCircle)\n",
    "#scatter.set_offsets([[5, 5]])\n",
    "\n",
    "scatter.set_offsets([[3, 11-3]])\n",
    "#fig.canvas.draw()\n",
    "#fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    scatter.set_offsets([[5, 5]])\n",
    "    return scatter,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def animate_dot(i):\n",
    "    print(i)\n",
    "    x = i\n",
    "    y = 11-i\n",
    "    scatter.set_offsets([[x, y]])\n",
    "    return scatter,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate_dot, init_func=init,\n",
    "                               frames=(np.arange(10)+1), interval=500, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "rc('animation', html='html5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These movies are a bit slicker than drawing using the matplotlib figure canvas updates, is this something we can/should incorporate into the non-interactive versions where you just watch something play.\n",
    "\n",
    "Thoughts to tweak the game, have random movement be something that always happens and directed movement as an optional intervention, maybe on a timer?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "P1C1_Sequence1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
