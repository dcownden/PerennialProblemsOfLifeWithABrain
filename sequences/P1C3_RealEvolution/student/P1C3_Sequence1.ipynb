{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C3_RealEvolution/student/P1C3_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C3_RealEvolution/student/P1C3_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **1.3.1: Populations and Inheritance**\n",
    "### Objective: We concluded the last chapter by discussing how neuroscience has traditionally focused on neural mechanisms of behaviour. While neural ontogeny has also been a focus of neuroscience, we argued that even greater focus on ontogeny could benefit both brain and behavioural sciences. Evolutionary thinking underpins this argument, and frames the brain as an evolved tool for rapid adaptation. To fully grasp this perspective, we first need to understand the core principles of evolutionary theory. Our aim here is to set out and explore these core principles of evolution through simple examples.\n",
    "\n",
    "In this sequence we will:\n",
    "\n",
    "* Outline the fundamental concepts of evolution and natural selection:\n",
    "  1. Key Players in Evolution:\n",
    "    * Populations and Individuals: The collective and singular units of evolution. Individuals are selected to reproduce, but it is the allele (a variant form of a gene at a specific gene locus) and trait frequencies within the population that evolve over time and generations.\n",
    "    * Genotype and Phenotype: Genetic makeup of an individual and its observable traits (including behavioural traits).\n",
    "    * Genotype-Phenotype Map (Ontogeny): How genotypes translate to phenotypes over an individual's lifespan.\n",
    "    * Genes and Alleles: Genes are the basic units of heredity that dictate traits through coded instructions. Alleles are the different variants of a gene, located at specific positions (loci) on a chromosome.\n",
    "  2. Dynamics of Evolution:\n",
    "    * Reproduction and Fitness: Who gets to pass their alleles, how many copies they pass on, to whom, and why. This process is guided by environmental conditions that act as a filter on the traits of individuals.\n",
    "    * Selection: The natural process that filters individuals based on their fitness, essentially acting as an environmental sieve that determines which traits are favorable for reproduction.\n",
    "    * Heritable Variation: Phenotypic differences between individuals that are underpinned (at least in part) by genetic differences and so can be passed down to the next generation.\n",
    "  3. Sources of Variation Driving Evolution:  \n",
    "    * Mutation: Random changes in genetic material that can produce wholly new genotypes.\n",
    "    * Recombination: The reshuffling of existing genes to create new genetic combinations.\n",
    "* Use a simple example to show how these elements fit together in an evolutionary process.\n",
    "* Show how this evolutionary process is fundamentally a form of optimization, tuning a population for better survival and reproduction over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Dependencies, Imports and Setup** You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
    "             'gw_widgets.py', 'gw_NN_RL.py']\n",
    "#filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C3_S1\"\n",
    "\n",
    "################################################################\n",
    "# Graph Viz Helper Functions\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
    "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
    "  ax.axis(\"off\")\n",
    "  #plt.tight_layout()\n",
    "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
    "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
    "  for edge in edge_labels:\n",
    "    src, dest, latex_str = edge\n",
    "    if graph.has_edge(src, dest):\n",
    "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
    "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
    "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
    "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\")\n",
    "      graph.delete_edge(src, dest)\n",
    "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
    "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
    "  return graph\n",
    "\n",
    "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
    "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
    "  for node in graph.nodes():\n",
    "    if not node.startswith(\"dummy\"):\n",
    "      node.attr['width'] = width\n",
    "      node.attr['height'] = height\n",
    "  return graph\n",
    "\n",
    "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels, output_path=\"graphviz_output.png\", dpi=300,\n",
    "                            figsize=(0.6, 0.3), fontsize=16):\n",
    "  \"\"\"\n",
    "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
    "\n",
    "  Parameters:\n",
    "    nodes_list (list): List of nodes in the graph.\n",
    "    edges_list (list): List of edges in the graph.\n",
    "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
    "    output_path (str): Path to save the rendered graph.\n",
    "    dpi (int): DPI for rendering the graph.\n",
    "    figsize (tuple): Figure size for the LaTeX labels.\n",
    "\n",
    "  Returns:\n",
    "    str: Path to the saved graph image.\n",
    "  \"\"\"\n",
    "\n",
    "  # Graph Creation and Configuration\n",
    "  G = pgv.AGraph(directed=True, strict=False, rankdir='LR', ranksep=0.5, nodesep=0.5)\n",
    "  G.add_nodes_from(nodes_list)\n",
    "  for edge in edges_list:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
    "  G = set_regular_node_sizes(G, width=1, height=1)\n",
    "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=figsize, fontsize=fontsize)\n",
    "\n",
    "  # Additional graph attributes\n",
    "  G.graph_attr['size'] = \"8,8\"\n",
    "  G.graph_attr['dpi'] = str(dpi)\n",
    "\n",
    "  # Render and save the graph\n",
    "  G.layout(prog='dot')\n",
    "  G.draw(output_path)\n",
    "\n",
    "  return output_path\n",
    "\n",
    "\n",
    "# The unerlying logic used by the evolution simulator and and the\n",
    "# 'twiddle genome' widget\n",
    "\n",
    "class StrikeNoStrikeEnv():\n",
    "\n",
    "  def __init__(self, num_receptors=4, num_food_items=4, num_non_food_items=4,\n",
    "               food_markers=None, non_food_markers=None, food_freq=0.5,\n",
    "               food_items_dist=None, non_food_items_dist=None,\n",
    "               reward=1, cost=1, rng=None, random_init=False,\n",
    "               hard_mode=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      num_receptors (int): Number of receptors\n",
    "      num_food_items (int): Number of food items\n",
    "      num_non_food_items (int): Number of non-food items\n",
    "      food_markers (np.ndarray): Food markers shape (num_food_items, num_receptors)\n",
    "      non_food_markers (np.ndarray): Non-food markers shape (num_non_food_items, num_receptors)\n",
    "      food_freq (float): Food frequency\n",
    "      reward (float): reward for successful strike\n",
    "      cost (float): cost of strike at non food\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng()\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "    self.num_receptors = num_receptors\n",
    "\n",
    "    # Create food markers\n",
    "    if food_markers is not None:\n",
    "      self.food_markers = np.array(food_markers, dtype=np.bool_)\n",
    "    else:\n",
    "      first_half_food = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                        size=(num_food_items, num_receptors // 2),\n",
    "                                        replace=True)\n",
    "      if hard_mode == True:\n",
    "        first_half_food[-1] = ~first_half_food[0]\n",
    "\n",
    "      second_half_food = np.ones((num_food_items, num_receptors - num_receptors // 2),\n",
    "                                  dtype=np.bool_)\n",
    "      self.food_markers = np.hstack([first_half_food, second_half_food])\n",
    "    # Create non-food markers\n",
    "    if non_food_markers is not None:\n",
    "      self.non_food_markers = np.array(non_food_markers, dtype=np.bool_)\n",
    "    else:\n",
    "      if hard_mode == True:\n",
    "        # In hard mode non-food markers look almost like copies of food markers\n",
    "        random_food_indices = self.rng.choice(np.arange(num_food_items), size=num_non_food_items, replace=True)\n",
    "        self.non_food_markers = np.copy(self.food_markers[random_food_indices])\n",
    "        # except for one randomly change one bit\n",
    "        random_bit_idx = self.rng.choice(np.arange(num_receptors // 2, num_receptors), size=num_non_food_items)\n",
    "        for i in range(num_non_food_items):\n",
    "          self.non_food_markers[i, random_bit_idx[i]] = 0\n",
    "      else:\n",
    "        # non-food markers are basically just random\n",
    "        first_half_non_food = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                              size=(num_non_food_items, num_receptors // 2),\n",
    "                                              replace=True)\n",
    "        second_half_non_food = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                               size=(num_non_food_items, num_receptors // 2),\n",
    "                                               replace=True)\n",
    "        # Ensure second half is not all 1's\n",
    "        while np.any(np.all(second_half_non_food, axis=1)):\n",
    "          idx = np.where(np.all(second_half_non_food, axis=1))[0]\n",
    "          second_half_non_food[idx] = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                                      size=(len(idx), num_receptors - num_receptors // 2),\n",
    "                                                      replace=True)\n",
    "          # keep checking condition because we set with random each time\n",
    "        self.non_food_markers = np.hstack([first_half_non_food, second_half_non_food])\n",
    "\n",
    "    # default items distribution is uniform, but can be otherwise\n",
    "    if food_items_dist == 'dirichlet_sample':\n",
    "      self.food_items_dist = self.rng.dirichlet(np.ones(len(self.food_markers)),size=1)[0]\n",
    "    elif food_items_dist is not None:\n",
    "      self.food_item_dist = food_items_dist\n",
    "    else:\n",
    "      self.food_item_dist = np.ones(self.food_markers.shape[0], dtype=float) / self.food_markers.shape[0]\n",
    "\n",
    "    if non_food_items_dist == 'dirichlet_sample':\n",
    "      self.non_food_items_dist = self.rng.dirichlet(np.ones(len(self.non_food_markers)),size=1)[0]\n",
    "    elif non_food_items_dist is not None:\n",
    "      self.non_food_item_dist = non_food_items_dist\n",
    "    else:\n",
    "      self.non_food_item_dist = np.ones(self.non_food_markers.shape[0], dtype=float) / self.non_food_markers.shape[0]\n",
    "\n",
    "    # Initialize rewards, costs, and food frequency\n",
    "    if random_init:\n",
    "      self.reward = self.rng.integers(1, 10)\n",
    "      self.cost = self.rng.integers(1, 10)\n",
    "      self.food_freq = np.round(self.rng.uniform(0.01, 0.99), 2)\n",
    "    else:\n",
    "      self.reward = reward\n",
    "      self.cost = cost\n",
    "      self.food_freq = food_freq\n",
    "\n",
    "\n",
    "  def compute_score(self, genotype,  n_trials=100):\n",
    "    n_food_trials = self.rng.binomial(n=n_trials, p=self.food_freq)\n",
    "    n_non_food_trials = n_trials - n_food_trials\n",
    "    food_indices = self.rng.choice(len(self.food_markers),\n",
    "                                   size=n_food_trials,\n",
    "                                   p=self.food_item_dist)\n",
    "    non_food_indices = self.rng.choice(len(self.non_food_markers),\n",
    "                                       size=n_non_food_trials,\n",
    "                                       p=self.non_food_item_dist)\n",
    "    food_items = self.food_markers[food_indices]\n",
    "    non_food_items = self.non_food_markers[non_food_indices]\n",
    "    # Using broadcasting to compute number of strikes in each case\n",
    "    # this happens when genotype lines up with food marker for all\n",
    "    # spots where genotype specifies a receptor\n",
    "    tp = np.sum(np.all((genotype & food_items)[:,genotype], axis=1))\n",
    "    fp = np.sum(np.all((genotype & non_food_items)[:,genotype], axis=1))\n",
    "\n",
    "    return tp * self.reward - fp * self.cost\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########\n",
    "# Define GenomeTwiddle locally before integrating into utils\n",
    "##########\n",
    "\n",
    "\n",
    "class GenomeTwiddle:\n",
    "  def __init__(self, seed=None, show_env=True, num_receptors=4,\n",
    "               num_food_types=4, num_non_food_types=4, mimics=False,\n",
    "               scramble_key=False, random_params=False):\n",
    "    self.rng = np.random.default_rng(seed)\n",
    "    self.show_env = show_env\n",
    "    self.num_receptors = num_receptors\n",
    "    step_for_freq_food = 0.01\n",
    "\n",
    "    if show_env and num_receptors==4:\n",
    "      self.food_markers = np.array([[1, 1, 1, 1],\n",
    "                                    [1, 0, 1, 1],\n",
    "                                    [0, 0, 1, 1],\n",
    "                                    [0, 1, 1, 1]],\n",
    "                                   dtype=np.bool_)\n",
    "      self.non_food_markers = np.array([[0, 0, 1, 0],\n",
    "                                        [1, 0, 0, 1],\n",
    "                                        [0, 1, 0, 1],\n",
    "                                        [0, 0, 1, 0]],\n",
    "                                       dtype=np.bool_)\n",
    "      self.food_item_dist = np.ones(len(self.food_markers)) / len(self.food_markers)\n",
    "      self.non_food_item_dist = np.ones(len(self.non_food_markers)) / len(self.non_food_markers)\n",
    "    else:\n",
    "      first_half_food = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                        size=(num_food_types, num_receptors // 2),\n",
    "                                        replace=True)\n",
    "      second_half_food = np.ones((num_food_types, num_receptors - num_receptors // 2),\n",
    "                                  dtype=np.bool_)\n",
    "      self.food_markers = np.hstack([first_half_food, second_half_food])\n",
    "      self.food_item_dist = self.rng.dirichlet(np.ones(len(self.food_markers)),size=1)[0]\n",
    "\n",
    "      if mimics == False:\n",
    "        first_half_non_food = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                              size=(num_non_food_types, num_receptors // 2),\n",
    "                                              replace=True)\n",
    "        second_half_non_food = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                               size=(num_non_food_types, num_receptors // 2),\n",
    "                                               replace=True)\n",
    "        # Ensure second half is not all 1's\n",
    "        while np.any(np.all(second_half_non_food, axis=1)):\n",
    "          idx = np.where(np.all(second_half_non_food, axis=1))[0]\n",
    "          second_half_non_food[idx] = self.rng.choice(np.array([0, 1],dtype=np.bool_),\n",
    "                                                      size=(len(idx), num_receptors - num_receptors // 2),\n",
    "                                                      replace=True)\n",
    "          # keep checking condition because we set with random each time\n",
    "        self.non_food_markers = np.hstack([first_half_non_food, second_half_non_food])\n",
    "      elif mimics == True:\n",
    "        # Randomly select indices from food_markers for each non_food_marker\n",
    "        selected_indices = self.rng.integers(len(self.food_markers), size=num_non_food_types)\n",
    "        self.non_food_markers = np.copy(self.food_markers[selected_indices])\n",
    "        # Flip one bit on each mimic\n",
    "        row_flip_indx = np.arange(num_non_food_types)\n",
    "        col_flip_indx = self.rng.integers(num_receptors // 2, num_receptors, size=num_non_food_types)\n",
    "        # Flip the bits at the selected indices\n",
    "        self.non_food_markers[row_flip_indx, col_flip_indx] = ~self.non_food_markers[row_flip_indx, col_flip_indx]\n",
    "\n",
    "      self.non_food_item_dist = self.rng.dirichlet(np.ones(len(self.non_food_markers)),size=1)[0]\n",
    "\n",
    "    if scramble_key:\n",
    "      # Generate a permutation of column indices\n",
    "      perm_indices = self.rng.permutation(self.num_receptors)\n",
    "      # Apply the permutation self.food_markers and self.non_food_markers\n",
    "      self.food_markers = self.food_markers[:, perm_indices]\n",
    "      self.non_food_markers = self.non_food_markers[:, perm_indices]\n",
    "\n",
    "    self.genotype_buttons = widgets.HBox([widgets.RadioButtons(\n",
    "        options=['0', '1'], description=f'Receptor {i+1}:',) for i in range(num_receptors)])\n",
    "\n",
    "    # set up the parameters of the simulation\n",
    "    if random_params:\n",
    "      default_r = self.rng.integers(1, 10)\n",
    "      default_c = self.rng.integers(1, 10)\n",
    "      default_ff = np.round(self.rng.uniform(0.01, 0.99) / step_for_freq_food) * step_for_freq_food\n",
    "    else:\n",
    "      default_r = 1\n",
    "      default_c = 1\n",
    "      default_ff = 0.5\n",
    "\n",
    "    self.freq_food = widgets.FloatSlider(\n",
    "        value=default_ff, min=0, max=1.0, step=0.01,\n",
    "        description='Food Freq:',)\n",
    "    self.reward_r = widgets.IntSlider(\n",
    "        value=default_r, min=0, max=10, description='Reward (r):',)\n",
    "    self.cost_c = widgets.IntSlider(\n",
    "        value=default_c, min=0, max=10, description='Cost (c):',)\n",
    "\n",
    "    self.food_marker_output = widgets.Output()\n",
    "    self.non_food_marker_output = widgets.Output()\n",
    "    food_markers_with_labels = [[f'Food Item {i+1}'] + list(marker) for i, marker in enumerate(self.food_markers)]\n",
    "    non_food_markers_with_labels = [[f'Non-Food Item {i+1}'] + list(marker) for i, marker in enumerate(self.non_food_markers)]\n",
    "    headers = [''] + [f'R{i+1}' for i in range(self.num_receptors)]\n",
    "\n",
    "    with self.food_marker_output:\n",
    "      print(f\"Food Markers:\\n{tabulate(food_markers_with_labels, headers=headers, tablefmt='grid')}\")\n",
    "    with self.non_food_marker_output:\n",
    "      print(f\"Non-Food Markers:\\n{tabulate(non_food_markers_with_labels, headers=headers, tablefmt='grid')}\")\n",
    "\n",
    "    self.marker_display = widgets.HBox([self.food_marker_output, self.non_food_marker_output])\n",
    "    self.output = widgets.Output()\n",
    "    self.compute_button = widgets.Button(description='Evaluate', disabled=False,\n",
    "                                         button_style='',)\n",
    "    self.compute_button.on_click(self.compute_trials)\n",
    "    self.sliders = widgets.VBox([widgets.Label('Parameters:'),\n",
    "        widgets.HBox([self.freq_food, self.reward_r, self.cost_c])])\n",
    "    self.compute_result = widgets.VBox([ widgets.Label('Result:'),\n",
    "                                        self.compute_button,\n",
    "                                         self.output])\n",
    "    # Conditional display\n",
    "    if self.show_env:\n",
    "      self.widget_box = widgets.VBox([self.genotype_buttons,\n",
    "                                      self.marker_display,\n",
    "                                      self.sliders,\n",
    "                                      self.compute_result])\n",
    "    else:\n",
    "      self.widget_box = widgets.VBox([self.genotype_buttons,\n",
    "                                      self.compute_result])\n",
    "\n",
    "  def display(self):\n",
    "    display(self.widget_box)\n",
    "\n",
    "  def compute_trials(self, button):\n",
    "    with self.output:\n",
    "      self.output.clear_output()\n",
    "      n_trials = 100\n",
    "\n",
    "      genotype = np.array(\n",
    "          [int(button.value) for button in self.genotype_buttons.children],\n",
    "          dtype=np.bool_)\n",
    "\n",
    "      n_food_trials = self.rng.binomial(n=n_trials, p=self.freq_food.value)\n",
    "      n_non_food_trials = n_trials - n_food_trials\n",
    "      food_indices = self.rng.choice(len(self.food_markers),\n",
    "                                     size=n_food_trials,\n",
    "                                     p=self.food_item_dist)\n",
    "      non_food_indices = self.rng.choice(len(self.non_food_markers),\n",
    "                                         size=n_non_food_trials,\n",
    "                                         p=self.non_food_item_dist)\n",
    "      food_items = self.food_markers[food_indices]\n",
    "      non_food_items = self.non_food_markers[non_food_indices]\n",
    "      # Using broadcasting to compute number of strikes in each case\n",
    "      tp = np.sum(np.all((genotype & food_items)[:, genotype], axis=1))\n",
    "      fp = np.sum(np.all((genotype & non_food_items)[:, genotype], axis=1))\n",
    "\n",
    "      fn = n_food_trials - tp\n",
    "      tn = n_non_food_trials - fp\n",
    "\n",
    "      score = tp * self.reward_r.value - fp * self.cost_c.value\n",
    "      if self.show_env:\n",
    "        contingency_table = [[\"\", \"Strike\", \"No-Strike\"],\n",
    "                             [\"Food\", tp, fn],\n",
    "                             [\"Non-Food\", fp, tn]]\n",
    "        print(f\"Contingency Table:\\n{tabulate(contingency_table, headers='firstrow', tablefmt='grid')}\")\n",
    "      print(f\"Score: {score}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### integrate into utils later\n",
    "\n",
    "def evolutionary_simulation(population_size=1000, num_generations=100,\n",
    "                            mutation_rate=0.01, num_receptors=4,\n",
    "                            num_food_items=4, num_non_food_items=4,\n",
    "                            reward=1, cost=1, food_freq=0.5,\n",
    "                            slct_temp=5.0, has_mutation=False,\n",
    "                            random_init=True, seed=None, make_plot=True,\n",
    "                            generation_printout = False, hard_mode='hard',\n",
    "                            cheating_population=False,\n",
    "                            food_markers=None, non_food_markers=None,\n",
    "                            initial_variant=None, stop_at_converged=True,\n",
    "                            n_trials=100):\n",
    "\n",
    "  rng = np.random.default_rng(seed)\n",
    "  # Initialize population\n",
    "  if initial_variant is not None:\n",
    "    population = np.repeat([initial_variant], population_size, axis=0)\n",
    "  elif random_init:\n",
    "    population = rng.integers(0, 2, size=(population_size, num_receptors),\n",
    "                              dtype=np.bool_)\n",
    "    if cheating_population:\n",
    "      half_pop_size = population_size // 2\n",
    "      first_half_zeros = np.zeros((half_pop_size, num_receptors // 2), dtype=np.bool_)\n",
    "      second_half_ones = np.ones((half_pop_size, num_receptors - num_receptors // 2), dtype=np.bool_)\n",
    "      population[:half_pop_size] = np.hstack([first_half_zeros, second_half_ones])\n",
    "  elif num_receptors == 4 and population_size == 400 and not random_init:\n",
    "    population = np.array([[1, 1, 1, 0],\n",
    "                           [1, 0, 1, 0],\n",
    "                           [0, 0, 0, 1],\n",
    "                           [0, 0, 1, 0]],\n",
    "                           dtype=np.bool_)\n",
    "    population = np.repeat(population, 100, axis=0)\n",
    "  else:\n",
    "    population = rng.integers(0, 2, size=(population_size, num_receptors),\n",
    "                              dtype=np.bool_)\n",
    "    if cheating_population:\n",
    "      print('cheating')\n",
    "      half_pop_size = population_size // 2\n",
    "      first_half_zeros = np.zeros((half_pop_size, num_receptors // 2), dtype=np.bool_)\n",
    "      second_half_ones = np.ones((half_pop_size, num_receptors - num_receptors // 2), dtype=np.bool_)\n",
    "      population[:half_pop_size] = np.hstack([first_half_zeros, second_half_ones])\n",
    "\n",
    "  # Initialize environment\n",
    "  if food_markers is not None and non_food_markers is not None:\n",
    "    print('using given food markers')\n",
    "    snse = StrikeNoStrikeEnv(num_receptors=num_receptors,\n",
    "                             food_markers=food_markers,\n",
    "                             non_food_markers=non_food_markers,\n",
    "                             reward=reward, cost=cost,\n",
    "                             food_freq=food_freq,\n",
    "                             rng=rng)\n",
    "  elif not random_init and num_receptors==4:\n",
    "    food_markers = np.array([[1, 1, 1, 1],\n",
    "                             [1, 0, 1, 1],\n",
    "                             [0, 0, 1, 1],\n",
    "                             [0, 1, 1, 1]],\n",
    "                            dtype=np.bool_)\n",
    "    non_food_markers = np.array([[0, 0, 1, 0],\n",
    "                                 [1, 0, 0, 1],\n",
    "                                 [0, 1, 0, 1],\n",
    "                                 [0, 0, 1, 0]],\n",
    "                                dtype=np.bool_)\n",
    "\n",
    "    snse = StrikeNoStrikeEnv(num_receptors=num_receptors,\n",
    "                             food_markers=food_markers,\n",
    "                             non_food_markers=non_food_markers,\n",
    "                             reward=reward, cost=cost,\n",
    "                             food_freq=food_freq, hard_mode=hard_mode,\n",
    "                             rng=rng)\n",
    "  else:\n",
    "    snse = StrikeNoStrikeEnv(num_receptors=num_receptors,\n",
    "                             num_food_items=num_food_items,\n",
    "                             num_non_food_items=num_non_food_items,\n",
    "                             reward=reward, cost=cost, hard_mode=hard_mode,\n",
    "                             food_freq=food_freq, rng=rng)\n",
    "\n",
    "  all_freqs_history = collections.defaultdict(lambda: [0]*num_generations)\n",
    "  all_fitness_history = collections.defaultdict(lambda: [None]*num_generations)\n",
    "  pop_fitness_history = []\n",
    "  observed_genotypes = set()\n",
    "\n",
    "\n",
    "  print(f\"Population size: {population_size}\")\n",
    "  #print(f\"Initial population {population[:10,:]}\")\n",
    "  if initial_variant is not None:\n",
    "    print(f\"Initial variant: {initial_variant}\")\n",
    "  print(f\"Food markers:\")\n",
    "  display(np.array(snse.food_markers,dtype=int))\n",
    "  print(f\"Non-food markers:\")\n",
    "  display(np.array(snse.non_food_markers, dtype=int))\n",
    "  print(f\"Reward: {snse.reward}\")\n",
    "  print(f\"Cost: {snse.cost}\")\n",
    "  print(f\"Food frequency: {snse.food_freq}\")\n",
    "  print(\"---------------------------------------\")\n",
    "\n",
    "  # Run simulation\n",
    "  for generation in tqdm(range(num_generations), desc='Simulating Generations'):\n",
    "    # Compute scores for each individual\n",
    "    scores = [snse.compute_score(geno, n_trials=n_trials) for geno in population]\n",
    "\n",
    "    # Normalize scores to probabilities with slct_temp adjustment\n",
    "    exp_scores = np.exp(np.array(scores) / slct_temp)\n",
    "    prob_scores = exp_scores / np.sum(exp_scores)\n",
    "\n",
    "    # Count and display population makeup\n",
    "    pop_counter = collections.Counter([tuple(np.array(ind, dtype=int))\n",
    "                                       for ind in population])\n",
    "    # Sort population counter\n",
    "    pop_counter = {k: v for k, v in\n",
    "                   sorted(pop_counter.items(), key=lambda item: item[1], reverse=True)}\n",
    "    # Calculate the average score for each genotype\n",
    "    genotype_to_scores = collections.defaultdict(list)\n",
    "    for ind, score in zip(population, scores):\n",
    "      genotype_key = tuple(np.array(ind, dtype=int))\n",
    "      genotype_to_scores[genotype_key].append(score)\n",
    "    avg_scores_by_type = {k: np.mean(v) for k, v in\n",
    "                          sorted(genotype_to_scores.items(), key=lambda item: np.mean(item[1]), reverse=True)}\n",
    "\n",
    "    newly_observed_genotypes = set(pop_counter.keys()) - observed_genotypes\n",
    "\n",
    "    for new_genotype in newly_observed_genotypes:\n",
    "      all_freqs_history[new_genotype][:generation] = [0]*generation\n",
    "      all_fitness_history[new_genotype][:generation] = [None]*generation\n",
    "    observed_genotypes.update(newly_observed_genotypes)\n",
    "\n",
    "    # Update history dictionaries\n",
    "    for genotype in observed_genotypes:\n",
    "      freq = pop_counter.get(genotype, 0)\n",
    "      all_freqs_history[genotype][generation] = freq\n",
    "      avg_score = avg_scores_by_type.get(genotype, None)\n",
    "      all_fitness_history[genotype][generation] = avg_score\n",
    "\n",
    "    pop_fitness_history.append(np.mean(scores))\n",
    "\n",
    "    if generation_printout:\n",
    "      print(f\"Generation {generation+1}: Mean Score = {np.mean(scores)}\")\n",
    "      print(f\"Population makeup: {pop_counter}\")\n",
    "      print(f'Average scores by type: {avg_scores_by_type}')\n",
    "      print('')\n",
    "    if len(pop_counter) == 1:\n",
    "      if stop_at_converged:\n",
    "        print(f\"Population has converged to a single genotype in generation {generation + 1}. Stopping simulation.\")\n",
    "        break\n",
    "    # Resample population based on scores\n",
    "    population = population[\n",
    "        rng.choice(np.arange(population_size),\n",
    "                   size=population_size, p=prob_scores)]\n",
    "    # mutation applied to next generation using XOR to introduce new variants\n",
    "    if has_mutation:\n",
    "      mutation_mask = rng.random(population.shape) < mutation_rate\n",
    "      population ^= mutation_mask\n",
    "\n",
    "  # simulation is done\n",
    "  last_10_percent = int(0.1 * num_generations)\n",
    "  avg_last_10_percent_fitness = np.mean(pop_fitness_history[-last_10_percent:])\n",
    "  print(f\"Average mean population fitness for the last 10% of rounds: {avg_last_10_percent_fitness:.4f}\")\n",
    "\n",
    "  def bits_to_int(bits):\n",
    "    return int(\"\".join(str(x) for x in bits), 2)\n",
    "\n",
    "  if make_plot:\n",
    "    # simulation is done, make a plot\n",
    "    avg_freqs = {genotype: np.mean(freqs[:generation+1]) for genotype, freqs in all_freqs_history.items() if genotype in observed_genotypes}\n",
    "    top_4_genotypes = sorted(avg_freqs, key=avg_freqs.get, reverse=True)[:4]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot for top 4 variant frequencies\n",
    "    ax[0].set_title(\"Top 4 Variant Frequencies\")\n",
    "    ax[0].set_xlabel(\"Generation\")\n",
    "    ax[0].set_ylabel(\"Frequency\")\n",
    "    for geno in top_4_genotypes:\n",
    "      ax[0].plot(all_freqs_history[geno][:generation+1], label=f\"Variant {bits_to_int(geno)}\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    # Plot for population fitness\n",
    "    ax[1].set_title(\"Population Fitness\")\n",
    "    ax[1].set_xlabel(\"Generation\")\n",
    "    ax[1].set_ylabel(\"Average Fitness\")\n",
    "\n",
    "    if num_generations > 100:\n",
    "      # Actual data with lower alpha value\n",
    "      ax[1].plot(pop_fitness_history[:generation+1], alpha=0.3, color='blue')\n",
    "      # Smoothed time average (rolling mean) in a strong color\n",
    "      smoothed_fitness = pd.Series(pop_fitness_history[:generation+1]).rolling(window=5).mean()\n",
    "      ax[1].plot(smoothed_fitness, color='blue')\n",
    "    else:\n",
    "      ax[1].plot(pop_fitness_history[:generation+1], color='blue')\n",
    "\n",
    "    remove_ip_clutter(fig)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "  return(pop_fitness_history)\n",
    "\n",
    "# test usage\n",
    "#fitness_hist = evolutionary_simulation(slct_temp=5.0, random_init=False, num_generations=25,\n",
    "#                        has_mutation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.3.1.1: A Bite-Sized Evolution Example\n",
    "\n",
    "We're going to explore the fundamentals of evolution using the following simple scenario. Drawing inspiration from the sand-striker worm (Eunice aphroditois), imagine an organism that lurks in its burrow, with just its chemosensory antennae sticking out. Whenever food that 'smells good' (as determined by its antennae receptors) passes by, the worm lunges from its lair to snatch the morsel, incapacitate it if need be, and drag it back home for a feast. Yum! We model this organism with a simplified 'genome'—an array of bits where each bit codes for the presence (1) or absence (0) of a distinct chemosensory receptor on the worm's antennae.\n",
    "\n",
    "The organism's life centers around one core problem: to 'strike' or 'not strike'. A successful strike at edible food yields a reward, $r$. While the immediate benefits of this come from fats, proteins, and calories, and the impact of these resources on survival and reproduction fluctuates based on the worm's condition, we simplify by saying each successful capture increases the number of viable gametes by $r$, thereby affecting the chances of the organism's offspring being in the next generation. Conversely, a missed strike has a cost, $c$. While these immediate costs manifest as increased predation risk or lost opportunity, which may have complex impacts on survival and reproduction depending on the worm's condition, we again simplify this cost as a constant gamete decrement by $c$.\n",
    "\n",
    "The environment our organism finds itself in, in this first minimal example, consists of a series of encounters with several different kinds of food and non-food items. Each kind of item is tagged with distinct chemical markers, akin to different 'scents' or 'flavours'. Note that these markers will sometimes partially (but never totally) overlap between items regardless of food / non-food categorization, depending on the particular details of the environment. This creates potential for misjudgment. Our organism's chemosensory antennae, coded by its genome, are sensitive to these markers. In each of these encounters, the organism faces its core 'strike' or 'not strike' problem, which it 'solves' with the following decision rule: the worm strikes only if ***all*** of its receptors pick up a marker and refrains from striking if ***any*** of its receptors are left unactivated. (When the organism has no receptors technically all of its non-existent receptors are activated and so it strikes in all situations.) This is a very simple decision rule, and certainly not the best one in terms of making use of the sensory receptors available, but it for illustrative purposes it is the one we will work with. In the simulation below see if you can find the optimal 'genome' for an organism. To get started, you can observe the 'markers' on the different food and non-food items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **TODO**: Picture of Chemosensory Strike-No-Strike Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **TODO**: Picture of Eunice Aphroditois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interaction: Twiddle the Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to tweak the genome. Click the evaluate button to see how the organism fares over 100 encounters with randomly selected food and non-food items.\n",
    "gt = GenomeTwiddle(show_env=True, num_receptors=4)\n",
    "gt.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So already in this very simple example some interesting things are happening. In our example, by construction, each location on the genome represents a 'gene' for a specific receptor with two possible alleles: the presence or absence of the corresponding chemosensory receptor on an antenna. However, we might also zoom out our analysis and look at traits at the behavioural level. For example, if we say that 'Striking at Food Item 2' (with markers '1011') is a trait, then we find that any of the genes for receptor 1, 3, or 4 might equally be considered a 'Gene for Striking at Food Item 2'. In a different environment, where the markers for food and non-food items vary—for instance, if 'Food Item 2' had markers '1000 instead—then only the gene for receptor 1 might be considered the 'Gene for Striking at Food Item 2'. This hints at the potential complexity of the mapping from gene to behaviour (through ontogeny and mechanisms) and also emphasizes the strong context-dependence of this mapping, i.e. its sensitivity to the specifics of the environment.\n",
    "\n",
    "Despite this complexity, as we'll soon see, the evolutionary process still manages to shape and select for complex traits. When you were twiddling the genes in the exercise above, you had the benefit of knowing what the markers were for the food and non-food items, the costs and rewards of different outcomes, the relative frequencies of food versus non-food items appearing, and the feedback about true and false negatives and positives. The evolutionary process is effectively blind to all of this. The problem that evolution faces is more like the following exercise, where the only feedback is a 'fitness score,' with no awareness of how that score came about. (Note that the individual organism may be aware of the differences between true and false positives, and maybe even between true and false negatives, but the evolutionary process itself is not.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interaction: Twiddle the Genes in Ingnorance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# @markdown **Run this cell** to tweak the genome. Click the evaluate button to see how the organism fares over 100 random trials\n",
    "gt = GenomeTwiddle(show_env=False, scramble_key=False, mimics=True)\n",
    "gt.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So, with just four receptors, this problem isn't so difficult. It's relatively easy to test various combinations and observe what works well and what doesn't. With the highly stochastic feedback it's hard to know if we have the exact best receptor configuration, but we can at least find a good configuration. What about when there are more receptors and food types and non-food types?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interaction: An Overwhelming Number of Genes and Receptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to tweak the genome. Click the evaluate button to see how the organism fares over 100 random trials\n",
    "gt = GenomeTwiddle(show_env=False, num_receptors=16, scramble_key=True,\n",
    "                   num_food_types=100, num_non_food_types=400, mimics=True)\n",
    "gt.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The curse of dimensionality (and noisey feedback) strikes again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.3.1.2: Evolution Acts on Populations\n",
    "\n",
    "One of the ways that evolution is able to search high-dimensional parameters spaces (big genomes) despite the curse of dimensionality is because it operates in a massively parallel manner. Many variants are evaluated in parallel within each generation, potentially as many as there are individuals in a population.\n",
    "\n",
    "In a very coarse first approximation, evolution can be thought of as changes in allele frequencies within a population. In this simplified framing evolution can be seen as the aggregate effects at the population level of selection acting on individuals. Populations evolve; individuals are selected.\n",
    "\n",
    "To see how evolution can search a large and opaque genotype space for a good solution to this 'strike' or 'no strike' problem, we will simulate evolution in a population of organisms. We'll apply the 'environmental seive' by having those individuals with the highest score in the 'strike' or 'no-strike' game be the ones most likely to have their offspring in the subsequent generation. In this first simplified simulation, we will work with asexual haploid organisms. Note that sex—and other forms of gene transfer—along with multiple chromosomes are the general rule in life. In the next sequences, we will explore how sex (and recombination enabled by diploidy) can radically amplify the effectiveness of evolution's pure brute-force parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "## **TODO** Bio Box: Evolution Beyond Allele Frequency Change\n",
    "Allele frequency change is a good first approximation to evolution, one one that is particularly appealing given the mutually reinforcing technologies of gene sequencing and quantitative genetics. However, this simplistic view completely fails to address many critical aspects of evolution, in particular the emergence of new genetic loci.  \n",
    "\n",
    "**More about deconstraints and \"plausability of life\" here.**\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Simulation: Selection Acting on a Population\n",
    "Run the cell below see brute force parallelism of evolution find good a solution for a 20-receptor/marker organism/environment system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fitness_hist = evolutionary_simulation(\n",
    "    num_receptors=20, slct_temp=5.0, random_init=False,\n",
    "    num_generations=100, has_mutation=False, cost=1, reward=1,\n",
    "    population_size=100, hard_mode=True, stop_at_converged=True,\n",
    "    num_food_items =2, num_non_food_items=20, seed=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So the simulation above worked well... perhaps. With 20 receptors, there are $2^{20} \\approx  1 \\text{million}$ different types to evaluate. However, our population only consisted of one hundred individuals. This process quickly selected the best variant in the existing population. But, what if there are better variants that were not in this initial population? Let's take a look at the selected variant and think about whether or not a better variant is possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# What receptors did the dominant variant have?\n",
    "binary_str = bin(66520)[2:]\n",
    "padded_binary_str = binary_str.zfill(20)\n",
    "print(padded_binary_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Comparing this receptor pattern to the food and non-food markers used in the simulation above we see that this pattern will cause the organism to always strike at one of the food items (the one with marker 10110100111111111111) but not the other, and to strike at just two of the ten possible non-food items. So it will avoid most false positive strikes, but miss out on half of the food opportunities. Looking at the food and non-food markers in the simulation above we can see that an organism with receptors 00000000001111111111 will have perfect discrimination between food and non-food items. However this 'best' variant was not in the initial population and a purely selective process will never generate novel better variants. We need a way of introducing variation into the population. We need mutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Simulation: Selection Acting on a Population with Mutation\n",
    "Run the cell below to see a similar evoluationary process play out again, but this time with mutation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fitness_hist = evolutionary_simulation(\n",
    "    num_receptors=20, slct_temp=5.0, random_init=False,\n",
    "    num_generations=200, has_mutation=True, mutation_rate=0.02,\n",
    "    population_size=100, hard_mode=True, n_trials=100, cost=1, reward=1,\n",
    "    num_food_items =2, num_non_food_items=20, seed=12, stop_at_converged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# This was the receptor pattern of the dominant variant\n",
    "binary_str = bin(1023)[2:]\n",
    "padded_binary_str = binary_str.zfill(20)\n",
    "print(padded_binary_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Mutation effectively introduced variation, allowing the parameter space to be searched more thoroughly. The mean population fitness stabilized around 42.7 after this simulation, compared to approximately 22.4 in the previous simulation without mutation. But can we do better? Unfortunately, we can't assume that just because this is the output of an evolutionary process, we have reached some kind of globally optimal solution. (Although in this case by inspecting the problem we can conclude that our evolutionary process with mutation did indeed find the best solution in this instance). One important reason for this is because evolution is effectively a hill climbing algorithm. Mutations make small steps in the genotype-parameter space, and if these mutations prove beneficial (which they often don't), they are more likely to be retained in subsequent generations (this is selection). These beneficial mutations then become the basis for further exploration of the genotype-parameter space through additional mutations. But, there is always the possibility that there is some really great solution that was not reachable by an 'adaptive path' of mutations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Simulation: Selection Caught in a Local Optimum\n",
    "Consider a scenario where two peaks represent local optima in the fitness landscape. These peaks are separated by a 'maladaptive valley,' a region of lower fitness. An evolving population that starts near one of these peaks would likely climb toward that local optimum due to natural selection favoring incremental improvements. However, reaching the higher peak on the other side of the valley would require a sequence of mutations that initially decrease fitness before eventually leading to a much higher fitness level. In such cases, natural selection would typically weed out these 'stepping-stone' mutations before they could lead to the higher peak, causing the population to get stuck at the lower local optimum. We can see just such an example of this scenario play out the simulation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fitness_hist = evolutionary_simulation(\n",
    "    num_receptors=3, slct_temp=5.0, random_init=False,\n",
    "    num_generations=1000, has_mutation=True, mutation_rate=0.0001,\n",
    "    population_size=100, seed=12,\n",
    "    cost=1.0, reward=2.0, food_freq=0.5,\n",
    "    food_markers=np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]]),\n",
    "    non_food_markers=np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]]),\n",
    "    initial_variant=np.array([1, 1, 1], dtype=np.bool_), stop_at_converged=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this example, the population starts out completely dominated by the all receptors '111' (aka variant 7). None of the food items ('001', '010', '100') trigger a strike for this variant, nor do any of the non-food items ('011', '101', '110'). As a result, this variant never strikes and neither gains nor loses fitness from the strike/no-strike aspect of their life, they get a score of zero. Occasionally, mutation introduces a new variant, but over the course of the thousand generations in the simulation, all of these are 1-bit mutants. While these mutants still don't strike at any of the food items, they do strike at one of the non-food items, incurring a cost of $c$ in one-sixth (half of the scenarios are non-food items and one-third of those are the specific non-food item that a particular mutant will strike). Because these mutants have lower fitness than the dominant type they are swiftly removed from the population by selection.\n",
    "\n",
    "Yet, there is a better solution. In this example, the reward for successful strikes is $r=2$, which is greater than the cost $c=1$ of striking non-food items. Given that food and non-food items are equally likely to be encountered, striking at everything results in positive expected fitness increment. This makes it a better strategy than striking at nothing, which has an expected reward of zero. However, in the simulation above, our process couldn't reach this superior solution. This is because these local optima in the fitness landscape were separated by a so-called 'fitness valley,' and there was no 'adaptive path' bridging them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the 3 marker/receptor genotype space and the 'fitness valley' of this particular example.\n",
    "def one_bit_difference(node1, node2):\n",
    "    \"\"\"Check if two nodes have a 1-bit difference.\"\"\"\n",
    "    return sum(a != b for a, b in zip(node1, node2)) == 1\n",
    "\n",
    "# Create a new graph\n",
    "G = pgv.AGraph(strict=True)\n",
    "\n",
    "# Add nodes for food types and non-food types\n",
    "nodes = [[0, 0, 0],\n",
    "         [1, 0, 0], [0, 1, 0], [0, 0, 1],\n",
    "         [1, 1, 0], [1, 0, 1], [0, 1, 1],\n",
    "         [1, 1, 1]]\n",
    "\n",
    "\n",
    "positions = {\n",
    "    str([0, 0, 0]): \"0,1\",\n",
    "    str([1, 0, 0]): \"2,2\",\n",
    "    str([0, 1, 0]): \"2,1\",\n",
    "    str([0, 0, 1]): \"2,0\",\n",
    "    str([1, 1, 0]): \"4,2\",\n",
    "    str([1, 0, 1]): \"4,1\",\n",
    "    str([0, 1, 1]): \"4,0\",\n",
    "    str([1, 1, 1]): \"6,1\"\n",
    "    }\n",
    "\n",
    "\n",
    "for node in nodes:\n",
    "    if node in [[1, 0, 0], [0, 1, 0], [0, 0, 1]]:\n",
    "        G.add_node(str(node), label=str(node), color=\"red\", pos=positions[str(node)], pin=True)\n",
    "    elif node in [[1, 1, 0], [1, 0, 1], [0, 1, 1]]:\n",
    "        G.add_node(str(node), label=str(node), color=\"red\", pos=positions[str(node)], pin=True)\n",
    "    else:\n",
    "        G.add_node(str(node), label=str(node), color=\"blue\", pos=positions[str(node)], pin=True)\n",
    "\n",
    "G.node_attr['width'] = '1'\n",
    "G.node_attr['height'] = '1'\n",
    "\n",
    "# Add edges for nodes with 1-bit difference\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(i, len(nodes)):\n",
    "        if one_bit_difference(nodes[i], nodes[j]):\n",
    "            G.add_edge(str(nodes[i]), str(nodes[j]))\n",
    "\n",
    "# Save the graph to a PNG file\n",
    "filename = \"evolution_graph.png\"\n",
    "G.layout(prog='neato')\n",
    "G.draw(filename, format='png')\n",
    "\n",
    "# Display the PNG file using Image\n",
    "Image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The blue nodes in the genotype space represent local optima of the fitness function, while the red nodes depict the fitness valley that separates them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Math Exercise: Calculating Expected Fitness Score\n",
    "\n",
    "The expected fitness value of genotype '000', which strikes at everything, is calculated as:\n",
    "$$\\text{# trials} \\cdot (\\text{food frequency} \\cdot r + (1 - \\text{food frequency}) \\cdot c) = 100 \\cdot (0.5 \\cdot 2 - 0.5 \\cdot 1) = 50$$\n",
    "Similarly the expected fitness value of genotype '111', which strikes at nothigh is calculated as 0.\n",
    "\n",
    "1. Calculate the expected fitness for the 1-bit genotypes ('100', '010', '001') situated in the 'fitness valley'. These all have the same expected fitness, since they strike at one of the three food items, and two of the three non-food items. Recall each food item and non-food item is equally likely to be the item present in a given trial.\n",
    "  \n",
    "2. Calculate the expected fitness for the 2-bit genotypes ('110', '101', '011') located in the 'fitness valley'. Again, they all share the same expected fitness, striking at none of the three food items, but one of the three non-food item. Again, each food item and non-food item having an equal likelihood of being the item present in a given trial.\n",
    "\n",
    "**Answers**\n",
    "\n",
    "1. $100 \\cdot(\\frac{1}{6} \\cdot 2 - \\frac{2}{6} \\cdot 1) = 0$\n",
    "2. $100 \\cdot ( 0 + \\frac{-1}{6}) \\approx -16.67$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "  <summary>Click to see the answers</summary>\n",
    "  <ol>\n",
    "    <li>$$100 \\cdot \\left(\\frac{1}{6} \\cdot 2 - \\frac{2}{6} \\cdot 1\\right) = 0$$</li>\n",
    "    <li>$$100 \\cdot \\left(0 + \\frac{-1}{6}\\right) \\approx -16.67$$</li>\n",
    "  </ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Simulation: Traversing the Fitness Valley with More Mutation\n",
    "One way to traverse such fitness valleys is by increasing the mutation rate, enabling the population to explore more widely. In the simulation above, with the mutation rate set at $\\mu=0.0001$, mutants differing from the dominant type by more than one bit were extremely rare (i.e., they didn't appear during our 1,000-generation simulation). Now, when we increase the mutation rate, in the simulation below we see that crossing this 'valley' becomes much more feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fitness_hist = evolutionary_simulation(\n",
    "    num_receptors=3, slct_temp=5.0, random_init=False,\n",
    "    num_generations=400, has_mutation=True, mutation_rate=0.01,\n",
    "    population_size=100, num_food_items =1, num_non_food_items=2, seed=12,\n",
    "    cost=1.0, reward=2.0, food_freq=0.5,\n",
    "    food_markers=np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]]),\n",
    "    non_food_markers=np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]]),\n",
    "    initial_variant=np.array([1, 1, 1], dtype=np.bool_), stop_at_converged=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Cranking up the mutation worked, in this case. Can mutation be too high though? Let's increase the mutation rate in the next simulation to $\\mu=0.1$ and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fitness_hist = evolutionary_simulation(\n",
    "    num_receptors=3, slct_temp=5.0, random_init=False,\n",
    "    num_generations=400, has_mutation=True, mutation_rate=0.1,\n",
    "    population_size=100, num_food_items =1, num_non_food_items=2, seed=12,\n",
    "    cost=1.0, reward=2.0, food_freq=0.5,\n",
    "    food_markers=np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0]]),\n",
    "    non_food_markers=np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]]),\n",
    "    initial_variant=np.array([1, 1, 1], dtype=np.bool_), stop_at_converged=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "With this higher mutation rate we observe that even though the optimal variant is identified, the high mutation rate results in a decrease in overall population fitness. In this simulation the mean population fitness is approximately 35.9, significantly lower than the expected fitness score of 50 for the best variant. This hints at a fundamental trade-off between exploration (seeking new solutions) and exploitation (utilizing current best solutions). Low mutation rates can trap the evolutionary process in suboptimal local fitness peaks, hindering the discovery of superior solutions. However, a high mutation rate reduces overall fitness, since mutations typically deviate from an already effective genotype. With this in mind, let's look a little more formally at how evolution can (and can't) be thought of as fitness optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.2.3.3: How Evolution 'Optimizes' Fitness  \n",
    "\n",
    "\n",
    "Because of selection, the population's dominant genotype is more likely to shift toward regions of higher fitness. Over time, this leads to populations typically being found in these high fitness regions, which explains the highly adapted traits observed in nature. In this sense both real-world evolutionary processes and the simulated versions we've explored can be thought of optimization algorithms finding good parameters for a fitness function. But there are some key differences. One such key difference is that evolution has no termination condition. Evolution doesn't stop when a 'good enough' or 'best' solution is reached because the process is blind to the overall structure of the problem being solved. So, even after stumbling upon a 'good' solution, mutations continue, and they mostly end up being counterproductive. When a complicated and efficient system is working well, most changes will make things worse!\n",
    "\n",
    "This is somewhat at odds with our observations last chapter where normative evolutionary thinking was used to explain the observation of many adaptive behaviours and traits out in the natural world. How can an ongoing restless process like evolution result in highly adapted organism?\n",
    "\n",
    "The answer lies in the dynamic properties of evolutionary processes. With enough selection pressure, evolution exhibits 'sticky attractors' within the landscape of genetic possibilities. These are states that, once attained, tend to be stable over long periods, even amidst ongoing mutations. The 'stickiness' of these attractors means that, despite continuous evolution, populations tend to remain within these high-fitness states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This discussion above, while illustrative, is somewhat abstract. To make this argument more concrete we will need to focus on a specific well defined system. Let's start.\n",
    "\n",
    "Consider a genotype space denoted $G$, consisting of sequences of bits of length $n$. There's a fitness function $f(g): G \\rightarrow \\mathbb{R}$ that maps each genotype $g$ in $G$ to a real number representing its fitness. For the sake of simplicity, we'll assume that fitness depends solely on an individual's genotype, not on the genotypes of others in the population, or the genotypes present in other evolving population, and that this function (which represents the environment) remains constant throughout the process. We will address changing fitness functions and fitness function that depend on the genotype of other individuals later in this chapter.\n",
    "\n",
    "For analytic simplicity, let's further assume that selection pressure is strong and mutation rates are low. This leads to two significant effects: the population is generally dominated by a single variant, and given the rarity of mutations, we need only consider whether a new mutant can supplant the current dominant variant to understand the long-term dynamics of this evolutionary process. This method of analysis, sometimes known as 'mutant invasion analysis', is a cornerstone of quantitative genetics and evolutionary game theory [further reading].\n",
    "\n",
    "By setting the mutation rate low in our simulation we can see these two effects in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Simulation with low mutation rate and strong selection\n",
    "fitness_hist = evolutionary_simulation(\n",
    "    num_receptors=3, slct_temp=5.0, random_init=False,\n",
    "    num_generations=600, has_mutation=True, mutation_rate=0.0001,\n",
    "    population_size=100, num_food_items =1, num_non_food_items=2, seed=12,\n",
    "    cost=1.0, reward=1.0, food_freq=0.5,\n",
    "    food_markers=np.array([[1, 1, 1]]),\n",
    "    non_food_markers=np.array([[1, 1, 0], [1, 0, 1], [0, 1, 1]]),\n",
    "    initial_variant=np.array([0, 0, 0], dtype=np.bool_), stop_at_converged=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Because we assume mutations are rare and selection is strong we can focus our analysis on the probability of our evolutionary process transitioning from a population with dominant variant $g$ to one with dominant variant $g'$. This transition probability is the product of two factors:\n",
    "1. The probability of a mutant type $g'$ arising in a population where $g$ is dominant.\n",
    "2. The fixation probability, $\\phi(g, g')$, which is the probability of $g'$ becoming the dominant variant once introduced by mutation.\n",
    "\n",
    "First let's consider the probability of a mutant arising. Given a per-bit mutation rate, $\\mu$, a genome length of $n$ bits, and a population size $N$, we can calculate:\n",
    "* Probability of no mutants showing in a generation: $$(1-\\mu)^{n \\cdot N}$$\n",
    "* Probability of exactly one 1-bit mutant in a generation: $$N \\cdot [n \\cdot \\mu \\cdot (1-\\mu)^{n-1}] \\cdot (1-\\mu)^{n \\cdot(N-1)}$$\n",
    "* Probability of more than one mutant, or a mutant with a multi-bit mutation, or some combination of these events: $$1 - N \\cdot [n \\cdot \\mu \\cdot (1-\\mu)^{n-1}] \\cdot (1-\\mu)^{n \\cdot(N-1)} - (1-\\mu)^{n \\cdot N}$$\n",
    "\n",
    "For example, with $\\mu = 1 \\times 10^{-6}$, $N=100$ and $n=4$ then the probability of no mutants in a generation is about 99.96%, and the probability of a single, 1-bit mutant is approximately 0.04%. More complex events have an effectively negligible probability of occurring. Being able to effectively ignore complex mutation events is what the 'rare mutation' assumption is about. The time between appearances of mutant variants will follow a Poisson distribution with new mutant variants being introduced every 2501 generations on average for these example parameters. We also assume that selection is sufficiently strong, so that a new mutant variant will likely either dominate or be eliminated before another mutation event occurs. Not having to consider multiple mutants invading at the same time is one of the things that our 'sufficiently strong' selection assumption is about.\n",
    "\n",
    "Next, the fixation probability $\\phi(g, g')$ will depend on the fitness function $f$, population size, and selection strength. For simplicity, we assume selection is so strong that less fit mutants can never become dominant, i.e. $\\phi(g,g') \\approx 0$ when $f(g) > f(g')$ and $\\phi(g, g') > 0$ when $f(g) \\leq f(g')$.\n",
    "\n",
    "Given this, the long term dynamics of our evolutionary process can be closely approximated by a Markov process with transition probabilities given by the transition matrix $P$. $P_{ij}$, the element in the $i^{th}$ row and the $j^{th}$ column of $P$ gives the probability of transitioning from the state indexed by $j$ into state indexed by $i$. Considering $g$ and $g'$ are unique bit strings, we can assign them natural numerical representations. The transition probabilities are then\n",
    "\n",
    "$$P_{g'g} =\n",
    "\\begin{cases}\n",
    "\\frac{1}{n} \\phi(g, g') & \\text{if } g' \\in \\Gamma(g) \\\\\n",
    "0 & \\text{if } g' \\notin \\Gamma(g) \\text{ and } g \\neq g' \\\\\n",
    "1 - \\frac{1}{n}\\sum\\limits_{g' \\in \\Gamma(g)} \\phi(g,g') & \\text{if } g = g'\n",
    "\\end{cases}$$\n",
    "\n",
    "Here, $\\Gamma(g)$ represents all sequences that differ from $g$ by exactly one bit. (The number of bit differences between two binary sequences is known as the Hamming distance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the theory of Markov processes an absorbing state is one that, once entered, cannot be left. Because of our assumption of strong selection our Markov process has such absorbing states and these correspond precisely to local maxima of the fitness function $f$. This simplifies our understanding of the long-term behavior of the evolutionary process. Specifically, over an infinitely long time horizon, the population will almost surely end up in one of these local optima and remain there. In other words, the absorbing states serve as 'sinks' that capture the evolutionary trajectory. This helps to explain why even though evolution is an ongoing process and blind to the underlying structure of the fitness function, it nevertheless often settles in at local optima of the fitness function. It is in this sense that the evolutionary process can be viewed as an optimization algorithm, and one that is guaranteed (under certain conditions, using the theory of Markov processes) to converge on local fitness maxima. Note though there are no guarantees about how these local fitness maxima related to the global maxima. This is because evolution is essentially a local 'hill-climbing' process with no awareness of the global shape of the fitness landscape. Its only awareness is of the smear of points in the genotype space that the population is currently able to sample, i.e. those points close enough to the dominant variant(s) given the mutation process.\n",
    "\n",
    "In this argument that links absorbing states to local fitness optima—hinging on our assumption that $\\phi(g,g') \\approx 0$ when $f(g) > f(g')$ and $\\phi(g, g') > 0$ when $f(g) \\leq f(g')$—we are not able to say much about which optima the evolutionary process will end up in using this analysis. In the appendix we present a more nuanced version of this argument. It uses a weaker assumption where less fit mutants do have a small, but non-negligible probability of becoming dominant, i.e. $0 < \\phi(g,g') < \\epsilon$ when $f(g) > f(g')$ and $\\phi(g, g') \\geq \\epsilon$ when $f(g) \\leq f(g')$. This allows for exploration of how the details of the genotype space (and in particular the geometry of that space induced by the mutation process) together with the fitness function determine which states an evolutionary process is likely to spend most of its time in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This concludes our introduction to the evolutionary process. In the remainder of this chapter we will:\n",
    "1. See how sex and recombination radically enhance the ability of an evolutionary process to discover good variants in vast genotype spaces.\n",
    "2. Get a richer sense of the ways that evolution **is not** optimization. Specifically we will develop an appreciation for the fact that the precise fitness function 'being optimized' is typically is a moving target, shifting from one generation to the next, often due to the very evolutionary changes the fitness function is driving.\n",
    "3. Explore the importance of within lifetime learning, both as a response to rapid environmental change within an individual's lifetime, but also as an accelerator of the evolutionary process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to take the quiz\n",
    "# @markdown **Run this cell** to take the quiz\n",
    "comprehension_quiz = [\n",
    "  {\n",
    "    \"question\": \"What is the impact of mutation on the evolutionary process?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"It always increases the fitness of individuals in a population.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Mutation does not always increase fitness; it typically introduces neutral or deleterious variations. Beneficial mutations are rare.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It introduces necessary variation to explore new genotypes.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct! Mutation is essential for introducing genetic variation, which allows populations to explore new genotypes and adapt over time.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It decreases genetic diversity within a population.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Mutation actually increases genetic diversity by introducing new genetic variations.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"It reduces the population size over time.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Mutation itself does not necessarily reduce population size; it's the selection process that might influence population numbers based on the fitness effects of mutations.\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How does evolution compare to a typical optimization algorithm?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"Evolution has a clear termination condition when a global maximum is reached.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Evolution lacks a termination condition and does not stop even when high-fitness solutions are found.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"Evolution is a process that continually explores and exploits, without awareness of the global fitness landscape.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Exactly! Evolution continuously explores new genotypes and exploits current adaptations without a concept of the overall fitness landscape. In this way it is like a 'black-box' optimization algorithm\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"Evolutionary processes always find the globally optimal solution.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Evolution does not necessarily find global optima; it often settles on local maxima due to its hill-climbing nature.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"Evolution stops mutations once a sufficiently good solution is found.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Evolution does not stop mutating genotypes even after finding high-fitness solutions, which can lead to further, fitness reducing variation.\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"In the context of evolutionary simulations, what does a low mutation rate typically lead to?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"A diverse population with many different genotypes coexisting.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"A low mutation rate usually results in less genetic diversity, not more.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"Rapid convergence to the global fitness peak.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Low mutation rates can lead to rapid convergence, but not necessarily to global peaks—often to local ones.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"The population is generally dominated by a single variant.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Correct! Low mutation rates can lead to populations being dominated by a single, high-fitness variant.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"An increase in the number of harmful mutations.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"A low mutation rate means fewer mutations overall, not an increase in harmful ones specifically.\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What role do 'sticky attractors' play in evolutionary processes?\",\n",
    "    \"type\": \"multiple_choice\",\n",
    "    \"answers\": [\n",
    "      {\n",
    "        \"answer\": \"They prevent the population from reaching any kind of fitness peak.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Sticky attractors do not prevent the attainment of fitness peaks; they are the peaks where populations tend to stabilize.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"They represent states of low fitness that populations tend to avoid.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Sticky attractors are not low-fitness states; they are high-fitness states that populations are drawn to.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"They are high-fitness states in the genotype space that populations are likely to remain in for long periods.\",\n",
    "        \"correct\": True,\n",
    "        \"feedback\": \"Exactly! Sticky attractors are robust high-fitness states where populations tend to remain stable over time.\"\n",
    "      },\n",
    "      {\n",
    "        \"answer\": \"They are synonymous with global maxima in the fitness function.\",\n",
    "        \"correct\": False,\n",
    "        \"feedback\": \"Sticky attractors correspond to local maxima, not necessarily global maxima, in the fitness landscape.\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "display_quiz(comprehension_quiz)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P1C3_Sequence1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
