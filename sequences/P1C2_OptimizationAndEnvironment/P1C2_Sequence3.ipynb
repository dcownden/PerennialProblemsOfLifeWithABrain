{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C2_OptimizationAndEnvironment/P1C2_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C2_OptimizationAndEnvironment/P1C2_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "____\n",
    "# 1.2.3: Normative Thinking\n",
    "\n",
    "### **Objective:** Up until this point, we have predominantly viewed optimization as a tool for policy improvement, adapting behavior to better suit a particular environmental niche. This is a prescriptive view of optimization: \"To improve the policy, implement these steps.\" However, optimization isn't solely prescriptive. It can also be used descriptively and predicatively. In this sequence we will learn about using optimization predictively. We call this 'Normative Thinking'.\n",
    "\n",
    "If we make a series of reasonable assumptions—like behaviour being the result of learning processes which in turn are shaped by evolutionary forces, both of which adapt according to environmental demands—we can posit that the optimal behavior predicted by a model will align with the behaviour exhibited by real-world organisms. Crucially though, the validity of such predictions hinges on the extent to which the ***relevant*** intricacies of the evolutionary organism-environment dynamic are captured by the model.\n",
    "\n",
    "In this sequence, we will:\n",
    "\n",
    "* Investigate a new foraging scenario where policy performance can be mathematically-derived.\n",
    "\n",
    "* Use deduction and reasoning to derive policy performance and determine the optimal policy.\n",
    "\n",
    "* Observe how a simple propose-and-test algorithm, unaware of the structure of the problem, converges on the same optimal outcome as our analytical approach.\n",
    "\n",
    "* Ponder the beauty and potential power of this connection between deductive reasoning about policies and 'empirical' observation. There are many different optimization methods and approaches, but all effective optimization approaches will lead to -mostly- the same place.\n",
    "\n",
    "* Look at how optimization approaches fit within a broader framework of scientific understanding generally and what the means for neuroscientific understanding more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note that sequence 1.2.3 used to be 'Competition and Interaction Transform Optimization into Game Theory' but that has been moved to the next chapter 1.3 focused on evolution. You can find it here https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C3_RealEvolution/student/P1C3_Sequence3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
    "             'gw_widgets.py', 'gw_NN_RL.py']\n",
    "#filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C2_S3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Graph Viz Helper Functions\n",
    "################################################################\n",
    "# @title Graphvis Helper Functions\n",
    "\n",
    "\n",
    "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
    "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
    "  ax.axis(\"off\")\n",
    "  #plt.tight_layout()\n",
    "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
    "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
    "  for edge in edge_labels:\n",
    "    src, dest, latex_str = edge\n",
    "    if graph.has_edge(src, dest):\n",
    "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
    "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
    "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
    "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\")\n",
    "      graph.delete_edge(src, dest)\n",
    "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
    "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
    "  return graph\n",
    "\n",
    "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
    "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
    "  for node in graph.nodes():\n",
    "    if not node.startswith(\"dummy\"):\n",
    "      node.attr['width'] = width\n",
    "      node.attr['height'] = height\n",
    "  return graph\n",
    "\n",
    "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels, output_path=\"graphviz_output.png\", dpi=300,\n",
    "                            figsize=(0.6, 0.3), fontsize=16):\n",
    "  \"\"\"\n",
    "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
    "\n",
    "  Parameters:\n",
    "    nodes_list (list): List of nodes in the graph.\n",
    "    edges_list (list): List of edges in the graph.\n",
    "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
    "    output_path (str): Path to save the rendered graph.\n",
    "    dpi (int): DPI for rendering the graph.\n",
    "    figsize (tuple): Figure size for the LaTeX labels.\n",
    "\n",
    "  Returns:\n",
    "    str: Path to the saved graph image.\n",
    "  \"\"\"\n",
    "\n",
    "  # Graph Creation and Configuration\n",
    "  G = pgv.AGraph(directed=True, strict=False, rankdir='LR', ranksep=0.5, nodesep=0.5)\n",
    "  G.add_nodes_from(nodes_list)\n",
    "  for edge in edges_list:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
    "  G = set_regular_node_sizes(G, width=1, height=1)\n",
    "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=figsize, fontsize=fontsize)\n",
    "\n",
    "  # Additional graph attributes\n",
    "  G.graph_attr['size'] = \"8,8\"\n",
    "  G.graph_attr['dpi'] = str(dpi)\n",
    "\n",
    "  # Render and save the graph\n",
    "  G.layout(prog='dot')\n",
    "  G.draw(output_path)\n",
    "\n",
    "  return output_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# make PatchyForageBoard class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "# @title PatchyForageBoard class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchyForageBoard():\n",
    "  \"\"\"\n",
    "  A collection of methods and parameters of a patchy foraging game board that\n",
    "  define the logic of the game, and allows for multiple critters on the same\n",
    "  board\n",
    "\n",
    "  game state is represented by primarily by food locations, forager locations,\n",
    "  predator locations, scores, and rounds left\n",
    "  food patch locations are stored on a batch x n_rows x n_cols numpy array,\n",
    "  forager and predator(when we have them) locations are stored as dictionaries\n",
    "  with integer keys corresponding to a forager/predatore 1, 2, 3 etc, and then\n",
    "  np.argwhere style tuples of arrays of (batch_array, row_array, col_array)\n",
    "  giving the locations\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game.\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "  ARRAY_PAD_VALUE = -200\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=10, n_cols=5,\n",
    "               num_foragers=1,\n",
    "               death_rate=0.04,\n",
    "               food_patch_prob = 0.5,\n",
    "               food_regen_prob=0.0,\n",
    "               forage_success_prob = 0.9,\n",
    "               food_extinct_prob = 0.2, rng=None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_foragers = num_foragers\n",
    "    self.death_rate = death_rate\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.forage_success_prob = forage_success_prob\n",
    "    self.food_extinct_prob = food_extinct_prob\n",
    "    self.food_regen_prob = food_regen_prob\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"\n",
    "    Set up starting board using game parameters\n",
    "\n",
    "    Returns:\n",
    "      state (dict):\n",
    "      The state dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new\n",
    "    \"\"\"\n",
    "    # note that is_over applies at the batch level not the batch x forager level\n",
    "    self.is_over = np.zeros(self.batch_size, dtype=bool)\n",
    "    self.rounds_alive = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.scores = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    # create an empty board array for food locs\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols),\n",
    "                           dtype=int)\n",
    "    # Place critters in top left corner of the board\n",
    "    self.forager_locs = {}\n",
    "    for c in (np.arange(self.num_foragers)+1):\n",
    "      self.forager_locs[c] = (np.arange(self.batch_size, dtype=int),\n",
    "                              np.zeros(self.batch_size, dtype=int),\n",
    "                              np.zeros(self.batch_size, dtype=int))\n",
    "    # Initial food patches on the board randomly\n",
    "    # each grid has an independent prob of being a pathc (to make the math\n",
    "    # easier later) so total number of patches on a board is binomially\n",
    "    # distributed\n",
    "    num_foods = self.rng.binomial(n=self.n_rows * self.n_cols,\n",
    "                                  p=self.food_patch_prob,\n",
    "                                  size=self.batch_size)\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(self.n_rows, self.n_cols,\n",
    "                                               num_foods[ii])\n",
    "      # food patch start locations (do each patch separate in case we\n",
    "      # want to have different kinds of patches)\n",
    "      for f_ in np.arange(num_foods[ii]):\n",
    "        self.pieces[(ii, rc_idx[0][f_],\n",
    "                         rc_idx[1][f_])] = - 1\n",
    "    # keep track of which foragers have missed how many times\n",
    "    # at what kind of patch\n",
    "    self.misses_new_patch = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.misses_known_patch = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.at_new_patch = np.ones((self.batch_size, self.num_foragers), dtype=bool)\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_alive': self.rounds_alive.copy(),\n",
    "             'is_over': self.is_over.copy(),\n",
    "             'forager_locs': copy.deepcopy(self.forager_locs),\n",
    "             'misses_new_patch': self.misses_new_patch.copy(),\n",
    "             'misses_known_patch': self.misses_known_patch.copy(),\n",
    "             'at_new_patch': self.at_new_patch.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\"\n",
    "    Sets the state given a board dictionary.\n",
    "\n",
    "    Args:\n",
    "      board (dict):\n",
    "      The board dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new patch.\n",
    "    \"\"\"\n",
    "    self.pieces = board['pieces'].copy()\n",
    "    self.forager_locs = copy.deepcopy(board['forager_locs'])\n",
    "    self.rounds_alive = board['rounds_alive'].copy()\n",
    "    self.scores = board['scores'].copy()\n",
    "    self.is_over = board['is_over'].copy()\n",
    "    self.misses_new_patch = board['misses_new_patch'].copy()\n",
    "    self.misses_known_patch = board['misses_known_patch'].copy()\n",
    "    self.at_new_patch = board['at_new_patch'].copy()\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\"\n",
    "    Returns the current board state.\n",
    "\n",
    "    Returns:\n",
    "      state (dict):\n",
    "      The state dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new patch.\n",
    "    \"\"\"\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_alive': self.rounds_alive.copy(),\n",
    "             'is_over': self.is_over.copy(),\n",
    "             'forager_locs': copy.deepcopy(self.forager_locs),\n",
    "             'misses_new_patch': self.misses_new_patch.copy(),\n",
    "             'misses_known_patch': self.misses_known_patch.copy(),\n",
    "             'at_new_patch': self.at_new_patch.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  ################# CORE GAME STATE UPDATE LOGIC ##############################\n",
    "  ################# execute_moves is main, uses these helper functions ########\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, which_critter):\n",
    "    \"\"\"\n",
    "    Execute the moves on the board. A move to the current location implies\n",
    "    foraging. If foraging, check if foraging is successful, update scores,\n",
    "    and check if the food goes extinct. If moving to a new location, simply\n",
    "    update the critter's location.\n",
    "\n",
    "    Args:\n",
    "      moves (tuple): A tuple of three arrays:\n",
    "        - batch_array: Specifies which board in the batch the move corresponds to.\n",
    "        - row_array: Specifies the target row for each move.\n",
    "        - col_array: Specifies the target column for each move.\n",
    "        Each array in the tuple has the same length. A move is represented by\n",
    "        the combination of a batch index, row index, and column index at the\n",
    "        same position in their respective arrays.\n",
    "      which_critter (int): Index to identify the critter. Starts from 1.\n",
    "\n",
    "    Returns: Nothing, just updates state related attributes of the board object\n",
    "\n",
    "    \"\"\"\n",
    "    #expand moves tuple\n",
    "    batch_moves, row_moves, col_moves = moves\n",
    "\n",
    "    # Get current locations of the critter\n",
    "    current_locs = self.forager_locs[which_critter]\n",
    "\n",
    "    # Iterate over each board in the batch\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # If the game is over for this board, skip\n",
    "      if self.is_over[ii]:\n",
    "        continue\n",
    "\n",
    "      # increment rounds alive if not over\n",
    "      self.rounds_alive[ii, which_critter - 1] += 1\n",
    "\n",
    "      # Get new location directly from the moves\n",
    "      new_row = int(row_moves[ii])\n",
    "      new_col = int(col_moves[ii])\n",
    "\n",
    "      # Check if the critter has moved to a new patch\n",
    "      if (new_row, new_col) != (current_locs[1][ii], current_locs[2][ii]):\n",
    "        # Moved to a new patch\n",
    "        self.misses_new_patch[ii, which_critter - 1] = 0\n",
    "        self.misses_known_patch[ii, which_critter - 1] = 0\n",
    "        self.at_new_patch[ii, which_critter - 1] = True\n",
    "\n",
    "      # If the critter's position has not changed, it's trying to forage\n",
    "      elif (new_row, new_col) == (current_locs[1][ii], current_locs[2][ii]):\n",
    "        # Check if there's food at the location\n",
    "        if self.pieces[ii, new_row, new_col] < 0:\n",
    "          # Check if foraging is successful\n",
    "          if self.rng.random() < self.forage_success_prob:\n",
    "            # Successful foraging, increase critter's score\n",
    "            self.scores[ii, which_critter - 1] += 1\n",
    "            # misses are zeroed and no longer at new patch\n",
    "            self.misses_new_patch[ii, which_critter - 1] = 0\n",
    "            self.misses_known_patch[ii, which_critter - 1] = 0\n",
    "            self.at_new_patch[ii, which_critter - 1] = False\n",
    "            # Check if food goes extinct (only on success)\n",
    "            if self.rng.random() < self.food_extinct_prob:\n",
    "              self.pieces[ii, new_row, new_col] = 0  # Set it to empty\n",
    "          else:\n",
    "            #unsuccessful foraging at patch with food\n",
    "            if self.at_new_patch[ii, which_critter - 1]:\n",
    "              # at a new patch\n",
    "              self.misses_new_patch[ii, which_critter - 1] += 1\n",
    "            else:\n",
    "              # at a known patch\n",
    "              self.misses_known_patch[ii, which_critter - 1] += 1\n",
    "        else:\n",
    "          #unsuccessful foraging at patch without food\n",
    "            if self.at_new_patch[ii, which_critter - 1]:\n",
    "              # at a new patch\n",
    "              self.misses_new_patch[ii, which_critter - 1] += 1\n",
    "            else:\n",
    "              # at a known patch\n",
    "              self.misses_known_patch[ii, which_critter - 1] += 1\n",
    "\n",
    "      # Always check if session is over\n",
    "      if self.rng.random() < self.death_rate:\n",
    "        self.is_over[ii] = True\n",
    "\n",
    "    # assume moves are legal and update locs for whole batch at once\n",
    "    self.forager_locs[which_critter] = (batch_moves, row_moves, col_moves)\n",
    "\n",
    "  ###### Getting Legal Moves and Perceptions #########################\n",
    "  ####################################################################\n",
    "  def get_neighbor_grc_indices(self, which_critter, radius, pad=False):\n",
    "    \"\"\"\n",
    "    Returns all grid positions within a certain cityblock distance radius from\n",
    "    the place corresponding to which_critter.\n",
    "\n",
    "    Args:\n",
    "        which_critter (int): The idex of the focal critter_food.\n",
    "        radius (int): The cityblock distance.\n",
    "        pad (bool): whether or not to pad the array, if padded all row, col\n",
    "          indexes are valid for the padded array, useful for getting percept\n",
    "          if not all indexes are correct for the original array, useful for\n",
    "          figuring out legal moves.\n",
    "\n",
    "    Returns:\n",
    "        an array of indices, each row is a g, r, c index for the neighborhoods\n",
    "        around the critters, can use the g value to know which board you are in.\n",
    "        if pad=True also returns the padded array (the indices in that case) are\n",
    "        for the padded array, so won't work on self.pieces, whereas if pad is\n",
    "        False the indices will be for the offsets in reference to the original\n",
    "        self.pieces, but note that some of these will be invalid, and will\n",
    "        need to be filtered out (as we do in get_legal)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    batch, rows, cols = self.forager_locs[which_critter]\n",
    "    # Create meshgrid for offsets\n",
    "    if pad is True:\n",
    "      padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "        (radius, radius)), constant_values=self.ARRAY_PAD_VALUE)\n",
    "      rows = rows + radius\n",
    "      cols = cols + radius\n",
    "\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    neighbors_rows = extended_rows + valid_row_offsets\n",
    "    neighbors_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    indices = np.column_stack((np.repeat(np.arange(batch_size),\n",
    "                                         neighbors_rows.shape[1]),\n",
    "                               neighbors_rows.ravel(),\n",
    "                               neighbors_cols.ravel()))\n",
    "    if pad is False:\n",
    "      return indices\n",
    "    elif pad is True:\n",
    "      return indices, padded_arr\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, which_critter, radius=1):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter.\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    critter_locs = np.array(self.forager_locs[which_critter])\n",
    "    # turn those row, col offsets into a set of legal offsets\n",
    "    legal_offsets = self.get_neighbor_grc_indices(which_critter, radius)\n",
    "    legal_offsets = {tuple(m_) for m_ in legal_offsets}\n",
    "\n",
    "    legal_destinations = np.where(np.ones(self.pieces.shape, dtype=bool))\n",
    "    legal_destinations = {tuple(coords) for coords in zip(*legal_destinations)}\n",
    "    # Add the current locations of the critters to legal_destinations\n",
    "    current_locations = {tuple(loc) for loc in critter_locs.T}\n",
    "    legal_destinations = legal_destinations.union(current_locations)\n",
    "\n",
    "    # legal moves are both legal offsets and legal destinations\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_perceptions(self, critter_food, radius):\n",
    "    idx, pad_pieces = self.get_neighbor_grc_indices(critter_food,\n",
    "                                                    radius, pad=True)\n",
    "    #percept_mask = np.zeros(pad_pieces.shape, dtype=bool)\n",
    "    #percept_mask[idx[:,0], idx[:,1]], idx[:,2]] = True\n",
    "    percept = pad_pieces[idx[:,0], idx[:,1], idx[:,2]]\n",
    "    return(percept.reshape(self.batch_size, -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# make PatchyForageGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "# @title PatchyForageGame class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchyForagingGame():\n",
    "  \"\"\"\n",
    "  A collection of methods and parameters of a patchy foraging game that allow\n",
    "  for interaction with and display of PatchyForageBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1, n_rows=10, n_cols=5, num_foragers=1,\n",
    "               death_rate=0.04, food_patch_prob=0.5, food_regen_prob=0.0,\n",
    "               forage_success_prob=0.9, food_extinct_prob=0.2, rng=None):\n",
    "    \"\"\"\n",
    "    Initializes an instance of the PatchyForagingGame with the specified parameters.\n",
    "    Args:\n",
    "    ... [same as in PatchyForageBoard]\n",
    "    \"\"\"\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_foragers = num_foragers\n",
    "    self.death_rate = death_rate\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.forage_success_prob = forage_success_prob\n",
    "    self.food_extinct_prob = food_extinct_prob\n",
    "    self.food_regen_prob = food_regen_prob\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns the initial state of the game.\n",
    "    \"\"\"\n",
    "    board = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                              n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                              death_rate=self.death_rate, food_patch_prob=self.food_patch_prob,\n",
    "                              forage_success_prob=self.forage_success_prob,\n",
    "                              food_extinct_prob=self.food_extinct_prob,\n",
    "                              food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    return board.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_shape(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only a subset\n",
    "    of these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to (batch,) row and column coordinate indexes of board locations.\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    return self.batch_size\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "  def get_rounds_alive(self, board):\n",
    "    return board['rounds_alive'].copy()\n",
    "\n",
    "  def get_square_symbol(self, piece, has_forager):\n",
    "    \"\"\"Returns the symbol representation of a board square.\"\"\"\n",
    "    if has_forager and piece < 0: return 'C'  # Critter on food patch\n",
    "    if has_forager: return 'P'  # Forager on an empty square\n",
    "    if piece == 0: return '.'  # Empty square\n",
    "    if piece < 0: return 'F'  # Food patch\n",
    "    return '?'  # Unknown piece type, for debugging\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th game in the batch of boards.\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for r_ in range(self.n_rows):\n",
    "      print(r_, \"|\", end=\"\")  # Print the row number\n",
    "      for c_ in range(self.n_cols):\n",
    "        piece = board['pieces'][g, r_, c_]  # Get the piece to print\n",
    "        # Check if the square is occupied by a forager\n",
    "        has_forager = False\n",
    "        for forager_num, locs in board['forager_locs'].items():\n",
    "          if g in locs[0] and r_ in locs[1] and c_ in locs[2]:\n",
    "            has_forager = True\n",
    "            break\n",
    "\n",
    "        print(self.get_square_symbol(piece, has_forager), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Alive: \" + str(board['rounds_alive'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "  def get_critter_rc(self, board, g, which_critter):\n",
    "    critter_locs = board['forager_locs'][which_critter]\n",
    "    return critter_locs[1][g], critter_locs[2][g]\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war field_of_view\n",
    "                 fov_opaque=False, #let human see through fog of war or not\n",
    "                 show_food=True,\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter',\n",
    "                 focal_critter_index = 0):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, size=550, show_food=show_food)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food, show_food=show_food)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(self.num_foragers):\n",
    "        critter_name = name if self.num_foragers == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == self.num_foragers, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(self.get_critter_rc(board, g, spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # plot field of view around the 'active player'\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque, fov=fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, board, which_critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to get the legal moves, as a set of batch, row, col triples\n",
    "    for the given board. Does return moves that are technically legal\n",
    "    but that will result in a blocking move\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "\n",
    "      which_critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          death_rate=self.death_rate,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves(which_critter, radius)\n",
    "    return legal_moves\n",
    "\n",
    "  def get_valid_actions(self, board, which_critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    legal_moves =  self.get_legal_moves(board, which_critter, radius)\n",
    "    g, r, c = zip(*legal_moves)\n",
    "    valids = np.zeros((self.batch_size, self.n_rows * self.n_cols))\n",
    "    valids[g, np.array(r) * self.n_cols + np.array(c)] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, which_critter, actions):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    assert batch_size == len(actions)\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          death_rate=self.death_rate,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.actions_to_moves(actions)\n",
    "    b.execute_moves(moves, which_critter)\n",
    "    return b.get_state()\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, which_critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right' 'still'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          death_rate=self.death_rate,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.critter_direction_to_move(board, offsets, which_critter)\n",
    "    b.execute_moves(moves, which_critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right', 'still'\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = board['forager_locs'][critter]\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "  def critter_directions_to_actions(self, board, directions, critter):\n",
    "    \"\"\"\n",
    "    Converts a list of direction strings to a list of action indices for the given board state and critter.\n",
    "\n",
    "    Args:\n",
    "      board (dict): The current state of the game.\n",
    "      directions (list of str): List of directions, where each direction is one of 'up', 'down', 'left', 'right', 'still'.\n",
    "      critter (int): The critter index.\n",
    "\n",
    "    Returns:\n",
    "      list of int: List of action indices corresponding to the directions.\n",
    "    \"\"\"\n",
    "    # Ensure the length of directions matches the batch size\n",
    "    assert len(directions) == board['pieces'].shape[0], \"Mismatch between directions length and batch size\"\n",
    "\n",
    "    # Convert directions to moves\n",
    "    moves = self.critter_direction_to_move(board, directions, critter)\n",
    "\n",
    "    # Convert moves to actions\n",
    "    actions = self.moves_to_actions(moves)\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, which_critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up',\n",
    "                   ( 0, 0): 'still'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, which_critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    critter_locs = board['forager_locs'][which_critter]\n",
    "    valid_directions = []\n",
    "    for g, batch_valid in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valid==1)[0]\n",
    "      critter_loc = np.array([[critter_locs[1][g],critter_locs[2][g]]])\n",
    "      # critter_loc shape is (1, 2)\n",
    "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
    "      offsets = moves - critter_loc\n",
    "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, which_critter):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          death_rate=self.death_rate,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    return(b.get_perceptions(radius, which_critter))\n",
    "\n",
    "\n",
    "  def play_game(self, players=[], visualize = False):\n",
    "    \"\"\"This method takes a list of players the same length as num_foragers,\n",
    "        and then plays a batch of games with them and returns the final board\n",
    "        states of each game\"\"\"\n",
    "    if len(players) != self.num_foragers:\n",
    "      raise ValueError(\"number of players different than expected\")\n",
    "\n",
    "    board = self.get_init_board()\n",
    "    if visualize == True:\n",
    "      self.display(board, 0)\n",
    "\n",
    "    for p_idx, player_ in enumerate(players):\n",
    "      if player_.critter_index != p_idx+1:\n",
    "        print(player_.critter_index)\n",
    "        print(p_idx + 1)\n",
    "        raise ValueError(\"player order does not match assigned critter index\")\n",
    "\n",
    "    while np.any(board['is_over'] == False):\n",
    "      for player_ in players:\n",
    "        old_scores = board['scores']\n",
    "        if player_.return_direction:\n",
    "          directions = player_.play(board)\n",
    "          a_player = self.critter_directions_to_actions(board, directions, player_.critter_index)\n",
    "        else: # player returns actions directly\n",
    "          a_player, _, _ = player_.play(board)\n",
    "        board = self.get_next_state(board, player_.critter_index, a_player)\n",
    "        if visualize == True:\n",
    "          self.display(board, 0)\n",
    "    return board\n",
    "\n",
    "\n",
    "  def plot_visualizations(board):\n",
    "    # Extracting scores and rounds_alive for all batches\n",
    "    scores = board['scores']\n",
    "    rounds_alive = board['rounds_alive']\n",
    "\n",
    "    # Calculating average scores per round for each batch\n",
    "    avg_scores_per_round = scores / rounds_alive\n",
    "\n",
    "    # Histogram of Average Score Per Round\n",
    "    plt.figure()\n",
    "    plt.hist(avg_scores_per_round, bins=30, edgecolor='black')\n",
    "    plt.xlabel('Average Score Per Round')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Average Score Per Round')\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter Plot of Averages vs. Rounds Alive\n",
    "    plt.figure()\n",
    "    plt.scatter(rounds_alive, avg_scores_per_round, c='blue', alpha=0.5)\n",
    "    plt.xlabel('Rounds Alive')\n",
    "    plt.ylabel('Average Score Per Round')\n",
    "    plt.title('Scatter Plot of Average Scores vs. Rounds Alive')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# make InteractivePatchyForage class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "# @title Interactive Patchy Foraging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractivePatchyForage():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, PatchyForage_game, init_board=None, has_fov=False,\n",
    "               radius=2, fov_opaque=False, show_food=True, show_misses=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human']):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a patchy foraging game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of PatchyForageGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'rounds_alive': The number of rounds each critter has been alive.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the PatchyForage_game.num_foragers\n",
    "        attribute. Order should align with critter_name.\n",
    "\n",
    "      Note: fov only turns on for the 'active' player.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.pfg = PatchyForage_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.show_food = show_food\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.active_player_index = 0\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.pfg.num_foragers):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] #implicit check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        # player objects expected to have a critter_index attribute\n",
    "        # we set it appropriately here so it aligns with the players list\n",
    "        # used to create the widget\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    # Initialize the sidebar for displaying misses if needed\n",
    "    self.show_misses = show_misses\n",
    "    if self.show_misses:\n",
    "      self.misses_sidebar = widgets.Output(layout=widgets.Layout(\n",
    "          min_width='12.5em', max_width='18.8em',\n",
    "          min_height='6.3em', overflow='auto'))\n",
    "      self.misses_new_patch = [0] * self.pfg.num_foragers\n",
    "      self.misses_known_patch = ['--'] * self.pfg.num_foragers\n",
    "      self.at_new_patch = [True] * self.pfg.num_foragers\n",
    "\n",
    "    if init_board is None:\n",
    "      self.board_state = self.pfg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.forage_button = widgets.Button(description=\"Forage\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.empty_space = widgets.Box(layout=widgets.Layout(height='2.5em'))\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.pfg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize,\n",
    "          show_food=self.show_food)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.pfg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                fov_opaque=self.fov_opaque,\n",
    "                                show_food=self.show_food,\n",
    "                                radius=self.radius, figsize=self.figsize)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([widgets.VBox([self.forage_button, self.left_button]),\n",
    "                                 widgets.VBox([self.up_button, self.down_button]),\n",
    "                                 widgets.VBox([self.empty_space, self.right_button])])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "      if len(self.players) == 1:\n",
    "        #one human player\n",
    "        self.output_and_score = widgets.VBox([self.scoreboard, self.output])\n",
    "        if self.show_misses:\n",
    "            self.final_display = widgets.HBox([self.board_and_buttons,\n",
    "                widgets.VBox([self.misses_sidebar, self.output_and_score])])\n",
    "        else:\n",
    "            self.final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                               self.output_and_score])\n",
    "      else:\n",
    "        # more than one player, one of them human\n",
    "        self.V_board_output= widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend,\n",
    "                                             self.misses_sidebar])\n",
    "        else:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # all players are ai\n",
    "      if len(self.players) == 1:\n",
    "        # one ai player\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox(\n",
    "              [widgets.VBox([self.b_fig.canvas, self.scoreboard]),\n",
    "               widgets.VBox([self.misses_sidebar, self.output,\n",
    "                             self.start_button])])\n",
    "        else:\n",
    "          self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "          self.final_display = self.HBox(\n",
    "              [widgets.VBox([self.b_fig.canvas, self.H_score_output_start])])\n",
    "      else:\n",
    "        # more than one ai player\n",
    "        self.V_board_output = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "          self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend,\n",
    "                                             self.misses_sidebar])\n",
    "        else:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['Best Eating Rate:'] + ['--'] * self.pfg.num_foragers,\n",
    "               ['Last Eating Rate:'] + ['--'] * self.pfg.num_foragers,\n",
    "               ['Average Eating Rate:'] + ['--'] * self.pfg.num_foragers,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    # If show_misses is enabled, initialize the misses_sidebar content\n",
    "    if self.show_misses:\n",
    "      with self.misses_sidebar:\n",
    "        table = [['Misses (New Patch):'] + ['0'] * self.pfg.num_foragers,\n",
    "                 ['Misses (Known Patch):'] + ['--'] * self.pfg.num_foragers]\n",
    "        if len(self.players) > 1:\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "          print(tabulate(table, headers=headers))\n",
    "        else: # len(self.players) == 1\n",
    "            print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.forage_button.on_click(self.on_forage_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def update_state_based_on_move(self, direction):\n",
    "    old_board = self.board_state.copy()\n",
    "    # index of players is 0 through num_critter-1,\n",
    "    # same player represented by value of index + 1 in\n",
    "    if (isinstance(self.players[self.active_player_index], str) and\n",
    "        'human' in self.players[self.active_player_index]):\n",
    "      direction = direction\n",
    "    else:\n",
    "      if self.players[self.active_player_index].return_direction:\n",
    "        directions = self.players[self.active_player_index].play(old_board)\n",
    "      else:\n",
    "        a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
    "        # print(a_player)\n",
    "        directions = self.pfg.action_to_critter_direction(old_board,\n",
    "                                                        self.active_player_index+1,\n",
    "                                                        a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = directions[0]\n",
    "    self.board_state = self.pfg.critter_oriented_get_next_state(\n",
    "          self.board_state, self.active_player_index+1, [direction])\n",
    "    return direction\n",
    "\n",
    "\n",
    "  def update_output_and_scores(self, direction, old_board):\n",
    "    old_scores = old_board['scores'][0]\n",
    "    old_row, old_col = self.pfg.get_critter_rc(old_board, 0,\n",
    "                                               self.active_player_index+1)\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    rounds_alive = self.board_state['rounds_alive'][0]\n",
    "    row, col = self.pfg.get_critter_rc(self.board_state, 0,\n",
    "                                       self.active_player_index+1)\n",
    "\n",
    "    did_eat = False\n",
    "    # Check if the forager moved or tried to forage and what happened\n",
    "    if (row, col) != (old_row, old_col):\n",
    "      # Moved to a new patch\n",
    "      self.misses_new_patch[self.active_player_index] = 0\n",
    "      self.misses_known_patch[self.active_player_index] = '--'\n",
    "      self.at_new_patch[self.active_player_index] = True\n",
    "      action_string = \"tried to move \" + direction + \" to ({}, {})\".format(row, col)\n",
    "      eating_string = \"They were too busy moving to look for food.\"\n",
    "    elif (row, col) == (old_row, old_col):\n",
    "      # they didn't move, tried to forage\n",
    "      action_string = \"tried to forage.\"\n",
    "      if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
    "        # They found food\n",
    "        eating_string = \"They found some food at the patch!\"\n",
    "        did_eat = True\n",
    "        if self.at_new_patch[self.active_player_index]:\n",
    "          # They found food at a new patch\n",
    "          self.misses_new_patch[self.active_player_index] = '--'\n",
    "          self.misses_known_patch[self.active_player_index] = 0\n",
    "          self.at_new_patch[self.active_player_index] = False\n",
    "        else:\n",
    "          # They found food at a known patch\n",
    "          # Reset count as they found food\n",
    "          self.misses_known_patch[self.active_player_index] = 0\n",
    "      else:\n",
    "        # They didn't find food\n",
    "        eating_string = \"They didn't find any food at the patch.\"\n",
    "        if self.at_new_patch[self.active_player_index]:\n",
    "          # They are at a new patch\n",
    "          self.misses_new_patch[self.active_player_index] += 1\n",
    "        else:\n",
    "          # They are at a known patch\n",
    "          self.misses_known_patch[self.active_player_index] += 1\n",
    "\n",
    "    #make the picture of the new board position\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.pfg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             show_food=self.show_food,\n",
    "                             radius=self.radius, legend_type=self.legend_type)\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        print(\"The critter {}\".format(action_string))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Alive: {}\\nFood Eaten: {}\\nFood Per Move: {:.2f}\".format(\n",
    "          rounds_alive[self.active_player_index],\n",
    "          new_scores[self.active_player_index],\n",
    "          new_scores[self.active_player_index] / rounds_alive[self.active_player_index]))\n",
    "      else:  # more than one player\n",
    "        print(\"Critter {} {}\".format(self.active_player_index+1, action_string))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Alive: {}\\nFood Eaten: {}\".format(\n",
    "          rounds_alive, new_scores))\n",
    "\n",
    "    if self.show_misses:\n",
    "      with self.misses_sidebar:\n",
    "        clear_output()\n",
    "        table = [['Misses (New Patch):'] + [str(miss) for miss in self.misses_new_patch],\n",
    "                 ['Misses (Known Patch):'] + [str(miss) for miss in self.misses_known_patch]]\n",
    "        if len(self.players) > 1:\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "          print(tabulate(table, headers=headers))\n",
    "        else: # len(self.players) == 1\n",
    "            print(tabulate(table))\n",
    "\n",
    "\n",
    "  def handle_game_end(self):\n",
    "    \"\"\"Handle the logic when the game is over.\"\"\"\n",
    "    self.final_scores.append(self.board_state['scores'][0] / self.board_state['rounds_alive'][0])\n",
    "    self.board_state = self.pfg.get_init_board()\n",
    "    for player in self.players:\n",
    "      if hasattr(player, 'last_direction'):\n",
    "        player.last_direction = ['right'] * self.pfg.batch_size\n",
    "    if self.show_misses:\n",
    "      self.misses_new_patch = [0] * self.pfg.num_foragers\n",
    "      self.misses_known_patch = ['--'] * self.pfg.num_foragers\n",
    "      self.at_new_patch = [True] * self.pfg.num_foragers\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      print('Game Over. Final Food per Move {}'.format(self.final_scores[-1]))\n",
    "      print('Resetting the board for another game')\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.pfg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                             self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             show_food=self.show_food,\n",
    "                             legend_type=self.legend_type)\n",
    "    with self.scoreboard:\n",
    "      clear_output()\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      if len(self.players) == 1:\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = [\n",
    "            ['Best Eating Rate: ', '{:.2f}'.format(np.max(np.array(self.final_scores)))],\n",
    "            ['Last Eating Rate: ', '{:.2f}'.format(self.final_scores[-1][0])],\n",
    "            ['Average Eating Rate', '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "        else:\n",
    "          table = [['Best Eating Rate:', '--'],\n",
    "                   ['Last Eating Rate:', '--'],\n",
    "                   ['Average Eating Rate:', '--']]\n",
    "        print(tabulate(table))\n",
    "      else: # len(self.players) > 1\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = []\n",
    "          # Assuming the batch size is 1 for now\n",
    "          current_scores = self.final_scores[-1]\n",
    "          max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "          average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "          table.append(['Besat Rates:'] +\n",
    "          [str(score) for score in max_scores])\n",
    "          table.append(['Last Rates:'] +\n",
    "            [str(score) for score in current_scores])\n",
    "          table.append(['Average Rates:'] +\n",
    "              ['{:.2f}'.format(score) for score in average_scores])\n",
    "        else:\n",
    "          table = [\n",
    "            ['High Score:'] + ['--'] * self.pfg.num_foragers,\n",
    "            ['Last Score:'] + ['--'] * self.pfg.num_foragers,\n",
    "            ['Average Score:'] + ['--'] * self.pfg.num_foragers,]\n",
    "        print(tabulate(table, headers=headers))\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "    self.forage_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "    self.forage_button.disabled = False\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('right')\n",
    "\n",
    "  def on_forage_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('still')\n",
    "\n",
    "  def execute_moves(self, human_direction=None):\n",
    "    ai_direction = None\n",
    "    while not self.board_state['is_over'][0]:\n",
    "      old_board = self.board_state.copy()\n",
    "      # Check if the current player is human\n",
    "      if self.players[self.active_player_index] == 'human':\n",
    "        if human_direction is None:\n",
    "          # If the human direction is not provided,\n",
    "          # it means the human has not yet made a move\n",
    "          # Break out and wait for one\n",
    "          break\n",
    "        else:\n",
    "          # The human made a move, so execute it and reset the human_direction\n",
    "          self.update_state_based_on_move(human_direction)\n",
    "          self.update_output_and_scores(human_direction, old_board)\n",
    "          human_direction = None  # Reset for next loop iteration\n",
    "      else:\n",
    "        # AI player\n",
    "        ai_direction = self.update_state_based_on_move('tbd')\n",
    "        self.update_output_and_scores(ai_direction, old_board)\n",
    "\n",
    "      # Move to the next player\n",
    "      self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "\n",
    "  def on_direction_button_click(self, direction):\n",
    "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
    "    self.execute_moves(human_direction=direction)\n",
    "    if self.board_state['is_over'][0]:\n",
    "        self.handle_game_end()\n",
    "    self.enable_direction_buttons()  # Re-enable buttons\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    self.execute_moves()\n",
    "    if self.board_state['is_over'][0]:\n",
    "        self.handle_game_end()\n",
    "    self.start_button.disabled = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# refactor Monte Carlo for boards that support multiple critters\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarlo():\n",
    "  \"\"\"\n",
    "  Implementation of Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, nnet, default_depth=5, random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def pis_vs_from_board(self, board, critter):\n",
    "    #helper function, to put board in canonical form that nn was trained on\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    co_pieces = board['pieces'].copy()\n",
    "    this_critter_locs = np.where(co_pieces == critter)\n",
    "    all_critter_locs = np.where(co_pieces >= 1)\n",
    "    # other critters are invisible to this player\n",
    "    co_pieces[all_critter_locs] = 0\n",
    "    # nnet trained to see self as 1\n",
    "    co_pieces[this_critter_locs] = 1\n",
    "    scalar_rounds_left = board['rounds_left'][0]\n",
    "    co_rounds_left = scalar_rounds_left // self.game.num_critters\n",
    "    if critter-1 < scalar_rounds_left % self.game.num_critters:\n",
    "       # add an extra if we haven't had this players turn yet in the round cycle\n",
    "       co_rounds_left = co_rounds_left + 1\n",
    "    co_rounds_left = np.array([co_rounds_left]*batch_size)\n",
    "    pis, vs = self.nnet.predict(co_pieces,\n",
    "                                board['scores'][:,critter-1],\n",
    "                                co_rounds_left)\n",
    "    return pis, vs\n",
    "\n",
    "\n",
    "  def simulate(self, board, actions, action_indexes, critter=1, depth=None):\n",
    "    \"\"\"\n",
    "    Helper function to simulate one Monte Carlo rollout\n",
    "\n",
    "    Args:\n",
    "      board: triple (batch_size x x_size x y_size np.array of board position,\n",
    "                     scalar of current score,\n",
    "                     scalar of rounds left\n",
    "      actions: batch size list/array of integer indexes for moves on each board\n",
    "      these are assumed to be legal, no check for validity of moves\n",
    "    Returns:\n",
    "      temp_v:\n",
    "        Terminal State\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    next_board = self.game.get_next_state(board, critter,\n",
    "                                          actions, action_indexes)\n",
    "    # in this version of the mc player, the existence of other players is\n",
    "    # ignored, in another version of mc other players moves might be simulated\n",
    "    next_board['active_player'] = critter-1\n",
    "\n",
    "    if depth is None:\n",
    "      depth = self.default_depth\n",
    "    # potentially expand the game tree here,\n",
    "    # but just do straight rollouts after this\n",
    "    # doesn't expand to deal with all random food generation possibilities\n",
    "    # just expands based on the actions given\n",
    "    expand_bs, _, _ = next_board['pieces'].shape\n",
    "\n",
    "    for i in range(depth):  # maxDepth\n",
    "      if next_board['rounds_left'][0] <= 0:\n",
    "        # check that game isn't over\n",
    "        # assumes all boards have the same rounds left\n",
    "        # no rounds left return scores as true values\n",
    "        terminal_vs = next_board['scores'][:,critter-1].copy()\n",
    "        return terminal_vs\n",
    "      else:\n",
    "        #pis, vs = self.nnet.predict(next_board['pieces'], next_board['scores'], next_board['rounds_left'])\n",
    "        pis, vs = self.pis_vs_from_board(next_board, critter)\n",
    "        valids = self.game.get_valid_actions(next_board, critter)\n",
    "        masked_pis = pis * valids\n",
    "        sum_pis = np.sum(masked_pis, axis=1)\n",
    "        probs = np.array(\n",
    "            [masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "             else valid / valid.sum()\n",
    "             for valid, masked_pi in zip(valids, masked_pis)])\n",
    "        samp = self.rng.uniform(size = expand_bs).reshape((expand_bs,1))\n",
    "        sampled_actions = np.argmax(probs.cumsum(axis=1) > samp, axis=1)\n",
    "      next_board = self.game.get_next_state(next_board, critter,\n",
    "                                            sampled_actions)\n",
    "      # in this version of the mc player, existence of other players is ignored\n",
    "      # in another better version other players moves might be simulated, either\n",
    "      # as copies of self, or as distinct environmental dynamics\n",
    "      next_board['active_player'] = critter-1\n",
    "\n",
    "\n",
    "    pis, vs = self.pis_vs_from_board(next_board, critter)\n",
    "    #pis, vs = self.nnet.predict(next_board['pieces'], next_board['scores'],\n",
    "    #                            next_board['rounds_left'])\n",
    "    #print(vs.shape)\n",
    "    return vs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title plotting functions\n",
    "#################################################\n",
    "# More plotting functions\n",
    "#################################################\n",
    "\n",
    "\n",
    "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
    "                    name=None):\n",
    "  \"\"\"\n",
    "  Plot vector field indicating critter direction probabilities.\n",
    "\n",
    "  Args:\n",
    "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
    "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
    "      and values as lists of direction probabilities corresponding to the\n",
    "      directions ['right', 'down', 'left', 'up'].\n",
    "    critter (int): Identifier for which critter directions are associated with.\n",
    "    deterministic (bool, optional): If True, the probabilities array is\n",
    "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
    "      and are larger. Defaults to False.\n",
    "  \"\"\"\n",
    "\n",
    "  #looks like direction ignores inverted axis\n",
    "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
    "                       'left': (-1, 0), 'up': (0, 1)}\n",
    "  # but offsets need to be aware of inverted\n",
    "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
    "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
    "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
    "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
    "  # same logic for colors\n",
    "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
    "  # Get the offset and color for this critter\n",
    "  critter_offset = critter_offsets[critter]\n",
    "  critter_color = critter_colors[critter]\n",
    "\n",
    "  # Add legend only if critter is not 0\n",
    "  custom_leg_handles = []\n",
    "  if critter != 0:\n",
    "    if name is None:\n",
    "      name = f'Critter {critter}'\n",
    "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
    "    # Add the legend for this critter\n",
    "    custom_leg_handles.append(legend_patch)\n",
    "\n",
    "  C, R, U, V, A = [], [], [], [], []\n",
    "\n",
    "  for loc in loc_prob_dict.keys():\n",
    "    row, col = loc\n",
    "    probs = loc_prob_dict[loc]\n",
    "    for dir_key, prob in probs.items():\n",
    "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
    "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
    "      U.append(direction_vectors[dir_key][0])\n",
    "      V.append(direction_vectors[dir_key][1])\n",
    "\n",
    "      if deterministic:\n",
    "        A.append(1 if prob == max(probs.values()) else 0)\n",
    "      else:\n",
    "        A.append(prob)\n",
    "\n",
    "  linewidth = 1.5 if deterministic else 0.5\n",
    "  scale = 15 if deterministic else 30\n",
    "\n",
    "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
    "            scale=scale, linewidth=linewidth)\n",
    "  return fig, ax, custom_leg_handles\n",
    "\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for further use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  # Create a new figure and axes with given figsize\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  # Set width and height padding, remove horizontal and vertical spacing\n",
    "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
    "  # Show right and top borders (spines) of the plot\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  # Set major ticks (where grid lines will be) on x and y axes\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Set labels for major ticks with font size of 8\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "  # Set minor ticks (no grid lines here) to be between major ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "  # Move x-axis ticks to the top of the plot\n",
    "  ax.xaxis.tick_top()\n",
    "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "  # Remove minor ticks (not the grid lines)\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "  # Set limits of x and y axes\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  # Invert y axis direction\n",
    "  ax.invert_yaxis()\n",
    "  # If title is provided, set it as the figure title\n",
    "  if title is not None:\n",
    "    fig.suptitle(title)\n",
    "  # Hide header and footer, disable toolbar and resizing of the figure\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  # Redraw the figure with these settings\n",
    "  fig.canvas.draw()\n",
    "  # Return figure and axes handles for further customization\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None, size=None,\n",
    "              show_food=True):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if size is None:\n",
    "    size=150\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=size, marker='o',\n",
    "                      color='red', label='Food')\n",
    "  if show_food:\n",
    "    rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "    #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "    #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "    food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]],\n",
    "                  size=None) -> List[Dict[str, object]]:\n",
    "  \"\"\"\n",
    "  Plots multiple types of \"critters\" on a grid implied by the given\n",
    "  fig, ax arguments.\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects.\n",
    "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
    "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
    "    critter.\n",
    "\n",
    "  Returns:\n",
    "    Updated critter_specs with handles.\n",
    "  \"\"\"\n",
    "  if size is None:\n",
    "    size=250\n",
    "  for spec in critter_specs:\n",
    "    # Ensure required keys are present\n",
    "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
    "      if key not in spec:\n",
    "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
    "    handle_ = spec.get('handle')\n",
    "    if handle_ is None:\n",
    "      handle_ = ax.scatter([], [], s=size, marker=spec['marker'],\n",
    "                           color=spec['color'], label=spec['name'],\n",
    "                           edgecolors='white', linewidths=1)\n",
    "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
    "    spec.update({'handle': handle_})\n",
    "  return critter_specs\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc,\n",
    "                 critter=None, critter_name='Critter'):\n",
    "  \"\"\"\n",
    "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h',\n",
    "                         color='blue', label=critter_name)\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov,\n",
    "             opaque=False, fov=None):\n",
    "  \"\"\"\n",
    "  Plots a mask on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter: ndarry(int) (row,col) of the critter\n",
    "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib Image object of mask, either new if no handle\n",
    "    was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
    "  mask_array = np.ones((n_rows, n_cols, 4))\n",
    "  mask_array[:, :, :3] = 0.5  # light grey color\n",
    "  if has_fov == True:\n",
    "    if opaque:\n",
    "      mask_array[:, :, 3] = 1.0  # 50% opacity\n",
    "    else:\n",
    "      mask_array[:, :, 3] = 0.5  # 50% opacity\n",
    "    # Create arrays representing the row and column indices\n",
    "    rows = np.arange(n_rows)[:, np.newaxis]\n",
    "    cols = np.arange(n_cols)[np.newaxis, :]\n",
    "    # Iterate over each critter location\n",
    "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
    "    # Set the region within the specified radius around the critter to transparent\n",
    "    mask_array[dist <= radius, 3] = 0\n",
    "  else:\n",
    "    mask_array[:, :, 3] = 0\n",
    "\n",
    "  if fov is None:\n",
    "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
    "  else:\n",
    "    fov.set_data(mask_array)\n",
    "\n",
    "  return fov\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title GridworldBoard class\n",
    "#######################################################################\n",
    "# extend GridworldGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game board that\n",
    "  define the logic of the game, and allows for multiple critters on the same\n",
    "  board\n",
    "\n",
    "  board state is represented by primarily by pieces, score, and rounds left\n",
    "  pieces is a batch x n_rows x n_cols numpy array positive integers are critter\n",
    "  locations 0's are empty space and -1's are food.\n",
    "\n",
    "  For pieces first dim is batch, second dim row , third is col,\n",
    "  so pieces[0][1][7] is the square in row 2, in column 8 of the first board in\n",
    "  the batch of boards.\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game.\n",
    "\n",
    "  active_player keeps track of which players turn it is\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  class CritterFoodType(Enum):\n",
    "    FOOD = \"food\"\n",
    "    PREY = \"prey\"\n",
    "    PREDATOR = \"predator\"\n",
    "\n",
    "  ARRAY_PAD_VALUE = -200\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_food=10, num_prey=1, num_pred=1,\n",
    "               lifetime=30, rng=None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_food = num_food\n",
    "    self.num_prey = num_prey\n",
    "    self.num_pred = num_pred\n",
    "    self.num_critters = num_pred + num_prey\n",
    "    self.lifetime = lifetime\n",
    "    self.pred_prey_threshold = self.num_prey\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "    self.check_sum = np.sum(np.arange(start=-self.num_food,\n",
    "                                      stop=self.num_critters+1))\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"Set up starting board using game parameters\"\"\"\n",
    "    #set rounds_left and score\n",
    "    self.rounds_left = (np.ones(self.batch_size) *\n",
    "                        self.lifetime)\n",
    "    self.is_over = np.zeros(self.batch_size, dtype=bool)\n",
    "    self.scores = np.zeros((self.batch_size, self.num_critters))\n",
    "    # create an empty board array.\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols),\n",
    "                           dtype=int)\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+num_critter because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(\n",
    "        self.n_rows, self.n_cols, self.num_food+self.num_critters)\n",
    "      # critter random start locations\n",
    "      for c_ in np.arange(self.num_critters):\n",
    "        self.pieces[(ii, rc_idx[0][c_], rc_idx[1][c_])] = c_ + 1\n",
    "      # food random start locations\n",
    "      for f_ in np.arange(self.num_food):\n",
    "        self.pieces[(ii, rc_idx[0][self.num_critters + f_],\n",
    "                         rc_idx[1][self.num_critters + f_])] = -f_ - 1\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy(),\n",
    "             'is_over': self.is_over.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\" board is dictionary giving game state a triple of np arrays\n",
    "      pieces:        numpy array (batch_size x n_rows x n_cols),\n",
    "      scores:        numpy array (batch_size x num_critters)\n",
    "      rounds_left:   numpy array (batch_size)\n",
    "    \"\"\"\n",
    "    self.pieces = board['pieces'].copy()\n",
    "    self.scores = board['scores'].copy()\n",
    "    self.rounds_left = board['rounds_left'].copy()\n",
    "    self.is_over = board['is_over'].copy()\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state, which is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy(),\n",
    "             'is_over': self.is_over.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  ################# CORE GAME STATE UPDATE LOGIC ##############################\n",
    "  ################# execute_moves is main, uses these helper functions ########\n",
    "  def get_critter_food_type(self, critter_food):\n",
    "    if critter_food <= -1:\n",
    "        critter_food_type = self.CritterFoodType.FOOD\n",
    "    elif critter_food > self.pred_prey_threshold:\n",
    "        critter_food_type = self.CritterFoodType.PREDATOR\n",
    "    else:\n",
    "        critter_food_type = self.CritterFoodType.PREY\n",
    "    return critter_food_type\n",
    "\n",
    "\n",
    "  def get_type_masks(self):\n",
    "    \"\"\"\n",
    "    Returns masks indicating the position types on the board.\n",
    "    Returns:\n",
    "        tuple: Tuple containing masks for empty spaces, food, prey, and predator.\n",
    "    \"\"\"\n",
    "    empt_mask = self.pieces == 0\n",
    "    food_mask = self.pieces <= -1\n",
    "    prey_mask = (1 <= self.pieces) & (self.pieces <= self.pred_prey_threshold)\n",
    "    pred_mask = self.pred_prey_threshold < self.pieces\n",
    "    return empt_mask, food_mask, prey_mask, pred_mask\n",
    "\n",
    "\n",
    "  def get_collisions(self, moves, critter_food, critter_food_type):\n",
    "    \"\"\"\n",
    "    Determine the collision results and update scores accordingly.\n",
    "    Args:\n",
    "        moves (tuple): Tuple of arrays indicating the moves.\n",
    "        critter_food (int): Index to identify the critter or food.\n",
    "        critter_food_type (enum): Type of the critter or food\n",
    "    Returns:\n",
    "        tuple: Tuple containing move collision messages and separates out the\n",
    "        moves by where they land i.e., empty spaces, food, prey, and predator.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    move_mask = np.zeros(self.pieces.shape, dtype=bool)\n",
    "    move_mask[moves] = True\n",
    "    (empt_mask, food_mask,\n",
    "     prey_mask, pred_mask) = self.get_type_masks()\n",
    "\n",
    "    move_coll_msg = np.zeros(batch_size)\n",
    "    empt_moves = np.where(empt_mask & move_mask)\n",
    "    food_moves = np.where(food_mask & move_mask)\n",
    "    prey_moves = np.where(prey_mask & move_mask)\n",
    "    pred_moves = np.where(pred_mask & move_mask)\n",
    "    move_coll_msg[empt_moves[0]] = 1\n",
    "\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      move_coll_msg[food_moves[0]] = 2\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      move_coll_msg[food_moves[0]] = 3\n",
    "      move_coll_msg[prey_moves[0]] = 4\n",
    "    # all collision types are blocking for food types\n",
    "\n",
    "    return (move_coll_msg, empt_moves, food_moves, prey_moves, pred_moves)\n",
    "\n",
    "\n",
    "  def update_scores(self, move_coll_msg, critter_food,\n",
    "                    critter_food_type, prey_moves):\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      self.scores[:, critter_food-1] += (move_coll_msg == 2)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      # predators that eat get a point\n",
    "      self.scores[:, critter_food-1] += (move_coll_msg == 4)\n",
    "      # prey that are eaten lose 10 points\n",
    "      who_eaten = self.pieces[prey_moves]\n",
    "      self.scores[prey_moves[0], who_eaten-1] -= 10\n",
    "    # food types don't get a score, it's a neuro book\n",
    "\n",
    "\n",
    "  def move_pieces(self, critter_food, move_coll_msg, moves):\n",
    "    \"\"\"\n",
    "    Move the pieces on the board based on the collision messages.\n",
    "\n",
    "    Args:\n",
    "        critter_food (int): Index to identify the critter or food.\n",
    "        move_coll_msg (np.array): Array of collision messages.\n",
    "        moves (tuple): Tuple of arrays indicating the moves.\n",
    "    \"\"\"\n",
    "    old_locs = np.where(self.pieces == critter_food)\n",
    "    vacated_old_locs = np.column_stack(old_locs)[np.where(move_coll_msg > 0)]\n",
    "    vacated_old_locs_idx = (vacated_old_locs[:,0],\n",
    "                            vacated_old_locs[:,1],\n",
    "                            vacated_old_locs[:,2])\n",
    "    self.pieces[vacated_old_locs_idx] = 0\n",
    "    new_locs = np.column_stack(moves)[np.where(move_coll_msg > 0)]\n",
    "    new_locs_idx = (new_locs[:,0], new_locs[:,1], new_locs[:,2])\n",
    "    self.pieces[new_locs_idx] = critter_food\n",
    "\n",
    "\n",
    "  def replace_destroyed(self, destroying_moves, old_pieces):\n",
    "    \"\"\"\n",
    "    Replace the destroyed pieces on the board.\n",
    "\n",
    "    Args:\n",
    "        destroying_moves (tuple): Tuple of arrays indicating the moves that\n",
    "        resulted in destruction.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = old_pieces.shape\n",
    "    g_gone = np.zeros(batch_size)\n",
    "    g_gone[destroying_moves[0]] = 1\n",
    "    which_gone = old_pieces[destroying_moves]\n",
    "    if np.sum(g_gone) > 0:\n",
    "      num_empty_after = (n_rows*n_cols - self.num_food - self.num_critters + 1)\n",
    "      p_new_locs = np.where(np.logical_and(\n",
    "        self.pieces == 0, g_gone.reshape(batch_size, 1, 1)))\n",
    "      food_sample_ = self.rng.choice(num_empty_after, size=int(np.sum(g_gone)))\n",
    "      food_sample = food_sample_ + np.arange(int(np.sum(g_gone)))*num_empty_after\n",
    "      new_loc_vals = self.pieces[(p_new_locs[0][food_sample],\n",
    "                   p_new_locs[1][food_sample],\n",
    "                   p_new_locs[2][food_sample])]\n",
    "      # this requires that p_new_locs and destroying moves are both\n",
    "      # lexographically sorted... but they are not always\n",
    "      self.pieces[(p_new_locs[0][food_sample],\n",
    "                   p_new_locs[1][food_sample],\n",
    "                   p_new_locs[2][food_sample])] = which_gone\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, critter_food):\n",
    "    \"\"\"\n",
    "    Execute the moves on the board, handle collisions, update scores,\n",
    "    and replace destroyed/eaten pieces.\n",
    "\n",
    "    Args:\n",
    "      moves (tuple): Tuple of arrays indicating the moves.\n",
    "      critter_food (int): Index to identify the critter or food.\n",
    "    \"\"\"\n",
    "    # what type of critter is moving\n",
    "    critter_food_type = self.get_critter_food_type(critter_food)\n",
    "    # what do they land on when the move\n",
    "    (move_coll_msg, empt_moves, food_moves,\n",
    "     prey_moves, pred_moves) = self.get_collisions(\n",
    "        moves, critter_food, critter_food_type)\n",
    "    # based on what they move onto increment/decrement scores\n",
    "    self.update_scores(move_coll_msg, critter_food,\n",
    "                       critter_food_type, prey_moves)\n",
    "    # move the pieces\n",
    "    old_pieces = self.pieces.copy()\n",
    "    self.move_pieces(critter_food, move_coll_msg, moves)\n",
    "    # eaten food and prey respawn\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      self.replace_destroyed(food_moves, old_pieces)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      self.replace_destroyed(food_moves, old_pieces)\n",
    "      self.replace_destroyed(prey_moves, old_pieces)\n",
    "    if np.any(np.sum(self.pieces, axis=(1,2)) != self.check_sum):\n",
    "      print('something went terribly wrong')\n",
    "      print(old_pieces)\n",
    "      print(critter_food)\n",
    "      print(moves)\n",
    "      print(self.pieces)\n",
    "\n",
    "\n",
    "  def get_neighbor_grc_indices(self, critter_food, radius, pad=False):\n",
    "    \"\"\"\n",
    "    Returns all grid positions within a certain cityblock distance radius from\n",
    "    the place corresponding to critter_food.\n",
    "\n",
    "    Args:\n",
    "        critter_food (int): The idex of the focal critter_food.\n",
    "        radius (int): The cityblock distance.\n",
    "        pad (bool): whether or not to pad the array, if padded all row, col\n",
    "          indexes are valid for the padded array, useful for getting percept\n",
    "          if not all indexes are correct for the original array, useful for\n",
    "          figuring out legal moves.\n",
    "\n",
    "    Returns:\n",
    "        an array of indices, each row is a g, r, c index for the neighborhoods\n",
    "        around the critters, can use the g value to know which board you are in.\n",
    "        if pad=True also returns the padded array (the indices in that case) are\n",
    "        for the padded array, so won't work on self.pieces, whereas if pad is\n",
    "        False the indices will be for the offsets in reference to the original\n",
    "        self.pieces, but note that some of these will be invalid, and will\n",
    "        need to be filtered out (as we do in get_legal)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    # Create meshgrid for offsets\n",
    "    if pad is True:\n",
    "      padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "        (radius, radius)), constant_values=self.ARRAY_PAD_VALUE)\n",
    "      batch, rows, cols = np.where(padded_arr == critter_food)\n",
    "    else:\n",
    "      batch, rows, cols = np.where(self.pieces == critter_food)\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    neighbors_rows = extended_rows + valid_row_offsets\n",
    "    neighbors_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    indices = np.column_stack((np.repeat(np.arange(batch_size),\n",
    "                                         neighbors_rows.shape[1]),\n",
    "                               neighbors_rows.ravel(),\n",
    "                               neighbors_cols.ravel()))\n",
    "    if pad is False:\n",
    "      return indices\n",
    "    elif pad is True:\n",
    "      return indices, padded_arr\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, critter_food, radius=1):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter, taking into account which moves\n",
    "    are blocking based on type.\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    critter_locs = np.array(np.where(self.pieces == critter_food))\n",
    "    # turn those row, col offsets into a set of legal offsets\n",
    "    legal_offsets = self.get_neighbor_grc_indices(critter_food, radius)\n",
    "    legal_offsets = {tuple(m_) for m_ in legal_offsets}\n",
    "\n",
    "    # Apply logic of where a successful move can be made, by which\n",
    "    # type of critter, be they food, prey, predator or something else\n",
    "    empt_mask, food_mask, prey_mask, pred_mask = self.get_type_masks()\n",
    "    critter_food_type = self.get_critter_food_type(critter_food)\n",
    "    #print(critter_food_type)\n",
    "    if critter_food_type == self.CritterFoodType.FOOD:\n",
    "      #food only drifts into empty places\n",
    "      legal_destinations = np.where(empt_mask)\n",
    "    elif critter_food_type == self.CritterFoodType.PREY:\n",
    "      legal_destinations = np.where(empt_mask | food_mask)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      legal_destinations = np.where(empt_mask | food_mask | prey_mask)\n",
    "    else:\n",
    "      raise ValueError(\"Unexpected value for critter_food_type.\")\n",
    "    legal_destinations = {tuple(coords) for coords in zip(*legal_destinations)}\n",
    "    # Add the current locations of the critters to legal_destinations\n",
    "    current_locations = {tuple(loc) for loc in critter_locs.T}\n",
    "    legal_destinations = legal_destinations.union(current_locations)\n",
    "\n",
    "    # legal moves are both legal offsets and legal destinations\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_legal_offsets(self, critter_food, radius):\n",
    "    \"\"\"\n",
    "    Identifies all legal offsets for a critter or food, so filter out moves\n",
    "    that are off the board, but does not filter out collisions that would be\n",
    "    blocking. For a random valid player likely better to use get_legal_moves,\n",
    "    but this is much quicker, because it doesn't check collision types, for\n",
    "    use by RL agents in training loops\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    batch, rows, cols = np.where(self.pieces == critter_food)\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    potential_moves_rows = extended_rows + valid_row_offsets\n",
    "    potential_moves_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    # Filter offsets that would take the critter outside the board\n",
    "    c1 = potential_moves_rows >= 0\n",
    "    c2 = potential_moves_rows <= n_rows-1\n",
    "    c3 = potential_moves_cols >= 0\n",
    "    c4 = potential_moves_cols <= n_cols-1\n",
    "    valid_move_mask = np.logical_and.reduce([c1, c2, c3, c4])\n",
    "\n",
    "    legal_offsets_rows = potential_moves_rows[valid_move_mask]\n",
    "    legal_offsets_cols = potential_moves_cols[valid_move_mask]\n",
    "    batch_indexes = np.repeat(batch, valid_row_offsets.shape[0])\n",
    "    legal_offsets = np.column_stack((batch_indexes[valid_move_mask.ravel()],\n",
    "                                     legal_offsets_rows.ravel(),\n",
    "                                     legal_offsets_cols.ravel()))\n",
    "    return legal_offsets, valid_move_mask\n",
    "\n",
    "\n",
    "  def get_perceptions(self, critter_food, radius):\n",
    "    idx, pad_pieces = self.get_neighbor_grc_indices(critter_food,\n",
    "                                                    radius, pad=True)\n",
    "    #percept_mask = np.zeros(pad_pieces.shape, dtype=bool)\n",
    "    #percept_mask[idx[:,0], idx[:,1]], idx[:,2]] = True\n",
    "    percept = pad_pieces[idx[:,0], idx[:,1], idx[:,2]]\n",
    "    return(percept.reshape(self.batch_size, -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title GridworldGame class\n",
    "#######################################################################\n",
    "# extend GridworldGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1, n_rows=7, n_cols=7,\n",
    "               num_food=10, num_prey=1, num_pred=1,\n",
    "               lifetime=30, rng=None, drift_player=None):\n",
    "    \"\"\"\n",
    "    Initializes an instance of the class with the specified parameters.\n",
    "    Args:\n",
    "      batch_size (int, optional): Number of instances in a batch. Default is 1.\n",
    "      n_rows (int, optional): Number of rows in the grid. Default is 7.\n",
    "      n_cols (int, optional): Number of columns in the grid. Default is 7.\n",
    "      num_food (int, optional): Number of food items. Default is 10.\n",
    "      num_prey (int, optional): Number of different agents running around\n",
    "        on each board in the batch eating food. Default is 1.\n",
    "      num_pred (int, optional): Number of different agents running around\n",
    "        on each board in the batch eating prey. Default is 1.\n",
    "      lifetime (int, optional): Time before critter's life ends, in terms of\n",
    "        time steps. Default is 30.\n",
    "      rng (numpy random number generator, optional): Random number generator\n",
    "        for reproducibility. If None, uses default RNG with a preset seed.\n",
    "      drift_player (player object, optional): a 'player' who moves the food\n",
    "        pieces around (drifting) if none, skip food movement\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for positive integer inputs\n",
    "    assert all(isinstance(i, int) and i >= 0\n",
    "               for i in [batch_size, n_rows, n_cols, num_food, num_prey, num_pred,\n",
    "                         lifetime]), \"All inputs must be non-negative integers.\"\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_food = num_food\n",
    "    self.num_prey = num_prey\n",
    "    self.num_pred = num_pred\n",
    "    self.num_critters = num_pred + num_prey\n",
    "    self.pred_prey_threshold = self.num_prey\n",
    "    # Check for num_food exceeding maximum possible value\n",
    "    max_food = n_rows * n_cols - self.num_critters\n",
    "    if num_food > max_food:\n",
    "      print(f'num_food is too large, setting it to maximum possible value: {max_food}')\n",
    "      num_food = max_food\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    # Set up random number generator\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "    self.drift_player = drift_player\n",
    "\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                       n_cols=self.n_cols, num_food=self.num_food,\n",
    "                       num_prey=self.num_prey, num_pred=self.num_pred,\n",
    "                       lifetime=self.lifetime, rng=self.rng)\n",
    "    return b.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_shape(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,r,c coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 0-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to r,c indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g=0):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return (board['pieces'][g].tobytes() + board['scores'][g].tobytes() +\n",
    "            board['rounds_left'][g].tobytes())\n",
    "\n",
    "\n",
    "  def get_square_symbol(self, piece):\n",
    "    \"\"\" Translate integer piece value to symbol for display\"\"\"\n",
    "    if piece == -1:\n",
    "      return \"X\"\n",
    "    elif piece == 0:\n",
    "      return \"-\"\n",
    "    elif piece >= 1:\n",
    "      return \"0\"\n",
    "    else:\n",
    "      return \"???????????????????????????\"\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g=0):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.get_square_symbol(square)\n",
    "                        for row in board['pieces'][g]\n",
    "                          for square in row])\n",
    "    board_s = board_s + '_' + str(board['scores'][g])\n",
    "    board_s = board_s + '_' + str(board['rounds_left'][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board['rounds_left'].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, \"|\", end=\"\")    # Print the row\n",
    "      for r_ in range(self.n_rows):\n",
    "        piece = board['pieces'][g,c_,r_]    # Get the piece to print\n",
    "        #print(piece)\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board['rounds_left'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g, critter_index):\n",
    "    return np.squeeze(np.array(np.where(board['pieces'][g]==critter_index)))\n",
    "\n",
    "\n",
    "  def plot_moves(self, board, player0, g=0, player1=None,\n",
    "                 fig=None, ax=None, p0_name='Player 0', p1_name='Player 1',\n",
    "                 figsize=(6,5), critter_name='Critter', title=None,\n",
    "                 deterministic=False):\n",
    "    \"\"\"\n",
    "    Uses plotting functions to make picture of the current board state, and what\n",
    "    a critter would do at each non-food location in the current board state\n",
    "    \"\"\"\n",
    "    def make_prob_dict(critter_locs, play):\n",
    "      offset_dict = {(0, 1): 'right',\n",
    "                     (0,-1): 'left',\n",
    "                     ( 1, 0): 'down',\n",
    "                     (-1, 0): 'up'}\n",
    "      index_probs = play[2].copy()\n",
    "      loc_prob_dict = {}\n",
    "      # for each non food locations\n",
    "      for g, loc_ in enumerate(critter_locs):\n",
    "        # this is the location as an r, c tuple\n",
    "        rc_tup = tuple((loc_[1], loc_[2]))\n",
    "        # the relevant probabilities\n",
    "        raw_probs = index_probs[g]\n",
    "        probs = raw_probs[raw_probs > 0]\n",
    "        indexes = np.argwhere(raw_probs > 0)\n",
    "        # turn the probability indexes into r, c coords\n",
    "        rows = np.floor_divide(indexes, gwg.n_cols)\n",
    "        cols = np.remainder(indexes, gwg.n_cols)\n",
    "        moves = np.squeeze(np.array([z for z in zip(rows, cols)]), axis=2)\n",
    "        #compute the offsets and turn them to strings\n",
    "        offsets = moves - loc_[1:]\n",
    "        str_offsets = np.array(list(map(offset_dict.get, map(tuple, offsets))))\n",
    "        # update the loc_prob_dict for plotting\n",
    "        prob_dict = dict(zip(str_offsets, probs))\n",
    "        loc_prob_dict.update({rc_tup: prob_dict})\n",
    "      return loc_prob_dict\n",
    "\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    food = plot_food(fig, ax, rc_food_plotting)\n",
    "\n",
    "    expanded_board = self.critter_everywhere_state_expansion(\n",
    "      board, player0.critter_index, to_expand=g)\n",
    "    critter_locs = np.argwhere(expanded_board['pieces']==player0.critter_index)\n",
    "    #play the expanded state\n",
    "    p0_play = player0.play(expanded_board)\n",
    "    #get the prob dict\n",
    "    p0_loc_prob_dict = make_prob_dict(critter_locs, p0_play)\n",
    "    # same for player1 if there is one\n",
    "    if player1 is not None:\n",
    "      p1_play = player1.play(expanded_board)\n",
    "      p1_loc_prob_dict = make_prob_dict(critter_locs, p1_play)\n",
    "\n",
    "    existing_handels, _ = ax.get_legend_handles_labels()\n",
    "    if player1 is None:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=0, deterministic=deterministic)\n",
    "      leg_handles = existing_handels\n",
    "    else:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=1, deterministic=deterministic, name=p0_name)\n",
    "      fig, ax, leg_handles_1 = plot_directions(fig, ax, p1_loc_prob_dict,\n",
    "        critter=2, deterministic=deterministic, name=p1_name)\n",
    "      leg_handles = existing_handels + leg_handles_0 + leg_handles_1\n",
    "\n",
    "    fig.legend(handles=leg_handles, loc=\"outside right upper\")\n",
    "    fig.canvas.draw()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war feild_of_view\n",
    "                 fov_opaque=False, #let human see trhough fog of war or not\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter',\n",
    "                 focal_critter_index = 0):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(self.num_critters):\n",
    "        critter_name = name if self.num_critters == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == self.num_critters, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(np.where(board['pieces'][g] == spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # plot field of view around the 'active player'\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque, fov=fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to get the legal moves, as set of batch, row, col triples\n",
    "    giving for the given board. Does return moves that are technically legal\n",
    "    but that will result in a blocking move, this is good for a random valid\n",
    "    player, so that the don't have a high probability of staying still if\n",
    "    there are lots of blocking moves.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                       n_cols=self.n_cols, num_food=self.num_food,\n",
    "                       num_prey=self.num_prey, num_pred=self.num_pred,\n",
    "                       lifetime=self.lifetime, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves(critter, radius)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_legal_offsets(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to the legal moves, as an array where each row is\n",
    "    a batch, row, col index giving legal moves on a given board. Includes\n",
    "    blocking moves, but excludes offsets that will take the critter off the\n",
    "    board\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                       n_cols=self.n_cols, num_food=self.num_food,\n",
    "                       num_prey=self.num_prey, num_pred=self.num_pred,\n",
    "                       lifetime=self.lifetime, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_offsets, valid_moves_mask =  b.get_legal_offsets(critter, radius)\n",
    "    return legal_offsets, valid_moves_mask\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    legal_moves =  self.get_legal_moves(board, critter, radius)\n",
    "    g, r, c = zip(*legal_moves)\n",
    "    valids = np.zeros((self.batch_size, self.n_rows * self.n_cols))\n",
    "    valids[g, np.array(r) * self.n_cols + np.array(c)] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, critter=1, g=0):\n",
    "    \"\"\"Displays possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    A=np.reshape(self.get_valid_actions(board, critter)[g],\n",
    "                 (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, \"|\", end=\"\")    # Print the row\n",
    "      for row in range(self.n_rows):\n",
    "        piece = A[col][row]    # Get the piece to print\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, critter):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                       n_cols=self.n_cols, num_food=self.num_food,\n",
    "                       num_prey=self.num_prey, num_pred=self.num_pred,\n",
    "                       lifetime=self.lifetime, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    return(b.get_perceptions(radius, critter))\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, critter, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if board['rounds_left'][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      b = GridworldBoard(batch_size=len(actions), n_rows=n_rows,\n",
    "                         n_cols=n_cols, num_food=self.num_food,\n",
    "                         num_prey=self.num_prey, num_pred=self.num_pred,\n",
    "                         lifetime=self.lifetime, rng=self.rng)\n",
    "\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        b.set_state(board)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        new_pieces = np.array([board['pieces'][ai].copy() for ai in a_indx])\n",
    "        new_scores = np.array([board['scores'][ai].copy() for ai in a_indx])\n",
    "        new_rounds_left = np.array([board['rounds_left'][ai].copy() for ai in a_indx])\n",
    "        new_active_player = copy(board['active_player'])\n",
    "        new_state = {'pieces': new_pieces,\n",
    "                     'scores': new_scores,\n",
    "                     'rounds_left': new_rounds_left,\n",
    "                     'active_player': new_active_player}\n",
    "        b.set_state(new_state)\n",
    "      moves = self.actions_to_moves(actions)\n",
    "      b.execute_moves(moves, critter)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                       n_cols=self.n_cols, num_food=self.num_food,\n",
    "                       num_prey=self.num_prey, num_pred=self.num_pred,\n",
    "                       lifetime=self.lifetime, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.critter_direction_to_move(board, offsets, critter)\n",
    "    b.execute_moves(moves, critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right'\n",
    "      critter: integer index for the critter we want moves for\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = np.where(board['pieces'] == critter)\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def direction_probs_to_flat_probs(self, board, direction_probs, critter):\n",
    "    \"\"\"\n",
    "    Converts direction probabilities in reference to the critter's location into\n",
    "    probability arrays on the flattened board.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      direction_probs: batch length list of dictionaries with keys\n",
    "        ['up', 'down', 'left', 'right'] and corresponding probabilities.\n",
    "\n",
    "    Returns:\n",
    "      probs_arrays: list of arrays, where each array is of length n_rows*n_cols\n",
    "                    and represents the flattened probability distribution for\n",
    "                    board in the batch.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {\n",
    "        'up': np.array((0, -1, 0)),\n",
    "        'down': np.array((0, 1, 0)),\n",
    "        'left': np.array((0, 0, -1)),\n",
    "        'right': np.array((0, 0, 1))}\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    probs_arrays = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for batch_index in range(batch_size):\n",
    "      prob_array = np.zeros(n_rows * n_cols)\n",
    "      for direction, prob in direction_probs[batch_index].items():\n",
    "          offset = offset_dict[direction]\n",
    "          new_loc = np.array(critter_locs)[:, batch_index] + offset\n",
    "          # Check bounces at boundaries\n",
    "          new_loc[1] = np.where(new_loc[1] >= n_rows, n_rows-2, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] >= n_cols, n_cols-2, new_loc[2])\n",
    "          new_loc[1] = np.where(new_loc[1] < 0, 1, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] < 0, 1, new_loc[2])\n",
    "          # Convert 2D location to flattened index\n",
    "          flattened_index = new_loc[1] * n_cols + new_loc[2]\n",
    "          prob_array[flattened_index] += prob\n",
    "      probs_arrays[batch_index, :] = prob_array\n",
    "    return list(probs_arrays)\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, critter, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right', 'still'\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 0): 'still',\n",
    "                   (0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up',\n",
    "                   ( 0, 0): 'still'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    critter_locs = np.column_stack(np.where(board['pieces'] == critter))\n",
    "    valid_directions = []\n",
    "    for g, batch_valid in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valid==1)[0]\n",
    "      critter_loc = critter_locs[critter_locs[:, 0] == g, 1:]\n",
    "      # critter_loc shape is (1, 2)\n",
    "      critter_loc = np.squeeze(critter_loc)\n",
    "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
    "      offsets = moves - critter_loc\n",
    "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "    Returns a batch size np.array of -1 if not ended, and scores for each game\n",
    "    in the batch if it is ended, note only returns scores if all games in the\n",
    "    batch have ended\n",
    "    \"\"\"\n",
    "    rounds_left = board['rounds_left']\n",
    "    scores = board['scores']\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores\n",
    "\n",
    "\n",
    "  def critter_everywhere_state_expansion(self, board_state,\n",
    "                                         critter=1, to_expand=0):\n",
    "    \"\"\"\n",
    "    Expand a given board state by placing a critter at each non-food location.\n",
    "\n",
    "    The function takes a game state and returns an expanded version of it. For\n",
    "    each board in the state, it creates a new version of the board for every\n",
    "    non-food location, placing a critter at that location. The scores and\n",
    "    remaining rounds are copied for each new board. The result is a new game state\n",
    "    with a larger number of boards, each representing a possible configuration\n",
    "    with a critter at a different location.\n",
    "\n",
    "    Args:\n",
    "      board_state (dict): A dictionary containing the current game state.\n",
    "      It should have the following keys:\n",
    "        - 'pieces': a 3D numpy array (batch x n_col x n_row) representing the game\n",
    "          board. -1 -> food, 0 -> empty cell, and 1 -> critter.\n",
    "        - 'scores': 1D numpy array of the score for each board in the batch.\n",
    "        - 'rounds_left': a 1D numpy array of the rounds left for\n",
    "          each board in the batch.\n",
    "      critter: integer index to place on the expanded board state\n",
    "      to_expand (list (int)): list of batch indices to have state expanded\n",
    "\n",
    "    Returns:\n",
    "      dict: A dictionary containing the expanded game state with the same keys\n",
    "        as the input. The number of boards will be larger than the input state.\n",
    "    \"\"\"\n",
    "    pieces = board_state['pieces'].copy()\n",
    "    scores = board_state['scores'].copy()\n",
    "    rounds_left = board_state['rounds_left'].copy()\n",
    "    active_player = copy.copy(board_state['active_player'])\n",
    "    # Determine non-food locations\n",
    "    non_food_locs = np.argwhere(pieces[to_expand] != -1)\n",
    "    #scrub all existing critter locations,\n",
    "    # maybe later only scrub specific critter type\n",
    "    pieces[pieces >= 1] = 0\n",
    "    # lists to store expanded states\n",
    "    expanded_pieces = []\n",
    "    expanded_scores = []\n",
    "    expanded_rounds_left = []\n",
    "    # Iterate over each non-food location\n",
    "    for i in range(non_food_locs.shape[0]):\n",
    "      # Create a copy of the board\n",
    "      expanded_board = np.copy(pieces[to_expand])\n",
    "      # Place the critter at the non-food location\n",
    "      # later consider only placing at non-food,\n",
    "      # non-other critter locs\n",
    "      expanded_board[tuple(non_food_locs[i])] = critter\n",
    "      # Add the expanded board to the list along score and rounds_left\n",
    "      expanded_pieces.append(expanded_board)\n",
    "      expanded_scores.append(scores[to_expand])\n",
    "      expanded_rounds_left.append(rounds_left[to_expand])\n",
    "    # Convert to arrays and create expanded board state\n",
    "    expanded_state = {'pieces': np.stack(expanded_pieces),\n",
    "                      'scores': np.array(expanded_scores),\n",
    "                      'rounds_left': np.array(expanded_rounds_left),\n",
    "                      'active_player': active_player}\n",
    "    return expanded_state\n",
    "\n",
    "\n",
    "  def play_game(self, players=[], collect_fov_data=False, fov_radius=2,\n",
    "                visualize = False):\n",
    "    \"\"\"This method takes a list of players the same length as num_critters,\n",
    "        and then plays a batch of games with them and returns the final board\n",
    "        state\"\"\"\n",
    "    if len(players) != self.num_critters:\n",
    "      raise ValueError(\"number of players different than expected\")\n",
    "\n",
    "    board = self.get_init_board()\n",
    "    if visualize == True:\n",
    "      self.display(board, 0)\n",
    "\n",
    "    if collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                         n_cols=self.n_cols, num_food=self.num_food,\n",
    "                         num_prey=self.num_prey, num_pred=self.num_pred,\n",
    "                         lifetime=self.lifetime, rng=self.rng)\n",
    "    for p_idx, player_ in enumerate(players):\n",
    "      if player_.critter_index != p_idx+1:\n",
    "        print(player_.critter_index)\n",
    "        print(p_idx + 1)\n",
    "        raise ValueError(\"player order does not match assigned critter index\")\n",
    "\n",
    "    for ii in range(self.lifetime):\n",
    "      for player_ in players:\n",
    "        old_scores = board['scores']\n",
    "        if collect_fov_data is True:\n",
    "          b.set_state(board)\n",
    "          percepts = b.get_perceptions(fov_radius)\n",
    "\n",
    "        a_player, _, _ = player_.play(board)\n",
    "        board = self.get_next_state(board, player_.critter_index, a_player)\n",
    "        if visualize == True:\n",
    "          self.display(board, 0)\n",
    "    return board\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title player zoo\n",
    "###########################################################################\n",
    "# make a separate player zoo\n",
    "###########################################################################\n",
    "\n",
    "class FoodDriftPlayer():\n",
    "  \"\"\"\n",
    "  A player the executes the drifting pattern of the food,\n",
    "  Treating move made by the environment as though maybe by another player as\n",
    "  a convenient coding abstraction... also just a good way to think about things\n",
    "  This will drift the food on the board based on the given offsets probabilities.\n",
    "  Collisions are handled by the execute moves logic of the board object\n",
    "\n",
    "  Parameters:\n",
    "  - offset_probs: Probabilities corresponding to each offset, note implicit\n",
    "    order dependence here\n",
    "  - wrapping, a boolean indicating whether drifting food can fall off the edge\n",
    "    of the board and re-appear on the other side of the board.\n",
    "\n",
    "\n",
    "  The play method returns a batch of moves\n",
    "    - nothing, just updates self.pieces\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=-1, offset_probs=[1.0, 0, 0, 0, 0],\n",
    "               wrapping=False, wrap_type='random'):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "          self.game.num_food  <= critter_index <= -1), \"Value is not a negative integer or exceeds the limit.\"\n",
    "    self.offset_probs = offset_probs\n",
    "    self.wrapping = wrapping\n",
    "    self.wrap_type = wrap_type\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    possible_offsets = np.array([[ 0, -1,  0], # up\n",
    "                                 [ 0,  1,  0], # down\n",
    "                                 [ 0,  0, -1], # left\n",
    "                                 [ 0,  0,  1], # right\n",
    "                                 [ 0,  0,  0]]) # still\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    # original food locations\n",
    "    food_locations = np.argwhere(board['pieces'] == self.critter_index)\n",
    "    # Sample offsets for each food location\n",
    "    num_food = food_locations.shape[0]\n",
    "    sampled_offsets = possible_offsets[self.rng.choice(\n",
    "        np.arange(possible_offsets.shape[0]),\n",
    "        size=num_food, replace=True, p=self.offset_probs)]\n",
    "    # Possible new food locations\n",
    "    p_new_locs = food_locations + sampled_offsets\n",
    "\n",
    "    #out of bounds rows and cols\n",
    "    oob_rows = (p_new_locs[:, 1] >= n_rows) | (p_new_locs[:, 1] < 0)\n",
    "    oob_cols = (p_new_locs[:, 2] >= n_cols) | (p_new_locs[:, 2] < 0)\n",
    "\n",
    "    if self.wrapping is True:\n",
    "      if self.wrap_type == 'random':\n",
    "        p_wrap_row_indexes = self.rng.choice(np.arange(n_rows),\n",
    "                                             size=num_food)\n",
    "        p_wrap_col_indexes = self.rng.choice(np.arange(n_cols),\n",
    "                                             size=num_food)\n",
    "        p_new_locs[oob_rows, 1] = p_wrap_row_indexes[oob_rows]\n",
    "        p_new_locs[oob_cols, 1] = p_wrap_col_indexes[oob_cols]\n",
    "      else:\n",
    "        #deterministic wrapping\n",
    "        p_new_locs[:, 1] = np.mod(p_new_locs[:, 1], n_rows)\n",
    "        p_new_locs[:, 2] = np.mod(p_new_locs[:, 2], n_cols)\n",
    "    else:\n",
    "      # they don't move if they hit an edge\n",
    "      p_new_locs[oob_rows, 1] = food_locations[oob_rows, 1]\n",
    "      p_new_locs[oob_cols, 2] = food_locations[oob_cols, 2]\n",
    "\n",
    "    a = p_new_locs[:,1] * n_cols + p_new_locs[:,2]\n",
    "    a_1hots = np.zeros((batch_size, n_cols*n_rows))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    return a, a_1hots, a_1hots, p_new_locs\n",
    "\n",
    "\n",
    "class RandomValidPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld, could be prey or pred... or even food\n",
    "  It leans hard on the game's get valid method and then just samples from there\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1, speed=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.speed = speed\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index, self.speed)\n",
    "    action_size = self.game.get_action_size()\n",
    "\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GeneralLinearPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a linear policy defined by the given weights. Content and\n",
    "  size of percept is parameterized, as is speed. Whether it is treated as a\n",
    "  prey or predator type by the game depends on the critter_index, specifically\n",
    "  0 < critter_index <= game.pred_prey_threshold --> prey type\n",
    "  game.pred_prey_threshold < critter_index --> predator type\n",
    "  note that many game loops re-assign critter index based on the order of the\n",
    "  player list handed to the game loop.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1, weights=None, fov_radius=1, speed=1,\n",
    "               has_food_percept = True,  has_edge_percept=False,\n",
    "               has_prey_percept = False, has_pred_percept=False,\n",
    "               get_probs=False, deterministic=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      weights: a numpy array that gives the connection strengths between the\n",
    "      'perception' neurons and the direction 'neurons'\n",
    "      fov_radius: int how far around itself the critter perceives\n",
    "      speed: int how many grid cells a critter can move in a round\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    # all critters need these things\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.get_probs = get_probs\n",
    "    # these things are specific to this kind of critter\n",
    "    self.deterministic = deterministic\n",
    "    self.fov_radius = fov_radius\n",
    "    self.speed = speed\n",
    "    self.W_out_shape = 2*self.speed**2 + 2*self.speed + 1\n",
    "    self.W_in_shape = 2*self.fov_radius**2 + 2*self.fov_radius + 1\n",
    "    self.has_food_percept = has_food_percept\n",
    "    self.has_edge_percept = has_edge_percept\n",
    "    self.has_prey_percept = has_prey_percept\n",
    "    self.has_pred_percept = has_pred_percept\n",
    "    self.W_layers = np.sum([has_food_percept, has_edge_percept,\n",
    "                            has_prey_percept, has_pred_percept])\n",
    "    if weights is None:\n",
    "      self.W = np.ones((self.W_layers, self.W_out_shape, self.W_in_shape))\n",
    "    else:\n",
    "      self.W = weights\n",
    "    if self.W.shape != (self.W_layers, self.W_out_shape, self.W_in_shape):\n",
    "      raise ValueError(\"Weights don't match expected shape given fov, speed, and percepts\")\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "\n",
    "  def direction_value_from_percept(self, percepts, W):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A batch by self.W_in_shape array representing the perceptions of\n",
    "        the organism. Indices correspond to spaces around the organism. The\n",
    "        values in the array can be -200 (out-of-bounds), 0 (empty space),\n",
    "        or negative integers (food), or positive integers below or equal\n",
    "        game.pred_prey_threshold for prey organisms or positive integers above\n",
    "        game.predy_prey_threshold for predator organisms\n",
    "      W: a W_layers x W_out_shape x W_in_shape weight matrix of parameters\n",
    "        representing the connection strengths between the perception inputs and\n",
    "        the possible output actions.\n",
    "\n",
    "    Returns:\n",
    "      output: raw output activations, these filtered by which moves are valid\n",
    "        and then softmax normalized later\n",
    "    \"\"\"\n",
    "    expanded_percepts = []\n",
    "    if self.has_food_percept:\n",
    "      x_food = np.asarray((percepts <= -1) & (percepts > -200), float) # batch x len W_in\n",
    "      expanded_percepts.append(x_food)\n",
    "    if self.has_edge_percept:\n",
    "      x_edge = np.asarray(percepts == -200, float) # batch x len W_in\n",
    "      expanded_percepts.append(x_edge)\n",
    "    if self.has_prey_percept:\n",
    "      x_prey = np.asarray((percepts > 0) &\n",
    "       (percepts <= self.game.pred_prey_threshold), float) # batch x len W_in\n",
    "      expanded_percepts.append(x_prey)\n",
    "    if self.has_pred_percept:\n",
    "      x_pred = np.asarray(percepts > self.game.pred_prey_threshold, float) # batch x len W_in\n",
    "      expanded_percepts.append(x_pred)\n",
    "    percept_stack = np.stack(expanded_percepts) # W_layers x batch x len_W_in\n",
    "\n",
    "    output_activations = np.tensordot(percept_stack, W, [[0, 2], [0, 2]])\n",
    "    # output activations is batch by W_out, so each row gives the raw output\n",
    "    # activations for that batch.\n",
    "    return output_activations\n",
    "\n",
    "\n",
    "  def play(self, board, temp=None, W=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "    if W is None:\n",
    "      W = self.W\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.critter_index,\n",
    "                                            self.fov_radius)\n",
    "    direction_v = self.direction_value_from_percept(perceptions, W)\n",
    "    #direction_v is batch x w_out, i.e. number of different moves\n",
    "    legal_offsets, valid_moves_mask = self.game.get_legal_offsets(\n",
    "        board, critter=self.critter_index, radius=self.speed)\n",
    "    flat_dv = direction_v[valid_moves_mask]\n",
    "    batch_indexes = legal_offsets[:,0]\n",
    "    # turn offsets into flat indexes, i.e. ints in range(n_rows*n_cols)\n",
    "    action_indexes = legal_offsets[:,1] * n_cols + legal_offsets[:,2]\n",
    "    # Set invalid positions to -inf\n",
    "    value_expand = np.ones((batch_size, n_rows*n_cols)) * -np.inf\n",
    "    # Fragile order dependency here in how legal offsets and flat_dv 'line up'\n",
    "    # and we can slot the direction values into value expand like this\n",
    "    value_expand[(batch_indexes, action_indexes)] = flat_dv\n",
    "\n",
    "    if self.deterministic:\n",
    "      sampled_actions = np.argmax(value_expand, axis=1)\n",
    "      a_1Hots = np.zeros((batch_size, n_rows * n_cols))\n",
    "      a_1Hots[np.arange(batch_size), sampled_actions] = 1.0\n",
    "      v_probs = a_1Hots\n",
    "    else:\n",
    "      # Subtract max for numerical stability\n",
    "      value_expand_shift = value_expand - np.max(value_expand,\n",
    "                                                 axis=1, keepdims=True)\n",
    "      # softmax temp scaling\n",
    "      value_expand_scale = value_expand_shift/temp\n",
    "      exp_value = np.exp(value_expand_scale)\n",
    "      # Normalize by the sum of the exponentiated values for each row\n",
    "      v_probs = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "      v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "      samp = self.game.rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "      sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "      a_1Hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "      a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    return sampled_actions, a_1Hots, v_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EatNearPredPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a parameterized policy defined by the given weights\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, weights=None, fov_radius=1, critter_index=2,\n",
    "               get_probs=False, deterministic=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      weights: 4 x 12 numpy array (assumes fov_radius = 2), that gives the\n",
    "        connection strengths between the 'perception' neurons and the direction\n",
    "        'neurons'\n",
    "      fov_radius: int how far around itself the critter perceives, weights is\n",
    "        expecting fov_radius = 1\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    # all critters need these things\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.get_probs = get_probs\n",
    "    # these things are specific to this kind of critter\n",
    "    self.deterministic = deterministic\n",
    "    if weights is None:\n",
    "      self.W = np.array(\n",
    "      [[20.,  0., 0.,  0.,  0.],\n",
    "       [ 0., 20., 0.,  0.,  0.],\n",
    "       [ 0.,  0., 0.,  0.,  0.],\n",
    "       [ 0.,  0., 0., 20.,  0.],\n",
    "       [ 0.,  0., 0.,  0., 20.]])\n",
    "    else:\n",
    "      self.W = weights\n",
    "    self.fov_radius = fov_radius\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "\n",
    "  def direction_value_from_percept(self, percepts, W):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A batch by 1D len 12 array representing the perceptions of the\n",
    "      organism. Indices correspond to spaces around the organism. The values in\n",
    "      the array can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "      W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "        between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "    Returns:\n",
    "      direction_probs: array of probabilities of taking each action.\n",
    "    \"\"\"\n",
    "    # a human interpretable overview of the percept structure\n",
    "    #percept_struct = [\n",
    "    #  'up', 'left', 'center', 'right',  'down']\n",
    "    # a human interpretable overview of the out structure\n",
    "    #output_struct = ['up', 'left', 'center' 'right', 'down']\n",
    "    # boolean representation of percept, no edges, no food, no other predators\n",
    "    # just 1's where prey is,\n",
    "    # x is batch x 5\n",
    "    x = np.asarray(((percepts >= 1) &\n",
    "     (percepts <= self.game.pred_prey_threshold)), int)\n",
    "    # W is 5 x 5\n",
    "    # this does the broadcasting we want\n",
    "    output_activations = (W @ x.T).T\n",
    "    # output activations is batch by 4\n",
    "    return output_activations\n",
    "\n",
    "\n",
    "  def play(self, board, temp=None, W=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "    if W is None:\n",
    "      W = self.W\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.critter_index,\n",
    "                                            self.fov_radius)\n",
    "    # note the fragile order based dependency on how legal offsets is written,\n",
    "    # and how output activations are interpreted\n",
    "    direction_v = self.direction_value_from_percept(perceptions, W)\n",
    "    flat_ds = direction_v.T.ravel()\n",
    "\n",
    "    critter_locs = np.array(np.where(board['pieces'] == self.critter_index))\n",
    "    legal_offsets = np.stack([\n",
    "    critter_locs + np.array([np.array([0, -1,  0])]*batch_size).T, # up\n",
    "    critter_locs + np.array([np.array([0,  0, -1])]*batch_size).T, # left\n",
    "    critter_locs + np.array([np.array([0,  0,  0])]*batch_size).T, # still\n",
    "    critter_locs + np.array([np.array([0,  0,  1])]*batch_size).T, # right\n",
    "    critter_locs + np.array([np.array([0,  1,  0])]*batch_size).T, # down\n",
    "    ])\n",
    "    legal_offsets = np.vstack(np.transpose(legal_offsets, (0, 2, 1)))\n",
    "\n",
    "    # conditions for offsets on the board\n",
    "    c1 = legal_offsets[:,1] >= 0\n",
    "    c2 = legal_offsets[:,1] <= n_rows-1\n",
    "    c3 = legal_offsets[:,2] >= 0\n",
    "    c4 = legal_offsets[:,2] <= n_cols-1\n",
    "    all_c = np.logical_and.reduce([c1, c2, c3, c4])\n",
    "\n",
    "    batch_indexes = legal_offsets[:,0][all_c]\n",
    "    # turn offsets into flat indexes, i.e. ints in range(n_rows*n_cols)\n",
    "    action_indexes = legal_offsets[:,1][all_c] * n_cols + legal_offsets[:,2][all_c]\n",
    "    direction_values = flat_ds[all_c]\n",
    "    # the fragile order dependency is here in that legal offsets and flat_ds\n",
    "    # 'line up' and can both be indexed by 'all_c' and we get what we want\n",
    "    value_expand = np.zeros((batch_size, n_rows*n_cols))\n",
    "    # i.e. we can slot the direction values into value expand like this\n",
    "    value_expand[(batch_indexes, action_indexes)] = direction_values\n",
    "    # valids is a batch x (n_rows * n_cols) binary np array, so we use it to\n",
    "    # index value_expand and set non-valid values to -inf\n",
    "    valids = gwg.get_valid_actions(board, self.critter_index)\n",
    "    # Set invalid positions to -inf\n",
    "    valid_value_expand = np.where(valids == 1, value_expand, -np.inf)\n",
    "    if self.deterministic:\n",
    "      sampled_actions = np.argmax(valid_value_expand, axis=1)\n",
    "      a_1Hots = np.zeros((batch_size, n_rows * n_cols))\n",
    "      a_1Hots[np.arange(batch_size), sampled_actions] = 1.0\n",
    "      v_probs = a_1Hots\n",
    "    else:\n",
    "      # Subtract max for numerical stability\n",
    "      value_expand_shift = valid_value_expand - np.max(valid_value_expand,\n",
    "                                                       axis=1, keepdims=True)\n",
    "      # softmax temp scaling\n",
    "      value_expand_scale = value_expand_shift/temp\n",
    "      exp_value = np.exp(value_expand_scale)\n",
    "      # Normalize by the sum of the exponentiated values for each row\n",
    "      v_probs = exp_value / np.sum(exp_value, axis=1, keepdims=True)\n",
    "      v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "      samp = self.game.rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "      sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "      a_1Hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "      a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    return sampled_actions, a_1Hots, v_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomDirectionPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function assigns a uniform probability to going up down left or right\n",
    "    independent of whether it is at an edge or corner or not. Then because of\n",
    "    bouncing off edges it will have a higher probability of moving away from\n",
    "    edges as opposed to along them than the random valid move player.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    action_probs = {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
    "\n",
    "    critter_oriented_moves = self.game.rng.choice(list(action_probs.keys()),\n",
    "                                                  size=(batch_size))\n",
    "    direction_probs = [action_probs] * batch_size\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs,\n",
    "                                                    self.critter_index)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarloBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Player based on Monte Carlo Algorithm\n",
    "\n",
    "  Note: Has dependencies in the gw_NN_RL.py util, namely a policy/value\n",
    "  network and the Monte Carlo class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet,\n",
    "               critter_index=1,\n",
    "               default_depth=1,\n",
    "               default_rollouts=1,\n",
    "               default_K=4,\n",
    "               default_temp=1.0,\n",
    "               random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.default_rollouts = default_rollouts\n",
    "    self.mc = MonteCarlo(self.game, self.nnet, self.default_depth)\n",
    "    self.default_K = default_K\n",
    "    self.default_temp = default_temp\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def play(self, board,\n",
    "           num_rollouts=None,\n",
    "           rollout_depth=None,\n",
    "           K=None,\n",
    "           softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Simulates a batch Monte Carlo based plays on the given board state.\n",
    "\n",
    "    Computes the probability of each valid move being played using a softmax\n",
    "    activation on the Monte Carlo based value (Q) of each action then selects a\n",
    "    move randomly for each game in the batch based on those probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if num_rollouts is None:\n",
    "      num_rollouts = self.default_rollouts\n",
    "    if rollout_depth is None:\n",
    "      rollout_depth = self.default_depth\n",
    "    if K is None:\n",
    "      K = self.default_K\n",
    "    if softmax_temp is None:\n",
    "      softmax_temp = self.default_temp\n",
    "\n",
    "    # figure out top k actions according to normalize action probability\n",
    "    # given by our policy network prediction\n",
    "    #co_pieces = board['pieces'].copy()\n",
    "    #this_critter_locs = np.where(co_pieces == self.critter_index+1)\n",
    "    #all_critter_locs = np.where(co_pieces >= 1)\n",
    "    # other critters are invisible to this player\n",
    "    #co_pieces[all_critter_locs] = 0\n",
    "    # nnet trained to see self as 1\n",
    "    #co_pieces[this_critter_locs] = 1\n",
    "    #scalar_rounds_left = board['rounds_left'][0]\n",
    "    #co_rounds_left = scalar_rounds_left // self.game.num_critters\n",
    "    #if self.critter_index-1 < scalar_rounds_left % self.game.num_critters:\n",
    "       # add an extra if we haven't had this players turn yet in the round cycle\n",
    "    #   co_rounds_left = co_rounds_left + 1\n",
    "    #co_rounds_left = np.array([co_rounds_left]*batch_size)\n",
    "    #pis, vs = self.nnet.predict(co_pieces,\n",
    "    #                            board['scores'][:,self.critter_index-1],\n",
    "    #                            co_rounds_left)\n",
    "    pis, vs = self.mc.pis_vs_from_board(board, self.critter_index)\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    masked_pis = pis * valids  # Masking invalid moves\n",
    "    sum_pis = np.sum(masked_pis, axis=1)\n",
    "    num_valid_actions = np.sum(valids, axis=1)\n",
    "    effective_topk = np.array(np.minimum(num_valid_actions, K), dtype= int)\n",
    "    probs = np.array([masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "                      else valid / valid.sum()\n",
    "                      for valid, masked_pi in zip(valids, masked_pis)])\n",
    "    partioned = np.argpartition(probs,-effective_topk)\n",
    "    topk_actions = [partioned[g,-(ii+1)]\n",
    "                      for g in range(batch_size)\n",
    "                        for ii in range(effective_topk[g])]\n",
    "    topk_actions_index = [ii\n",
    "                            for ii, etk in enumerate(effective_topk)\n",
    "                              for _ in range(etk)]\n",
    "    values = np.zeros(len(topk_actions))\n",
    "    # Do some rollouts\n",
    "    for _ in range(num_rollouts):\n",
    "      values = values + self.mc.simulate(board, topk_actions,\n",
    "                                         topk_actions_index,\n",
    "                                         critter=self.critter_index,\n",
    "                                         depth=rollout_depth)\n",
    "    values = values / num_rollouts\n",
    "\n",
    "    value_expand = np.zeros((batch_size, n_rows*n_cols))\n",
    "    value_expand[(topk_actions_index, topk_actions)] = values\n",
    "    value_expand_shift = value_expand - np.max(value_expand, axis=1, keepdims=True)\n",
    "    value_expand_scale = value_expand_shift/softmax_temp\n",
    "    v_probs = np.exp(value_expand_scale) / np.sum(\n",
    "        np.exp(value_expand_scale), axis=1, keepdims=True)\n",
    "    v_probs = v_probs * valids\n",
    "    v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "    samp = self.rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "    sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "    a_1Hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    return sampled_actions, a_1Hots, v_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleRulePlayer():\n",
    "  \"\"\"\n",
    "  A Predator Player based on the following simple policy:\n",
    "  If there is any prey immediately nearby move towards it,\n",
    "  otherwise move to a random valid location.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, fov_radius=1, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.fov_radius = fov_radius\n",
    "\n",
    "\n",
    "  def simple_action_from_percept(self, percept):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A 1D array (len 4 if fov_radius = 1)representing the perception\n",
    "        of the organism. Indices correspond to spaces around the organism. The\n",
    "        values in the array can be -200 (out-of-bounds), 0 (empty space),\n",
    "        negative integers > -200 (food),\n",
    "        positive integers <= self.game.pred_prey_threshold are prey\n",
    "        positive integers > self.game.pred_prey_threshold are predators\n",
    "\n",
    "    Returns:\n",
    "      action: a str, one of 'up', 'down', 'left', 'right'. If food in one or\n",
    "        more of the spaces immediately beside the organism, the function will\n",
    "        return a random choice among these directions. If there is no food\n",
    "        nearby, the function will return a random direction.\n",
    "    \"\"\"\n",
    "    # a human interpretable overview of the percept structure if fov = 2\n",
    "    percept_struct = ['up', 'left', 'right', 'down']\n",
    "    # Defines directions corresponding to different perception indices\n",
    "    direction_struct = [\n",
    "      'None', 'None', 'up', 'None',\n",
    "      'None', 'left', 'right', 'None',\n",
    "      'None', 'down', 'None', 'None']\n",
    "    # these are what count as nearby in the percept\n",
    "    nearby_directions = ['near up', 'near left', 'near right', 'near down']\n",
    "    # Get the corresponding indices in the percept array\n",
    "    nearby_indices = [percept_struct.index(dir_) for dir_ in nearby_directions]\n",
    "    # Identify the directions where food is located\n",
    "    food_indices = [index for index in nearby_indices if percept[index] <= -1]\n",
    "    food_directions = [direction_struct[index] for index in food_indices]\n",
    "\n",
    "    action_probs = {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
    "    if len(food_directions) > 0:  # If there is any food nearby\n",
    "      # If there is any food nearby randomly choose a direction with food\n",
    "      action = self.game.rng.choice(food_directions)  # Move towards a random one\n",
    "      for direction in food_directions:\n",
    "        action_probs[direction] = 1.0 /len(food_directions)\n",
    "    else:\n",
    "      # If there is no food nearby, move randomly\n",
    "      action = self.game.rng.choice(['up', 'down', 'left', 'right'])\n",
    "      for direction in ['up', 'down', 'left', 'right']:\n",
    "        action_probs[direction] = 0.25\n",
    "\n",
    "    return action, action_probs\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius,\n",
    "                                            self.critter_index)\n",
    "\n",
    "    critter_oriented_moves = []\n",
    "    direction_probs = []\n",
    "    for g in range(batch_size):\n",
    "      action, action_probs = self.simple_action_from_percept(perceptions[g])\n",
    "      critter_oriented_moves.append(action)\n",
    "      direction_probs.append(action_probs)\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                direction_probs,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PerceptParamPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a parameterized policy defined by the given weights\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, weights=None, fov_radius=2, critter_index=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      weights: 4 x 12 numpy array (assumes fov_radius = 2), that gives the\n",
    "        connection strengths between the 'perception' neurons and the direction\n",
    "        'neurons'\n",
    "      fov_radius: int how far around itself the critter perceives, weights is\n",
    "        expecting fov_radius = 2\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    if weights is None:\n",
    "      self.W = np.array(\n",
    "      [[1., 1., 4., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 4., 1., 1.],\n",
    "       [0., 1., 0., 0., 1., 4., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 4., 1., 0., 0., 1., 0.]])\n",
    "    else:\n",
    "      self.W = weights\n",
    "    self.fov_radius = fov_radius\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "\n",
    "  def param_action_from_percept(self, percept, valid_directions, W,\n",
    "                                softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A 1D len 12 array representing the perception of the organism.\n",
    "        Indices correspond to spaces around the organism. The values in the\n",
    "        array can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "      W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "        between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "    Returns:\n",
    "      direction: a str, one of 'up', 'down', 'left', 'right'. If food in one or\n",
    "        more of the spaces immediately beside the organism, the function will\n",
    "        return a random choice among these directions. If there is no food\n",
    "        nearby, the function will return a random direction.\n",
    "      direction_probs: dictionary with probabilities of taking each action.\n",
    "    \"\"\"\n",
    "    if len(valid_directions) == 0:\n",
    "      # if there is no where legit to move, stay put\n",
    "      return 'still', {direction: 0 for direction in output_struct}\n",
    "\n",
    "    if softmax_temp is None:\n",
    "      # very low temp, basically deterministic for this range of values\n",
    "      softmax_temp = self.default_softmax_temp\n",
    "    # a human interpretable overview of the percept structure\n",
    "    percept_struct = [\n",
    "      'far up', 'left up', 'near up', 'right up',\n",
    "      'far left', 'near left', 'near right', 'far right',\n",
    "      'left down', 'near down', 'right down', 'far down']\n",
    "    # a human interpretable overview of the out structure\n",
    "    output_struct = ['up', 'down', 'left', 'right']\n",
    "    # boolean representation of percept, no edges, just 1's where food is,\n",
    "    # zero otherwise, also means other organisms are invisible\n",
    "    x = np.asarray(percept <= -1, int)\n",
    "    output_activations = W @ x\n",
    "\n",
    "    # softmax shift by max, scale by temp\n",
    "    shift_scale_ex = np.exp((output_activations -\n",
    "                             np.max(output_activations))/softmax_temp)\n",
    "    # set invalid direction activations to zero\n",
    "    invalid_directions = [direction for direction in output_struct\n",
    "                           if direction not in valid_directions]\n",
    "    invalid_indices = [output_struct.index(direction)\n",
    "                        for direction in valid_directions]\n",
    "    sm = shift_scale_ex / shift_scale_ex.sum() #normalized\n",
    "    # set invalid direction probabilities to zero\n",
    "    invalid_directions = [direction for direction in output_struct\n",
    "                           if direction not in valid_directions]\n",
    "    invalid_indices = [output_struct.index(direction)\n",
    "                        for direction in invalid_directions]\n",
    "    sm[invalid_indices] = 0\n",
    "    probs_sm = sm / sm.sum(axis=0) #re-normalized again for fp issues\n",
    "    direction = self.game.rng.choice(output_struct, p=probs_sm)\n",
    "    direction_probs = {direction: prob\n",
    "                        for direction, prob in zip(output_struct, probs_sm)}\n",
    "    return direction, direction_probs\n",
    "\n",
    "\n",
    "  def play(self, board, temp=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius,\n",
    "                                            self.critter_index)\n",
    "    critter_oriented_moves = []\n",
    "    direction_probs = []\n",
    "\n",
    "    # Get valid actions for each game in the batch\n",
    "    valid_directions = self.game.get_valid_directions(board, self.critter_index)\n",
    "    for g in range(batch_size):\n",
    "      direction, batch_direction_probs = self.param_action_from_percept(\n",
    "        perceptions[g], valid_directions[g], self.W, softmax_temp=temp)\n",
    "      critter_oriented_moves.append(direction)\n",
    "      direction_probs.append(batch_direction_probs)\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs, self.critter_index)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title interactive gridworld widgets\n",
    "\n",
    "########################################\n",
    "# widgets refactor for multi-critter\n",
    "#########################################\n",
    "# Interactive Gridworld Game Widgets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None, has_fov=False,\n",
    "               radius=2, fov_opaque=False, collect_fov_data=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human']):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the gridworld_game.num_critters\n",
    "        attribute. Order should align with critter_name.\n",
    "\n",
    "      Note: fov is going to look pretty janky with more than one player, maybe\n",
    "      we get fov to only turn on for the 'active' player?\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.active_player_index = 0\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.gwg.num_critters):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] #implict check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        # player objects expected to have a critter_index attribute\n",
    "        # we set it appropriately here so it aligns with the players list\n",
    "        # used to create the widget\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    if self.collect_fov_data is True:\n",
    "      # keep raw records of percept and eating for manipulation later\n",
    "      self.percept_eat_records = []\n",
    "      # keep data in contingency table of how many food items were in\n",
    "      # the percept, and whether or not food was eaten\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.fov_eat_table_display = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='25.0em', min_height='18.8em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if self.collect_fov_data and self.any_human_players:\n",
    "      self.legend_type = None # don't keep regenerating the legend\n",
    "      # do legend separately if showing observations and no human player\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          legend_type='separate', figsize=self.figsize, has_fov=self.has_fov,\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque)\n",
    "    elif len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                fov_opaque=self.fov_opaque,\n",
    "                                radius=self.radius, figsize=self.figsize)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "      if len(self.players) == 1:\n",
    "        #one human player\n",
    "        self.output_and_score = widgets.HBox([self.scoreboard, self.output])\n",
    "        self.no_table_final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                                  self.output_and_score])\n",
    "        if self.collect_fov_data == True:\n",
    "          # a single human player collecting data\n",
    "          self.final_display = widgets.HBox([self.no_table_final_display,\n",
    "                                           self.fov_eat_table_display])\n",
    "        else: # self.collect_fov_data == False:\n",
    "          # a single human player not collecting data\n",
    "          self.final_display = self.no_table_final_display\n",
    "      else:\n",
    "        # more than one player, one of them human\n",
    "        self.V_board_output = widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # player is some kind of ai\n",
    "      if self.collect_fov_data == True:\n",
    "        # an ai player with recording\n",
    "        # in this case legend is separate\n",
    "        self.V_score_start_output_legend = widgets.VBox([self.scoreboard,\n",
    "          self.start_button,  self.output, self.b_fig_legend.canvas])\n",
    "        self.V_board_table = widgets.VBox([self.b_fig.canvas,\n",
    "                                           self.fov_eat_table_display])\n",
    "        self.final_display = widgets.HBox([self.V_board_table,\n",
    "                                           self.V_score_start_output_legend])\n",
    "      else:\n",
    "        if len(self.players) == 1:\n",
    "          # an ai player without recording\n",
    "          self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "          self.final_display = widgets.VBox([\n",
    "            self.b_fig.canvas, self.H_score_output_start])\n",
    "        else:\n",
    "          # more than one ai player\n",
    "          self.V_board_output = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "          self.V_scoreboard_start_legend = widgets.VBox([\n",
    "              self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.fov_eat_table_display:\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    # index of players is 0 through num_critter-1,\n",
    "    # same player represented by value of index + 1 in\n",
    "    old_scores = old_board['scores'][0]\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                         self.gwg.num_food, self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "      b.set_state(old_board)\n",
    "      percept = b.get_perceptions(self.radius)[0]\n",
    "\n",
    "    if (isinstance(self.players[self.active_player_index], str) and\n",
    "        'human' in self.players[self.active_player_index]):\n",
    "      direction = which_button\n",
    "    else:\n",
    "      a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
    "      # print(a_player)\n",
    "      a_player = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                      self.active_player_index+1,\n",
    "                                                      a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = a_player[0]\n",
    "\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "          self.board_state, self.active_player_index+1, [direction])\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = np.floor(self.gwg.lifetime -\n",
    "                         rounds_left / self.gwg.num_critters)\n",
    "    if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
    "      #eating happened\n",
    "      eating_string = \"They ate the food/prey there!\"\n",
    "      did_eat = 1\n",
    "    else: #eating didn't happen\n",
    "      eating_string = \"There's no food/prey there.\"\n",
    "      did_eat = 0\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0,\n",
    "                                       self.active_player_index+1)\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             radius=self.radius, legend_type=self.legend_type)\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = {'perception': percept.copy(),\n",
    "                  'state': old_board,\n",
    "                  'did_eat': bool(did_eat)}\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1) # number of food items in FoV\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        print(\"The critter (tried) to move \" + direction +\n",
    "              \" and is now at ({}, {}).\".format(row,col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\\nFood Per Move: {:.2f}\".format(\n",
    "            rounds_left, new_scores[self.active_player_index],\n",
    "            new_scores[self.active_player_index] / num_moves))\n",
    "      else: # more than one players\n",
    "        print(\"Critter {} (tried) to move \".format(self.active_player_index+1) +\n",
    "              direction +\n",
    "              \" and is now at ({}, {}).\".format(row, col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\".format(\n",
    "            rounds_left, new_scores))\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_scores)\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores))\n",
    "        print('Resetting the board for another game')\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                               self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                               has_fov=self.has_fov, radius=self.radius,\n",
    "                               fov_opaque=self.fov_opaque,\n",
    "                               legend_type=self.legend_type)\n",
    "    with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.players) == 1:\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = [\n",
    "              ['High Score:', str(np.max(np.array(self.final_scores)))],\n",
    "              ['Last Score:', str(self.final_scores[-1])],\n",
    "              ['Average Score',\n",
    "              '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "          else:\n",
    "            table = [['High Score:', '--'],\n",
    "                     ['Last Score:', '--'],\n",
    "                     ['Average Score:', '--']]\n",
    "          print(tabulate(table))\n",
    "        else: # len(self.players) > 1\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = []\n",
    "            # Assuming the batch size is 1 for now\n",
    "            current_scores = self.final_scores[-1]\n",
    "            max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "            average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "            table.append(['High Scores:'] +\n",
    "              [str(score) for score in max_scores])\n",
    "            table.append(['Last Scores:'] +\n",
    "              [str(score) for score in current_scores])\n",
    "            table.append(['Average Scores:'] +\n",
    "              ['{:.2f}'.format(score) for score in average_scores])\n",
    "          else:\n",
    "            table = [\n",
    "              ['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "          print(tabulate(table, headers=headers))\n",
    "\n",
    "    with self.fov_eat_table_display:\n",
    "      clear_output()\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "\n",
    "  def human_ai_player_loop(self, direction):\n",
    "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
    "    # Execute the move of the human who clicked the button\n",
    "    self.button_output_update(direction)\n",
    "    # Move to the next player\n",
    "    def update_player_and_rounds():\n",
    "      \"\"\"Update the player index and decrement rounds if a full loop is completed.\"\"\"\n",
    "      self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "      if self.active_player_index == 0:\n",
    "        self.board_state['rounds_left'] -= 1\n",
    "    update_player_and_rounds()\n",
    "    # Do AI moves if there are any\n",
    "    while self.players[self.active_player_index] != 'human':\n",
    "      self.button_output_update('tbd')\n",
    "      # Move to the next player\n",
    "      update_player_and_rounds()\n",
    "    # Next player is human turn buttons on for them\n",
    "    self.enable_direction_buttons()\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    for ii in range(self.gwg.lifetime*self.gwg.num_critters):\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.2)\n",
    "    self.start_button.disabled = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Head2HeadGridworld():\n",
    "  \"\"\"\n",
    "  A widget for interacting with a gridworld game while an artificial player\n",
    "  plays on an identical board or watching two artificial players play, again\n",
    "  with identical starting positions (though RNG not synched between the two\n",
    "  boards, so not like duplicate bridge). We are not going to worry about having\n",
    "  more than 1 critter type playing in head to head, (maybe we will to talk\n",
    "  about cooperation... maybe).\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None,\n",
    "               player0='human', p0_short_name='YOU', p0_long_name=None,\n",
    "               player1=None, p1_short_name='THEM', p1_long_name=None,\n",
    "               has_fov=False, radius=2, fov_opaque=False,\n",
    "               collect_fov_data=False, critter_name='Critter',\n",
    "               figsize=(5,4.5), has_temp_slider=False):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batch_size of 2\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      player0: object with a play method that takes a board state\n",
    "        as an argument and returns a move. If none will use a random player\n",
    "        if the special string 'human' is passed make arrow keys for that player\n",
    "      player1: same deal as player0, never more than 1 human player\n",
    "      has_fov: bool, whether or not to display field of view around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "    \"\"\"\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.final_scores = []\n",
    "    self.player0 = player0\n",
    "    self.p0_short_name = p0_short_name\n",
    "    self.p0_long_name = p0_long_name\n",
    "    self.player1 = player1\n",
    "    self.p1_short_name = p1_short_name\n",
    "    self.p1_long_name = p1_long_name\n",
    "    self.no_human = True\n",
    "    if self.player0 == 'human':\n",
    "      assert self.player1 != 'human'\n",
    "      self.no_human = False\n",
    "    if self.player1 == 'human':\n",
    "      assert self.player0 != 'human'\n",
    "      self.no_human = False\n",
    "    self.p0_next_move = None\n",
    "    self.p1_next_move = None\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.critter_name = critter_name\n",
    "    self.figsize = figsize\n",
    "    if player0 is None:\n",
    "      self.player0 = RandomValidPlayer(self.gwg)\n",
    "    else:\n",
    "      self.player0 = player0\n",
    "    if player1 is None:\n",
    "      self.player1 = RandomValidPlayer(self.gwg)\n",
    "    else:\n",
    "      self.player1 = player1\n",
    "    self.has_temp_slider = has_temp_slider\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      self.percept_eat_records = []\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    #print(self.board_state)\n",
    "\n",
    "    # both players have same starting board\n",
    "    self.board_state['pieces'][1] = self.board_state['pieces'][0].copy()\n",
    "\n",
    "    # Initialize widgets and buttons\n",
    "    if self.has_temp_slider:\n",
    "      self.sft_slider_label = widgets.Label(value='Softmax Temperature')\n",
    "      self.sft_slider = widgets.FloatSlider(value=1.0, min=0.05,\n",
    "                                            max=5.0, step=0.05)\n",
    "      self.softmax_temp_slider = widgets.VBox([self.sft_slider_label,\n",
    "                                               self.sft_slider])\n",
    "    self.output0 = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.output1 = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='20em', max_width='21em', min_height='6.3em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "                                    layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "                                      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "                                      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "                                       layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em', margin='0.6em 0 0 0'))  # 0.6em top margin\n",
    "\n",
    "    #set up buttons and outputs and layouts\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    plt.ioff()\n",
    "    (self.b_fig0, self.b_ax0, self.b_crit_specs0, self.b_food0, self.b_fov0,\n",
    "     self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "        self.board_state, g=0, legend_type='separate', figsize=self.figsize,\n",
    "        has_fov=self.has_fov, radius=self.radius, fov_opaque=self.fov_opaque,\n",
    "        name=self.critter_name, title=self.p0_long_name)\n",
    "    (self.b_fig1, self.b_ax1, self.b_crit_specs1, self.b_food1, self.b_fov1\n",
    "     ) = self.gwg.plot_board(self.board_state, g=1, legend_type=None,\n",
    "                             figsize=self.figsize, has_fov=self.has_fov,\n",
    "                             radius=self.radius, fov_opaque=self.fov_opaque,\n",
    "                             title=self.p1_long_name)\n",
    "    # player 0 is human\n",
    "    self.board_buttons_and_output0 = widgets.VBox(\n",
    "      [self.b_fig0.canvas, self.buttons, self.output0])\n",
    "    # player 1 is human\n",
    "    self.board_buttons_and_output1 = widgets.VBox(\n",
    "      [self.b_fig1.canvas, self.buttons, self.output1])\n",
    "    # non human players\n",
    "    self.board_and_output0 = widgets.VBox([self.b_fig0.canvas, self.output0])\n",
    "    self.board_and_output1 = widgets.VBox([self.b_fig1.canvas, self.output1])\n",
    "\n",
    "    self.legend_and_scores = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                           self.scoreboard])\n",
    "    if self.has_temp_slider:\n",
    "      self.legend_scores_start = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                               self.scoreboard,\n",
    "                                               self.softmax_temp_slider,\n",
    "                                               self.start_button])\n",
    "    else:\n",
    "      self.legend_scores_start = widgets.VBox([self.b_fig_legend.canvas,\n",
    "                                               self.scoreboard,\n",
    "                                               self.start_button])\n",
    "    if self.player0 == 'human':\n",
    "      self.final_display = widgets.HBox([self.board_buttons_and_output0,\n",
    "                                         self.legend_and_scores,\n",
    "                                         self.board_and_output1])\n",
    "    elif self.player1 == 'human':\n",
    "      self.final_display = widgets.HBox([self.board_and_output0,\n",
    "                                         self.legend_and_scores,\n",
    "                                         self.board_buttons_and_output1])\n",
    "    else: # no human player\n",
    "      self.final_display = widgets.HBox([self.board_and_output0,\n",
    "                                          self.legend_scores_start,\n",
    "                                          self.board_and_output1])\n",
    "    # initial text outputs\n",
    "    # if there's a temp slider check who, if anyone uses it\n",
    "    self.p0_uses_temp = False\n",
    "    self.p1_uses_temp = False\n",
    "    if self.has_temp_slider:\n",
    "      if self.player0=='human':\n",
    "        pass\n",
    "      else:\n",
    "        try:\n",
    "          _ = self.player0.play(self.board_state, temp=1.0)\n",
    "          self.p0_uses_temp = True\n",
    "        except TypeError: pass\n",
    "      if self.player1 == 'human':\n",
    "        pass\n",
    "      else:\n",
    "        try:\n",
    "          _ = self.player1.play(self.board_state, temp=1.0)\n",
    "          self.p1_uses_temp = True\n",
    "        except TypeError: pass\n",
    "      if not self.p0_uses_temp and not self.p1_uses_temp:\n",
    "        with self.output0:\n",
    "          print(\"Warning: neither player supports temperature adjustment. \"\n",
    "                \"The slider will have no effect.\")\n",
    "    with self.output0:\n",
    "      if self.no_human == False:\n",
    "        print('Click a button to start.')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.scoreboard:\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Avg. Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    old_scores = old_board['scores'][:,0] #both batches only one critter type\n",
    "    self.active_player = old_board['active_player']\n",
    "    self.disable_buttons()\n",
    "    if self.player0 == 'human':\n",
    "      a_player0 = which_button\n",
    "    else:\n",
    "      if self.p0_next_move is not None:\n",
    "        a_player0_ = self.p0_next_move\n",
    "        self.p0_next_move = None\n",
    "      else:\n",
    "        with self.output0:\n",
    "          print(\"AI is thinking...\")\n",
    "        if self.p0_uses_temp:\n",
    "          a_player0_, _, _ = self.player0.play(old_board,\n",
    "                                               temp=self.sft_slider.value)\n",
    "        else:\n",
    "          a_player0_, _, _ = self.player0.play(old_board)\n",
    "      a_player0_ = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                        self.active_player+1,\n",
    "                                                        a_player0_)\n",
    "      a_player0 = a_player0_[0]\n",
    "    if self.player1 == 'human':\n",
    "      a_player1 = which_button\n",
    "    else:\n",
    "      if self.p1_next_move is not None:\n",
    "        a_player1_ = self.p1_next_move\n",
    "        self.p1_next_move = None\n",
    "      else:\n",
    "        with self.output1:\n",
    "          print(\"AI is thinking...\")\n",
    "        if self.p1_uses_temp:\n",
    "          a_player1_, _, _ = self.player1.play(old_board,\n",
    "                                               temp=self.sft_slider.value)\n",
    "        else:\n",
    "          a_player1_, _, _ = self.player1.play(old_board)\n",
    "      a_player1_ = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                        self.active_player+1,\n",
    "                                                        a_player1_)\n",
    "      a_player1 = a_player1_[1]\n",
    "    self.enable_buttons()\n",
    "\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "        self.board_state, self.active_player+1, [a_player0, a_player1])\n",
    "\n",
    "    # Try to precompute next AI player move(s) if there are any rounds left\n",
    "    if self.board_state['rounds_left'][0] > 0:\n",
    "      if self.player0 != 'human':\n",
    "        if self.p0_uses_temp:\n",
    "          self.p0_next_move, _, _ = self.player0.play(\n",
    "            self.board_state, temp=self.sft_slider.value)\n",
    "        else:\n",
    "          self.p0_next_move, _, _ = self.player0.play(self.board_state)\n",
    "      if self.player1 != 'human':\n",
    "        if self.p1_uses_temp:\n",
    "          self.p1_next_move, _, _ = self.player1.play(\n",
    "            self.board_state, temp=self.sft_slider.value)\n",
    "        else:\n",
    "          self.p1_next_move, _, _ = self.player1.play(self.board_state)\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                         self.gwg.num_food, self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "      b.set_state(old_board)\n",
    "      percept = b.get_perceptions(self.radius)\n",
    "\n",
    "    new_scores = self.board_state['scores'][:,0] #both batches one critter type\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = self.gwg.lifetime - rounds_left\n",
    "\n",
    "    if new_scores[0] > old_scores[0]:\n",
    "      eating_string0 = \"They ate the food there!\"\n",
    "    else:\n",
    "      eating_string0 = \"There's no food there.\"\n",
    "    if new_scores[1] > old_scores[1]:\n",
    "      eating_string1 = \"They ate the food there!\"\n",
    "    else:\n",
    "      eating_string1 = \"There's no food there.\"\n",
    "    did_eat = int(new_scores[0] > old_scores[0])\n",
    "\n",
    "    row0, col0 = self.gwg.get_critter_rc(self.board_state, 0, 1)\n",
    "    (self.b_fig0, self.b_ax0, self.b_crit_specs0, self.b_food0, self.b_fov0\n",
    "     ) = self.gwg.plot_board(self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "                             self.b_crit_specs0, self.b_food0, self.b_fov0,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             legend_type=None)\n",
    "    row1, col1 = self.gwg.get_critter_rc(self.board_state, 1, 1)\n",
    "    (self.b_fig1, self.b_ax1, self.b_crit_specs1, self.b_food1, self.b_fov1\n",
    "     ) = self.gwg.plot_board(self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "                             self.b_crit_specs1, self.b_food1, self.b_fov1,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             legend_type=None)\n",
    "\n",
    "    with self.output0:\n",
    "      clear_output()\n",
    "      if self.player0 == 'human':\n",
    "        print(\"You clicked the \" + which_button +\n",
    "              \" button and your critter is now at ({}, {}).\".format(row0,col0))\n",
    "      else:\n",
    "        print(\"This player (tried) to move \" + a_player0 +\n",
    "              \" and is now at ({}, {}).\".format(row0,col0))\n",
    "      print(eating_string0)\n",
    "      print(\"Rounds Left: {} \\nFood Eaten: {} \\nFood Per Move: {:.2f}\".format(\n",
    "          rounds_left, new_scores[0], new_scores[0] / num_moves))\n",
    "    with self.output1:\n",
    "      clear_output()\n",
    "      if self.player1 == 'human':\n",
    "        print(\"You clicked the \" + which_button +\n",
    "              \" button and your critter is now at ({}, {}).\".format(row1,col1))\n",
    "      else:\n",
    "        print(\"This player (tried) to move \" + a_player1 +\n",
    "              \" and is now at ({}, {}).\".format(row1,col1))\n",
    "      print(eating_string1)\n",
    "      print(\"Rounds Left: {} \\nFood Eaten: {} \\nFood Per Move: {:.2f}\".format(\n",
    "        rounds_left, new_scores[1], new_scores[1] / num_moves))\n",
    "\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = (percept.copy(), did_eat, old_board)\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1, axis=1)\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_scores)\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "      self.board_state['pieces'][1] = self.board_state['pieces'][0].copy()\n",
    "      (self.b_fig0, self.b_ax0, self.b_crit_specs0, self.b_food0, self.b_fov0\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig0, self.b_ax0,\n",
    "                              self.b_crit_specs0, self.b_food0, self.b_fov0,\n",
    "                              has_fov=self.has_fov, radius=self.radius,\n",
    "                              fov_opaque=self.fov_opaque,\n",
    "                              legend_type=None)\n",
    "      (self.b_fig1, self.b_ax1, self.b_crit_specs1, self.b_food1, self.b_fov1\n",
    "       ) = self.gwg.plot_board(self.board_state, 1, self.b_fig1, self.b_ax1,\n",
    "                               self.b_crit_specs1, self.b_food1, self.b_fov1,\n",
    "                               has_fov=self.has_fov, radius=self.radius,\n",
    "                               fov_opaque=self.fov_opaque,\n",
    "                               legend_type=None)\n",
    "      with self.output0:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores[0]))\n",
    "        print('Resetting the board for another game')\n",
    "      with self.output1:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores[1]))\n",
    "        print('Resetting the board for another game')\n",
    "    with self.scoreboard:\n",
    "      clear_output()\n",
    "      self.b_fig_legend.canvas.draw()\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      if len(self.final_scores) > 0:\n",
    "        table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', str(np.max(np.array(self.final_scores)[:,0])),\n",
    "                          str(np.max(np.array(self.final_scores)[:,1]))],\n",
    "          ['Last Score:', str(self.final_scores[-1][0]),\n",
    "                          str(self.final_scores[-1][1])],\n",
    "          ['Average Score',\n",
    "            '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,0])),\n",
    "            '{:.2f}'.format(np.mean(np.array(self.final_scores)[:,1]))]]\n",
    "      else:\n",
    "        table = [['', self.p0_short_name, self.p1_short_name],\n",
    "          ['High Score:', '--', '--'],\n",
    "          ['Last Score:', '--', '--'],\n",
    "          ['Average Score:', '--', '--']]\n",
    "      print(tabulate(table))\n",
    "\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.button_output_update('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.button_output_update('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.button_output_update('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.button_output_update('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    if self.has_temp_slider:\n",
    "      self.softmax_temp_slider.disabled = True\n",
    "    for ii in range(self.gwg.lifetime):\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.2)\n",
    "    self.start_button.disabled = False\n",
    "    if self.has_temp_slider:\n",
    "      self.softmax_temp_slider.disabled = False\n",
    "\n",
    "  def disable_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "\n",
    "  def enable_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ExploreWeightsWidget:\n",
    "  def __init__(self, game):\n",
    "    self.game = game\n",
    "    self.n_rows, self.n_cols = 4, 12  # four directions, twelve fov cells\n",
    "    self.row_labels = ['Up', 'Down', 'Left', 'Right']\n",
    "    self.col_labels = [\n",
    "      'Far<br>Up', 'Left<br>Up', 'Near<br>Up', 'Right<br>Up',\n",
    "      'Far<br>Left', 'Near<br>Left', 'Near<br>Right', 'Far<br>Right',\n",
    "      'Left<br>Down', 'Near<br>Down', 'Right<br>Down', 'Far<br>Down'\n",
    "      ]\n",
    "\n",
    "    # Create column headers\n",
    "    col_header = [widgets.Label(value='', layout=widgets.Layout(width='50px'))] + \\\n",
    "                 [widgets.HTML(value=label, layout=widgets.Layout(width='60px', min_height='60px')) for label in self.col_labels]\n",
    "\n",
    "    self.text_fields = [widgets.HBox(col_header)]\n",
    "\n",
    "    for label in self.row_labels:\n",
    "      row_fields = [widgets.FloatText(value=0.0, step=0.1, layout=widgets.Layout(width='60px'))\n",
    "                    for _ in range(self.n_cols)]\n",
    "      row_label = widgets.Label(value=f\"{label}:\", layout=widgets.Layout(width='50px'))\n",
    "      self.text_fields.append(widgets.HBox([row_label] + row_fields))\n",
    "\n",
    "    # Create a button to start the game\n",
    "    self.run_button = widgets.Button(description=\"Run Game\")\n",
    "    self.run_button.on_click(self.run_game)\n",
    "\n",
    "    # set up fig and create placeholders for vertical lines and histograms\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    self.current_color = colors[0]\n",
    "    self.best_color = colors[1]\n",
    "    self.prev_color = colors[2]\n",
    "    self.fig, self.ax = plt.subplots(figsize=(6,4))\n",
    "    self.ax.set_xlim([0,25])\n",
    "    self.ax.set_ylim([0,1])\n",
    "    remove_ip_clutter(self.fig)\n",
    "    self.current_avg_line = self.ax.axvline(-1, color=self.current_color,\n",
    "                                            linestyle='dashed', linewidth=3,\n",
    "                                            label='Current')\n",
    "    self.prev_avg_line = self.ax.axvline(-1, color=self.prev_color,\n",
    "                                         linestyle='dashed', linewidth=3,\n",
    "                                         label='Previous')\n",
    "    self.best_avg_line = self.ax.axvline(-1, color=self.best_color,\n",
    "                                         linestyle='dashed', linewidth=3,\n",
    "                                         label='Best')\n",
    "    if self.game.batch_size > 1:\n",
    "      #only do hist for batches\n",
    "      self.current_hist_bars = self.ax.bar([0]*10, [0]*10,\n",
    "                                           color=self.current_color,\n",
    "                                           label='Current Run')\n",
    "      self.prev_hist_bars = self.ax.bar([0]*10, [0]*10, color=self.prev_color,\n",
    "                                        label='Previous Run', alpha=0.5)\n",
    "    self.fig.legend(loc='outside right upper')\n",
    "    self.fig.canvas.draw()\n",
    "\n",
    "    # Output widget to display game output and any other information\n",
    "    self.out = widgets.Output()\n",
    "\n",
    "    # keep track of important values\n",
    "    self.best_avg_score = float('-inf')\n",
    "    self.best_params = None\n",
    "    self.prev_scores = []\n",
    "    self.scores = np.array([])\n",
    "\n",
    "    # Button to set the best weights\n",
    "    self.best_button = widgets.Button(description=\"Set Best Weights\")\n",
    "    self.best_button.on_click(self.set_best_weights)\n",
    "\n",
    "    # Add a ToggleButton for symmetry\n",
    "    self.symmetry_toggle = widgets.ToggleButton(value=False,\n",
    "                                                description='Enforce Symmetry',\n",
    "                                                disabled=False,\n",
    "                                                button_style='',\n",
    "                                                tooltip='Toggle symmetry enforcement',\n",
    "                                                icon='check')\n",
    "    self.symmetry_toggle.observe(self.toggle_symmetry, 'value')\n",
    "\n",
    "    self.final_display = widgets.VBox([\n",
    "      *self.text_fields,\n",
    "      widgets.HBox([self.run_button, self.best_button, self.symmetry_toggle]),\n",
    "      self.fig.canvas,\n",
    "      self.out])\n",
    "\n",
    "    self.links = []  # To keep track of the jslink objects\n",
    "\n",
    "\n",
    "  def run_game(self, *args):\n",
    "    weights = []\n",
    "    for hbox in self.text_fields[1:]:  # Skip the header row\n",
    "      row_weights = [field.value for field in hbox.children[1:]]  # Skip the label at the first position\n",
    "      weights.append(row_weights)\n",
    "    weights = np.array(weights)\n",
    "    ppp = PerceptParamPlayer(self.game, weights=weights)\n",
    "    # Run the game\n",
    "    final_board = self.game.play_game(players=[ppp], visualize=False)\n",
    "\n",
    "    self.scores = final_board['scores'].flatten()\n",
    "    avg_score = np.mean(self.scores)\n",
    "\n",
    "    if avg_score > self.best_avg_score:\n",
    "      self.best_avg_score = avg_score\n",
    "      self.best_params = weights\n",
    "\n",
    "    if self.game.batch_size > 1:\n",
    "      # Compute and update histogram data\n",
    "      counts, edges = np.histogram(self.scores, bins=10)\n",
    "      counts = counts/np.sum(counts)\n",
    "      prev_counts, prev_edges = np.histogram(self.prev_scores[-1], bins=10) if len(self.prev_scores) > 0 else ([0]*10, edges)\n",
    "      prev_sum = np.sum(prev_counts)\n",
    "      if prev_sum > 0:\n",
    "        prev_counts = prev_counts / np.sum(prev_counts)\n",
    "      # Update the height of bars for the current scores\n",
    "      for bar, height, left in zip(self.current_hist_bars, counts, edges[:-1]):\n",
    "          bar.set_height(height)\n",
    "          bar.set_x(left)\n",
    "          bar.set_width(edges[1] - edges[0])\n",
    "      # Update the height of bars for the previous scores\n",
    "      for bar, height, left in zip(self.prev_hist_bars, prev_counts, prev_edges[:-1]):\n",
    "          bar.set_height(height)\n",
    "          bar.set_x(left)\n",
    "          bar.set_width(prev_edges[1] - prev_edges[0])\n",
    "    # set vline data\n",
    "    self.current_avg_line.set_xdata([avg_score])\n",
    "    if len(self.prev_scores) > 0:\n",
    "        prev_avg_score = np.mean(self.prev_scores[-1])\n",
    "        self.prev_avg_line.set_xdata([prev_avg_score])\n",
    "    self.best_avg_line.set_xdata([self.best_avg_score])\n",
    "\n",
    "    #update the fig\n",
    "    self.fig.legend(loc='outside right upper')\n",
    "    self.fig.canvas.draw()\n",
    "    # Display the output\n",
    "    with self.out:\n",
    "      clear_output()\n",
    "\n",
    "      if self.game.batch_size > 1:\n",
    "        print(f\"Average Score This Time: {avg_score}\")\n",
    "        if len(self.prev_scores) > 0:\n",
    "          prev_avg_score = np.mean(self.prev_scores[-1])\n",
    "          print(f\"Average Score Last Time: {prev_avg_score}\")\n",
    "        print(f\"Best Ever Average Score: {self.best_avg_score}\")\n",
    "      else:\n",
    "        print(f\"Score This Run: {avg_score}\")\n",
    "        if len(self.prev_scores) > 0:\n",
    "          print(f\"Score Last Run: {prev_avg_score}\")\n",
    "        print(f\"Best Score: {self.best_avg_score}\")\n",
    "\n",
    "    self.prev_scores.append(self.scores.copy())\n",
    "\n",
    "\n",
    "  def link_symmetric_widgets(self, change):\n",
    "    # Each row's symmetry permutation indices\n",
    "    symmetry_indices = {\n",
    "      'Up':    [ 0,  1,  2,  1,  3,  4,  4,  3,  5,  6,  5,  7],\n",
    "      'Down':  [ 7,  5,  6,  5,  3,  4,  4,  3,  1,  2,  1,  0],\n",
    "      'Left':  [ 3,  1,  4,  5,  0,  2,  6,  7,  1,  4,  5,  3],\n",
    "      'Right': [ 3,  5,  4,  1,  7,  6,  2,  0,  5,  4,  1,  3]}\n",
    "\n",
    "    if change['new']:  # If the toggle button is activated\n",
    "      base_row = self.text_fields[1].children[1:]  # The 'Up' row\n",
    "      base_perm = np.array(symmetry_indices['Up'])  # Convert to a NumPy array for easier manipulation\n",
    "\n",
    "      for row_label, hbox in zip(self.row_labels, self.text_fields[1:]):  # Include all rows\n",
    "        link_row = hbox.children[1:]  # Skip the label\n",
    "        perm = np.array(symmetry_indices[row_label])  # Convert to a NumPy array for easier manipulation\n",
    "        for i, j in enumerate(perm):\n",
    "          base_index = np.flatnonzero(base_perm == j)[0]\n",
    "          if row_label != 'Up' or base_index != i:  # Skip self-links\n",
    "            link = widgets.jslink((base_row[base_index], 'value'), (link_row[i], 'value'))\n",
    "            self.links.append(link)\n",
    "\n",
    "\n",
    "  def unlink_symmetric_widgets(self):\n",
    "    for link in self.links:\n",
    "      link.unlink()\n",
    "    self.links.clear()\n",
    "\n",
    "\n",
    "  def toggle_symmetry(self, change):\n",
    "    if change.new:\n",
    "      self.link_symmetric_widgets(change)\n",
    "    else:\n",
    "      self.unlink_symmetric_widgets()\n",
    "\n",
    "\n",
    "  def set_best_weights(self, *args):\n",
    "    if self.best_params is not None:\n",
    "      for i, hbox in enumerate(self.text_fields[1:]):\n",
    "        for j, field in enumerate(hbox.children[1:]):\n",
    "          field.value = self.best_params[i][j]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ComplexMoveInteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game when more\n",
    "  complicated (fast) moves are allowed\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None, has_fov=False,\n",
    "               radius=2, fov_opaque=False, collect_fov_data=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human']):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "        if left out will initialize with a random board state\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the gridworld_game.num_critters\n",
    "        attribute. Order should align with critter_name.\n",
    "\n",
    "      Note: fov is going to look pretty janky with more than one player, maybe\n",
    "      we get fov to only turn on for the 'active' player?\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.active_player_index = 0\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.gwg.num_critters):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] #implict check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        # player objects expected to have a critter_index attribute\n",
    "        # we set it appropriately here so it aligns with the players list\n",
    "        # used to create the widget\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    if init_board is None:\n",
    "      self.board_state = self.gwg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    if self.collect_fov_data is True:\n",
    "      # keep raw records of percept and eating for manipulation later\n",
    "      self.percept_eat_records = []\n",
    "      # keep data in contingency table of how many food items were in\n",
    "      # the percept, and whether or not food was eaten\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '21.0em', min_width='21.0em', max_width='22.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.fov_eat_table_display = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='25.0em', min_height='18.8em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.move_options = ['Stay', 'Up', 'Down', 'Left', 'Right']\n",
    "    self.move1_dropdown = widgets.Dropdown(\n",
    "      options=self.move_options,\n",
    "      value='Stay',\n",
    "      description='First Move Part:')\n",
    "    self.move2_dropdown = widgets.Dropdown(\n",
    "      options=self.move_options,\n",
    "      value='Stay',\n",
    "      description='Second Move Part:')\n",
    "    self.confirm_button = widgets.Button(description=\"Confirm Move\")\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.game_running = False\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if self.collect_fov_data and self.any_human_players:\n",
    "      self.legend_type = None # don't keep regenerating the legend\n",
    "      # do legend separately if showing observations and no human player\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          legend_type='separate', figsize=self.figsize, has_fov=self.has_fov,\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque,\n",
    "          focal_critter_index=self.active_player_index)\n",
    "    elif len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize,\n",
    "          focal_critter_index=self.active_player_index)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                fov_opaque=self.fov_opaque,\n",
    "                                radius=self.radius, figsize=self.figsize,\n",
    "                                focal_critter_index=self.active_player_index)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas, self.buttons])\n",
    "      if self.players[0] != 'human':\n",
    "        # first player isn't human disable direction buttons\n",
    "        self.disable_direction_buttons()\n",
    "\n",
    "      if len(self.players) == 1:\n",
    "        #one human player\n",
    "        self.output_and_score = widgets.HBox([self.scoreboard, self.output])\n",
    "        self.no_table_final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                                  self.output_and_score])\n",
    "        if self.collect_fov_data == True:\n",
    "          # a single human player collecting data\n",
    "          self.final_display = widgets.HBox([self.no_table_final_display,\n",
    "                                           self.fov_eat_table_display])\n",
    "        else: # self.collect_fov_data == False:\n",
    "          # a single human player not collecting data\n",
    "          self.final_display = self.no_table_final_display\n",
    "      else:\n",
    "        # more than one player, at least one of them human\n",
    "        self.V_board_output = widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # player is some kind of ai\n",
    "      if self.collect_fov_data == True:\n",
    "        # an ai player with recording\n",
    "        # in this case legend is separate\n",
    "        self.V_score_start_output_legend = widgets.VBox([self.scoreboard,\n",
    "          self.start_button,  self.output, self.b_fig_legend.canvas])\n",
    "        self.V_board_table = widgets.VBox([self.b_fig.canvas,\n",
    "                                           self.fov_eat_table_display])\n",
    "        self.final_display = widgets.HBox([self.V_board_table,\n",
    "                                           self.V_score_start_output_legend])\n",
    "      else:\n",
    "        if len(self.players) == 1:\n",
    "          # an ai player without recording\n",
    "          self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "          self.final_display = widgets.VBox([\n",
    "            self.b_fig.canvas, self.H_score_output_start])\n",
    "        else:\n",
    "          # more than one ai player\n",
    "          self.V_board_output = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "          self.V_scoreboard_start_legend = widgets.VBox([\n",
    "              self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.fov_eat_table_display:\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    next_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "    # index of players is 0 through num_critter-1,\n",
    "    # same player represented by value of index + 1 in\n",
    "    old_scores = old_board['scores'][0]\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size, n_rows, n_cols,\n",
    "                         self.gwg.num_food, self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "      b.set_state(old_board)\n",
    "      percept = b.get_perceptions(self.radius)[0]\n",
    "\n",
    "    direction = None\n",
    "    if (isinstance(self.players[self.active_player_index], str) and\n",
    "        'human' in self.players[self.active_player_index]):\n",
    "      direction = which_button\n",
    "      self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "          self.board_state, self.active_player_index+1, [direction])\n",
    "    else:\n",
    "      a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
    "      self.board_state = self.gwg.get_next_state(\n",
    "          self.board_state, self.active_player_index+1, a_player)\n",
    "      a_r = np.floor_divide(a_player[0], self.gwg.n_cols)\n",
    "      a_c = np.remainder(a_player[0], self.gwg.n_cols)\n",
    "\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = np.floor(self.gwg.lifetime -\n",
    "                         rounds_left / self.gwg.num_critters)\n",
    "    if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
    "      #eating happened\n",
    "      eating_string = \"They ate the food/prey there!\"\n",
    "      did_eat = 1\n",
    "    else: #eating didn't happen\n",
    "      eating_string = \"There's no food/prey there.\"\n",
    "      did_eat = 0\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0,\n",
    "                                       self.active_player_index+1)\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             radius=self.radius, legend_type=self.legend_type,\n",
    "                             focal_critter_index=next_player_index)\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = {'perception': percept.copy(),\n",
    "                  'state': old_board,\n",
    "                  'did_eat': bool(did_eat)}\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1) # number of food items in FoV\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        if direction is not None:\n",
    "          print(\"The critter (tried) to move \" +\n",
    "                direction +\n",
    "                \" and is now at ({}, {}).\".format(row, col))\n",
    "        else:\n",
    "          print(\"The critter (tried) to move \" +\n",
    "                \"to \" + \"({}, {})\".format(a_r, a_c) +\n",
    "                \" and is now at ({}, {}).\".format(row, col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nScores: {:.2f}\".format(\n",
    "            rounds_left, new_scores[self.active_player_index]))\n",
    "      else: # more than one players\n",
    "        if direction is not None:\n",
    "          print(\"Critter {} (tried) to move \".format(self.active_player_index+1) +\n",
    "                direction +\n",
    "                \" and is now at ({}, {}).\".format(row, col))\n",
    "        else:\n",
    "          print(\"Critter {} (tried) to move \".format(self.active_player_index+1) +\n",
    "                \"to \" + \"({}, {})\".format(a_r, a_c) +\n",
    "                \" and is now at ({}, {}).\".format(row, col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nScores: {}\".format(\n",
    "            rounds_left, new_scores))\n",
    "    if rounds_left == 0:\n",
    "      self.final_scores.append(new_scores)\n",
    "      self.game_running = False\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print('Game Over. Final Score {}'.format(new_scores))\n",
    "        print('Resetting the board for another game')\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                               self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                               has_fov=self.has_fov, radius=self.radius,\n",
    "                               fov_opaque=self.fov_opaque,\n",
    "                               legend_type=self.legend_type,\n",
    "                               focal_critter_index=next_player_index)\n",
    "      # start_button can be clicked now\n",
    "      self.start_button.disabled = False\n",
    "    with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.players) == 1:\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = [\n",
    "              ['High Score:', str(np.max(np.array(self.final_scores)))],\n",
    "              ['Last Score:', str(self.final_scores[-1])],\n",
    "              ['Average Score',\n",
    "              '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "          else:\n",
    "            table = [['High Score:', '--'],\n",
    "                     ['Last Score:', '--'],\n",
    "                     ['Average Score:', '--']]\n",
    "          print(tabulate(table))\n",
    "        else: # len(self.players) > 1\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = []\n",
    "            # Assuming the batch size is 1 for now\n",
    "            current_scores = self.final_scores[-1]\n",
    "            max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "            average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "            table.append(['High Scores:'] +\n",
    "              [str(score) for score in max_scores])\n",
    "            table.append(['Last Scores:'] +\n",
    "              [str(score) for score in current_scores])\n",
    "            table.append(['Average Scores:'] +\n",
    "              ['{:.2f}'.format(score) for score in average_scores])\n",
    "          else:\n",
    "            table = [\n",
    "              ['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "          print(tabulate(table, headers=headers))\n",
    "\n",
    "    with self.fov_eat_table_display:\n",
    "      clear_output()\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # last thing after all display stuff is done\n",
    "    self.update_player_and_rounds()\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "\n",
    "  def update_player_and_rounds(self):\n",
    "    \"\"\"Update the player index and decrement rounds if a full loop is completed.\"\"\"\n",
    "    self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "    if self.active_player_index == len(self.players) - 1:\n",
    "      self.board_state['rounds_left'] -= 1\n",
    "\n",
    "  def human_ai_player_loop(self, direction):\n",
    "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
    "    self.game_running = True\n",
    "    # Execute the move of the human who clicked the button\n",
    "    self.button_output_update(direction)\n",
    "    # Move to the next player\n",
    "    # Do AI moves if there are any\n",
    "    while self.players[self.active_player_index] != 'human' and self.game_running:\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.5)\n",
    "    # Next player is human turn buttons on for them\n",
    "    self.enable_direction_buttons()\n",
    "\n",
    "  def start_game(self):\n",
    "    # If first player is AI start button kicks off this loop\n",
    "    self.game_running = True\n",
    "    while self.players[self.active_player_index] != 'human' and self.game_running:\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.5)\n",
    "    # Next player is human, turn buttons on for them\n",
    "    self.enable_direction_buttons()\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    self.start_game()\n",
    "    self.enable_direction_buttons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# **1.2.3.1 Foraging in Patchy Environment**\n",
    "\n",
    "Optimal foraging theory posits that organisms forage to maximize their net energy intake per unit time. We're going to explore the implications of that idea using a highly simplified model of foraging. The scenario is defined as follows:\n",
    "\n",
    "* The goal is to collect food as efficiently as possible, i.e. have the highest possible food per move.\n",
    "* The environment is made up of patches(grid cells). Each patch may or may not contain food.\n",
    "* Foraging on a patch with food is usually, but not always successful, whereas foraging on a patch with no food is never successful.\n",
    "* Every turn the organism can either 1) try to forage on its current patch or 2) move on to a new patch.\n",
    "* Food patches can become exhausted after successful foraging, so the organism needs to be aware of when to move on to a fresh patch\n",
    "* The number of moves before the foraging session ends is limited, so efficiency is key.\n",
    "\n",
    "To get a sense of the problem run the cell below to play through a version of this problem where you have complete knowledge, i.e. you can see all the patches, which ones have food, and when they become exhausted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Omniscient Patchy Foraging\n",
    "# @markdown **Run this cell** to play the patchy foraging game.\n",
    "\n",
    "pfg = PatchyForagingGame(death_rate=0.05, food_patch_prob=0.5,\n",
    "                         forage_success_prob=0.9, food_extinct_prob=0.2)\n",
    "omni_ipfg = InteractivePatchyForage(pfg, show_food=True, show_misses=True,\n",
    "                                    figsize=(4,5))\n",
    "display(omni_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(omni_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When you can see where the patches are and can tell when they run out, this scenario isn't too difficult. Now let's look at this same scenario but where the food patches are hidden (cryptic). In this case, a forager has to make guesses about whether food is present based on the past success (or failure) of their foraging attempts. How well can you do in this 'cryptic patches' scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Cryptic Patchy Foraging\n",
    "# @markdown **Run this cell** to play the patchy foraging game with cryptic patches.\n",
    "\n",
    "pfg = PatchyForagingGame(death_rate=0.05, food_patch_prob=0.5,\n",
    "                         forage_success_prob=0.9, food_extinct_prob=0.2)\n",
    "cryptic_ipfg = InteractivePatchyForage(pfg, show_food=False,\n",
    "                                       show_misses=True,\n",
    "                                       figsize=(4,5))\n",
    "display(cryptic_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(cryptic_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Was the cryptic scenario harder? Hopefully your score was higher when you had full visibility of the food patches and their exhaustion status. Yet, without this visibility, how does a forager make the best decision? Our game display hints at two critical pieces of information for the forager:\n",
    "1. The number of unsuccessful foraging attempts at a new patch\n",
    "2. The number of unsuccessful foraging attempts since the last successful foraging attempt at that same patch.\n",
    "\n",
    "If the organism can't directly detect food patches or tell when they've become exhausted these metrics are crucial for inference.\n",
    "A simple 'threshold' policy can be defined using these metrics:\n",
    "* Move to a new patch after $\\tau_{new}$ failed foraging attempts at a fresh patch.\n",
    "* Move to a new patch after $\\tau_{eat}$ consecutive failed foraging attempts at a patch where there has been successful foraging previously.\n",
    "\n",
    "Interestingly, given the particular details of this problem, such a threshold is provably optimal (see appendix/exercises), assuming that the organism is able to avoid revisiting patches it has already been too and that fresh patches are always available throughout the foraging session.\n",
    "\n",
    "Now, let's implement this threshold policy and observe its performance in our foraging scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TODO for students: Replace the ...'s in the play method to implement a\n",
    "# a simple threshold policy\n",
    "raise NotImplementedError(\"Exercise: implement threshold policy\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class SimpleThresholdPlayer():\n",
    "  \"\"\"\n",
    "  Player that moves in a sweeping pattern and decides whether to forages\n",
    "  or move to a new patch based on threshold number of failed foraging attempts.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1, threshold_new=1, threshold_known=2,\n",
    "               return_direction=True):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.threshold_new = threshold_new\n",
    "    self.threshold_known = threshold_known\n",
    "    self.last_direction = ['right'] * self.game.batch_size\n",
    "    self.return_direction = return_direction\n",
    "\n",
    "  def play(self, board):\n",
    "    batch_size = board['pieces'].shape[0]\n",
    "    chosen_directions = []\n",
    "    valid_directions_batch = self.game.get_valid_directions(board, self.critter_index)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      valid_directions_for_this_board = valid_directions_batch[i]\n",
    "      is_at_new_patch = board['at_new_patch'][i, self.critter_index - 1]\n",
    "      # Decide to forage or move\n",
    "      if is_at_new_patch:\n",
    "        # print('at new patch')\n",
    "        misses = board['misses_new_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = ...\n",
    "      else:\n",
    "        # print('at known patch')\n",
    "        misses = board['misses_known_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = ...\n",
    "\n",
    "      if ...:\n",
    "        # print('move to new patch')\n",
    "        chosen_directions.append(self._get_next_direction(i, valid_directions_for_this_board))\n",
    "      else:\n",
    "        # print('forage at current patch')\n",
    "        chosen_directions.append('still')\n",
    "\n",
    "    if self.return_direction:\n",
    "      return chosen_directions\n",
    "    else:\n",
    "      # Convert chosen directions to actions\n",
    "      actions = self.game.critter_directions_to_actions(board, chosen_directions, self.critter_index)\n",
    "      action_size = self.game.get_action_size()\n",
    "      a_1hots = np.zeros((batch_size, action_size))\n",
    "      a_1hots[np.arange(batch_size), actions] = 1.0\n",
    "      return actions, a_1hots, a_1hots\n",
    "\n",
    "  def _get_next_direction(self, board_idx, valid_directions_for_this_board):\n",
    "    \"\"\"\n",
    "    Get the next direction based on left-right-down sweeping pattern.\n",
    "    \"\"\"\n",
    "    if self.last_direction[board_idx] == 'right':\n",
    "      if 'right' in valid_directions_for_this_board:\n",
    "        return 'right'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'left'\n",
    "        return 'down'\n",
    "\n",
    "    if self.last_direction[board_idx] == 'left':\n",
    "      if 'left' in valid_directions_for_this_board:\n",
    "        return 'left'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'right'\n",
    "        return 'down'\n",
    "\n",
    "    return 'still'  # Default to 'still' if none of the conditions are met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "class SimpleThresholdPlayer():\n",
    "  \"\"\"\n",
    "  Player that moves in a sweeping pattern and decides whether to forages\n",
    "  or move to a new patch based on threshold number of failed foraging attempts.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1, threshold_new=1, threshold_known=2,\n",
    "               return_direction=True):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.threshold_new = threshold_new\n",
    "    self.threshold_known = threshold_known\n",
    "    self.last_direction = ['right'] * self.game.batch_size\n",
    "    self.return_direction = return_direction\n",
    "\n",
    "  def play(self, board):\n",
    "    batch_size = board['pieces'].shape[0]\n",
    "    chosen_directions = []\n",
    "\n",
    "    valid_directions_batch = self.game.get_valid_directions(board, self.critter_index)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      valid_directions_for_this_board = valid_directions_batch[i]\n",
    "      is_at_new_patch = board['at_new_patch'][i, self.critter_index - 1]\n",
    "      # Decide to forage or move\n",
    "      if is_at_new_patch:\n",
    "        # print('at new patch')\n",
    "        misses = board['misses_new_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_new\n",
    "      else:\n",
    "        # print('at known patch')\n",
    "        misses = board['misses_known_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_known\n",
    "\n",
    "      if misses >= threshold:\n",
    "        # print('move to new patch')\n",
    "        chosen_directions.append(self._get_next_direction(i, valid_directions_for_this_board))\n",
    "      else:\n",
    "        # print('forage at current patch')\n",
    "        chosen_directions.append('still')\n",
    "\n",
    "    if self.return_direction:\n",
    "      return chosen_directions\n",
    "    else:\n",
    "      # Convert chosen directions to actions\n",
    "      actions = self.game.critter_directions_to_actions(board, chosen_directions, self.critter_index)\n",
    "      action_size = self.game.get_action_size()\n",
    "      a_1hots = np.zeros((batch_size, action_size))\n",
    "      a_1hots[np.arange(batch_size), actions] = 1.0\n",
    "      return actions, a_1hots, a_1hots\n",
    "\n",
    "  def _get_next_direction(self, board_idx, valid_directions_for_this_board):\n",
    "    \"\"\"\n",
    "    Get the next direction based on left-right-down sweeping pattern.\n",
    "    \"\"\"\n",
    "    if self.last_direction[board_idx] == 'right':\n",
    "      if 'right' in valid_directions_for_this_board:\n",
    "        return 'right'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'left'\n",
    "        return 'down'\n",
    "\n",
    "    if self.last_direction[board_idx] == 'left':\n",
    "      if 'left' in valid_directions_for_this_board:\n",
    "        return 'left'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'right'\n",
    "        return 'down'\n",
    "\n",
    "    return 'still'  # Default to 'still' if none of the conditions are met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have a forager, run the cell below to watch them forage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Threshold Foraging\n",
    "# @markdown **Run this cell** to watch our simple threshold forager play.\n",
    "\n",
    "# to_remove solution\n",
    "\n",
    "class SimpleThresholdPlayer():\n",
    "  \"\"\"\n",
    "  Player that moves in a sweeping pattern and decides whether to forages\n",
    "  or move to a new patch based on threshold number of failed foraging attempts.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1, threshold_new=1, threshold_known=2,\n",
    "               return_direction=True):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.threshold_new = threshold_new\n",
    "    self.threshold_known = threshold_known\n",
    "    self.last_direction = ['right'] * self.game.batch_size\n",
    "    self.return_direction = return_direction\n",
    "\n",
    "  def play(self, board):\n",
    "    batch_size = board['pieces'].shape[0]\n",
    "    chosen_directions = []\n",
    "\n",
    "    valid_directions_batch = self.game.get_valid_directions(board, self.critter_index)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      valid_directions_for_this_board = valid_directions_batch[i]\n",
    "      is_at_new_patch = board['at_new_patch'][i, self.critter_index - 1]\n",
    "      # Decide to forage or move\n",
    "      if is_at_new_patch:\n",
    "        # print('at new patch')\n",
    "        misses = board['misses_new_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_new\n",
    "      else:\n",
    "        # print('at known patch')\n",
    "        misses = board['misses_known_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_known\n",
    "\n",
    "      if misses >= threshold:\n",
    "        # print('move to new patch')\n",
    "        chosen_directions.append(self._get_next_direction(i, valid_directions_for_this_board))\n",
    "      else:\n",
    "        # print('forage at current patch')\n",
    "        chosen_directions.append('still')\n",
    "\n",
    "    if self.return_direction:\n",
    "      return chosen_directions\n",
    "    else:\n",
    "      # Convert chosen directions to actions\n",
    "      actions = self.game.critter_directions_to_actions(board, chosen_directions, self.critter_index)\n",
    "      action_size = self.game.get_action_size()\n",
    "      a_1hots = np.zeros((batch_size, action_size))\n",
    "      a_1hots[np.arange(batch_size), actions] = 1.0\n",
    "      return actions, a_1hots, a_1hots\n",
    "\n",
    "  def _get_next_direction(self, board_idx, valid_directions_for_this_board):\n",
    "    \"\"\"\n",
    "    Get the next direction based on left-right-down sweeping pattern.\n",
    "    \"\"\"\n",
    "    if self.last_direction[board_idx] == 'right':\n",
    "      if 'right' in valid_directions_for_this_board:\n",
    "        return 'right'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'left'\n",
    "        return 'down'\n",
    "\n",
    "    if self.last_direction[board_idx] == 'left':\n",
    "      if 'left' in valid_directions_for_this_board:\n",
    "        return 'left'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'right'\n",
    "        return 'down'\n",
    "\n",
    "    return 'still'  # Default to 'still' if none of the conditions are met\n",
    "\n",
    "pfg = PatchyForagingGame(death_rate=0.05, food_patch_prob=0.5,\n",
    "                         forage_success_prob=0.9, food_extinct_prob=0.2)\n",
    "stp = SimpleThresholdPlayer(pfg, critter_index=1, threshold_new=1, threshold_known=2)\n",
    "stp_ipfg = InteractivePatchyForage(pfg, players=[stp],\n",
    "                                   show_food=True,\n",
    "                                   show_misses=True,\n",
    "                                   figsize=(4,5))\n",
    "display(stp_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(stp_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "How did our simple threshold policy compare to your performance when playing with cryptic patches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# **1.2.3.2 Deductive Reasoning About Policy Performance**\n",
    "Now that we have had some 'hands-on' experience with this foraging problem, we're going to dig into the detailed mechanics of the model scenario, and see if what we can deduce about policy performance and what an optimal policy should be. In doing this we will introduce the concept of ***Value***, a powerful and indispensable tool for reasoning about policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Defining the Problem**\n",
    "First we need define our model problem precisely.\n",
    "\n",
    "* **Patchy Environment**: The foraging environment consists of discrete patches (represented as grid cells). At the start of the simulation each patch has a probability $p_e$ of containing food. The forager starts at a fresh patch.\n",
    "\n",
    "* **Possible Actions**: In each turn, the organism has two options:\n",
    "  - Try to forage at its current patch.\n",
    "  - Move to a new patch.\n",
    "\n",
    "* **Foraging Success**: When a patch contains food, foraging is often successful but not always guaranteed. In this model, foraging at a patch with food is successful with probability $p_s$. Conversely, foraging on a food-less patch is certain to be unsuccessful.\n",
    "\n",
    "* **Patch Exhaustion**: After each foraging success, there is a probability $p_x$ that the patch becomes exhausted. In such cases, the patch won't provide any more food.\n",
    "\n",
    "* **Session Limit**: The number of rounds that the forager will have is stochastic and unknown to the forager. Specifically, the duration of the session, is a random variable $T$ following a geometric distribution. The probability of the session ends at give round $T=t$ is given by:\n",
    "$$P(T = t) = \\gamma^{t-1}(1-\\gamma).$$\n",
    "Here $\\gamma$ is the probability of the session continuing each round and $1-\\gamma$ is the probability that the foraging session ends.\n",
    "\n",
    "* **Rewards**: Every successful foraging attempt gives the organism a reward of 1 point. If the foraging attempt is unsuccessful, no points are awarded for that round. We denote the reward received on round $t$ as $R_t$.\n",
    "\n",
    "* **Goal**: The overarching objective for the organism is to maximize its *expected cumulative reward* over the entire session. Formally, the forager aims to maximize:\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\sum_{t=1}^{T} R_t \\right] = \\sum_{t=1}^{\\infty} P(T \\geq t) \\cdot \\mathbb{E}\\left[ R_t \\right] = \\sum_{t=1}^{\\infty} \\gamma^{t-1} \\cdot \\mathbb{E}\\left[ R_t \\right]\n",
    "$$\n",
    "Given the geometric distribution of the session's end time, we can see that future rewards are naturally *discounted* by the probability of that future round actually being reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that the details of the foraging problem are precisely laid out. We can start reasoning about policy effectiveness. To do this we will think about two important quantities. One, which we will call $V_{new}$, is the *expected **future** cumulative reward* when newly arrived at a fresh patch. The other, which we will call $V_{eat}$, is the *expected **future** cumulative reward* of being at a patch immediately after a foraging success. Importantly, both these quantities are forward looking. They do not depend in any way on prior actions and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Stochastic Dynamics**\n",
    "Before starting to compute $V_{eat}$ and $V_{new}$ we need to take a little detour and map out all the possible situations a forager can encounter together with with associated probabilities of each situation occurring. To start, a forager is always in a New Patch (**NP**). From there one of two things can be true.\n",
    "1. Either this is New Patch With Food(**NPWF**), which is the case with probability $p_e$, or\n",
    "2. this is a New Patch with No Food (**NPNF**) which is the case with probability $1-p_e$.\n",
    "\n",
    "This situation is visualized below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize these probabilities\n",
    "\n",
    "nodes_list = [\"NP\", \"NPWF\", \"NPNF\"]\n",
    "edges_list = [(\"NP\", \"NPWF\"), (\"NP\", \"NPNF\")]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"NP\", \"NPWF\", \"p_e\"),\n",
    "    (\"NP\", \"NPNF\", \"1-p_e\"),\n",
    "]\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels)\n",
    "Image(output_path, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "When the forager is at a New Patch with No Food (**NPNF**), they will persistently forage there until either:\n",
    "1. They hit their threshold number of attempts $\\tau_{new}$ and move on to a new patch, the **NP** state. This happens with probability $\\gamma^{\\tau_{new}}$.  \n",
    "2. The session ends before they reach this threshold. This happens with probability $1-\\gamma^{\\tau_{new}}$.\n",
    "\n",
    "Let's add this to our visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize these probabilities\n",
    "\n",
    "nodes_list = [\"NP\", \"NPWF\", \"NPNF\", \"END\"]\n",
    "edges_list = [(\"NP\", \"NPWF\"), (\"NP\", \"NPNF\"), (\"NPNF\", \"END\"), (\"NPNF\", \"NP\")]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"NP\", \"NPWF\", \"p_e\"),\n",
    "    (\"NP\", \"NPNF\", \"1-p_e\"),\n",
    "    (\"NPNF\", \"END\", \"1-\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPNF\", \"NP\", \"\\gamma^{\\\\tau_{new}}\"),\n",
    "]\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels)\n",
    "Image(output_path, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now things get trickier. At a New Patch With Food multiple outcomes are possible, but for simplification we'll lump these into two broad cases:\n",
    "1. Nothing 'interesting', i.e. eating or dying, happens before the threshold number of turns passes and the forager moves on to a New Patch. We will denote the probability of nothing 'interesting' happening on a single forager turn as $p_n=(1-p_s)\\gamma$, the probability that the forager doesn't find food and survives to the next round. Then extending this, the probability of $\\tau_{new}$ uninteresting rounds in a row is ${p_n}^{\\tau_{new}}$.\n",
    "2. Something 'interesting' (food or death) happens before the threshold number of rounds passes. The probability this is $1 - {p_n}^{\\tau_{new}}$. Let's call this the New Patch Something Happens state (**NPSH**).\n",
    "\n",
    "Let's integrate this into our visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize these probabilities\n",
    "\n",
    "nodes_list = [\"NP\", \"NPWF\", \"NPNF\", \"END\", \"NPSH\"]\n",
    "edges_list = [(\"NP\", \"NPWF\"), (\"NP\", \"NPNF\"), (\"NPNF\", \"END\"), (\"NPNF\", \"NP\"),\n",
    "              (\"NPWF\", \"NP\"), (\"NPWF\", \"NPSH\")]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"NP\", \"NPWF\", \"p_e\"),\n",
    "    (\"NP\", \"NPNF\", \"1-p_e\"),\n",
    "    (\"NPNF\", \"END\", \"1-\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPNF\", \"NP\", \"\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NP\", \"{p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NPSH\", \"1 - {p_n}^{\\\\tau_{new}}\")\n",
    "]\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels)\n",
    "Image(output_path, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "At a New Patch Something Happens state (**NPSH**), the forager experiences either death, eating, or both simultaneously. Let's break down the probabilities of each event.\n",
    "\n",
    "For each round we have\n",
    "\n",
    "1. No eating, no death: $(1-p_s) \\gamma $\n",
    "2. Eating, no death: $p_s \\gamma$\n",
    "3. No eating, death: $(1-p_s) (1-\\gamma)$\n",
    "4. Eating and death: $p_s (1-\\gamma)$\n",
    "\n",
    "However, **NPSH** is when one of cases 2, 3, or 4 occurred, on at least one round before $\\tau_{new}$ rounds pass. We can focus our attention on the first interesting thing that happened as this is what determines our transition. Given that a first event occurs on a particular round, the probabilities for the possible types of first events on that round are:\n",
    "* Eating, no death: $\\frac{p_s \\gamma}{1 - (1-p_s)\\gamma}$\n",
    "* No eating, death: $\\frac{(1-p_s) (1-\\gamma)}{1 - (1-p_s)\\gamma}$\n",
    "* Eating and death: $\\frac{p_s (1-\\gamma)}{1 - (1-p_s)\\gamma}$\n",
    "\n",
    "Now, because the probabilities of death and foraging success don't change from round to round, these are also the probabilities of each of those cases being the first interesting thing that happens, conditional on something interesting happening.\n",
    "\n",
    "Two of these transitions lead to the **END** state, but one of the transitions results in a point for the forager whereas the other does not. If the forager eats without dying they transition to the Successful Eating **SE** state. Now, let's visualize these additions to our transition graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize these probabilities\n",
    "\n",
    "nodes_list = [\"NP\", \"NPWF\", \"NPNF\", \"END\", \"NPSH\", \"SE\"]\n",
    "edges_list = [(\"NP\", \"NPWF\"), (\"NP\", \"NPNF\"), (\"NPNF\", \"END\"), (\"NPNF\", \"NP\"),\n",
    "              (\"NPWF\", \"NP\"), (\"NPWF\", \"NPSH\"),\n",
    "              (\"NPSH\", \"SE\"), (\"NPSH\", \"END\"), (\"NPSH\", \"END\")]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"NP\", \"NPWF\", \"p_e\"),\n",
    "    (\"NP\", \"NPNF\",  \"1-p_e\"),\n",
    "    (\"NPNF\", \"END\", \"1-\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPNF\", \"NP\", \"\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NP\", \"{p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NPSH\", \"1 - {p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"NPSH\", \"SE\", \"\\\\frac{p_s \\\\gamma}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"NPSH\", \"END\", \"\\\\frac{(1-p_s) (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"NPSH\", \"END\", \"\\\\frac{p_s (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\")\n",
    "]\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels)\n",
    "Image(output_path, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The Successful Eating state (**SE**) is a lot like the **NP** state, except, now instead of $p_e$ giving the probability of food existing or not at the patch, the patch exhaustion probability $p_x$ determines whether this Successful Eating patch becomes a Successful Eating With Food patch (**SEWF**) or a Successful Eating No Food patch (**SENF**). Adding these states to our visualization we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize these probabilities\n",
    "\n",
    "nodes_list = [\"NP\", \"NPWF\", \"NPNF\", \"END\", \"NPSH\", \"SE\", \"SEWF\", \"SENF\"]\n",
    "\n",
    "edges_list = [(\"NP\", \"NPWF\"), (\"NP\", \"NPNF\"), (\"NPNF\", \"END\"), (\"NPNF\", \"NP\"),\n",
    "                      (\"NPWF\", \"NP\"), (\"NPWF\", \"NPSH\"),\n",
    "                      (\"NPSH\", \"SE\"), (\"NPSH\", \"END\"), (\"NPSH\", \"END\"),\n",
    "                      (\"SE\", \"SEWF\"), (\"SE\", \"SENF\")]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"NP\", \"NPWF\", \"p_e\"),\n",
    "    (\"NP\", \"NPNF\",  \"1-p_e\"),\n",
    "    (\"NPNF\", \"END\", \"1-\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPNF\", \"NP\", \"\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NP\", \"{p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NPSH\", \"1 - {p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"NPSH\", \"SE\", \"\\\\frac{p_s \\\\gamma}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"NPSH\", \"END\", \"\\\\frac{(1-p_s) (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"NPSH\", \"END\", \"\\\\frac{p_s (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"SE\", \"SEWF\", \"1-p_x\"),\n",
    "    (\"SE\", \"SENF\", \"p_x\")\n",
    "]\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels)\n",
    "Image(output_path, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "And again, much like the transition probabilities out of New Patch With Food (**NPWF**) and the New Patch with No Food (**NPNF**) are determined largely by $\\tau_{new}$, the transition probabilities out of Successful Eating patch With Food (**SEWF**) and Successful Eating patch with No Food (**SENF**) follow the same logic but are determined by $\\tau_{eat}$. Adding these transitions and the Successful Eating Something Happened (**SESH**) state, and its transitions, which are identical to the the transitions out of the ***NPSH** state, completes our visualization of the probabilistic dynamics of this foraging problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize these probabilities\n",
    "\n",
    "nodes_list = [\"NP\", \"NPWF\", \"NPNF\", \"END\", \"NPSH\", \"SE\", \"SEWF\", \"SENF\"]\n",
    "\n",
    "edges_list = [(\"NP\", \"NPWF\"), (\"NP\", \"NPNF\"),\n",
    "              (\"NPNF\", \"END\"), (\"NPNF\", \"NP\"),\n",
    "              (\"NPWF\", \"NP\"), (\"NPWF\", \"NPSH\"),\n",
    "              (\"NPSH\", \"SE\"), (\"NPSH\", \"END\"), (\"NPSH\", \"END\"),\n",
    "              (\"SE\", \"SEWF\"), (\"SE\", \"SENF\"),\n",
    "              (\"SENF\", \"END\"), (\"SENF\", \"NP\"),\n",
    "              (\"SEWF\", \"NP\"), (\"SEWF\", \"SESH\"),\n",
    "              (\"SESH\", \"SE\"), (\"SESH\", \"END\"), (\"SESH\", \"END\")]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"NP\", \"NPWF\", \"p_e\"),\n",
    "    (\"NP\", \"NPNF\",  \"1-p_e\"),\n",
    "    (\"NPNF\", \"END\", \"1-\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPNF\", \"NP\", \"\\gamma^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NP\", \"{p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"NPWF\", \"NPSH\", \"1 - {p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"NPSH\", \"SE\", \"\\\\frac{p_s \\\\gamma}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"NPSH\", \"END\", \"\\\\frac{(1-p_s) (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"NPSH\", \"END\", \"\\\\frac{p_s (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"SE\", \"SEWF\", \"1-p_x\"),\n",
    "    (\"SE\", \"SENF\", \"p_x\"),\n",
    "    (\"SENF\", \"END\", \"1-\\gamma^{\\\\tau_{eat}}\"),\n",
    "    (\"SENF\", \"NP\", \"\\gamma^{\\\\tau_{eat}}\"),\n",
    "    (\"SEWF\", \"NP\", \"{p_n}^{\\\\tau_{eat}}\"),\n",
    "    (\"SEWF\", \"SESH\", \"1 - {p_n}^{\\\\tau_{new}}\"),\n",
    "    (\"SESH\", \"SE\", \"\\\\frac{p_s \\\\gamma}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"SESH\", \"END\", \"\\\\frac{(1-p_s) (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\"),\n",
    "    (\"SESH\", \"END\", \"\\\\frac{p_s (1-\\\\gamma)}{1 - (1-p_s)\\\\gamma}\")\n",
    "]\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels)\n",
    "Image(output_path, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "States:\n",
    "\n",
    "1. **NP**: New Patch - Represents the initial state when a forager enters a new patch.\n",
    "2. **NPWF**: New Patch With Food - A state in which the forager is in a new patch, and there's food available.\n",
    "3. **NPNF**: New Patch No Food - A state in which the forager is in a new patch, but there's no food.\n",
    "4. **NPSH**: New Patch Something Happens - Represents events that can occur in a new patch: eating, death, or both.\n",
    "5. **SE**: Successful Eating - Indicates a forager successfully eating in a patch.\n",
    "6. **SEWF**: Successful Eating With Food - A state post-successful eating where food remains in the patch.\n",
    "7. **SENF**: Successful Eating No Food - A state post-successful eating where no food remains in the patch.\n",
    "8. **SESH**: Successful Eating Something Happens - Represents events that can occur post-successful eating.\n",
    "9. **END**: End - Represents the end state, which could be due to the forager dying or another terminal event.\n",
    "\n",
    "Note that **NP**, **NPWF**, and **NPNF** are all the same from the forager's perspective, similarly **SE**, **SEWF**, and **SENF** are all the same from the forager's perspective. We've introduced these states to keep track of the underlying dynamics, not the forager's experience of the environment. The **NPSH** and **SESH** states are also just helpful 'book keeping' states for helping us think about the underlying probabilities of the system and don't directly correspond to the forager's experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "  Transitions:\n",
    "  \n",
    "  * **NP** to **NPWF**: $p_e$ - Probability of finding food in a new patch.\n",
    "  \n",
    "  * **NP** to **NPNF**: $1-p_e$ - Probability of not finding food in a new patch.\n",
    "  \n",
    "  * **NPNF** to **END**: $1-\\gamma^{\\tau_{new}}$ - Probability of a terminal event in a food-less patch after a set number of rounds.\n",
    "  \n",
    "  * **NPNF** to **NP**: $\\gamma^{\\tau_{new}}$ - Probability of returning to a new patch after surviving in a food-less patch for a set number of rounds.\n",
    "  \n",
    "  * **NPWF** to **NP**: ${p_n}^{\\tau_{new}}$ - Probability of no event occurring in a food-rich patch and returning to find a new patch.\n",
    "  \n",
    "  * **NPWF** to **NPSH**: $1 - {p_n}^{\\tau_{new}}$ - Probability of an event (eating, death) in a food-rich patch within a set number of rounds.\n",
    "  \n",
    "  * **NPSH** to **SE**: $\\frac{p_s \\gamma}{1 - (1-p_s)\\gamma}$ - Probability of eating without dying in the NPSH state.\n",
    "  \n",
    "  * **NPSH** to **END** (due to death without eating): $\\frac{(1-p_s) (1-\\gamma)}{1 - (1-p_s)\\gamma}$.\n",
    "  \n",
    "  * **NPSH** to **END** (due to eating then dying): $\\frac{p_s (1-\\gamma)}{1 - (1-p_s)\\gamma}$.\n",
    "  \n",
    "  * **SE** to **SEWF**: $1-p_x$ - Probability of food still existing post successful eating.\n",
    "  \n",
    "  * **SE** to **SENF**: $p_x$ - Probability of no food existing post successful eating.\n",
    "  * **SENF** to **END**: $1-\\gamma^{\\tau_{eat}}$ - Probability of a terminal event in a post-eating, food-less patch after a set number of rounds.\n",
    "  \n",
    "  * **SENF** to **NP**: $\\gamma^{\\tau_{eat}}$ - Probability of returning to a new patch after surviving in a post-eating, food-less patch for a set number of rounds.\n",
    "  \n",
    "  * **SEWF** to **NP**: ${p_n}^{\\tau_{eat}}$ - Probability of no event occurring in a post-eating, food-rich patch and then returning to find a new patch.\n",
    "  \n",
    "  * **SEWF** to **SESH**: 1 - ${p_n}^{\\tau_{eat}}$ - Probability of an event occurring in a post-eating, food-rich patch within a set number of rounds.\n",
    "  \n",
    "  * **SESH** to **SE**: $\\frac{p_s \\gamma}{1 - (1-p_s)\\gamma}$ - Probability of eating without dying in the SESH state.\n",
    "  \n",
    "  * **SESH** to **END** (due to death without eating): $\\frac{(1-p_s) (1-\\gamma)}{1 - (1-p_s)\\gamma}$.\n",
    "  \n",
    "  * **SESH** to **END** (due to eating then dying): $\\frac{p_s (1-\\gamma)}{1 - (1-p_s)\\gamma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Value Functions**\n",
    "\n",
    "Those transition probabilities ended up being a bit much but we're now in a good position to both start writing down some critical ***Values*** and to appreciate how the concept of ***Value*** as **expected future cumulative reward** can help simplify a problem. Specifically we will compute:\n",
    "\n",
    "1. $V(\\text{NP})$: Value of being in a new patch.\n",
    "2. $V(\\text{SE})$: Value of being in a patch immediately after successful foraging.\n",
    "\n",
    "We will compute these in terms of:\n",
    "3. $V(\\text{NPWF})$: Value of being in a patch with food, given a threshold $\\tau_{new}$ for unsuccessful foraging attempts before moving on.\n",
    "4. $V(\\text{NPNF})$: Value of being in a patch without food, given a threshold\n",
    "$\\tau_{new}$ for unsuccessful foraging attempts before moving on.\n",
    "5. $V(\\text{SEWF})$: Value of being in a patch with food, given a threshold $\\tau_{eat}$ for unsuccessful foraging attempts before moving on.\n",
    "6. $V(\\text{SENF})$: Value of being in a patch without food, given a threshold\n",
    "$\\tau_{eat}$ for unsuccessful foraging attempts before moving on.\n",
    "\n",
    "But first we notice that 3 and and 5 are very similar, as are 4 and 6 so we can simplify and write:\n",
    "* $V_{\\tau}(\\text{WF}$): Value of being in a patch with food, given threshold $\\tau$.\n",
    "* $V_{\\tau}(\\text{NF})$: Value of being in a patch with no food, given threshold $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Upon arriving at a New Patch, there's a $p_e$ probability it has food and a $1-p_e$ probability it doesn't.\n",
    "\n",
    "$$\n",
    "V(\\text{NP}) = p_e \\cdot V_{\\tau_{new}}(\\text{WF}) + (1-p_e) \\cdot V_{\\tau_{new}}(\\text{NF})\n",
    "$$\n",
    "\n",
    "After Successful Eating, there's a $p_x$ probability the food runs out and a $1-p_x$ probability it remains.\n",
    "\n",
    "$$\n",
    "V(\\text{SE}) = (1-p_x) \\cdot V_{\\tau_{eat}}(\\text{WF}) + p_x \\cdot V_{\\tau_{eat}}(\\text{NF})\n",
    "$$\n",
    "\n",
    "Given a threshold $\\tau$ for unsuccessful attempts:\n",
    "- The forager will move on to new patch after $\\tau$ unsuccessful attempts, which happens with probability $(1-p_s)^\\tau$ \\gamma^{\\tau}. They will survive this journey to the new patch with probability $\\gamma$\n",
    "- Or, the organism finds food in one of the first $\\tau$ attempts.\n",
    "\n",
    "Thus, the value of being at a patch that With Food, given the organism will leave after $\\tau$ failed foraging attempts is:\n",
    "\n",
    "$$\n",
    "V_{\\tau}(\\text{WF}) = (1-p_s)^\\tau \\cdot \\gamma^{\\tau+1} \\cdot V(\\text{NP}) + p_s \\cdot (1+\\gamma V(\\text{SE})) \\cdot \\sum_{i=0}^{\\tau-1} (1-p_s)^i \\cdot \\gamma^i\n",
    "$$\n",
    "\n",
    "For an organism in a patch with no food, there is no chance of finding food on any attempt, and so after $\\tau$ unsuccessful attempts, it moves on to a new patch, and it survives this journey with probability $\\gamma$. Thus the value of being in such a state is:\n",
    "\n",
    "$$\n",
    "V_{\\tau}(\\text{NF}) = \\gamma^{\\tau+1} \\cdot V_{\\text{new}}\n",
    "$$\n",
    "\n",
    "With each unsuccessful forage attempt that does not pass the threshold, the organism stays in the current state, but the value is discounted by $\\gamma$ the probability of the the session ending. Note that the transitions the the ***END** state that introduced a lot of complications in our state transition diagram are all nicely rolled into these recurrence relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Optimal Thresholds**\n",
    "\n",
    "These recurrence relations between the values of different states allows us to write the $V(\\text{NP})$ and $V(\\text{SE})$ in terms of each other, the parameters of the model and the thresholds.\n",
    "\n",
    "Substituting $V_{\\tau_{new}}(\\text{WF})$ and $V_{\\tau_{new}}(\\text{WF})$ into $V(NP)$ we have:\n",
    "$$V(\\text{NP}) = $$\n",
    "$$\n",
    "p_e \\cdot \\left[ (1-p_s)^{\\tau_{new}} \\cdot \\gamma^{\\tau_{new}+1} \\cdot V(\\text{NP}) + p_s \\cdot (1+\\gamma V(\\text{SE})) \\cdot \\sum_{i=0}^{\\tau_{new}-1} (1-p_s)^i \\cdot \\gamma^i \\right]\n",
    "$$\n",
    "$$\n",
    "+ (1-p_e) \\cdot \\gamma^{\\tau_{new}+1} \\cdot V(\\text{NP})\n",
    "$$\n",
    "\n",
    "Similarly substituting $V_{\\tau_{eat}}(\\text{WF})$ and $V_{\\tau_{eat}}(\\text{WF})$ into $V(SE)$ we have:\n",
    "$$V(\\text{SE}) = $$\n",
    "$$(1-p_x) \\cdot \\left[ (1-p_s)^{\\tau_{eat}+1} \\cdot \\gamma^{\\tau_{eat}} \\cdot V(\\text{NP}) + p_s \\cdot (1+\\gamma V(\\text{SE})) \\cdot \\sum_{i=0}^{\\tau_{eat}-1} (1-p_s)^i \\cdot \\gamma^i \\right]\n",
    "$$\n",
    "$$\n",
    "+ p_x \\cdot \\gamma^{\\tau_{eat}+1} \\cdot V(\\text{NP})\n",
    "$$\n",
    "\n",
    "We can further re-arrange these into a nice system of linear equations\n",
    "\n",
    "$$ \\mathbf{v} = A \\mathbf{v} + \\mathbf{b} $$ with\n",
    "\n",
    "$$\\mathbf{v} = \\begin{bmatrix}\n",
    "V(\\text{NP}) \\\\\n",
    "V(\\text{SE}) \\\\\n",
    "\\end{bmatrix}, $$\n",
    "\n",
    "and with\n",
    "\n",
    "$$ A = \\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where\n",
    "\n",
    "$$a_{11} = p_e \\cdot (1-p_s)^{\\tau_{new}} \\cdot \\gamma^{\\tau_{new}+1} + (1-p_e) \\cdot \\gamma^{\\tau_{new}+1}$$\n",
    "\n",
    "$$a_{12} = p_e \\cdot p_s \\cdot \\gamma \\cdot \\sum_{i=0}^{\\tau_{new}-1} (1-p_s)^i \\cdot \\gamma^i$$\n",
    "\n",
    "$$a_{21} = (1-p_x) \\cdot (1-p_s)^{\\tau_{eat}+1} \\cdot \\gamma^{\\tau_{eat}} + p_x \\cdot \\gamma^{\\tau_{eat}+1}$$\n",
    "\n",
    "$$a_{22} = (1-p_x) \\cdot p_s \\cdot \\gamma \\cdot \\sum_{i=0}^{\\tau_{eat}-1} (1-p_s)^i \\cdot \\gamma^i,$$\n",
    "\n",
    "and with\n",
    "\n",
    "$$ \\mathbf{b} = \\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\end{bmatrix}, $$\n",
    "\n",
    "where\n",
    "$$ b_1 = p_e \\cdot p_s \\cdot \\sum_{i=0}^{\\tau_{new}-1} (1-p_s)^i \\cdot \\gamma^{i+1} $$\n",
    "$$b_2 = (1-p_x) \\cdot p_s \\cdot \\sum_{i=0}^{\\tau_{eat}-1} (1-p_s)^i \\cdot \\gamma^{i+1}.$$\n",
    "\n",
    "When all of the parameters of the model are known and the thresholds are set, this becomes a system of linear equations with two variables, $V(NP)$ and $V(SE)$, that can be solved using linear algebra. Let's write a little function that computes $A$ and $\\mathbf{b}$ and then solves for $\\mathbf{v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TODO for students: Uncomment each line starting with ... and replace each\n",
    "# instance of ... with one of a11, a12, a21, a22, b1 and b2\n",
    "# to assign the correct computations to the correct elements of A and b\n",
    "raise NotImplementedError(\"Exercise: implement linear system of equations\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def compute_A_b_solve_v(p_e, p_x, p_s, gamma, tau_new, tau_eat, verbose=True):\n",
    "  # Summation terms for tau_new and tau_eat\n",
    "  sum_new = sum([(1-p_s)**i * gamma**i for i in range(tau_new)])\n",
    "  sum_eat = sum([(1-p_s)**i * gamma**i for i in range(tau_eat)])\n",
    "\n",
    "  # Coefficients for the matrix A and vector b\n",
    "  # ... = p_e * (1-p_s)**(tau_new) * gamma**(tau_new+1) + (1-p_e) * gamma**(tau_new+1)\n",
    "  # ... = p_e * p_s * gamma * sum_new\n",
    "  # ... = p_e * p_s * sum_new\n",
    "\n",
    "  # ... = (1 - p_x) * (1-p_s)**tau_eat * gamma**(tau_eat+1) + p_x * gamma**(tau_eat+1)\n",
    "  # ... = (1 - p_x) * p_s * gamma * sum_eat\n",
    "  # ... = (1 - p_x) * p_s * sum_eat\n",
    "\n",
    "  A = np.array([[a11, a12],\n",
    "                [a21, a22]])\n",
    "\n",
    "  b = np.array([b1, b2])\n",
    "\n",
    "  I = np.identity(2)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"Matrix A:\")\n",
    "    print(A)\n",
    "    print(\"\\nVector b:\")\n",
    "    print(b)\n",
    "    print(\"\\nMatrix I - A:\")\n",
    "    print(I - A)\n",
    "\n",
    "  # Solve the system of equations\n",
    "  v = np.linalg.solve(I - A, b)\n",
    "\n",
    "  return v[0], v[1]\n",
    "\n",
    "# Parameters from earlier simulations\n",
    "p_e = 0.5\n",
    "p_x = 0.2\n",
    "p_s = 0.9\n",
    "gamma = (1 - 0.05)\n",
    "tau_new = 1\n",
    "tau_eat = 0\n",
    "\n",
    "V_NP, V_SE = compute_A_b_solve_v(p_e, p_x, p_s, gamma, tau_new, tau_eat)\n",
    "print(\"\\nV(NP) =\", V_NP)\n",
    "print(\"V(SE) =\", V_SE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "def compute_A_b_solve_v(p_e, p_x, p_s, gamma, tau_new, tau_eat, verbose=True):\n",
    "  # Summation terms for tau_new and tau_eat\n",
    "  sum_new = sum([(1-p_s)**i * gamma**i for i in range(tau_new)])\n",
    "  sum_eat = sum([(1-p_s)**i * gamma**i for i in range(tau_eat)])\n",
    "\n",
    "  # Coefficients for the matrix A and vector b\n",
    "  a11 = p_e * (1-p_s)**(tau_new) * gamma**(tau_new+1) + (1-p_e) * gamma**(tau_new+1)\n",
    "  a12 = p_e * p_s * gamma * sum_new\n",
    "  b1  = p_e * p_s * sum_new\n",
    "\n",
    "  a21 = (1 - p_x) * (1-p_s)**tau_eat * gamma**(tau_eat+1) + p_x * gamma**(tau_eat+1)\n",
    "  a22 = (1 - p_x) * p_s * gamma * sum_eat\n",
    "  b2  = (1 - p_x) * p_s * sum_eat\n",
    "\n",
    "  A = np.array([[a11, a12],\n",
    "                [a21, a22]])\n",
    "\n",
    "  b = np.array([b1, b2])\n",
    "\n",
    "  I = np.identity(2)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"Matrix A:\")\n",
    "    print(A)\n",
    "    print(\"\\nVector b:\")\n",
    "    print(b)\n",
    "    print(\"\\nMatrix I - A:\")\n",
    "    print(I - A)\n",
    "\n",
    "  # Solve the system of equations\n",
    "  v = np.linalg.solve(I - A, b)\n",
    "\n",
    "  return v[0], v[1]\n",
    "\n",
    "# Parameters from earlier simulations\n",
    "p_e = 0.5\n",
    "p_x = 0.2\n",
    "p_s = 0.9\n",
    "gamma = (1 - 0.05)\n",
    "tau_new = 1\n",
    "tau_eat = 0\n",
    "\n",
    "V_NP, V_SE = compute_A_b_solve_v(p_e, p_x, p_s, gamma, tau_new, tau_eat)\n",
    "print(\"\\nV(NP) =\", V_NP)\n",
    "print(\"V(SE) =\", V_SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "At first glance this function may not seem all the different from other policy evaluation functions we have used in the past. However, there is both a profound and a practical difference. Our previous evaluation functions have relied on running many simulations of a game scenario (often in parallel) to get an ***estimate*** of policy effectiveness based on a stochastic sample of possible scenarios. This analytic approach is radically different. Here we have ***directly computed*** the true *expected* performance of the policy. It is not an estimate. In theory we could further analyze the value of being in a New Patch, $V(NP)$, as an implicit function of the policy parameters $\\tau_{new}$ and $\\tau_{eat}$ and directly derive the optimal thresholds. However, such an analysis quickly becomes quite complicated, and in practice it is much easier to search the parameter space using this analytic value function in an optimization process. Here, the parameter space of the policy is quite small. There are only two positive integer parameters. Use the widget below to find the best threshold policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to evaluate the expected value of this foraging scenario under different threshold combinations\n",
    "\n",
    "class ValueHeatmapWidget:\n",
    "\n",
    "  def __init__(self, p_e, p_x, p_s, gamma):\n",
    "    self.p_e = p_e\n",
    "    self.p_x = p_x\n",
    "    self.p_s = p_s\n",
    "    self.gamma = gamma\n",
    "    self.heatmap_data = np.zeros((12, 12))\n",
    "    self.fig, self.ax = plt.subplots(figsize=(10, 8))\n",
    "    self.ax.set_xlabel('Eat Threshold')\n",
    "    self.ax.set_ylabel('New Patch Threshold')\n",
    "    self.ax.set_xlim(-0.5, 10.5)\n",
    "    self.ax.set_ylim((11.5, -0.5))\n",
    "    self.ax.xaxis.tick_top()\n",
    "    self.ax.xaxis.set_label_position('top')\n",
    "    self.ax.spines['top'].set_visible(True)\n",
    "    self.ax.spines['right'].set_visible(True)\n",
    "    self.ax.spines['bottom'].set_visible(True)\n",
    "    self.ax.spines['left'].set_visible(True)\n",
    "    remove_ip_clutter(self.fig)\n",
    "    #y_ticks = self.ax.get_yticks().tolist()\n",
    "    # Adjust the labels\n",
    "    #y_labels = [str(int(tick+1)) for tick in y_ticks if 0 <= tick < 11]\n",
    "    # Set the new labels without altering the tick positions\n",
    "    #self.ax.set_yticklabels(y_labels)\n",
    "    self.text_annotations = []\n",
    "\n",
    "    # Create sliders\n",
    "    self.tau_new_slider = widgets.IntSlider(min=1, max=11, step=1, value=5)\n",
    "    self.tau_eat_slider = widgets.IntSlider(min=0, max=10, step=1, value=5)\n",
    "    self.tau_new_label = widgets.Label(value=\"New Patch Threshold\")\n",
    "    self.tau_eat_label = widgets.Label(value=\"Successful Eating Threshold\")\n",
    "    self.tau_new_box = widgets.VBox([self.tau_new_label, self.tau_new_slider])\n",
    "    self.tau_eat_box = widgets.VBox([self.tau_eat_label, self.tau_eat_slider])\n",
    "\n",
    "    # Create compute button\n",
    "    self.compute_button = widgets.Button(description=\"Compute Value\")\n",
    "    self.compute_button.on_click(self.update_plot)\n",
    "\n",
    "    # Put the display together with the VBoxes and button\n",
    "    self.sliders = widgets.VBox([self.tau_new_box, self.tau_eat_box, self.compute_button])\n",
    "    self.final_display = widgets.HBox([self.sliders, self.fig.canvas])\n",
    "\n",
    "  def update_cax(self, data):\n",
    "    if hasattr(self, 'cax'):\n",
    "      self.cax.set_data(data)\n",
    "      self.cax.autoscale()\n",
    "      self.colorbar.remove()\n",
    "    else:\n",
    "      self.cax = self.ax.matshow(data, cmap='gray_r')\n",
    "\n",
    "    self.colorbar = self.fig.colorbar(self.cax, ax=self.ax)\n",
    "\n",
    "  def update_plot(self, change=None):\n",
    "    tau_new = self.tau_new_slider.value\n",
    "    tau_eat = self.tau_eat_slider.value\n",
    "    V_NP, _ = compute_A_b_solve_v(p_e, p_x, p_s, gamma,\n",
    "                                    tau_new, tau_eat, verbose=False)\n",
    "    # Update heatmap_data\n",
    "    self.heatmap_data[tau_new, tau_eat] = V_NP\n",
    "\n",
    "    # Plot\n",
    "    self.update_cax(self.heatmap_data)\n",
    "\n",
    "    for txt in self.text_annotations:\n",
    "      txt.remove()\n",
    "    self.text_annotations.clear()\n",
    "\n",
    "    # Annotate heatmap\n",
    "    for i in range(11):\n",
    "      for j in range(12):\n",
    "        c = self.heatmap_data[j, i]\n",
    "        if c != 0:\n",
    "          txt = self.ax.text(i, j, f\"{c:.2f}\", va='center', ha='center', color='w' if c > 0.5 else 'k')\n",
    "          self.text_annotations.append(txt)\n",
    "\n",
    "    self.fig.canvas.draw()\n",
    "\n",
    "\n",
    "def compute_A_b_solve_v(p_e, p_x, p_s, gamma, tau_new, tau_eat, verbose=True):\n",
    "  # Summation terms for tau_new and tau_eat\n",
    "  sum_new = sum([(1-p_s)**i * gamma**i for i in range(tau_new)])\n",
    "  sum_eat = sum([(1-p_s)**i * gamma**i for i in range(tau_eat)])\n",
    "\n",
    "  # Coefficients for the matrix A and vector b\n",
    "  a11 = p_e * (1-p_s)**(tau_new) * gamma**(tau_new+1) + (1-p_e) * gamma**(tau_new+1)\n",
    "  a12 = p_e * p_s * gamma * sum_new\n",
    "  b1  = p_e * p_s * sum_new\n",
    "\n",
    "  a21 = (1 - p_x) * (1-p_s)**tau_eat * gamma**(tau_eat+1) + p_x * gamma**(tau_eat+1)\n",
    "  a22 = (1 - p_x) * p_s * gamma * sum_eat\n",
    "  b2  = (1 - p_x) * p_s * sum_eat\n",
    "\n",
    "  A = np.array([[a11, a12],\n",
    "                [a21, a22]])\n",
    "\n",
    "  b = np.array([b1, b2])\n",
    "\n",
    "  I = np.identity(2)\n",
    "\n",
    "  if verbose:\n",
    "    print(\"Matrix A:\")\n",
    "    print(A)\n",
    "    print(\"\\nVector b:\")\n",
    "    print(b)\n",
    "    print(\"\\nMatrix I - A:\")\n",
    "    print(I - A)\n",
    "\n",
    "  # Solve the system of equations\n",
    "  v = np.linalg.solve(I - A, b)\n",
    "\n",
    "  return v[0], v[1]\n",
    "\n",
    "# Using the class\n",
    "vh_widget = ValueHeatmapWidget(p_e=0.5, p_x=0.2, p_s=0.9, gamma=(1 - 0.05))\n",
    "display(vh_widget.fig.canvas)\n",
    "clear_output()\n",
    "display(vh_widget.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[Answer](## \"Answer: New Patch Threshold = 1, Successful Eating Threshold = 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To help appreciate the practical difference between these two approaches we will time and compare a simulation based approach to policy evaluation, together with this analytic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Simulation based policy evaluation\n",
    "%%timeit\n",
    "pfg = PatchyForagingGame(batch_size=50, food_patch_prob=0.5, food_extinct_prob=0.2,\n",
    "                         death_rate=0.05, food_regen_prob=0)\n",
    "stp = SimpleThresholdPlayer(pfg, critter_index=1, threshold_new=1, threshold_known=2)\n",
    "result = pfg.play_game([stp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Analytical policy evaluation\n",
    "%%timeit\n",
    "p_e = 0.5\n",
    "p_x = 0.2\n",
    "p_s = 0.9\n",
    "gamma = (1 - 0.05)\n",
    "tau_new = 1\n",
    "tau_eat = 0\n",
    "\n",
    "V_NP, V_SE = compute_A_b_solve_v(p_e, p_x, p_s, gamma, tau_new, tau_eat, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "With some thought and effort we can get an exact answer in less the 1/1000 of the time it takes to get a rough estimate based on simulation. In this particular case the parameter space (after we introduced the idea of a threshold policy) is relatively small so these difference don't affect the overall tractability of the problem, but in other more complicated parameter spaces this kind of difference in function evaluation cost can mean the difference between getting a solution in minutes or getting a solution in weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# **1.2.3.3 Propose and Test versus Reasoning**\n",
    "\n",
    "Now that we've reasoned about policies and used our analysis to determine the optimal policy let's see how our old standby approach 'Propose and Test' does with this same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Propose and Test\n",
    "# @markdown Run this cell to use the propose and test method to find good parameters for this patchy foraging problem.\n",
    "\n",
    "def evaluate(params, game):\n",
    "  stp = SimpleThresholdPlayer(game, critter_index=1, threshold_new=params[0],\n",
    "                              threshold_known=params[1])\n",
    "  result = game.play_game([stp])\n",
    "  return(np.mean(result['scores']))\n",
    "\n",
    "\n",
    "def patchy_propose_and_test(batch_size=400,\n",
    "                            initial_params=None,\n",
    "                            max_rejected=200,\n",
    "                            verbose=True):\n",
    "\n",
    "  game = PatchyForagingGame(batch_size=batch_size, food_patch_prob=0.5,\n",
    "                            food_extinct_prob=0.2, death_rate=0.05,\n",
    "                            food_regen_prob=0)\n",
    "  tested_params_dict = {}\n",
    "  # Initialize parameters\n",
    "  if initial_params is None:\n",
    "    initial_params = 5*np.ones(2, dtype=int)\n",
    "  best_params = initial_params\n",
    "  best_avg_score = evaluate(best_params, game)\n",
    "  print(f\"Initial score: {best_avg_score}\")\n",
    "  rejected_count = 0\n",
    "  total_tests = 0  # Number of iterations\n",
    "  if verbose:\n",
    "    intermediate_params = []\n",
    "    intermediate_params.append(best_params)\n",
    "    intermediate_values = [best_avg_score]\n",
    "    iterations = [0]\n",
    "\n",
    "  # Propose-and-test loop\n",
    "  start_time = time.time()\n",
    "  while rejected_count < max_rejected:\n",
    "    total_tests +=1\n",
    "    # Propose new parameters: sample custom list of small discrete steps\n",
    "    # centered at best_params\n",
    "    delta_dim1 = np.random.choice([-2, -1, 0, 1, 2])\n",
    "    delta_dim2 = np.random.choice([-2, -1, 0, 1, 2])\n",
    "    delta_params = np.array([delta_dim1, delta_dim2])\n",
    "    proposal_params = best_params + delta_params\n",
    "    # Ensure that the values for the first dimension don't go below 0\n",
    "    proposal_params[0] = max(0, proposal_params[0])\n",
    "    proposal_params[1] = max(1, proposal_params[1])\n",
    "\n",
    "    if tuple(proposal_params) in tested_params_dict.keys():  # Step 2: Check if the proposed parameters have been tested before.\n",
    "      #print('already tested' + str(proposal_params))\n",
    "      rejected_count += 1\n",
    "      continue\n",
    "\n",
    "    #print('now testing' + str(proposal_params))\n",
    "    avg_score = evaluate(proposal_params, game)\n",
    "    tested_params_dict[tuple(proposal_params)] = avg_score\n",
    "\n",
    "    if avg_score > best_avg_score:\n",
    "      best_params = proposal_params\n",
    "      best_avg_score = avg_score\n",
    "      if verbose:\n",
    "        print('best params so far:')\n",
    "        display(best_params)\n",
    "        print(f\"Best score so far: {best_avg_score}\")\n",
    "        print(f\"Found after a total of {time.time() - start_time:.2f} seconds\")\n",
    "        rejected_count = 0\n",
    "    else:\n",
    "      rejected_count += 1\n",
    "\n",
    "  end_time = time.time()\n",
    "  elapsed_time = end_time - start_time\n",
    "  iterations.append(total_tests)\n",
    "\n",
    "  if verbose:\n",
    "    # Print the best found parameters and score\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Average Score:\", best_avg_score)\n",
    "    print(\"Parameter combinations tested:\", total_tests)\n",
    "    print(f\"Time taken for the optimization loop: {elapsed_time:.2f} seconds\")\n",
    "    return (best_params, best_avg_score, tested_params_dict)\n",
    "  else:\n",
    "    return best_params, best_avg_score\n",
    "\n",
    "best_params, best_score, tested_params_dict = patchy_propose_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Questions:\n",
    "1. Did the propose and test method using simulation based evaluation find the same optimal parameters as we did using analytical policy evaluation? [Answer](## \"Using simulation based evaluation sometimes finds the true optimal parameters as determined using analytical policy evaluation, but due to the stochastic nature of this evaluation method sometimes propose and test only finds parameters that are close to optimal.\")\n",
    "2. What about the best scores reported by the propose and test algorithm using simulation based evaluation, are these higher, lower or about the same as determined using an analytical approach? [Answer](## \"The best scores reported by the propose and test algorithm using simulation based evaluation are usually close to the true expected value, but again due to stochastic evaluation they are never exactly the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Multiple Perspectives on Optima**\n",
    "\n",
    "In this sequence we have looked at the same patchy foraging problem in two different ways.\n",
    "\n",
    "The first perspective was to interact with and observe a 'full' simulation of the problem, where an organism plays through rounds of a game according to some policy and receives a score. This is how we have approached everything so far in this book. Although this perspective and this game are highly simplified compared to lived reality, this general approach of 'run it and see' is exactly what evolution is doing. Variations on the policy emerge due to recombination and mutation, organisms live their lives, and the effectiveness of their policy is 'evaluated' via natural selection, roughly as the number of offspring surviving to the next generation. Importantly, the evolutionary selective process needs no analytic insight into the nature of the environment in which organisms are born and execute their policies. It is a black box, 'run it and see' approach.\n",
    "\n",
    "The second perspective, which we introduced here was to think about the nature of the problem, to write out a symbolic mathematical expressions that described the problem, and then through deductive reasoning arrive at an exact analytical method of evaluating threshold policy performance in this problem.\n",
    "\n",
    "These perspectives, for the most part, converged on the same optimal policy. Why is this? The optimal solution of a problem is inherent to the problem itself, not the method used to find the solution. Thus, different optimization approaches applied to the same problem, will arrive at the same conclusions, to the extent that each different approach is effective. This insight has been a powerful tool in understanding evolved behaviours and traits, with optimality models helping to explain a staggering array of animal behaviours (and the behaviour of all kinds of life). These are just a few examples:\n",
    "\n",
    "1. **Humming Bird Foraging**: Many animals are under selection pressure to maximize their net energy intake while minimizing the risk. The sequence in which hummingbirds feed on the flowers in their territory can be understood as minimizing flight distance and thus saving energy and time. This behavior exemplifies an energy-efficient strategy that boosts survival and reproductive chances.\n",
    "\n",
    "2. **Web Building in Spiders**: Considering the metabolic cost of producing silk, spiders build their webs to capture the most prey while using the least amount of material. Different spider species, depending on their ecological niche, have optimized their web designs accordingly - from the dense tangle webs of the funnel-web spiders to the expansive orb webs of garden spiders.\n",
    "\n",
    "3. **Echolocation in Bats**: Bats optimize the frequency and duration of their echolocation calls based on their environment and prey type. In cluttered environments, bats often use shorter frequency-modulated calls to detect nearby objects. However, in open spaces, they employ longer calls for detection over a more extended range. This behavioral adaptation can be viewed as an optimization problem where bats adjust their call parameters for maximum prey detection.\n",
    "\n",
    "4. **Migratory Paths of Birds**: Migratory birds traverse vast distances, optimizing their routes to conserve energy, avoid predators, and capitalize on food sources. For example, the Arctic tern, traveling from its Arctic breeding grounds to the Antarctic coast, chooses a path that, although not the shortest, exploits prevailing wind patterns and abundant food sources.\n",
    "\n",
    "5. **Root Architecture in Plants**: The way a plant structures its root system is crucial for both nutrient uptake and stability. For instance, in areas prone to drought, plants like mesquite trees invest in deep taproots that reach far into the soil, tapping into groundwater sources. In contrast, plants in nutrient-rich but unstable environments, like riverbanks, invest in expansive but shallow root systems to maximize nutrient absorption while anchoring themselves against fast-moving water.\n",
    "\n",
    "6. **Leaf Orientation and Sun Tracking**: Sunflower fields during a sunny day offer a mesmerizing sight: rows of flowers all facing the sun. This behavior, known as heliotropism, allows the plant to maximize sunlight absorption. By tracking the sun across the sky, sunflowers optimize photosynthesis rates. Similarly, desert plants like the creosote bush have small, vertically-oriented leaves, reducing their exposure to the intense midday sun and minimizing water loss due to transpiration.\n",
    "\n",
    "7. **Thermoregulation in Reptiles**: Cold-blooded reptiles, such as lizards and snakes, primarily thermoregulate through behaviour (not metabolically, like warm-blooded mammals and birds). Take the diurnal behavior of desert lizards: by basking in the morning sun, they quickly elevate their body temperature, optimizing their physiological functions for the day ahead. As midday approaches with its peak heat, these reptiles retreat to cooler shades or burrows, ensuring they don't overheat. This can be understood as an optimization strategy, balancing the need for warmth to maintain metabolic rates with the imperative to avoid lethal high temperatures.\n",
    "\n",
    "8. **Camouflage and Coloration**: The appearance of many plants and animals appearances can be understood as optimization for avoiding detection. Consider the peppered moth in industrial England. Historically, the majority of these moths were light-colored, blending seamlessly with the lichen-covered trees. However, during the Industrial Revolution, pollution darkened the trees, giving an advantage to rare dark-colored moths. As these darker moths thrived in their new environment, their numbers surged, demonstrating a rapid optimization of camouflage in response to environmental changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Each of the examples above takes an 'Ultimate' evolutionary functional perspective to explain behaviours or traits. However, this is but one of four perspectives from which to understand a behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Different Kinds of Understanding: Tinbergen's Four Questions**\n",
    "\n",
    "Building on work by Julian Huxley, Niko Tinbergen described four \"whys\" or perspectives for understanding an observed behaviour (adapted from Aristotle's four causes). These continue to ground and inform interdisciplinary work across the behavioural sciences.\n",
    "\n",
    "1. **Mechanistic** (Sensory and Biophysical Causes): This approach seeks to understand the immediate causes of a behaviour from the sensory input that elicits it through to the biophysical processes of the organism which produce the behaviour in response to a particular environmental input. While the 'Ultimate' perspective might explain why a bird sings in terms of its evolutionary function, e.g. to attract a mate, the mechanistic perspective looks at how a bird's vocal cords, brain circuits, and hormones function together to produce the song in response to certain environmental conditions, but not others.\n",
    "\n",
    "2. **Ontogenetic** (Developmental Causes): This perspective examines how a behavior develops in an individual over their lifetime. It encompasses aspects of both learning and maturation. For example, while many birds are born with the inherent capability to sing, they often refine their songs through interactions with their environment and other birds, improving their repertoire and accuracy over time.\n",
    "\n",
    "3. **Functional** (Adaptive Evolutionary Causes): This is the perspective we have focused on so far. It seeks to understand behavior in terms of the selective advantage it confers to the organism. How does the behavior increase the chances of survival and reproduction? The hummingbird foraging and the spider web-building examples fit into this category, showcasing behaviors that have been optimized over generations for increased fitness.\n",
    "\n",
    "4. **Phylogenetic** (Historical Evolutionary Causes): This perspective traces the behavior's lineage through evolutionary history. Why does a particular species exhibit this behavior, but not another? Did the common ancestor of these species also exhibit this behavior? How has evolutionary history set the conditions and constraints on which adaptive new forms are possible? For instance, the vertebrate eye (including the human eye) has a blind spot, whereas octopus eyes do not. In each of these two lineages, the eye originally developed in one of two distinct ways. Once this initial development program for the vertebrate eye was set, there were no intermediate forms that were both adaptive and would have enabled it to evolve without a blind spot.\n",
    "\n",
    "Scientific work can be done using one, several, or all of these perspectives combined. No single perspective is inherently better or more important. What is deeply valuable though, for any project within the life sciences, is knowing which kinds of understanding are being aimed at, and how work from the other perspectives might guide and constrain understanding in the area(s) of focus.\n",
    "\n",
    "In this book we will often take a functional perspective. However, understanding how phylogeny, ontogeny, and mechanism constrain and determine the optimization problems to be analyzed is critical to the relevance and validity of the functional perspective.\n",
    "\n",
    "Where does this four-perspective framework situate neuroscience within the broader behavioral sciences? Traditionally, neuroscience has largely been part of a mechanistic understanding of behaviour, i.e. how do the brain circuits compute and produce muscles activations that generate appropriate behaviour given environmental inputs. Learning is also obvious and important part of what brains do. To the extent that neuroscience research is focused on learning, i.e. adaptive neural plasticity, it is also a part of the ontogenetic understanding of behaviour.\n",
    "\n",
    "We believe that new kinds of understanding, in particular, understanding grounded in an ontogenetic and functional perspective, have recently become possible based on new insights into the optimization of high-dimensional systems emerging from Machine Learning research. At the same time, again based on insights from Machine Learning research, we believe that neuroscience as a field is approaching the natural limits of what can be comprehended about the brain and its behavioural outputs in a certain mechanistic sense. Given this, we advocate a shift away from attempts at 'circuit diagram understanding' of how brains compute and determine behaviour. In the field of ML 'circuit diagrams' of large complicated models that generate adaptive behaviour in complex environments are readily available for comprehensive and detailed inspection, all within completely controllable artificial environments. Yet, despite this total visibility, a satisfactory 'understanding' of how these many parameter systems compute and determine behaviour, so called 'interpretability', is so far largely absent. Conversely, ML researchers do have understanding of the learning algorithms that produce these complex and largely inscrutable models, i.e. the ontogeny of the these models, and the mechanisms of this ontogeny are relaitvely clear. It is for this reason that we encourage a 'learning first' approach to neuroscience, focused on understanding the adaptive neural plasticity dynamics governing brain development and the ongoing learning and behavioural plasticity that this supports. We posit that this, together with the mechanistic underpinnings of these learning dynamics, all considered in light of the overarching adaptive functional role of the brain, will lead to a predictive and integrative understanding of the brain.\n",
    "\n",
    "In the next chapter, we will focus primarily on evolution, aiming to elucidate the general adaptive functions of the brain. Then, in subsequent parts of the book we will connect different classes of machine learning algorithms to the specific adaptive challenges they might address. In each case we will show how insights from effective ML algorithms on a specific class of adaptive problem can both integrate existing empirical neuroscience work and inform future empirical work, within a 'learning first' perspective on brain function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P1C2_Sequence3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
