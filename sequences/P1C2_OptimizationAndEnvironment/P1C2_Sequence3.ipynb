{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C2_OptimizationAndEnvironment/P1C2_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C2_OptimizationAndEnvironment/P1C2_Sequence3.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "____\n",
    "# **1.2.3: Normative Thinking**\n",
    "\n",
    "### **Objective:**\n",
    "Up until this point, we have predominantly viewed optimization as a tool for policy improvement, adapting behavior to better suit a particular environmental niche. This is a prescriptive view of optimization: \"To improve the policy, implement these steps.\" However, optimization isn't solely prescriptive. It can also be used descriptively and predicatively. In this sequence we will learn about using optimization predictively. We call this 'Normative Thinking'.\n",
    "\n",
    "If we make a series of reasonable assumptions—like behaviour being the result of learning processes which in turn are shaped by evolutionary forces, both of which adapt according to environmental demands—we can posit that the optimal behavior predicted by a model will align with the behaviour exhibited by real-world organisms. Crucially though, the validity of such predictions hinges on the extent to which the ***relevant*** intricacies of the evolutionary organism-environment dynamic are captured by the model.\n",
    "\n",
    "In this sequence, we will:\n",
    "\n",
    "* Investigate a new foraging scenario where policy performance can be mathematically-derived.\n",
    "\n",
    "* Use deduction and reasoning to derive policy performance and determine the optimal policy.\n",
    "\n",
    "* Observe how a simple propose-accept-reject algorithm, unaware of the structure of the problem, converges on the same optimal outcome as our analytical approach.\n",
    "\n",
    "* Ponder the beauty and potential power of this connection between deductive reasoning about policies and 'empirical' observation. There are many different optimization methods and approaches, but all effective optimization approaches will lead to -mostly- the same place.\n",
    "\n",
    "* Look at how optimization approaches fit within a broader framework of scientific understanding generally and what the means for neuroscientific understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Dependencies, Imports and Setup.** You don't need to worry about how this code works – but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image, IFrame\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
    "             'gw_widgets.py', 'gw_NN_RL.py']\n",
    "#filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C2_S3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Graph Viz Helper Functions\n",
    "################################################################\n",
    "\n",
    "\n",
    "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
    "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
    "  ax.axis(\"off\")\n",
    "  #plt.tight_layout()\n",
    "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
    "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
    "  for edge in edge_labels:\n",
    "    src, dest, latex_str = edge\n",
    "    if graph.has_edge(src, dest):\n",
    "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
    "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
    "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
    "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\")\n",
    "      graph.delete_edge(src, dest)\n",
    "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
    "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
    "  return graph\n",
    "\n",
    "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
    "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
    "  for node in graph.nodes():\n",
    "    if not node.startswith(\"dummy\"):\n",
    "      node.attr['width'] = width\n",
    "      node.attr['height'] = height\n",
    "  return graph\n",
    "\n",
    "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                            node_colors = {},\n",
    "                            node_labels = {},\n",
    "                            output_path=\"graphviz_output.png\", dpi=300,\n",
    "                            figsize=(0.6, 0.3), fontsize=16):\n",
    "  \"\"\"\n",
    "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
    "\n",
    "  Parameters:\n",
    "    nodes_list (list): List of nodes in the graph.\n",
    "    edges_list (list): List of edges in the graph.\n",
    "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
    "    output_path (str): Path to save the rendered graph.\n",
    "    dpi (int): DPI for rendering the graph.\n",
    "    figsize (tuple): Figure size for the LaTeX labels.\n",
    "\n",
    "  Returns:\n",
    "    str: Path to the saved graph image.\n",
    "  \"\"\"\n",
    "\n",
    "  # Graph Creation and Configuration\n",
    "  G = pgv.AGraph(directed=True, strict=False, rankdir='LR', ranksep=0.5, nodesep=0.5)\n",
    "  for node in nodes_list:\n",
    "    color = node_colors.get(node, \"black\")\n",
    "    label = node_labels.get(node, node)\n",
    "    G.add_node(node, color=color, label=label)\n",
    "  for edge in edges_list:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
    "  G = set_regular_node_sizes(G, width=1, height=1)\n",
    "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=figsize, fontsize=fontsize)\n",
    "\n",
    "  # Additional graph attributes\n",
    "  G.graph_attr['size'] = \"8,8\"\n",
    "  G.graph_attr['dpi'] = str(dpi)\n",
    "\n",
    "  # Render and save the graph\n",
    "  G.layout(prog='dot')\n",
    "  G.draw(output_path)\n",
    "\n",
    "  return output_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# make PatchyForageBoard class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchyForageBoard():\n",
    "  \"\"\"\n",
    "  A collection of methods and parameters of a patchy foraging game board that\n",
    "  define the logic of the game, and allows for multiple critters on the same\n",
    "  board\n",
    "\n",
    "  game state is represented by primarily by food locations, forager locations,\n",
    "  predator locations, scores, and rounds left\n",
    "  food patch locations are stored on a batch x n_rows x n_cols numpy array,\n",
    "  forager and predator(when we have them) locations are stored as dictionaries\n",
    "  with integer keys corresponding to a forager/predatore 1, 2, 3 etc, and then\n",
    "  np.argwhere style tuples of arrays of (batch_array, row_array, col_array)\n",
    "  giving the locations\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game.\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "  ARRAY_PAD_VALUE = -200\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=10, n_cols=5,\n",
    "               num_foragers=1,\n",
    "               max_foraging_attempts=20,\n",
    "               food_patch_prob = 0.4,\n",
    "               food_regen_prob=0.0,\n",
    "               forage_success_prob = 0.7,\n",
    "               food_extinct_prob = 0.1, rng=None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_foragers = num_foragers\n",
    "    self.max_foraging_attempts = max_foraging_attempts\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.forage_success_prob = forage_success_prob\n",
    "    self.food_extinct_prob = food_extinct_prob\n",
    "    self.food_regen_prob = food_regen_prob\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"\n",
    "    Set up starting board using game parameters\n",
    "\n",
    "    Returns:\n",
    "      state (dict):\n",
    "      The state dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new\n",
    "    \"\"\"\n",
    "    # note that is_over applies at the batch level not the batch x forager level\n",
    "    self.is_over = np.zeros(self.batch_size, dtype=bool)\n",
    "    self.foraging_attempts = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.scores = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    # create an empty board array for food locs\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols),\n",
    "                           dtype=int)\n",
    "    # Place critters in top left corner of the board\n",
    "    self.forager_locs = {}\n",
    "    for c in (np.arange(self.num_foragers)+1):\n",
    "      self.forager_locs[c] = (np.arange(self.batch_size, dtype=int),\n",
    "                              np.zeros(self.batch_size, dtype=int),\n",
    "                              np.zeros(self.batch_size, dtype=int))\n",
    "    # Initial food patches on the board randomly\n",
    "    # each grid has an independent prob of being a pathc (to make the math\n",
    "    # easier later) so total number of patches on a board is binomially\n",
    "    # distributed\n",
    "    num_foods = self.rng.binomial(n=self.n_rows * self.n_cols,\n",
    "                                  p=self.food_patch_prob,\n",
    "                                  size=self.batch_size)\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(self.n_rows, self.n_cols,\n",
    "                                               num_foods[ii])\n",
    "      # food patch start locations (do each patch separate in case we\n",
    "      # want to have different kinds of patches)\n",
    "      for f_ in np.arange(num_foods[ii]):\n",
    "        self.pieces[(ii, rc_idx[0][f_],\n",
    "                         rc_idx[1][f_])] = - 1\n",
    "    # keep track of which foragers have missed how many times\n",
    "    # at what kind of patch\n",
    "    self.misses_new_patch = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.misses_known_patch = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.at_new_patch = np.ones((self.batch_size, self.num_foragers), dtype=bool)\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'foraging_attempts': self.foraging_attempts.copy(),\n",
    "             'is_over': self.is_over.copy(),\n",
    "             'forager_locs': copy.deepcopy(self.forager_locs),\n",
    "             'misses_new_patch': self.misses_new_patch.copy(),\n",
    "             'misses_known_patch': self.misses_known_patch.copy(),\n",
    "             'at_new_patch': self.at_new_patch.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\"\n",
    "    Sets the state given a board dictionary.\n",
    "\n",
    "    Args:\n",
    "      board (dict):\n",
    "      The board dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new patch.\n",
    "    \"\"\"\n",
    "    self.pieces = board['pieces'].copy()\n",
    "    self.forager_locs = copy.deepcopy(board['forager_locs'])\n",
    "    self.foraging_attempts = board['foraging_attempts'].copy()\n",
    "    self.scores = board['scores'].copy()\n",
    "    self.is_over = board['is_over'].copy()\n",
    "    self.misses_new_patch = board['misses_new_patch'].copy()\n",
    "    self.misses_known_patch = board['misses_known_patch'].copy()\n",
    "    self.at_new_patch = board['at_new_patch'].copy()\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\"\n",
    "    Returns the current board state.\n",
    "\n",
    "    Returns:\n",
    "      state (dict):\n",
    "      The state dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new patch.\n",
    "    \"\"\"\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'foraging_attempts': self.foraging_attempts.copy(),\n",
    "             'is_over': self.is_over.copy(),\n",
    "             'forager_locs': copy.deepcopy(self.forager_locs),\n",
    "             'misses_new_patch': self.misses_new_patch.copy(),\n",
    "             'misses_known_patch': self.misses_known_patch.copy(),\n",
    "             'at_new_patch': self.at_new_patch.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  ################# CORE GAME STATE UPDATE LOGIC ##############################\n",
    "  ################# execute_moves is main, uses these helper functions ########\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, which_critter):\n",
    "    \"\"\"\n",
    "    Execute the moves on the board. A move to the current location implies\n",
    "    foraging. If foraging, check if foraging is successful, update scores,\n",
    "    and check if the food goes extinct. If moving to a new location, simply\n",
    "    update the critter's location.\n",
    "\n",
    "    Args:\n",
    "      moves (tuple): A tuple of three arrays:\n",
    "        - batch_array: Specifies which board in the batch the move corresponds to.\n",
    "        - row_array: Specifies the target row for each move.\n",
    "        - col_array: Specifies the target column for each move.\n",
    "        Each array in the tuple has the same length. A move is represented by\n",
    "        the combination of a batch index, row index, and column index at the\n",
    "        same position in their respective arrays.\n",
    "      which_critter (int): Index to identify the critter. Starts from 1.\n",
    "\n",
    "    Returns: Nothing, just updates state related attributes of the board object\n",
    "\n",
    "    \"\"\"\n",
    "    #expand moves tuple\n",
    "    batch_moves, row_moves, col_moves = moves\n",
    "\n",
    "    # Get current locations of the critter\n",
    "    current_locs = self.forager_locs[which_critter]\n",
    "\n",
    "    # Iterate over each board in the batch\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # If the game is over for this board, skip\n",
    "      if self.is_over[ii]:\n",
    "        continue\n",
    "\n",
    "      # Get new location directly from the moves\n",
    "      new_row = int(row_moves[ii])\n",
    "      new_col = int(col_moves[ii])\n",
    "\n",
    "      # Check if the critter has moved to a new patch\n",
    "      if (new_row, new_col) != (current_locs[1][ii], current_locs[2][ii]):\n",
    "        # Moved to a new patch\n",
    "        self.misses_new_patch[ii, which_critter - 1] = 0\n",
    "        self.misses_known_patch[ii, which_critter - 1] = 0\n",
    "        self.at_new_patch[ii, which_critter - 1] = True\n",
    "\n",
    "      # If the critter's position has not changed, it's trying to forage\n",
    "      elif (new_row, new_col) == (current_locs[1][ii], current_locs[2][ii]):\n",
    "        # increment foraging attempt only if foraging\n",
    "        self.foraging_attempts[ii, which_critter - 1] += 1\n",
    "        # Check if there's food at the location\n",
    "        if self.pieces[ii, new_row, new_col] < 0:\n",
    "          # Check if foraging is successful\n",
    "          if self.rng.random() < self.forage_success_prob:\n",
    "            # Successful foraging, increase critter's score\n",
    "            self.scores[ii, which_critter - 1] += 1\n",
    "            # misses are zeroed and no longer at new patch\n",
    "            self.misses_new_patch[ii, which_critter - 1] = 0\n",
    "            self.misses_known_patch[ii, which_critter - 1] = 0\n",
    "            self.at_new_patch[ii, which_critter - 1] = False\n",
    "            # Check if food goes extinct (only on success)\n",
    "            if self.rng.random() < self.food_extinct_prob:\n",
    "              self.pieces[ii, new_row, new_col] = 0  # Set it to empty\n",
    "          else:\n",
    "            #unsuccessful foraging at patch with food\n",
    "            if self.at_new_patch[ii, which_critter - 1]:\n",
    "              # at a new patch\n",
    "              self.misses_new_patch[ii, which_critter - 1] += 1\n",
    "            else:\n",
    "              # at a known patch\n",
    "              self.misses_known_patch[ii, which_critter - 1] += 1\n",
    "        else:\n",
    "          #unsuccessful foraging at patch without food\n",
    "            if self.at_new_patch[ii, which_critter - 1]:\n",
    "              # at a new patch\n",
    "              self.misses_new_patch[ii, which_critter - 1] += 1\n",
    "            else:\n",
    "              # at a known patch\n",
    "              self.misses_known_patch[ii, which_critter - 1] += 1\n",
    "\n",
    "      # Always check if session is over\n",
    "      if self.foraging_attempts[ii] >= self.max_foraging_attempts:\n",
    "        self.is_over[ii] = True\n",
    "\n",
    "    # assume moves are legal and update locs for whole batch at once\n",
    "    self.forager_locs[which_critter] = (batch_moves, row_moves, col_moves)\n",
    "\n",
    "  ###### Getting Legal Moves and Perceptions #########################\n",
    "  ####################################################################\n",
    "  def get_neighbor_grc_indices(self, which_critter, radius, pad=False):\n",
    "    \"\"\"\n",
    "    Returns all grid positions within a certain cityblock distance radius from\n",
    "    the place corresponding to which_critter.\n",
    "\n",
    "    Args:\n",
    "        which_critter (int): The idex of the focal critter_food.\n",
    "        radius (int): The cityblock distance.\n",
    "        pad (bool): whether or not to pad the array, if padded all row, col\n",
    "          indexes are valid for the padded array, useful for getting percept\n",
    "          if not all indexes are correct for the original array, useful for\n",
    "          figuring out legal moves.\n",
    "\n",
    "    Returns:\n",
    "        an array of indices, each row is a g, r, c index for the neighborhoods\n",
    "        around the critters, can use the g value to know which board you are in.\n",
    "        if pad=True also returns the padded array (the indices in that case) are\n",
    "        for the padded array, so won't work on self.pieces, whereas if pad is\n",
    "        False the indices will be for the offsets in reference to the original\n",
    "        self.pieces, but note that some of these will be invalid, and will\n",
    "        need to be filtered out (as we do in get_legal)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    batch, rows, cols = self.forager_locs[which_critter]\n",
    "    # Create meshgrid for offsets\n",
    "    if pad is True:\n",
    "      padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "        (radius, radius)), constant_values=self.ARRAY_PAD_VALUE)\n",
    "      rows = rows + radius\n",
    "      cols = cols + radius\n",
    "\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    neighbors_rows = extended_rows + valid_row_offsets\n",
    "    neighbors_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    indices = np.column_stack((np.repeat(np.arange(batch_size),\n",
    "                                         neighbors_rows.shape[1]),\n",
    "                               neighbors_rows.ravel(),\n",
    "                               neighbors_cols.ravel()))\n",
    "    if pad is False:\n",
    "      return indices\n",
    "    elif pad is True:\n",
    "      return indices, padded_arr\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, which_critter, radius=1):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter.\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    critter_locs = np.array(self.forager_locs[which_critter])\n",
    "    # turn those row, col offsets into a set of legal offsets\n",
    "    legal_offsets = self.get_neighbor_grc_indices(which_critter, radius)\n",
    "    legal_offsets = {tuple(m_) for m_ in legal_offsets}\n",
    "\n",
    "    legal_destinations = np.where(np.ones(self.pieces.shape, dtype=bool))\n",
    "    legal_destinations = {tuple(coords) for coords in zip(*legal_destinations)}\n",
    "    # Add the current locations of the critters to legal_destinations\n",
    "    current_locations = {tuple(loc) for loc in critter_locs.T}\n",
    "    legal_destinations = legal_destinations.union(current_locations)\n",
    "\n",
    "    # legal moves are both legal offsets and legal destinations\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_perceptions(self, critter_food, radius):\n",
    "    idx, pad_pieces = self.get_neighbor_grc_indices(critter_food,\n",
    "                                                    radius, pad=True)\n",
    "    #percept_mask = np.zeros(pad_pieces.shape, dtype=bool)\n",
    "    #percept_mask[idx[:,0], idx[:,1]], idx[:,2]] = True\n",
    "    percept = pad_pieces[idx[:,0], idx[:,1], idx[:,2]]\n",
    "    return(percept.reshape(self.batch_size, -1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# make PatchyForageGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchyForagingGame():\n",
    "  \"\"\"\n",
    "  A collection of methods and parameters of a patchy foraging game that allow\n",
    "  for interaction with and display of PatchyForageBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1, n_rows=10, n_cols=5, num_foragers=1,\n",
    "               max_foraging_attempts=20, food_patch_prob=0.3, food_regen_prob=0.0,\n",
    "               forage_success_prob=0.6, food_extinct_prob=0.2, rng=None):\n",
    "    \"\"\"\n",
    "    Initializes an instance of the PatchyForagingGame with the specified parameters.\n",
    "    Args:\n",
    "    ... [same as in PatchyForageBoard]\n",
    "    \"\"\"\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_foragers = num_foragers\n",
    "    self.max_foraging_attempts = max_foraging_attempts\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.forage_success_prob = forage_success_prob\n",
    "    self.food_extinct_prob = food_extinct_prob\n",
    "    self.food_regen_prob = food_regen_prob\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns the initial state of the game.\n",
    "    \"\"\"\n",
    "    board = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                              n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                              max_foraging_attempts=self.max_foraging_attempts, food_patch_prob=self.food_patch_prob,\n",
    "                              forage_success_prob=self.forage_success_prob,\n",
    "                              food_extinct_prob=self.food_extinct_prob,\n",
    "                              food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    return board.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_shape(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only a subset\n",
    "    of these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to (batch,) row and column coordinate indexes of board locations.\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    return self.batch_size\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "  def get_foraging_attempts(self, board):\n",
    "    return board['foraging_attempts'].copy()\n",
    "\n",
    "  def get_square_symbol(self, piece, has_forager):\n",
    "    \"\"\"Returns the symbol representation of a board square.\"\"\"\n",
    "    if has_forager and piece < 0: return 'C'  # Critter on food patch\n",
    "    if has_forager: return 'P'  # Forager on an empty square\n",
    "    if piece == 0: return '.'  # Empty square\n",
    "    if piece < 0: return 'F'  # Food patch\n",
    "    return '?'  # Unknown piece type, for debugging\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th game in the batch of boards.\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for r_ in range(self.n_rows):\n",
    "      print(r_, \"|\", end=\"\")  # Print the row number\n",
    "      for c_ in range(self.n_cols):\n",
    "        piece = board['pieces'][g, r_, c_]  # Get the piece to print\n",
    "        # Check if the square is occupied by a forager\n",
    "        has_forager = False\n",
    "        for forager_num, locs in board['forager_locs'].items():\n",
    "          if g in locs[0] and r_ in locs[1] and c_ in locs[2]:\n",
    "            has_forager = True\n",
    "            break\n",
    "\n",
    "        print(self.get_square_symbol(piece, has_forager), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Foraging Attempts: \" + str(board['foraging_attempts'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "  def get_critter_rc(self, board, g, which_critter):\n",
    "    critter_locs = board['forager_locs'][which_critter]\n",
    "    return critter_locs[1][g], critter_locs[2][g]\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war field_of_view\n",
    "                 fov_opaque=False, #let human see through fog of war or not\n",
    "                 show_food=True,\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter',\n",
    "                 focal_critter_index = 0):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, size=550, show_food=show_food)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food, show_food=show_food)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(self.num_foragers):\n",
    "        critter_name = name if self.num_foragers == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == self.num_foragers, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(self.get_critter_rc(board, g, spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # plot field of view around the 'active player'\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque, fov=fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, board, which_critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to get the legal moves, as a set of batch, row, col triples\n",
    "    for the given board. Does return moves that are technically legal\n",
    "    but that will result in a blocking move\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "\n",
    "      which_critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          max_foraging_attempts=self.max_foraging_attempts,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves(which_critter, radius)\n",
    "    return legal_moves\n",
    "\n",
    "  def get_valid_actions(self, board, which_critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    legal_moves =  self.get_legal_moves(board, which_critter, radius)\n",
    "    g, r, c = zip(*legal_moves)\n",
    "    valids = np.zeros((self.batch_size, self.n_rows * self.n_cols))\n",
    "    valids[g, np.array(r) * self.n_cols + np.array(c)] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, which_critter, actions):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    assert batch_size == len(actions)\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          max_foraging_attempts=self.max_foraging_attempts,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.actions_to_moves(actions)\n",
    "    b.execute_moves(moves, which_critter)\n",
    "    return b.get_state()\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, which_critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right' 'still'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          max_foraging_attempts=self.max_foraging_attempts,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.critter_direction_to_move(board, offsets, which_critter)\n",
    "    b.execute_moves(moves, which_critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right', 'still'\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = board['forager_locs'][critter]\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "  def critter_directions_to_actions(self, board, directions, critter):\n",
    "    \"\"\"\n",
    "    Converts a list of direction strings to a list of action indices for the given board state and critter.\n",
    "\n",
    "    Args:\n",
    "      board (dict): The current state of the game.\n",
    "      directions (list of str): List of directions, where each direction is one of 'up', 'down', 'left', 'right', 'still'.\n",
    "      critter (int): The critter index.\n",
    "\n",
    "    Returns:\n",
    "      list of int: List of action indices corresponding to the directions.\n",
    "    \"\"\"\n",
    "    # Ensure the length of directions matches the batch size\n",
    "    assert len(directions) == board['pieces'].shape[0], \"Mismatch between directions length and batch size\"\n",
    "\n",
    "    # Convert directions to moves\n",
    "    moves = self.critter_direction_to_move(board, directions, critter)\n",
    "\n",
    "    # Convert moves to actions\n",
    "    actions = self.moves_to_actions(moves)\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, which_critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up',\n",
    "                   ( 0, 0): 'still'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, which_critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    critter_locs = board['forager_locs'][which_critter]\n",
    "    valid_directions = []\n",
    "    for g, batch_valid in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valid==1)[0]\n",
    "      critter_loc = np.array([[critter_locs[1][g],critter_locs[2][g]]])\n",
    "      # critter_loc shape is (1, 2)\n",
    "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
    "      offsets = moves - critter_loc\n",
    "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, which_critter):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = PatchyForageBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                          n_cols=self.n_cols, num_foragers=self.num_foragers,\n",
    "                          max_foraging_attempts=self.max_foraging_attempts,\n",
    "                          food_patch_prob=self.food_patch_prob,\n",
    "                          forage_success_prob=self.forage_success_prob,\n",
    "                          food_extinct_prob=self.food_extinct_prob,\n",
    "                          food_regen_prob=self.food_regen_prob, rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    return(b.get_perceptions(radius, which_critter))\n",
    "\n",
    "\n",
    "  def play_game(self, players=[], visualize = False):\n",
    "    \"\"\"This method takes a list of players the same length as num_foragers,\n",
    "        and then plays a batch of games with them and returns the final board\n",
    "        states of each game\"\"\"\n",
    "    if len(players) != self.num_foragers:\n",
    "      raise ValueError(\"number of players different than expected\")\n",
    "\n",
    "    board = self.get_init_board()\n",
    "    if visualize == True:\n",
    "      self.display(board, 0)\n",
    "\n",
    "    for p_idx, player_ in enumerate(players):\n",
    "      if player_.critter_index != p_idx+1:\n",
    "        print(player_.critter_index)\n",
    "        print(p_idx + 1)\n",
    "        raise ValueError(\"player order does not match assigned critter index\")\n",
    "\n",
    "    while np.any(board['is_over'] == False):\n",
    "      for player_ in players:\n",
    "        old_scores = board['scores']\n",
    "        if player_.return_direction:\n",
    "          directions = player_.play(board)\n",
    "          a_player = self.critter_directions_to_actions(board, directions, player_.critter_index)\n",
    "        else: # player returns actions directly\n",
    "          a_player, _, _ = player_.play(board)\n",
    "        board = self.get_next_state(board, player_.critter_index, a_player)\n",
    "        if visualize == True:\n",
    "          self.display(board, 0)\n",
    "    return board\n",
    "\n",
    "\n",
    "  def plot_visualizations(board):\n",
    "    # Extracting scores and foraging_attempts for all batches\n",
    "    scores = board['scores']\n",
    "    foraging_attempts = board['foraging_attempts']\n",
    "\n",
    "    # Calculating average scores per round for each batch\n",
    "    avg_scores_per_round = scores / foraging_attempts\n",
    "\n",
    "    # Histogram of Average Score Per Round\n",
    "    plt.figure()\n",
    "    plt.hist(avg_scores_per_round, bins=30, edgecolor='black')\n",
    "    plt.xlabel('Average Score Per Round')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Average Score Per Round')\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter Plot of Averages vs. Foraging Attempts\n",
    "    plt.figure()\n",
    "    plt.scatter(foraging_attempts, avg_scores_per_round, c='blue', alpha=0.5)\n",
    "    plt.xlabel('Foraging Attempts')\n",
    "    plt.ylabel('Average Score Per Round')\n",
    "    plt.title('Scatter Plot of Average Scores vs. Foraging Attempts')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################################\n",
    "# make InteractivePatchyForage class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractivePatchyForage():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, PatchyForage_game, init_board=None, has_fov=False,\n",
    "               radius=2, fov_opaque=False, show_food=True, show_misses=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human']):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a patchy foraging game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of PatchyForageGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the PatchyForage_game.num_foragers\n",
    "        attribute. Order should align with critter_name.\n",
    "\n",
    "      Note: fov only turns on for the 'active' player.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.pfg = PatchyForage_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.show_food = show_food\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.active_player_index = 0\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.pfg.num_foragers):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] #implicit check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        # player objects expected to have a critter_index attribute\n",
    "        # we set it appropriately here so it aligns with the players list\n",
    "        # used to create the widget\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    # Initialize the sidebar for displaying misses if needed\n",
    "    self.show_misses = show_misses\n",
    "    if self.show_misses:\n",
    "      self.misses_sidebar = widgets.Output(layout=widgets.Layout(\n",
    "          min_width='12.5em', max_width='18.8em',\n",
    "          min_height='6.3em', overflow='auto'))\n",
    "      self.misses_new_patch = [0] * self.pfg.num_foragers\n",
    "      self.misses_known_patch = ['--'] * self.pfg.num_foragers\n",
    "      self.at_new_patch = [True] * self.pfg.num_foragers\n",
    "\n",
    "    if init_board is None:\n",
    "      self.board_state = self.pfg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.forage_button = widgets.Button(description=\"Forage\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.empty_space = widgets.Box(layout=widgets.Layout(height='2.5em'))\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.pfg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize,\n",
    "          show_food=self.show_food)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.pfg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                fov_opaque=self.fov_opaque,\n",
    "                                show_food=self.show_food,\n",
    "                                radius=self.radius, figsize=self.figsize)\n",
    "    if len(self.players) == 1:\n",
    "      if hasattr(self.players[0], 'reset_thresholds'):\n",
    "        self.new_threshold_slider = widgets.IntSlider(\n",
    "          value=self.players[0].threshold_new,\n",
    "          min=0, max=10, description='Tau New')\n",
    "        self.known_threshold_slider = widgets.IntSlider(\n",
    "          value=self.players[0].threshold_known,\n",
    "          min=0, max=10, description='Tau Known')\n",
    "\n",
    "\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([widgets.VBox([self.forage_button, self.left_button]),\n",
    "                                 widgets.VBox([self.up_button, self.down_button]),\n",
    "                                 widgets.VBox([self.empty_space, self.right_button])])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "      if len(self.players) == 1:\n",
    "        #one human player\n",
    "        self.output_and_score = widgets.VBox([self.scoreboard, self.output])\n",
    "        if self.show_misses:\n",
    "            self.final_display = widgets.HBox([self.board_and_buttons,\n",
    "                widgets.VBox([self.misses_sidebar, self.output_and_score])])\n",
    "        else:\n",
    "            self.final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                               self.output_and_score])\n",
    "      else:\n",
    "        # more than one player, one of them human\n",
    "        self.V_board_output= widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend,\n",
    "                                             self.misses_sidebar])\n",
    "        else:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # all players are ai\n",
    "      if len(self.players) == 1:\n",
    "        # one ai player\n",
    "        if hasattr(self.players[0], 'reset_thresholds'):\n",
    "          self.sliders = widgets.VBox([self.new_threshold_slider, self.known_threshold_slider])\n",
    "          if self.show_misses:\n",
    "            self.final_display = widgets.HBox(\n",
    "                [widgets.VBox([self.b_fig.canvas, self.scoreboard]),\n",
    "                 widgets.VBox([self.misses_sidebar, self.output,\n",
    "                               self.sliders,\n",
    "                               self.start_button])])\n",
    "          else:\n",
    "            self.H_score_output_start = widgets.HBox([\n",
    "              self.scoreboard, self.output, self.sliders, self.start_button])\n",
    "            self.final_display = self.HBox(\n",
    "                [widgets.VBox([self.b_fig.canvas, self.H_score_output_start])])\n",
    "        else:\n",
    "          if self.show_misses:\n",
    "            self.final_display = widgets.HBox(\n",
    "                [widgets.VBox([self.b_fig.canvas, self.scoreboard]),\n",
    "                 widgets.VBox([self.misses_sidebar, self.output,\n",
    "                               self.start_button])])\n",
    "          else:\n",
    "            self.H_score_output_start = widgets.HBox([\n",
    "              self.scoreboard, self.output, self.start_button])\n",
    "            self.final_display = self.HBox(\n",
    "                [widgets.VBox([self.b_fig.canvas, self.H_score_output_start])])\n",
    "      else:\n",
    "        # more than one ai player\n",
    "        self.V_board_output = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "          self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend,\n",
    "                                             self.misses_sidebar])\n",
    "        else:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['Best Eating Rate:'] + ['--'] * self.pfg.num_foragers,\n",
    "               ['Last Eating Rate:'] + ['--'] * self.pfg.num_foragers,\n",
    "               ['Average Eating Rate:'] + ['--'] * self.pfg.num_foragers,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    # If show_misses is enabled, initialize the misses_sidebar content\n",
    "    if self.show_misses:\n",
    "      with self.misses_sidebar:\n",
    "        table = [['Misses (New Patch):'] + ['0'] * self.pfg.num_foragers,\n",
    "                 ['Misses (Known Patch):'] + ['--'] * self.pfg.num_foragers]\n",
    "        if len(self.players) > 1:\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "          print(tabulate(table, headers=headers))\n",
    "        else: # len(self.players) == 1\n",
    "            print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.forage_button.on_click(self.on_forage_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def update_state_based_on_move(self, direction):\n",
    "    old_board = self.board_state.copy()\n",
    "    # index of players is 0 through num_critter-1,\n",
    "    # same player represented by value of index + 1 in\n",
    "    if (isinstance(self.players[self.active_player_index], str) and\n",
    "        'human' in self.players[self.active_player_index]):\n",
    "      direction = direction\n",
    "    else:\n",
    "      if self.players[self.active_player_index].return_direction:\n",
    "        directions = self.players[self.active_player_index].play(old_board)\n",
    "      else:\n",
    "        a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
    "        # print(a_player)\n",
    "        directions = self.pfg.action_to_critter_direction(old_board,\n",
    "                                                        self.active_player_index+1,\n",
    "                                                        a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = directions[0]\n",
    "    self.board_state = self.pfg.critter_oriented_get_next_state(\n",
    "          self.board_state, self.active_player_index+1, [direction])\n",
    "    return direction\n",
    "\n",
    "\n",
    "  def update_output_and_scores(self, direction, old_board):\n",
    "    old_scores = old_board['scores'][0]\n",
    "    old_row, old_col = self.pfg.get_critter_rc(old_board, 0,\n",
    "                                               self.active_player_index+1)\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    foraging_attempts = self.board_state['foraging_attempts'][0]\n",
    "    row, col = self.pfg.get_critter_rc(self.board_state, 0,\n",
    "                                       self.active_player_index+1)\n",
    "\n",
    "    did_eat = False\n",
    "    # Check if the forager moved or tried to forage and what happened\n",
    "    if (row, col) != (old_row, old_col):\n",
    "      # Moved to a new patch\n",
    "      self.misses_new_patch[self.active_player_index] = 0\n",
    "      self.misses_known_patch[self.active_player_index] = '--'\n",
    "      self.at_new_patch[self.active_player_index] = True\n",
    "      action_string = \"tried to move \" + direction + \" to ({}, {})\".format(row, col)\n",
    "      eating_string = \"They were too busy moving to look for food.\"\n",
    "    elif (row, col) == (old_row, old_col):\n",
    "      # they didn't move, tried to forage\n",
    "      action_string = \"tried to forage.\"\n",
    "      if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
    "        # They found food\n",
    "        eating_string = \"They found some food at the patch!\"\n",
    "        did_eat = True\n",
    "        if self.at_new_patch[self.active_player_index]:\n",
    "          # They found food at a new patch\n",
    "          self.misses_new_patch[self.active_player_index] = '--'\n",
    "          self.misses_known_patch[self.active_player_index] = 0\n",
    "          self.at_new_patch[self.active_player_index] = False\n",
    "        else:\n",
    "          # They found food at a known patch\n",
    "          # Reset count as they found food\n",
    "          self.misses_known_patch[self.active_player_index] = 0\n",
    "      else:\n",
    "        # They didn't find food\n",
    "        eating_string = \"They didn't find any food at the patch.\"\n",
    "        if self.at_new_patch[self.active_player_index]:\n",
    "          # They are at a new patch\n",
    "          self.misses_new_patch[self.active_player_index] += 1\n",
    "        else:\n",
    "          # They are at a known patch\n",
    "          self.misses_known_patch[self.active_player_index] += 1\n",
    "\n",
    "    #make the picture of the new board position\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.pfg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             show_food=self.show_food,\n",
    "                             radius=self.radius, legend_type=self.legend_type)\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        print(\"The critter {}\".format(action_string))\n",
    "        print(eating_string)\n",
    "        foraging_attempts_count = foraging_attempts[self.active_player_index]\n",
    "        new_score = new_scores[self.active_player_index]\n",
    "        food_per_attempt = \"-\" if foraging_attempts_count == 0 else \"{:.2f}\".format(new_score / foraging_attempts_count)\n",
    "        print(\"Foraging Attempts: {}\\nFood Eaten: {}\\nFood Per Attempt: {}\".format(\n",
    "              foraging_attempts_count,\n",
    "              new_score,\n",
    "              food_per_attempt))\n",
    "        print(\"Foraging Attempts Left: {}\".format(self.pfg.max_foraging_attempts - foraging_attempts_count))\n",
    "      else:  # more than one player\n",
    "        print(\"Critter {} {}\".format(self.active_player_index+1, action_string))\n",
    "        print(eating_string)\n",
    "        print(\"Foraging Attempts: {}\\nFood Eaten: {}\".format(\n",
    "          foraging_attempts, new_scores))\n",
    "\n",
    "    if self.show_misses:\n",
    "      with self.misses_sidebar:\n",
    "        clear_output()\n",
    "        table = [['Misses (New Patch):'] + [str(miss) for miss in self.misses_new_patch],\n",
    "                 ['Misses (Known Patch):'] + [str(miss) for miss in self.misses_known_patch]]\n",
    "        if len(self.players) > 1:\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "          print(tabulate(table, headers=headers))\n",
    "        else: # len(self.players) == 1\n",
    "            print(tabulate(table))\n",
    "\n",
    "\n",
    "  def handle_game_end(self):\n",
    "    \"\"\"Handle the logic when the game is over.\"\"\"\n",
    "    self.final_scores.append(self.board_state['scores'][0] / self.board_state['foraging_attempts'][0])\n",
    "    self.board_state = self.pfg.get_init_board()\n",
    "    for player in self.players:\n",
    "      if hasattr(player, 'last_direction'):\n",
    "        player.last_direction = ['right'] * self.pfg.batch_size\n",
    "    if self.show_misses:\n",
    "      self.misses_new_patch = [0] * self.pfg.num_foragers\n",
    "      self.misses_known_patch = ['--'] * self.pfg.num_foragers\n",
    "      self.at_new_patch = [True] * self.pfg.num_foragers\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      print('Game Over. Final Food per Attempt {}'.format(self.final_scores[-1]))\n",
    "      print('Resetting the board for another game')\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.pfg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                             self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             show_food=self.show_food,\n",
    "                             legend_type=self.legend_type)\n",
    "    with self.scoreboard:\n",
    "      clear_output()\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      if len(self.players) == 1:\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = [\n",
    "            ['Best Eating Rate: ', '{:.2f}'.format(np.max(np.array(self.final_scores)))],\n",
    "            ['Last Eating Rate: ', '{:.2f}'.format(self.final_scores[-1][0])],\n",
    "            ['Average Eating Rate', '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "        else:\n",
    "          table = [['Best Eating Rate:', '--'],\n",
    "                   ['Last Eating Rate:', '--'],\n",
    "                   ['Average Eating Rate:', '--']]\n",
    "        print(tabulate(table))\n",
    "      else: # len(self.players) > 1\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.pfg.num_foragers)]\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = []\n",
    "          # Assuming the batch size is 1 for now\n",
    "          current_scores = self.final_scores[-1]\n",
    "          max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "          average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "          table.append(['Besat Rates:'] +\n",
    "          [str(score) for score in max_scores])\n",
    "          table.append(['Last Rates:'] +\n",
    "            [str(score) for score in current_scores])\n",
    "          table.append(['Average Rates:'] +\n",
    "              ['{:.2f}'.format(score) for score in average_scores])\n",
    "        else:\n",
    "          table = [\n",
    "            ['High Score:'] + ['--'] * self.pfg.num_foragers,\n",
    "            ['Last Score:'] + ['--'] * self.pfg.num_foragers,\n",
    "            ['Average Score:'] + ['--'] * self.pfg.num_foragers,]\n",
    "        print(tabulate(table, headers=headers))\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "    self.forage_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "    self.forage_button.disabled = False\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('right')\n",
    "\n",
    "  def on_forage_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('still')\n",
    "\n",
    "  def execute_moves(self, human_direction=None):\n",
    "    ai_direction = None\n",
    "    while not self.board_state['is_over'][0]:\n",
    "      old_board = self.board_state.copy()\n",
    "      # Check if the current player is human\n",
    "      if self.players[self.active_player_index] == 'human':\n",
    "        if human_direction is None:\n",
    "          # If the human direction is not provided,\n",
    "          # it means the human has not yet made a move\n",
    "          # Break out and wait for one\n",
    "          break\n",
    "        else:\n",
    "          # The human made a move, so execute it and reset the human_direction\n",
    "          self.update_state_based_on_move(human_direction)\n",
    "          self.update_output_and_scores(human_direction, old_board)\n",
    "          human_direction = None  # Reset for next loop iteration\n",
    "      else:\n",
    "        # AI player\n",
    "        ai_direction = self.update_state_based_on_move('tbd')\n",
    "        self.update_output_and_scores(ai_direction, old_board)\n",
    "\n",
    "      # Move to the next player\n",
    "      self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "\n",
    "  def on_direction_button_click(self, direction):\n",
    "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
    "    self.execute_moves(human_direction=direction)\n",
    "    if self.board_state['is_over'][0]:\n",
    "        self.handle_game_end()\n",
    "    self.enable_direction_buttons()  # Re-enable buttons\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    has_threshold_sliders_flag = False\n",
    "    if len(self.players) == 1 and hasattr(self.players[0], 'reset_thresholds'):\n",
    "      self.players[0].reset_thresholds(self.new_threshold_slider.value,\n",
    "                                       self.known_threshold_slider.value)\n",
    "      self.new_threshold_slider.disabled = True\n",
    "      self.known_threshold_slider.disabled = True\n",
    "      has_threshold_sliders_flag = True\n",
    "\n",
    "    self.start_button.disabled = True\n",
    "    self.execute_moves()\n",
    "    if self.board_state['is_over'][0]:\n",
    "        self.handle_game_end()\n",
    "    self.start_button.disabled = False\n",
    "    if has_threshold_sliders_flag:\n",
    "      self.new_threshold_slider.disabled = False\n",
    "      self.known_threshold_slider.disabled = False\n",
    "\n",
    "\n",
    "\n",
    "#################################################\n",
    "# More plotting functions\n",
    "#################################################\n",
    "\n",
    "\n",
    "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
    "                    name=None):\n",
    "  \"\"\"\n",
    "  Plot vector field indicating critter direction probabilities.\n",
    "\n",
    "  Args:\n",
    "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
    "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
    "      and values as lists of direction probabilities corresponding to the\n",
    "      directions ['right', 'down', 'left', 'up'].\n",
    "    critter (int): Identifier for which critter directions are associated with.\n",
    "    deterministic (bool, optional): If True, the probabilities array is\n",
    "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
    "      and are larger. Defaults to False.\n",
    "  \"\"\"\n",
    "\n",
    "  #looks like direction ignores inverted axis\n",
    "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
    "                       'left': (-1, 0), 'up': (0, 1)}\n",
    "  # but offsets need to be aware of inverted\n",
    "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
    "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
    "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
    "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
    "  # same logic for colors\n",
    "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
    "  # Get the offset and color for this critter\n",
    "  critter_offset = critter_offsets[critter]\n",
    "  critter_color = critter_colors[critter]\n",
    "\n",
    "  # Add legend only if critter is not 0\n",
    "  custom_leg_handles = []\n",
    "  if critter != 0:\n",
    "    if name is None:\n",
    "      name = f'Critter {critter}'\n",
    "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
    "    # Add the legend for this critter\n",
    "    custom_leg_handles.append(legend_patch)\n",
    "\n",
    "  C, R, U, V, A = [], [], [], [], []\n",
    "\n",
    "  for loc in loc_prob_dict.keys():\n",
    "    row, col = loc\n",
    "    probs = loc_prob_dict[loc]\n",
    "    for dir_key, prob in probs.items():\n",
    "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
    "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
    "      U.append(direction_vectors[dir_key][0])\n",
    "      V.append(direction_vectors[dir_key][1])\n",
    "\n",
    "      if deterministic:\n",
    "        A.append(1 if prob == max(probs.values()) else 0)\n",
    "      else:\n",
    "        A.append(prob)\n",
    "\n",
    "  linewidth = 1.5 if deterministic else 0.5\n",
    "  scale = 15 if deterministic else 30\n",
    "\n",
    "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
    "            scale=scale, linewidth=linewidth)\n",
    "  return fig, ax, custom_leg_handles\n",
    "\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for further use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  # Create a new figure and axes with given figsize\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  # Set width and height padding, remove horizontal and vertical spacing\n",
    "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
    "  # Show right and top borders (spines) of the plot\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  # Set major ticks (where grid lines will be) on x and y axes\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Set labels for major ticks with font size of 8\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "  # Set minor ticks (no grid lines here) to be between major ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "  # Move x-axis ticks to the top of the plot\n",
    "  ax.xaxis.tick_top()\n",
    "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "  # Remove minor ticks (not the grid lines)\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "  # Set limits of x and y axes\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  # Invert y axis direction\n",
    "  ax.invert_yaxis()\n",
    "  # If title is provided, set it as the figure title\n",
    "  if title is not None:\n",
    "    fig.suptitle(title)\n",
    "  # Hide header and footer, disable toolbar and resizing of the figure\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  # Redraw the figure with these settings\n",
    "  fig.canvas.draw()\n",
    "  # Return figure and axes handles for further customization\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None, size=None,\n",
    "              show_food=True):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if size is None:\n",
    "    size=150\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=size, marker='o',\n",
    "                      color='red', label='Food')\n",
    "  if show_food:\n",
    "    rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "    #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "    #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "    food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]],\n",
    "                  size=None) -> List[Dict[str, object]]:\n",
    "  \"\"\"\n",
    "  Plots multiple types of \"critters\" on a grid implied by the given\n",
    "  fig, ax arguments.\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects.\n",
    "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
    "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
    "    critter.\n",
    "\n",
    "  Returns:\n",
    "    Updated critter_specs with handles.\n",
    "  \"\"\"\n",
    "  if size is None:\n",
    "    size=250\n",
    "  for spec in critter_specs:\n",
    "    # Ensure required keys are present\n",
    "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
    "      if key not in spec:\n",
    "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
    "    handle_ = spec.get('handle')\n",
    "    if handle_ is None:\n",
    "      handle_ = ax.scatter([], [], s=size, marker=spec['marker'],\n",
    "                           color=spec['color'], label=spec['name'],\n",
    "                           edgecolors='white', linewidths=1)\n",
    "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
    "    spec.update({'handle': handle_})\n",
    "  return critter_specs\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc,\n",
    "                 critter=None, critter_name='Critter'):\n",
    "  \"\"\"\n",
    "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h',\n",
    "                         color='blue', label=critter_name)\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov,\n",
    "             opaque=False, fov=None):\n",
    "  \"\"\"\n",
    "  Plots a mask on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter: ndarry(int) (row,col) of the critter\n",
    "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib Image object of mask, either new if no handle\n",
    "    was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
    "  mask_array = np.ones((n_rows, n_cols, 4))\n",
    "  mask_array[:, :, :3] = 0.5  # light grey color\n",
    "  if has_fov == True:\n",
    "    if opaque:\n",
    "      mask_array[:, :, 3] = 1.0  # 50% opacity\n",
    "    else:\n",
    "      mask_array[:, :, 3] = 0.5  # 50% opacity\n",
    "    # Create arrays representing the row and column indices\n",
    "    rows = np.arange(n_rows)[:, np.newaxis]\n",
    "    cols = np.arange(n_cols)[np.newaxis, :]\n",
    "    # Iterate over each critter location\n",
    "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
    "    # Set the region within the specified radius around the critter to transparent\n",
    "    mask_array[dist <= radius, 3] = 0\n",
    "  else:\n",
    "    mask_array[:, :, 3] = 0\n",
    "\n",
    "  if fov is None:\n",
    "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
    "  else:\n",
    "    fov.set_data(mask_array)\n",
    "\n",
    "  return fov\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# **1.2.3.1 Foraging in Patchy Environment**\n",
    "\n",
    "Optimal foraging theory posits that organisms forage to maximize their net energy intake per unit time. We're going to explore the implications of that idea using a highly simplified model of foraging. The scenario is defined as follows:\n",
    "\n",
    "* The goal is to collect food as efficiently as possible, i.e. have the highest possible food per move.\n",
    "* The environment is made up of patches (grid cells). Each patch may or may not contain food.\n",
    "* Foraging on a patch with food is usually, but not always, results in finding food to eat. In contrast foraging on a patch with no food is never results in food being found.\n",
    "* Every turn the organism can either\n",
    "  1. Try to forage on its current patch or\n",
    "  2. Move on to a new patch.\n",
    "* Food patches can sometimes become exhausted after successful foraging, so pathes that once had food can swith to being patches without food. Thus the organism may want to leave a patch after some successful foraging there.\n",
    "* The number of chances to forage before the foraging session ends is limited, so efficiency is key.\n",
    "\n",
    "To get a sense of the problem run the cell below to play through a version of this problem where you have complete knowledge, i.e. you can see all the patches, which ones have food, which don't, and when food patches become exhausted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interaction: Omniscient Patchy Foraging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to play the patchy foraging game.\n",
    "\n",
    "pfg = PatchyForagingGame(max_foraging_attempts=20, food_patch_prob=0.3,\n",
    "                         forage_success_prob=0.6, food_extinct_prob=0.2)\n",
    "omni_ipfg = InteractivePatchyForage(pfg, show_food=True, show_misses=True,\n",
    "                                    figsize=(4,5))\n",
    "display(omni_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(omni_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interaction: Cryptic Patchy Foraging\n",
    "When you can see where the patches are and can tell when they run out, this scenario isn't too difficult. Now let's look at this same scenario but where the food patches are hidden (cryptic). In this case, a forager has to make guesses about whether food is present based on the past success (or failure) of their foraging attempts. How well can you do in this 'Cryptic Patchy Foraging' scenario?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to play the patchy foraging game with cryptic patches.\n",
    "\n",
    "pfg = PatchyForagingGame(max_foraging_attempts=20, food_patch_prob=0.3,\n",
    "                         forage_success_prob=0.6, food_extinct_prob=0.2)\n",
    "cryptic_ipfg = InteractivePatchyForage(pfg, show_food=False,\n",
    "                                       show_misses=True,\n",
    "                                       figsize=(4,5))\n",
    "display(cryptic_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(cryptic_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Was the cryptic scenario harder? Hopefully your score was higher when you had full visibility of the food patches and their exhaustion status. Yet, without this visibility, how does a forager make the best decision? Our game display hints at two critical pieces of information for the forager:\n",
    "1. The number of unsuccessful foraging attempts at a new patch\n",
    "2. The number of unsuccessful foraging attempts since the last successful foraging attempt at that same patch.\n",
    "\n",
    "If the organism can't directly detect food patches or tell when they've become exhausted these metrics are crucial for inference.\n",
    "A simple 'threshold' policy can be defined using these metrics:\n",
    "* Move to a new patch after $\\tau_{new}$ failed foraging attempts at a fresh patch.\n",
    "* Move to a new patch after $\\tau_{eat}$ consecutive failed foraging attempts at a patch where there has been successful foraging previously.\n",
    "\n",
    "Interestingly, given the particular details of this problem, such a threshold is provably optimal, assuming that the organism is able to avoid revisiting patches it has already been too and that fresh patches are always available throughout the foraging session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise: Implement Threshold Patchy Foraging Policy\n",
    "Now, let's implement this threshold policy and observe its performance in our foraging scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# TODO for students: Replace the ...'s in the play method to implement a\n",
    "# a simple threshold policy\n",
    "raise NotImplementedError(\"Exercise: implement threshold policy\")\n",
    "################################################################################\n",
    "\n",
    "\n",
    "class SimpleThresholdPlayer():\n",
    "  \"\"\"\n",
    "  Player that moves in a sweeping pattern and decides whether to forage\n",
    "  or move to a new patch based on threshold number of failed foraging attempts.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1, threshold_new=1, threshold_known=2,\n",
    "               return_direction=True):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.threshold_new = threshold_new\n",
    "    self.threshold_known = threshold_known\n",
    "    self.last_direction = ['right'] * self.game.batch_size\n",
    "    self.return_direction = return_direction\n",
    "\n",
    "  def play(self, board):\n",
    "    batch_size = board['pieces'].shape[0]\n",
    "    chosen_directions = []\n",
    "    valid_directions_batch = self.game.get_valid_directions(board, self.critter_index)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      valid_directions_for_this_board = valid_directions_batch[i]\n",
    "      is_at_new_patch = board['at_new_patch'][i, self.critter_index - 1]\n",
    "      # Decide to forage or move\n",
    "      if is_at_new_patch:\n",
    "        # print('at new patch')\n",
    "        misses = board['misses_new_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = ...\n",
    "      else:\n",
    "        # print('at known patch')\n",
    "        misses = board['misses_known_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = ...\n",
    "\n",
    "      if ...:\n",
    "        # print('move to new patch')\n",
    "        chosen_directions.append(self._get_next_direction(i, valid_directions_for_this_board))\n",
    "      else:\n",
    "        # print('forage at current patch')\n",
    "        chosen_directions.append('still')\n",
    "\n",
    "    if self.return_direction:\n",
    "      return chosen_directions\n",
    "    else:\n",
    "      # Convert chosen directions to actions\n",
    "      actions = self.game.critter_directions_to_actions(board, chosen_directions, self.critter_index)\n",
    "      action_size = self.game.get_action_size()\n",
    "      a_1hots = np.zeros((batch_size, action_size))\n",
    "      a_1hots[np.arange(batch_size), actions] = 1.0\n",
    "      return actions, a_1hots, a_1hots\n",
    "\n",
    "  def _get_next_direction(self, board_idx, valid_directions_for_this_board):\n",
    "    \"\"\"\n",
    "    Get the next direction based on left-right-down sweeping pattern.\n",
    "    \"\"\"\n",
    "    if self.last_direction[board_idx] == 'right':\n",
    "      if 'right' in valid_directions_for_this_board:\n",
    "        return 'right'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'left'\n",
    "        return 'down'\n",
    "\n",
    "    if self.last_direction[board_idx] == 'left':\n",
    "      if 'left' in valid_directions_for_this_board:\n",
    "        return 'left'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'right'\n",
    "        return 'down'\n",
    "\n",
    "    return 'still'  # Default to 'still' if none of the conditions are met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "\n",
    "class SimpleThresholdPlayer():\n",
    "  \"\"\"\n",
    "  Player that moves in a sweeping pattern and decides whether to forages\n",
    "  or move to a new patch based on threshold number of failed foraging attempts.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1, threshold_new=1, threshold_known=2,\n",
    "               return_direction=True):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.threshold_new = threshold_new\n",
    "    self.threshold_known = threshold_known\n",
    "    self.last_direction = ['right'] * self.game.batch_size\n",
    "    self.return_direction = return_direction\n",
    "\n",
    "  def play(self, board):\n",
    "    batch_size = board['pieces'].shape[0]\n",
    "    chosen_directions = []\n",
    "\n",
    "    valid_directions_batch = self.game.get_valid_directions(board, self.critter_index)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      valid_directions_for_this_board = valid_directions_batch[i]\n",
    "      is_at_new_patch = board['at_new_patch'][i, self.critter_index - 1]\n",
    "      # Decide to forage or move\n",
    "      if is_at_new_patch:\n",
    "        # print('at new patch')\n",
    "        misses = board['misses_new_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_new\n",
    "      else:\n",
    "        # print('at known patch')\n",
    "        misses = board['misses_known_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_known\n",
    "\n",
    "      if misses >= threshold:\n",
    "        # print('move to new patch')\n",
    "        chosen_directions.append(self._get_next_direction(i, valid_directions_for_this_board))\n",
    "      else:\n",
    "        # print('forage at current patch')\n",
    "        chosen_directions.append('still')\n",
    "\n",
    "    if self.return_direction:\n",
    "      return chosen_directions\n",
    "    else:\n",
    "      # Convert chosen directions to actions\n",
    "      actions = self.game.critter_directions_to_actions(board, chosen_directions, self.critter_index)\n",
    "      action_size = self.game.get_action_size()\n",
    "      a_1hots = np.zeros((batch_size, action_size))\n",
    "      a_1hots[np.arange(batch_size), actions] = 1.0\n",
    "      return actions, a_1hots, a_1hots\n",
    "\n",
    "  def _get_next_direction(self, board_idx, valid_directions_for_this_board):\n",
    "    \"\"\"\n",
    "    Get the next direction based on left-right-down sweeping pattern.\n",
    "    \"\"\"\n",
    "    if self.last_direction[board_idx] == 'right':\n",
    "      if 'right' in valid_directions_for_this_board:\n",
    "        return 'right'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'left'\n",
    "        return 'down'\n",
    "\n",
    "    if self.last_direction[board_idx] == 'left':\n",
    "      if 'left' in valid_directions_for_this_board:\n",
    "        return 'left'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'right'\n",
    "        return 'down'\n",
    "\n",
    "    return 'still'  # Default to 'still' if none of the conditions are met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have a forager, run the cell below to watch them forage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interaction: Adjustable Threshold Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to watch our simple threshold forager play. You can use the sliders between runs to try out different thresholds\n",
    "\n",
    "\n",
    "\n",
    "class SimpleThresholdPlayer():\n",
    "  \"\"\"\n",
    "  Player that moves in a sweeping pattern and decides whether to forages\n",
    "  or move to a new patch based on threshold number of failed foraging attempts.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1, threshold_new=1, threshold_known=2,\n",
    "               return_direction=True):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.threshold_new = threshold_new\n",
    "    self.threshold_known = threshold_known\n",
    "    self.last_direction = ['right'] * self.game.batch_size\n",
    "    self.return_direction = return_direction\n",
    "\n",
    "  def reset_thresholds(self, threshold_new, threshold_known):\n",
    "    self.threshold_new = threshold_new\n",
    "    self.threshold_known = threshold_known\n",
    "\n",
    "  def play(self, board):\n",
    "    batch_size = board['pieces'].shape[0]\n",
    "    chosen_directions = []\n",
    "\n",
    "    valid_directions_batch = self.game.get_valid_directions(board, self.critter_index)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "      valid_directions_for_this_board = valid_directions_batch[i]\n",
    "      is_at_new_patch = board['at_new_patch'][i, self.critter_index - 1]\n",
    "      # Decide to forage or move\n",
    "      if is_at_new_patch:\n",
    "        # print('at new patch')\n",
    "        misses = board['misses_new_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_new\n",
    "      else:\n",
    "        # print('at known patch')\n",
    "        misses = board['misses_known_patch'][i, self.critter_index - 1]\n",
    "        # print('misses:', misses)\n",
    "        threshold = self.threshold_known\n",
    "\n",
    "      if misses > threshold:\n",
    "        # print('move to new patch')\n",
    "        chosen_directions.append(self._get_next_direction(i, valid_directions_for_this_board))\n",
    "      else:\n",
    "        # print('forage at current patch')\n",
    "        chosen_directions.append('still')\n",
    "\n",
    "    if self.return_direction:\n",
    "      return chosen_directions\n",
    "    else:\n",
    "      # Convert chosen directions to actions\n",
    "      actions = self.game.critter_directions_to_actions(board, chosen_directions, self.critter_index)\n",
    "      action_size = self.game.get_action_size()\n",
    "      a_1hots = np.zeros((batch_size, action_size))\n",
    "      a_1hots[np.arange(batch_size), actions] = 1.0\n",
    "      return actions, a_1hots, a_1hots\n",
    "\n",
    "  def _get_next_direction(self, board_idx, valid_directions_for_this_board):\n",
    "    \"\"\"\n",
    "    Get the next direction based on left-right-down sweeping pattern.\n",
    "    \"\"\"\n",
    "    if self.last_direction[board_idx] == 'right':\n",
    "      if 'right' in valid_directions_for_this_board:\n",
    "        return 'right'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'left'\n",
    "        return 'down'\n",
    "\n",
    "    if self.last_direction[board_idx] == 'left':\n",
    "      if 'left' in valid_directions_for_this_board:\n",
    "        return 'left'\n",
    "      elif 'down' in valid_directions_for_this_board:\n",
    "        self.last_direction[board_idx] = 'right'\n",
    "        return 'down'\n",
    "\n",
    "    return 'still'  # Default to 'still' if none of the conditions are met\n",
    "\n",
    "pfg = PatchyForagingGame(max_foraging_attempts=20, food_patch_prob=0.3,\n",
    "                         forage_success_prob=0.6, food_extinct_prob=0.2,)\n",
    "stp = SimpleThresholdPlayer(pfg, critter_index=1, threshold_new=1, threshold_known=3)\n",
    "stp_ipfg = InteractivePatchyForage(pfg, players=[stp],\n",
    "                                   show_food=True,\n",
    "                                   show_misses=True,\n",
    "                                   figsize=(4,5))\n",
    "display(stp_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(stp_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "How did our simple threshold policy compare to your performance when playing with cryptic patches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# **1.2.3.2 Deductive Reasoning About Policy Performance**\n",
    "Now that we have had some 'hands-on' experience with this foraging problem, we're going to dig into the detailed mechanics of this model scenario, and see what we can deduce about policy performance and the optimal policy. In doing this we will introduce **Bayes' rule**, the cornerstone of inference, and an indispensable tool for reasoning about uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Defining the Problem**\n",
    "First we need define our model problem precisely.\n",
    "\n",
    "* **Patchy Environment**: The foraging environment consists of discrete patches (represented as grid cells). At the start of the simulation each patch has a probability $p_e \\in (0,1)$ of containing food. The forager starts at a fresh patch.\n",
    "\n",
    "* **Possible Actions**: In each turn, the organism has two options:\n",
    "  - Try to forage at its current patch.\n",
    "  - Move to a new patch.\n",
    "\n",
    "* **Foraging Success**: When a patch contains food, foraging is often successful but not always guaranteed. In this model, foraging at a patch with food is successful with probability $p_s \\in (0,1)$. Conversely, foraging on a food-less patch is certain to be unsuccessful.\n",
    "\n",
    "* **Patch Exhaustion**: After each foraging success, there is a probability $p_x \\in (0,1)$ that the patch becomes exhausted. In such cases, the patch won't provide any more food.\n",
    "\n",
    "* **Session Limit**: There is a fixed number of foraging attempts, $T$, before the session end.\n",
    "\n",
    "* **Rewards**: Every successful foraging attempt gives the organism a reward of 1 point. If the foraging attempt is unsuccessful, no points are awarded for that round. We denote the reward received on round $t$ as $R_t$.\n",
    "\n",
    "* **Goal**: The overarching objective for the organism is to maximize its *expected cumulative reward* over the entire session. Formally, the forager aims to maximize:\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\sum_{t=1}^{T} R_t \\right] = \\sum_{t=1}^{T} \\mathbb{E}\\left[ R_t \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Those are the basic mechanics of this foraging problem. Now let's visualize this stochastic process using a probability tree. This tree will help us track the different scenarios in the foraging process and their associated probabilities. Each node in this tree represents a state — for example, 'being at a new patch' or 'being at new patch after an unsuccessful foraging attempt.' The edges between nodes signify the transition from one state to another, with the probabilities on these edges indicating how likely each transition is. Importantly, the sum of the probabilities leading out of each node equals one, reflecting the complete set of possible outcomes at each decision point. To start we will just think about all the different possible scenarios that might occur after attempting to forage once at a new patch. Run the cell below to see what that looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize these probabilities\n",
    "\n",
    "nodes_list = [\"New Patch\", \"Has Food\", \"No Food\", \"Didn't Find Food\", \"Found the Food\", \"No Food to Find\"]\n",
    "edges_list = [(\"New Patch\", \"Has Food\"), (\"New Patch\", \"No Food\"), (\"Has Food\", \"Found the Food\"),\n",
    "              (\"Has Food\", \"Didn't Find Food\"), (\"No Food\", \"No Food to Find\")]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"New Patch\", \"Has Food\", \"p_e\"),\n",
    "    (\"New Patch\", \"No Food\", \"1-p_e\"),\n",
    "    (\"Has Food\", \"Didn't Find Food\", \"1-p_s\"),\n",
    "    (\"Has Food\", \"Found the Food\", \"p_s\"),\n",
    "    (\"No Food\", \"No Food to Find\", \"1\"),\n",
    "\n",
    "]\n",
    "\n",
    "node_colors = {\n",
    "    \"New Patch\": \"red\",\n",
    "    \"Has Food\": \"red\",\n",
    "    \"No Food\": \"red\",\n",
    "    \"Didn't Find Food\": \"blue\",\n",
    "    \"Found the Food\": \"green\",\n",
    "    \"No Food to Find\": \"blue\"\n",
    "}\n",
    "\n",
    "node_labels = {\n",
    "    \"Didn't Find Food\": \"No Food Found\",\n",
    "    \"No Food to Find\": \"No Food Found\"\n",
    "}\n",
    "\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                                      node_colors=node_colors,\n",
    "                                      node_labels=node_labels)\n",
    "Image(output_path, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As we trace a path from the root of the tree ($\\text{New Patch}$) to any node, we multiply the probabilities along this path to calculate the likelihood of that specific scenario occurring. This multiplicative approach embodies the 'and' logic of sequential events in our stochastic process. Moreover, when we encounter situations that can arise from multiple paths — such as ($\\text{No Food Found}$), which happens in two different scenarios — we sum the probabilities of these paths, employing an 'or' logic to encompass all routes leading to that outcome.\n",
    "\n",
    "In this visualization, nodes that have the same color are indistinguishable from the perspective of the forager. When a forager arrives at a new patch, maybe the patch has food, maybe it doesn't. The forager doesn't know. To indicate that the forager can't tell these situation apart, these nodes in the graph above are coloured red. Similarly if the forager fails to find food after one round of searching it doesn't know if it failed because there was no food to find in the first place, or if there was actually food there, and it just didn't get it. However, just because the forager can't know with certainty which situation it is in, it is possible to assign probabilities of different situations being the case given knowledge of the underlying stochastic process.\n",
    "\n",
    "Correctly calculating these probabilities is exactly what we will need to do to figure out the optimal foraging policy. Recall that the goal of the forager is to maximize $ \\sum_{t=1}^{T} \\mathbb{E}\\left[ R_t \\right] $, and\n",
    "$$ \\mathbb{E}\\left[ R_t \\right] = p_s \\cdot P(\\text{Food Present for Foraging Attempt } t)$$\n",
    "\n",
    "We know that the probability of food being present at fresh new patch is $p_e$, but what about patches where there has been some history of successful and/or unsucessful foraging? What is the correct way to compute the probability that food is present in these cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## **Bayes' Rule**\n",
    "\n",
    "To start figuring out the probability that food is present at a patch, let's first use the diagram above to just write out the different situations that the forager might find themselves in, and the probability that these situation occur.\n",
    "\n",
    "| State | Probability State Occurs |\n",
    "|-----------------------|--------------------------|\n",
    "| Succesful foraging at new patch | $p_e \\cdot p_s$ |\n",
    "| Failed foraging at new patch with food | $ (1-p_s) \\cdot p_e $ |\n",
    "| Failed foraging at new patch with no food | $1-p_e$ |\n",
    "\n",
    "That's good, but it doesn't directly tell us what the probability of there being food at the forager's current patch is. For the case where foraging was successful this is straightforward: Either the patch is exhausted and there is no food left with probability $p_x$, or it is not exhausted and food is still present with probability $1-p_x$. For the failed foraging case though the calculation is a little more involved. We derive this now.\n",
    "\n",
    "\n",
    "| Observable State | Probability Observable State Occurs | Probability of Food Given Observables of the State |\n",
    "|-----------------------|--------------------------|-------------------------------------|\n",
    "| Succesful foraging at new patch | $p_e \\cdot p_s$ |  $1-p_x$ |\n",
    "| Failed foraging at new patch | $ (1-p_s) \\cdot p_e  + (1-p_e)$ | $\\frac{(1-p_s)\\cdot p_e}{(1-p_s)\\cdot p_e + (1-p_e)}$ |\n",
    "\n",
    "It is intuitive (to us at least, hopefully to you too) that as the number of failed foraging attempts at a patch increases, the chance that there is no food at the patch also increases. Now obviously, failed foraging attempts do not have any causal impact on the presence or absence of food at the patch (at least not in this model). However, failed foraging attempts do provide **evidence for** the possibility of being at a patch with no food, and **evidence against** the possibility of being at a patch with food. This is because the presence or absence of food at the patch does have a causal impact on foraging success, with foraging success being more likely when food is present and less likely (impossible even in this model) when food is not present. Using this causal relation between the presense/absence of food and foraging success, we can use the observation of foraging success/failure to make an **inference** about whether or not food is present at the patch even though presence/absence is not directly observable.\n",
    "\n",
    "It turns out that there is one correct way, known as Bayes' rule (or theorem), to make such inferences, i.e. to update beliefs about the world based on observed evidence. In its simplest form Bayes' rule states\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "Here, $A$ and $B$ are events and $P(A|B)$ and $P(B|A)$ are conditional probabilities. Specifically $P(A|B)$ is the probaility of $A$ being the case (or occuring) given that $B$ has occured (or is the case) and $P(B|A)$ is the probability of $B$ being the case if $A$ has occured. To apply Bayes' rule to our situation we let $A$ be the 'event' that food really is present at the new patch (which the forager can't directly observe), and we let $B$ be the 'event' that they don't find any food on their first foraging attempt. This gives us\n",
    "\n",
    "$$ P(\\text{Has Food}| \\text{No Food Found}) = \\frac{P(\\text{No Food Found}|\\text{Has Food}) \\cdot P(\\text{Has Food})}{P(\\text{No Food Found})} $$\n",
    "\n",
    "These values can be read off of our probability tree diagram above giving:\n",
    "\n",
    "$$ P(\\text{Has Food}| \\text{No Food Found}) = \\frac{(1-p_s) \\cdot (p_e)}{(1-p_s) \\cdot (p_e) + 1 \\cdot (1-p_e)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Another perspective on where this probability is coming from is that we are only considering the pathways in the tree that are possible given the observation of no food found. Then, to figure out the probability of food being at the patch we look at total probability of the paths where there is actually food at the patch (there just one in this example), relative to the total probability of all paths that are possible given the observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to display the relevant paths through the probability tree. This is a rough place holder image.\n",
    "\n",
    "main_url = 'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C2_OptimizationAndEnvironment/static/marked_up_foraging_prob_tree1.png'\n",
    "backup_url = 'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/book-review/sequences/P1C2_OptimizationAndEnvironment/static/marked_up_foraging_prob_tree1.png'\n",
    "\n",
    "# Function to check if URL is accessible\n",
    "def is_url_accessible(url):\n",
    "  try:\n",
    "    response = requests.head(url, allow_redirects=True, timeout=5)\n",
    "    return response.status_code == 200\n",
    "  except requests.RequestException:\n",
    "    return False\n",
    "\n",
    "# Function to download the PDF\n",
    "def download_image(url, local_filename):\n",
    "  try:\n",
    "    with requests.get(url, stream=True) as r:\n",
    "      if r.status_code == 200:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "          for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "        return True\n",
    "  except Exception as e:\n",
    "    print(f\"Error downloading file: {e}\")\n",
    "  return False\n",
    "\n",
    "# Decide which URL to use and download the file\n",
    "local_image_filename = 'local_marked_up_tree1.png'\n",
    "if not os.path.exists(local_image_filename):\n",
    "  if not download_image(main_url, local_image_filename):\n",
    "    download_image(backup_url, local_image_filename)\n",
    "\n",
    "# Display the image\n",
    "Image(filename=local_image_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Reasoning Exercises: Going Through the Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Case 1. What to do after a failed foraging attempt at a new patch\n",
    "\n",
    "Now that we have those probabilities computed we can ask, is it better for the forager to stay at a patch where it has just failed to forage, or is it better for it move on to a new patch?\n",
    "\n",
    "At a new patch the probability of finding food is $$p_e \\cdot p_s$$\n",
    "\n",
    "At a new patch where a forager has been unsuccesful once, the probability of finding food is $$\\frac{1-p_s}{(1-p_s) \\cdot p_e + (1-p_e)}\\cdot p_e \\cdot p_s = \\frac{1-p_s}{1-p_e \\cdot p_s }\\cdot p_e \\cdot p_s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Quick Math Excercise:**\n",
    "\n",
    "Given that $0 < p_s < 1$ and $0 < p_e < 1$ convince yourself that $$0 < \\frac{1-p_s}{1-p_e \\cdot p_s } < 1$$\n",
    "**Bonus:** Prove this to yourself\n",
    "\n",
    "**Bonus Answer**\n",
    "1. If $p_s$ and $p_e$ are both positive and less than one, then their product is also positive and less than one, so is 1 minus their prodcut, that is\n",
    "$0 < 1-p_e \\cdot p_s < 1$\n",
    "2. Given 1, and that $p_s$ is positve and between zero and one, then $1-p_s$ is positive, and so $0 < \\frac{1-p_s}{1-p_e \\cdot p_s }$\n",
    "3. $p_s > p_s \\cdot p_e \\implies 1 - p_s < 1 - p_s \\cdot p_e \\implies \\frac{1-p_s}{1-p_e \\cdot p_s } < 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "What this tells us is that the probability of finding food at a new patch where the forager has had a failure is always going to be less than the probability of finding food a fresh new patch. Therefore an optimal forager should always move on to a fresh new patch after an immediate failure at a new patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Case 2. What to do after succesful foraging\n",
    "\n",
    "In order to figure this out we're going to have to expand our probability tree out one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to visualize the probability tree\n",
    "\n",
    "nodes_list = [\"New0\", \"Has Food0\", \"No Food0\",\n",
    "              \"Didn't Find Food\", \"Found the Food\",\n",
    "              \"No Food to Find\",\n",
    "              \"New1\", #\"Has Food1\", \"No Food1\",\n",
    "              \"New2\", #\"Has Food2\", \"No Food2\",\n",
    "              \"Still Food\", \"Food Ran Out\"]\n",
    "edges_list = [(\"New0\", \"Has Food0\"), (\"New0\", \"No Food0\"), (\"Has Food0\", \"Found the Food\"),\n",
    "              (\"Has Food0\", \"Didn't Find Food\"), (\"No Food0\", \"No Food to Find\"),\n",
    "              (\"Didn't Find Food\", \"New1\"),\n",
    "              #(\"New1\", \"Has Food1\"), (\"New1\", \"No Food1\"),\n",
    "              (\"Found the Food\", \"Still Food\"), (\"Found the Food\", \"Food Ran Out\"),\n",
    "              (\"No Food to Find\", \"New2\"),\n",
    "              #(\"New2\", \"Has Food2\"), (\"New2\", \"No Food2\")\n",
    "              ]\n",
    "\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"New0\", \"Has Food0\", \"p_e\"),\n",
    "    (\"New0\", \"No Food0\", \"1-p_e\"),\n",
    "    (\"Has Food0\", \"Didn't Find Food\", \"1-p_s\"),\n",
    "    (\"Has Food0\", \"Found the Food\", \"p_s\"),\n",
    "    (\"No Food0\", \"No Food to Find\", \"1\"),\n",
    "    (\"No Food to Find\", \"New2\", \"1\"),\n",
    "    (\"Didn't Find Food\", \"New1\", \"1\"),\n",
    "    (\"Found the Food\", \"Still Food\", \"1-p_x\"),\n",
    "    (\"Found the Food\", \"Food Ran Out\", \"p_x\"),\n",
    "    #(\"New1\", \"Has Food1\", \"p_e\"),\n",
    "    #(\"New1\", \"No Food1\", \"1-p_e\"),\n",
    "    #(\"New2\", \"Has Food2\", \"p_e\"),\n",
    "    #(\"New2\", \"No Food2\", \"1-p_e\")\n",
    "]\n",
    "\n",
    "node_colors = {\n",
    "    \"New0\": \"red\",\n",
    "    \"Has Food0\": \"red\",\n",
    "    \"No Food0\": \"red\",\n",
    "    \"New1\": \"red\",\n",
    "    \"Has Food1\": \"red\",\n",
    "    \"No Food1\": \"red\",\n",
    "    \"New2\": \"red\",\n",
    "    \"Has Food2\": \"red\",\n",
    "    \"No Food2\": \"red\",\n",
    "    \"Didn't Find Food\": \"blue\",\n",
    "    \"Found the Food\": \"green\",\n",
    "    \"No Food to Find\": \"blue\",\n",
    "    \"Still Food\": \"green\",\n",
    "    \"Food Ran Out\": \"green\"\n",
    "}\n",
    "\n",
    "node_labels = {\n",
    "    \"New0\": \"New\",\n",
    "    \"New1\": \"New\",\n",
    "    \"New2\": \"New\",\n",
    "    \"Has Food0\": \"Has Food\",\n",
    "    \"Has Food1\": \"Has Food\",\n",
    "    \"Has Food2\": \"Has Food\",\n",
    "    \"No Food0\": \"No Food\",\n",
    "    \"No Food1\": \"No Food\",\n",
    "    \"No Food2\": \"No Food\",\n",
    "    \"Didn't Find Food\": \"No Food Found\",\n",
    "    \"No Food to Find\": \"No Food Found\",\n",
    "    \"Finally Found Food\": \"Found the Food\",\n",
    "    \"Still Food\": \"More Food There\",\n",
    "    \"Still No Food to Find\": \"Still No Food\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                                      node_colors=node_colors,\n",
    "                                      node_labels=node_labels)\n",
    "Image(output_path, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So after successful foraging the probability that there is still food at the patch is $1-p_x$. Thus, after a succsfull forage at a new patch if $1-p_x > p_e$ it is better to stay and continue foraging, but if $1-p_x < p_e$ then it is better to move on to a fresh patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Case 3. What to do after one failed foraging attempt following successful foraging\n",
    "\n",
    "In cases where $1-p_x > p_e$, i.e. food persisting at a patch is more likely than finding a new patch with food, and it makes sense for the forager to forage again at the same patch after a successful foraging attempt. In these cases, we then need to know what a forager should do if this subsequent attempt fails. Again, let's expand the visualization to help us think this through. To focus in on our particular question we will use the \"$\\text{Found the Food}$\" node as the root node of this next probability tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Run this cell to visualize the probability tree\n",
    "\n",
    "nodes_list = [\n",
    "  \"Found the Food\",\n",
    "  \"Still Food\", \"Food Ran Out\",\n",
    "  \"Found More Food\", \"Didn't Find Food\", \"No Food to Find\"\n",
    "]\n",
    "edges_list = [\n",
    "  (\"Found the Food\", \"Still Food\"),\n",
    "  (\"Found the Food\", \"Food Ran Out\"),\n",
    "  (\"Still Food\", \"Found More Food\"),\n",
    "  (\"Still Food\", \"Didn't Find Food\"),\n",
    "  (\"Food Ran Out\", \"No Food to Find\"),\n",
    "]\n",
    "\n",
    "latex_edge_labels = [\n",
    "  (\"Found the Food\", \"Still Food\", \"1-p_x\"),\n",
    "  (\"Found the Food\", \"Food Ran Out\", \"p_x\"),\n",
    "  (\"Still Food\", \"Found More Food\", \"p_s\"),\n",
    "  (\"Still Food\", \"Didn't Find Food\", \"1-p_s\"),\n",
    "  (\"Food Ran Out\", \"No Food to Find\", \"1\"),\n",
    "]\n",
    "\n",
    "node_colors = {\n",
    "  \"Found the Food\": \"green\",\n",
    "  \"Still Food\": \"green\",\n",
    "  \"Food Ran Out\": \"green\",\n",
    "  \"Found More Food\": \"green\",\n",
    "  \"Didn't Find Food\": \"blue\",\n",
    "  \"No Food to Find\": \"blue\",\n",
    "}\n",
    "\n",
    "node_labels = {\n",
    "  \"Still Food\": \"More Food There\",\n",
    "  \"Didn't Find Food\": \"No Food Found\",\n",
    "  \"No Food to Find\": \"No Food Found\"\n",
    "}\n",
    "\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                                      node_colors=node_colors,\n",
    "                                      node_labels=node_labels)\n",
    "Image(output_path, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This should look very familiar. This is basically the same situation we had previously where the forager can't definatively tell the difference between a failed foraging attempt caused by there not being anything to find, and a failed foraging attempt where the patch has food and the forager just didn't find it. Just as before Bayes' rule allows us to compute how likely it is that there is still food at the patch when an unsuccessful foraging attempt follows a successful foraging event. Specifically\n",
    "\n",
    "$$ P(\\text{More Food}| \\text{No Food Found}) = \\frac{P(\\text{No Food Found}|\\text{More Food}) \\cdot P(\\text{More Food})}{P(\\text{No Food Found})} $$\n",
    "\n",
    "Again, these values can be read off of our probability tree diagram above giving:\n",
    "\n",
    "$$ P(\\text{More Food}| \\text{No Food Found}) = \\frac{(1-p_s) \\cdot (1-p_x)}{(1-p_s) \\cdot (1-p_x) + p_x} $$\n",
    "\n",
    "Given this, and the fact that the forager should always forage in the place most likely to have food, it should move to a fresh new patch if and only if\n",
    "$$\\frac{(1-p_s) \\cdot (1-p_x)}{(1-p_s) \\cdot (1-p_x) + p_x} < p_e $$\n",
    "\n",
    "We haven't yet given any specific values for the parameters of this proccess, but if we did have some we could plug them into this formula to see if foraging after one failed foraging attempt following successful foraging is a good or a bad idea. Let's say\n",
    "* $p_e=0.3$\n",
    "* $p_s=0.6$\n",
    "* $p_x=0.2$\n",
    "\n",
    "These just happen to be the parameter values used in the simulations earlier.\n",
    "Plugging those values into our formulae we have that the probability of food at the current patch is roughly $0.642...$ which is greater than $p_e=0.4$ so for this particular parameter set, foraging again at the same patch with one failure after a success is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Probability of there still being food at the current patch after one failed attempt following a successful attempt\n",
    "p_e = 0.3\n",
    "p_s = 0.6\n",
    "p_x = 0.2\n",
    "((1 - p_s) * (1 - p_x)) / ((1 - p_s) * (1 - p_x) + p_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Case 4. What to do after $n$ failed foraging attempts following successful foraging\n",
    "\n",
    "We know that after a single failed foraging attempt at a new patch the forager should move on. The intuition here is that a failed foraging attempt gives evidence for the new patch being one with no food, so better to just start again at a fesh patch. (Note that there are no travel costs here, if there were things would be trickier.) Conversely, after having found food at a patch, if exhaustion is relatively unlikely, then the evidence provided by a failed attempt might still not be enough to counter the evidence of a previous success. So, for some parameter values, trying again at the same patch is the best thing to do.\n",
    "\n",
    "Expanding these probability trees is fun, but could become a bit tedious if we had to do it a lot. However, we do need some way to compute this more general quantity:\n",
    "\n",
    "$$ P(\\text{More Food}| \\text{No Food Found } n \\text{ Times}) = \\frac{P(\\text{No Food Found } n \\text{ Times}|\\text{More Food}) \\cdot P(\\text{More Food})}{P(\\text{No Food Found } n \\text{ Times})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Probability Exercise**\n",
    "\n",
    "Plug the correct values into the formula above.\n",
    "\n",
    "Answer:\n",
    "\n",
    "$$ P(\\text{More Food}| \\text{No Food Found } n \\text{ Times}) = \\frac{(1-p_s)^n \\cdot (1-p_x)}{(1-p_s)^n \\cdot (1-p_x) + p_x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Then for our given parameter values we can see how the liklihood of there being food at the patch diminishes with each unsuccessfull foraging attempt, until it passes the threshold probability $p_e$ where it makes sense to start foraging at a new patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "p_e = 0.3  # Probability of food being present at a new patch\n",
    "p_s = 0.6  # Probability of foraging success\n",
    "p_x = 0.2  # Probability of patch exhaustion after success\n",
    "\n",
    "def prob_still_food_after_n_fails(n, p_s, p_x):\n",
    "  return ((1 - p_s) ** n * (1 - p_x)) / ((1 - p_s) ** n * (1 - p_x) + p_x)\n",
    "\n",
    "# Calculating probabilities different n values\n",
    "n_values = np.arange(0, 6)  # From 0 to 5\n",
    "prob_values = [prob_still_food_after_n_fails(n, p_s, p_x) for n in n_values]\n",
    "\n",
    "# Plotting the probabilities\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(n_values, prob_values, marker='o', color='blue', label='At Current Patch')\n",
    "ax.axhline(y=p_e, color='green', linestyle='--', label='At New Patch')\n",
    "\n",
    "ax.set_title('Probability Food Present Conditional on n Foraging Failures')\n",
    "ax.set_xlabel('Number of Times No Food Found (n)')\n",
    "ax.set_ylabel('Probability of Food at Patch')\n",
    "ax.grid(False)\n",
    "ax.legend()\n",
    "remove_ip_clutter(fig)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Looking at this figure it is clear that even after 2 failures, for these parameter values at least, it is still better to forage at the current patch. However, after 3 consecutive foraging failures enough evidence against there being food present at the current patch has accumulated and its time for the forager to move on to a new patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Math Exercise: Optimal Thresholds\n",
    "\n",
    "Summarizing our analysis thus far.\n",
    "* No matter what the parameter values, a single foraging failure at a new patch should prompt the forager to move on to a new patch (provided there is an abundant supply of fresh patches, e.g. more new patches than there are foraging rounds). This is because a single foraging failure at a new patch provides evidence for their being no food at that patch, making a fresh patch preferable to foraging again in the same place.\n",
    "* If patch exhaustion through foraging is unlikely relative to the baseline probability of food presnce at a fresh patch, then having found food at a patch provides evidence for the continued presense of food at that same patch. Specifically if $1-p_x < p_e$, then foraging at a patch where food was just found is the best thing to do. Conversely if patch exhaustion through foraging is likely relative to the baseline probability of food presence at a fresh patch, then having found food at a patch provides evidence for the absence of food at that same patch. Moving to a fresh patch after successuful foraging is  if $1-p_x > p_e$ is the best thing to do.\n",
    "* In the case where $1-p_x < p_e$ and so continued foraging at the same patch after success is good, subsequent failed foraging attempts provide evidence that the patch was indeed exhausted and no food is present. The probability of food being present after $n$ failed foragin attempts is\n",
    "$$ \\frac{(1-p_s)^n \\cdot (1-p_x)}{(1-p_s)^n \\cdot (1-p_x) + p_x} $$\n",
    "When this quantity is greater than $p_e$ continued attempts at the same patch is the best thing to do, but when this quantity is less than $p_e$ it is time to move on to a new patch.\n",
    "\n",
    "Taking this one step further we can algebrically solve for $n$ as a function of the parameters to determine what the optimal policy is for any given set of parameters.\n",
    "\n",
    "**Exercise** Algebrically manipulate the expression\n",
    "$$ \\frac{(1-p_s)^n \\cdot (1-p_x)}{(1-p_s)^n \\cdot (1-p_x) + p_x} > p_e$$\n",
    "To determine the maximum number $n$ of consecutive foraging failures after a successful foraging attempt where it is still optimal to continue foraging at the same patch.\n",
    "\n",
    "Answer:\n",
    "$$\n",
    "\\begin{align}\n",
    "(1-p_s)^n \\cdot (1-p_x) &> p_e \\cdot ((1-p_s)^n \\cdot (1-p_x) + p_x) \\\\\n",
    "(1-p_e)\\cdot (1-p_s)^n \\cdot (1-p_x) &> p_e \\cdot p_x \\\\\n",
    "(1-p_s)^n &> \\frac{p_e \\cdot p_x}{(1-p_e)\\cdot(1-p_x)} \\\\\n",
    "n \\ln(1-p_s) &> \\ln \\left( \\frac{p_e \\cdot p_x}{(1-p_e)\\cdot(1-p_x)} \\right) \\\\\n",
    "n &< \\frac{\\ln(p_e) + \\ln(p_x) - \\ln(1-p_e) - \\ln(1-p_x)}{\\ln(1-p_s)}\n",
    "\\end{align}\n",
    "$$\n",
    "So when\n",
    "$$\n",
    "n \\leq \\left\\lfloor \\frac{\\ln(p_e) + \\ln(p_x) - \\ln(1-p_e) - \\ln(1-p_x)}{\\ln(1-p_s)} \\right\\rfloor\n",
    "$$\n",
    "The forager should keep trying the same patch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So this analysis seems sound, but let's check that when we use these supposedly optimal thresholds, our forager does indeed perform better than when using other sub-optimal threshold foraging rules. For the parameter values\n",
    "* $p_e=0.3$\n",
    "* $p_s=0.6$\n",
    "* $p_x=0.2$\n",
    "\n",
    "This gives us\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "p_e = 0.3\n",
    "p_s = 0.6\n",
    "p_x = 0.2\n",
    "np.floor((np.log(p_e) + np.log(p_x) - np.log(1-p_e) - np.log(1-p_x)) / np.log(1-p_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So for these parameters, the forager should try again after two fails, but move on to a fresh patch after the third, and never forage again after a failed attempt at a new patch. Run the cell below to check performance for different thresholds. Note that the threshold is maximum number of consecutive misses before the forager moves on. So the optimal threshold is $0$ for new patches and $2$ for patches with a prior success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interaction: Manual Exploration fof Threshold Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to evaluate the expected reward of this foraging scenario under different threshold combinations\n",
    "\n",
    "class ValueHeatmapWidget:\n",
    "\n",
    "  def __init__(self, p_e, p_x, p_s, max_foraging_attempts):\n",
    "    self.rng = np.random.default_rng()\n",
    "    self.p_e = p_e\n",
    "    self.p_x = p_x\n",
    "    self.p_s = p_s\n",
    "    self.max_foraging_attempts = max_foraging_attempts\n",
    "    self.heatmap_data = np.zeros((12, 12))\n",
    "    self.fig, self.ax = plt.subplots(figsize=(10, 8))\n",
    "    self.ax.set_xlabel('Successful Forage Threshold')\n",
    "    self.ax.set_ylabel('New Patch Threshold')\n",
    "    self.ax.set_xlim(-0.5, 5.5)\n",
    "    self.ax.set_ylim((6.5, -0.5))\n",
    "    self.ax.xaxis.tick_top()\n",
    "    self.ax.xaxis.set_label_position('top')\n",
    "    self.ax.spines['top'].set_visible(True)\n",
    "    self.ax.spines['right'].set_visible(True)\n",
    "    self.ax.spines['bottom'].set_visible(True)\n",
    "    self.ax.spines['left'].set_visible(True)\n",
    "    remove_ip_clutter(self.fig)\n",
    "    #y_ticks = self.ax.get_yticks().tolist()\n",
    "    # Adjust the labels\n",
    "    #y_labels = [str(int(tick+1)) for tick in y_ticks if 0 <= tick < 11]\n",
    "    # Set the new labels without altering the tick positions\n",
    "    #self.ax.set_yticklabels(y_labels)\n",
    "    self.text_annotations = []\n",
    "\n",
    "    # Create sliders\n",
    "    self.tau_new_slider = widgets.IntSlider(min=0, max=5, step=1, value=2)\n",
    "    self.tau_eat_slider = widgets.IntSlider(min=0, max=5, step=1, value=2)\n",
    "    self.tau_new_label = widgets.Label(value=\"New Patch Threshold\")\n",
    "    self.tau_eat_label = widgets.Label(value=\"Successful Eating Threshold\")\n",
    "    self.tau_new_box = widgets.VBox([self.tau_new_label, self.tau_new_slider])\n",
    "    self.tau_eat_box = widgets.VBox([self.tau_eat_label, self.tau_eat_slider])\n",
    "\n",
    "    # Create compute button\n",
    "    self.compute_button = widgets.Button(description=\"Compute Value\")\n",
    "    self.compute_button.on_click(self.update_plot)\n",
    "\n",
    "    # Put the display together with the VBoxes and button\n",
    "    self.sliders = widgets.VBox([self.tau_new_box, self.tau_eat_box, self.compute_button])\n",
    "    self.final_display = widgets.HBox([self.sliders, self.fig.canvas])\n",
    "\n",
    "  def update_cax(self, data):\n",
    "    if hasattr(self, 'cax'):\n",
    "      self.cax.set_data(data)\n",
    "      self.cax.autoscale()\n",
    "      self.colorbar.remove()\n",
    "    else:\n",
    "      self.cax = self.ax.matshow(data, cmap='gray_r')\n",
    "\n",
    "    self.colorbar = self.fig.colorbar(self.cax, ax=self.ax)\n",
    "\n",
    "  def update_plot(self, change=None):\n",
    "    tau_new = self.tau_new_slider.value\n",
    "    tau_eat = self.tau_eat_slider.value\n",
    "    pfg = PatchyForagingGame(batch_size=1000,\n",
    "                         food_patch_prob=self.p_e,\n",
    "                         food_extinct_prob=self.p_x,\n",
    "                         forage_success_prob=self.p_s,\n",
    "                         max_foraging_attempts=self.max_foraging_attempts,\n",
    "                         food_regen_prob=0,\n",
    "                         rng=self.rng)\n",
    "    stp = SimpleThresholdPlayer(pfg, critter_index=1,\n",
    "                                threshold_new=tau_new,\n",
    "                                threshold_known=tau_eat)\n",
    "    result = pfg.play_game([stp])\n",
    "    score = np.mean(result['scores'])\n",
    "\n",
    "    # Update heatmap_data\n",
    "    self.heatmap_data[tau_new, tau_eat] = score\n",
    "\n",
    "    # Plot\n",
    "    self.update_cax(self.heatmap_data)\n",
    "\n",
    "    for txt in self.text_annotations:\n",
    "      txt.remove()\n",
    "    self.text_annotations.clear()\n",
    "\n",
    "    # Annotate heatmap\n",
    "    for i in range(11):\n",
    "      for j in range(12):\n",
    "        c = self.heatmap_data[j, i]\n",
    "        if c != 0:\n",
    "          txt = self.ax.text(i, j, f\"{c:.2f}\", va='center', ha='center', color='w' if c > 0.5 else 'k')\n",
    "          self.text_annotations.append(txt)\n",
    "\n",
    "    self.fig.canvas.draw()\n",
    "\n",
    "# Using the class\n",
    "vh_widget = ValueHeatmapWidget(p_e=0.3, p_x=0.2, p_s=0.6,\n",
    "                               max_foraging_attempts=20)\n",
    "display(vh_widget.fig.canvas)\n",
    "clear_output()\n",
    "display(vh_widget.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully our optimal strategy is really seeming optimal.\n",
    "\n",
    "Two things to notice about what we've done here:\n",
    "1. Unlike previous optimization processes in the book where we've tried different parameters and evaluated them until eventually we've found good parameters, here we reasoned about the properties of the optimal strategy and then deduced what it should be. No algorithms, no function evaluations, just thinking.\n",
    "2. With an algorithmic optimization process, the algorithm needs to be run, and a different optimal policy found, each time the parameters of the underlying problem change. Here, we've derived the optimal strategy as a function of the parameters. Thus for any set of parameters, we can instantly determine the optimal strategy, without needing to re-run the optimization process. Very efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# **1.2.3.3 Propose-Accept-Reject versus Reasoning**\n",
    "\n",
    "Now that we've reasoned about policies and used our analysis to determine the optimal policy let's see how our simple optimization approach propose-accept-reject does with this same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Simulation: Learning a Patchy Foraging Policy with Propose-Accept-Reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to use propose-accept-reject to find good parameters for this patchy foraging problem.\n",
    "\n",
    "def evaluate(params, game):\n",
    "  stp = SimpleThresholdPlayer(game, critter_index=1, threshold_new=params[0],\n",
    "                              threshold_known=params[1])\n",
    "  result = game.play_game([stp])\n",
    "  return(np.mean(result['scores']))\n",
    "\n",
    "\n",
    "def patchy_propose_and_test(batch_size=1000,\n",
    "                            initial_params=None,\n",
    "                            max_rejected=200,\n",
    "                            verbose=True):\n",
    "\n",
    "  game = PatchyForagingGame(batch_size=batch_size,\n",
    "                            food_patch_prob=0.3,\n",
    "                            food_extinct_prob=0.2,\n",
    "                            forage_success_prob=0.6,\n",
    "                            max_foraging_attempts=20,\n",
    "                            food_regen_prob=0)\n",
    "  tested_params_dict = {}\n",
    "  # Initialize parameters\n",
    "  if initial_params is None:\n",
    "    initial_params = 5*np.ones(2, dtype=int)\n",
    "  best_params = initial_params\n",
    "  best_avg_score = evaluate(best_params, game)\n",
    "  print(f\"Initial score: {best_avg_score}\")\n",
    "  rejected_count = 0\n",
    "  total_tests = 0  # Number of iterations\n",
    "  if verbose:\n",
    "    intermediate_params = []\n",
    "    intermediate_params.append(best_params)\n",
    "    intermediate_values = [best_avg_score]\n",
    "    iterations = [0]\n",
    "\n",
    "  # Propose-and-test loop\n",
    "  start_time = time.time()\n",
    "  while rejected_count < max_rejected:\n",
    "    total_tests +=1\n",
    "    # Propose new parameters: sample custom list of small discrete steps\n",
    "    # centered at best_params\n",
    "    delta_dim1 = np.random.choice([-2, -1, 0, 1, 2])\n",
    "    delta_dim2 = np.random.choice([-2, -1, 0, 1, 2])\n",
    "    delta_params = np.array([delta_dim1, delta_dim2])\n",
    "    proposal_params = best_params + delta_params\n",
    "    # Ensure that the values for the first dimension don't go below 0\n",
    "    proposal_params[0] = max(0, proposal_params[0])\n",
    "    proposal_params[1] = max(1, proposal_params[1])\n",
    "\n",
    "    if tuple(proposal_params) in tested_params_dict.keys():  # Step 2: Check if the proposed parameters have been tested before.\n",
    "      #print('already tested' + str(proposal_params))\n",
    "      rejected_count += 1\n",
    "      continue\n",
    "\n",
    "    #print('now testing' + str(proposal_params))\n",
    "    avg_score = evaluate(proposal_params, game)\n",
    "    tested_params_dict[tuple(proposal_params)] = avg_score\n",
    "\n",
    "    if avg_score > best_avg_score:\n",
    "      best_params = proposal_params\n",
    "      best_avg_score = avg_score\n",
    "      if verbose:\n",
    "        print('best params so far:')\n",
    "        display(best_params)\n",
    "        print(f\"Best score so far: {best_avg_score}\")\n",
    "        print(f\"Found after a total of {time.time() - start_time:.2f} seconds\")\n",
    "        rejected_count = 0\n",
    "    else:\n",
    "      rejected_count += 1\n",
    "\n",
    "  end_time = time.time()\n",
    "  elapsed_time = end_time - start_time\n",
    "  iterations.append(total_tests)\n",
    "\n",
    "  if verbose:\n",
    "    # Print the best found parameters and score\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Average Score:\", best_avg_score)\n",
    "    print(\"Parameter combinations tested:\", total_tests)\n",
    "    print(f\"Time taken for the optimization loop: {elapsed_time:.2f} seconds\")\n",
    "    return (best_params, best_avg_score, tested_params_dict)\n",
    "  else:\n",
    "    return best_params, best_avg_score\n",
    "\n",
    "best_params, best_score, tested_params_dict = patchy_propose_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Reflection Questions:**\n",
    "1. Did the propose and test method using simulation based evaluation find the same optimal parameters as we did using analytical policy evaluation?\n",
    "2. What about the best scores reported by the propose and test algorithm using simulation based evaluation, are these higher, lower or about the same as determined using an analytical approach?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<details>\n",
    "  <summary>Answers</summary>\n",
    "  <p>\n",
    "  1. The best scores reported by the propose and test algorithm using simulation based evaluation are usually close to the true expected value, but again due to stochastic evaluation they are never exactly the same. </p>\n",
    "  <p>\n",
    "  2. Using simulation based evaluation sometimes finds the true optimalparameters as determined using analytical policy evaluation, but due to the stochastic nature of this evaluation method sometimes propose and test only finds parameters that are close to optimal.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "## Bio Box: Multiple Perspectives on Optima\n",
    "\n",
    "In this sequence we have looked at the same patchy foraging problem in two different ways.\n",
    "\n",
    "The first perspective was to interact with and observe a 'full' simulation of the problem, where an organism plays through rounds of a game according to some policy and receives a score. This is how we have approached everything so far in this book. Although this perspective and this game are highly simplified compared to lived reality, this general approach of 'run it and see' is exactly what evolution is doing. Variations on the policy emerge due to recombination and mutation, organisms live their lives, and the effectiveness of their policy is 'evaluated' via natural selection, roughly as the number of offspring surviving to the next generation. Importantly, the evolutionary selective process needs no analytic insight into the nature of the environment in which organisms are born and execute their policies. Evolution is a black box, 'run it and see' approach, much like propose-accept-reject running in parellel.\n",
    "\n",
    "The second perspective, which we introduced here was to think about the nature of the problem, to write out a symbolic mathematical expressions that described the problem, and then through deductive reasoning arrive at an exact analytical method of determining the optimal policy for this problem.\n",
    "\n",
    "These perspectives, for the most part, converged on the same optimal policy. Why is this? The optimal solution of a problem is inherent to the problem itself, not the method used to find the solution. Thus, different optimization approaches applied to the same problem, will arrive at the same conclusions, to the extent that each different optimization approach is effective. This insight has been a powerful tool in understanding evolved behaviours and traits, with optimality models helping to explain a staggering array of animal behaviours (and the behaviour of all kinds of life). These are just a few examples:\n",
    "\n",
    "1. **Humming Bird Foraging**: Many animals are under selection pressure to maximize their energy intake while minimizing the risk of predation and energetic costs. The sequence in which hummingbirds feed on the flowers in their territory can be understood as minimizing flight distance and thus saving energy and time. This behavior exemplifies an energy-efficient strategy that boosts survival and reproductive chances.\n",
    "\n",
    "2. **Web Building in Spiders**: Considering the metabolic cost of producing silk, spiders build their webs to capture the most prey while using the least amount of material. Different spider species, depending on their ecological niche, have optimized their web designs accordingly - from the dense tangle webs of the funnel-web spiders to the expansive orb webs of garden spiders, each design maximizes prey capture rates while minimizing silk production cost.\n",
    "\n",
    "3. **Echolocation in Bats**: Bats optimize the frequency and duration of their echolocation calls based on their environment and prey type. In cluttered environments, bats often use shorter frequency-modulated calls to detect nearby objects. However, in open spaces, they employ longer calls for detection over a more extended range. This behavioral adaptation can be viewed as an optimization problem where bats adjust their call parameters for maximum prey detection.\n",
    "\n",
    "4. **Migratory Paths of Birds**: Migratory birds traverse vast distances, optimizing their routes to conserve energy, avoid predators, and capitalize on food sources. For example, the Arctic tern, traveling from its Arctic breeding grounds to the Antarctic coast, chooses a path that, although not the shortest, exploits prevailing wind patterns and abundant food sources.\n",
    "\n",
    "5. **Root Architecture in Plants**: The way a plant structures its root system is crucial for both nutrient uptake and stability. For instance, in areas prone to drought, plants like mesquite trees invest in deep taproots that reach far into the soil, tapping into groundwater sources. In contrast, plants in nutrient-rich but unstable environments, like riverbanks, invest in expansive but shallow root systems to maximize nutrient absorption while anchoring themselves against fast-moving water.\n",
    "\n",
    "6. **Leaf Orientation and Sun Tracking**: Sunflower fields during a sunny day offer a mesmerizing sight: rows of flowers all facing the sun. This behavior, known as heliotropism, allows the plant to maximize sunlight absorption. By tracking the sun across the sky, sunflowers optimize photosynthesis rates. Similarly, desert plants like the creosote bush have small, vertically-oriented leaves, reducing their exposure to the intense midday sun and minimizing water loss due to transpiration.\n",
    "\n",
    "7. **Thermoregulation in Reptiles**: Cold-blooded reptiles, such as lizards and snakes, primarily thermoregulate through behaviour (not metabolically, like warm-blooded mammals and birds). Take the diurnal behavior of desert lizards: by basking in the morning sun, they quickly elevate their body temperature, optimizing their physiological functions for the day ahead. As midday approaches with its peak heat, these reptiles retreat to cooler shades or burrows, ensuring they don't overheat. This can be understood as an optimization strategy, balancing the need for warmth to maintain metabolic rates with the imperative to avoid lethal high temperatures.\n",
    "\n",
    "8. **Camouflage and Coloration**: The appearance of many plants and animals appearances can be understood as optimization for avoiding detection. Consider the peppered moth in industrial England. Historically, the majority of these moths were light-colored, blending seamlessly with the lichen-covered trees. However, during the Industrial Revolution, pollution darkened the trees, giving an advantage to rare dark-colored moths. As these darker moths thrived in their new environment, their numbers surged, demonstrating a rapid optimization of camouflage in response to environmental changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Each of the examples above takes an 'Ultimate' evolutionary functional perspective to explain behaviours or traits. However, this is but one of four perspectives from which to understand a behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bio Box: Different Kinds of Understanding and Tinbergen's Four Questions\n",
    "\n",
    "Building on work by Julian Huxley, Niko Tinbergen described four \"whys\" or perspectives for understanding an observed behaviour (adapted from Aristotle's four causes). These continue to ground and inform interdisciplinary work across the behavioural sciences.\n",
    "\n",
    "1. **Mechanistic** (Sensory and Biophysical Causes): This approach seeks to understand the immediate causes of a behaviour from the sensory input that elicits it through to the biophysical processes of the organism which produce the behaviour in response to a particular environmental input. While the 'Ultimate' perspective might explain why a bird sings in terms of its evolutionary function, e.g. to attract a mate, the mechanistic perspective looks at how a bird's vocal cords, brain circuits, and hormones function together to produce the song in response to certain environmental conditions, but not others.\n",
    "\n",
    "2. **Ontogenetic** (Developmental Causes): This perspective examines how a behavior develops in an individual over their lifetime. It encompasses aspects of both learning and maturation. For example, while many birds are born with the inherent capability to sing, they often refine their songs through interactions with their environment and other birds, improving their repertoire and accuracy over time.\n",
    "\n",
    "3. **Functional** (Adaptive Evolutionary Causes): This is the perspective we have focused on so far. It seeks to understand behavior in terms of the selective advantage it confers to the organism. How does the behavior increase the chances of survival and reproduction? The hummingbird foraging and the spider web-building examples fit into this category, showcasing behaviors that have been optimized over generations for increased fitness.\n",
    "\n",
    "4. **Phylogenetic** (Historical Evolutionary Causes): This perspective traces the behavior's lineage through evolutionary history. Why does a particular species exhibit this behavior, but not another? Did the common ancestor of these species also exhibit this behavior? How has evolutionary history set the conditions and constraints on which adaptive new forms are possible? For instance, the vertebrate eye (including the human eye) has a blind spot, whereas octopus eyes do not. In each of these two lineages, the eye originally developed in one of two distinct ways. Once this initial development program for the vertebrate eye was set, there were no intermediate forms that were both adaptive and would have enabled it to evolve without a blind spot.\n",
    "\n",
    "Scientific work can be done using one, several, or all of these perspectives combined. No single perspective is inherently better or more important. What is deeply valuable though, for any project within the life sciences, is knowing which kinds of understanding are being aimed at, and how work from the other perspectives might guide and constrain understanding in the area(s) of focus.\n",
    "\n",
    "In this book we will often take a functional perspective. However, understanding how phylogeny, ontogeny, and mechanism constrain and determine the optimization problems to be analyzed is critical to the relevance and validity of the functional perspective.\n",
    "\n",
    "Where does this four-perspective framework situate neuroscience within the broader behavioral sciences? Traditionally, neuroscience has largely been part of a mechanistic understanding of behaviour, i.e. how do the brain circuits compute and produce muscles activations that generate appropriate behaviour given environmental inputs. Learning is also obvious and important part of what brains do. To the extent that neuroscience research is focused on learning, i.e. adaptive neural plasticity, it is also a part of the ontogenetic understanding of behaviour.\n",
    "\n",
    "We believe that new kinds of understanding, in particular, understanding grounded in an ontogenetic and functional perspective, have recently become possible based on new insights into the optimization of high-dimensional systems emerging from Machine Learning research. At the same time, again based on insights from Machine Learning research, we believe that neuroscience as a field is approaching the natural limits of what can be comprehended about the brain and its behavioural outputs in a certain mechanistic sense. Given this, we advocate a shift away from attempts at 'circuit diagram understanding' of how brains compute and determine behaviour. In the field of ML 'circuit diagrams' of large complicated models that generate adaptive behaviour in complex environments are readily available for comprehensive and detailed inspection, all within completely controllable artificial environments. Yet, despite this total visibility, a satisfactory 'understanding' of how these many parameter systems compute and determine behaviour, so called 'interpretability', is so far largely absent. Conversely, ML researchers do have understanding of the learning algorithms that produce these complex and largely inscrutable models, i.e. the ontogeny of the these models, and the mechanisms of this ontogeny are relaitvely clear. It is for this reason that we encourage a 'learning first' approach to neuroscience, focused on understanding the adaptive neural plasticity dynamics governing brain development and the ongoing learning and behavioural plasticity that this supports. We posit that this, together with the mechanistic underpinnings of these learning dynamics, all considered in light of the overarching adaptive functional role of the brain, will lead to a predictive and integrative understanding of the brain.\n",
    "\n",
    "In the next chapter, we will focus primarily on evolution, aiming to elucidate the general adaptive functions of the brain. Then, in subsequent parts of the book we will connect different classes of machine learning algorithms to the specific adaptive challenges they can address in the context of the brain. In each case we will show how insights from effective ML algorithms on a specific class of adaptive problem can both integrate existing empirical neuroscience work and inform future empirical work, within a 'learning first' perspective on brain function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P1C2_Sequence3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
