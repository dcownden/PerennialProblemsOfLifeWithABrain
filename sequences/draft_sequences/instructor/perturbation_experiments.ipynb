{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/draft_sequences/perturbation_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/draft_sequences/perturbation_experiments.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here we explore the convergence properties of two different weight perturbation update rules, one uses the 'raw' parameterwise correlations between reward change and parameter perturbation, the other uses a \"proper\" fitting of the hyperplane passing through the test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def z(x, y):\n",
    "  return x**2 - y**2\n",
    "\n",
    "def perturb_and_eval(x, y, sigma):\n",
    "  perturbation = rng.normal(0, sigma, size=2)\n",
    "  delta_x = perturbation[0]\n",
    "  delta_y = perturbation[1]\n",
    "  perturbed_x = x + delta_x\n",
    "  perturbed_y = y + delta_y\n",
    "  perturbed_z = z(perturbed_x, perturbed_y)\n",
    "  delta_z = perturbed_z - z(x, y)\n",
    "  test = (perturbed_x, perturbed_y, perturbed_z)\n",
    "  delta = (delta_x, delta_y, delta_z)\n",
    "  return test, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "perturb_and_eval(4, 0, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "tests = []\n",
    "deltas = []\n",
    "for i in range(5000):\n",
    "  test, delta = perturb_and_eval(4, 0, 1.0)\n",
    "  tests.append(test)\n",
    "  deltas.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "delta_a = np.array(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "naive_est_x = delta_a[:,2] / delta_a[:,0]\n",
    "naive_est_y = delta_a[:,2] / delta_a[:,1]\n",
    "b = 50  # Example threshold\n",
    "clipped_naive_est_x = np.clip(naive_est_x, -b, b)\n",
    "clipped_naive_est_y = np.clip(naive_est_y, -b, b)\n",
    "cov_est_x = np.sum(delta_a[:,0] * delta_a[:,2]) / np.sum((delta_a[:,0])**2)\n",
    "cov_est_y = np.sum(delta_a[:,1] * delta_a[:,2]) / np.sum((delta_a[:,1])**2)\n",
    "print(np.mean(naive_est_x))\n",
    "print(np.mean(naive_est_y))\n",
    "print(cov_est_x)\n",
    "print(cov_est_y)\n",
    "print(np.mean(clipped_naive_est_x))\n",
    "print(np.mean(clipped_naive_est_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "np.max(naive_est_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-_4n6PG3PLnN"
   },
   "outputs": [],
   "source": [
    "def z(x, y):\n",
    "  return -5 * x**2 - 3 * y**2 + x * y\n",
    "\n",
    "def dz(x, y):\n",
    "  return (-10 * x + y, -6 * y + x)\n",
    "\n",
    "def perturb(x, y, sigma):\n",
    "  perturbation = rng.normal(0, sigma, size=2)\n",
    "  return x + perturbation[0], y + perturbation[1]\n",
    "\n",
    "def perturb_and_eval(x, y, sigma):\n",
    "  perturbation = rng.normal(0, sigma, size=2)\n",
    "  delta_x = perturbation[0]\n",
    "  delta_y = perturbation[1]\n",
    "  perturbed_x = x + delta_x\n",
    "  perturbed_y = y + delta_y\n",
    "  perturbed_z = z(perturbed_x, perturbed_y)\n",
    "  delta_z = z(x, y) - perturbed_z\n",
    "  return delta_x, delta_y, delta_z\n",
    "\n",
    "def avg_est1(delta_x1, delta_y1, delta_z1):\n",
    "  x_update = delta_z1 / delta_x1\n",
    "  y_update = delta_z1 / delta_y1\n",
    "  return x_update, y_update\n",
    "\n",
    "def avg_est2(delta_x1, delta_y1, delta_z1, delta_x2, delta_y2, delta_z2):\n",
    "  x_update = (delta_z1/delta_x1 + delta_z2/delta_x2) / 2\n",
    "  y_update = (delta_z1/delta_y1 + delta_z2/delta_y2) / 2\n",
    "  return x_update, y_update\n",
    "\n",
    "def fit_est(delta_x1, delta_y1, delta_z1, delta_x2, delta_y2, delta_z2):\n",
    "  x_update = ((delta_y1 * delta_z2 - delta_z1 * delta_y2) /\n",
    "              (delta_x1 * delta_y2 - delta_x2 * delta_y1))\n",
    "  y_update = ((delta_x1 * delta_z2 - delta_z1 * delta_x2) /\n",
    "              (delta_x1 * delta_y2 - delat_x2 * delta_y1))\n",
    "  return x_update, y_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def z(x, y):\n",
    "    return -5 * x**2 - 3 * y**2 + x * y\n",
    "\n",
    "def dz(x, y):\n",
    "    return (-10 * x + y, -6 * y + x)\n",
    "\n",
    "def perturb_and_eval(x, y, sigma):\n",
    "    perturbation = rng.normal(0, sigma, size=2)\n",
    "    delta_x = perturbation[0]\n",
    "    delta_y = perturbation[1]\n",
    "    perturbed_x = x + delta_x\n",
    "    perturbed_y = y + delta_y\n",
    "    perturbed_z = z(perturbed_x, perturbed_y)\n",
    "    delta_z = perturbed_z - z(x, y)\n",
    "    return delta_x, delta_y, delta_z\n",
    "\n",
    "def avg_est1(delta_x, delta_y, delta_z):\n",
    "    if delta_x == 0 or delta_y == 0:\n",
    "        return None\n",
    "    x_update = delta_z / delta_x\n",
    "    y_update = delta_z / delta_y\n",
    "    return x_update, y_update\n",
    "\n",
    "def avg_est2(delta_x1, delta_y1, delta_z1, delta_x2, delta_y2, delta_z2):\n",
    "    if delta_x1 == 0 or delta_y1 == 0 or delta_x2 == 0 or delta_y2 == 0:\n",
    "        return None\n",
    "    x_update = ((delta_z1 / delta_x1) + (delta_z2 / delta_x2)) / 2\n",
    "    y_update = ((delta_z1 / delta_y1) + (delta_z2 / delta_y2)) / 2\n",
    "    return x_update, y_update\n",
    "\n",
    "def fit_est(delta_x1, delta_y1, delta_z1, delta_x2, delta_y2, delta_z2):\n",
    "    denominator = (delta_x1 * delta_y2 - delta_x2 * delta_y1)\n",
    "    if denominator == 0:\n",
    "        return None\n",
    "    x_update = (delta_y1 * delta_z2 - delta_z1 * delta_y2) / denominator\n",
    "    y_update = (delta_z1 * delta_x2 - delta_x1 * delta_z2) / denominator\n",
    "    return x_update, y_update\n",
    "\n",
    "# Parameters\n",
    "sigma = 0.1\n",
    "n_samples = 1000\n",
    "true_x, true_y = dz(1, 1) # True derivatives at the point (1,1)\n",
    "\n",
    "# Storage for estimates\n",
    "estimates1 = []\n",
    "estimates2 = []\n",
    "estimates_fit = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    delta_x1, delta_y1, delta_z1 = perturb_and_eval(1, 1, sigma)\n",
    "    delta_x2, delta_y2, delta_z2 = perturb_and_eval(1, 1, sigma)\n",
    "\n",
    "    est1 = avg_est1(delta_x1, delta_y1, delta_z1)\n",
    "    est2 = avg_est2(delta_x1, delta_y1, delta_z1, delta_x2, delta_y2, delta_z2)\n",
    "    est_fit = fit_est(delta_x1, delta_y1, delta_z1, delta_x2, delta_y2, delta_z2)\n",
    "\n",
    "    if est1:\n",
    "        estimates1.append(est1)\n",
    "    if est2:\n",
    "        estimates2.append(est2)\n",
    "    if est_fit:\n",
    "        estimates_fit.append(est_fit)\n",
    "\n",
    "# Convert to numpy arrays for easier manipulation\n",
    "estimates1 = np.array(estimates1)\n",
    "estimates2 = np.array(estimates2)\n",
    "estimates_fit = np.array(estimates_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Adjusted plotting with separate histograms for better visibility\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 12),)  # 2 rows for X and Y, 3 columns for each estimation method\n",
    "\n",
    "# Plotting for X slope estimates\n",
    "axs[0, 0].hist(estimates1[:, 0], bins=30, alpha=0.7, label='Avg Est 1 (X)')\n",
    "axs[0, 0].axvline(x=true_x, color='r', linestyle='--', label='True derivative (X)')\n",
    "axs[0, 0].set_title('Avg Est 1 (X)')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[0, 1].hist(estimates2[:, 0], bins=30, alpha=0.7, label='Avg Est 2 (X)')\n",
    "axs[0, 1].axvline(x=true_x, color='r', linestyle='--', label='True derivative (X)')\n",
    "axs[0, 1].set_title('Avg Est 2 (X)')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "axs[0, 2].hist(estimates_fit[:, 0], bins=30, alpha=0.7, label='Fit Est (X)')\n",
    "axs[0, 2].axvline(x=true_x, color='r', linestyle='--', label='True derivative (X)')\n",
    "axs[0, 2].set_title('Fit Est (X)')\n",
    "axs[0, 2].legend()\n",
    "\n",
    "# Plotting for Y slope estimates\n",
    "axs[1, 0].hist(estimates1[:, 1], bins=30, alpha=0.7, label='Avg Est 1 (Y)')\n",
    "axs[1, 0].axvline(x=true_y, color='r', linestyle='--', label='True derivative (Y)')\n",
    "axs[1, 0].set_title('Avg Est 1 (Y)')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "axs[1, 1].hist(estimates2[:, 1], bins=30, alpha=0.7, label='Avg Est 2 (Y)')\n",
    "axs[1, 1].axvline(x=true_y, color='r', linestyle='--', label='True derivative (Y)')\n",
    "axs[1, 1].set_title('Avg Est 2 (Y)')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "axs[1, 2].hist(estimates_fit[:, 1], bins=30, alpha=0.7, label='Fit Est (Y)')\n",
    "axs[1, 2].axvline(x=true_y, color='r', linestyle='--', label='True derivative (Y)')\n",
    "axs[1, 2].set_title('Fit Est (Y)')\n",
    "axs[1, 2].legend()\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Slope Estimate', ylabel='Frequency')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "np.mean(estimates1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "np.mean(estimates2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "np.mean(estimates_fit, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "true_x, true_y = dz(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(true_x, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate sample data for X and Y\n",
    "np.random.seed(42) # For reproducibility\n",
    "X = np.random.normal(0, 1, 10000) # 100 data points from a normal distribution for X\n",
    "Y = np.random.normal(0, 1, 10000) # 100 data points from a normal distribution for Y\n",
    "\n",
    "# Calculate Z = X^2 + Y^2\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "# Calculate the sample means\n",
    "mean_X = np.mean(X)\n",
    "mean_Y = np.mean(Y)\n",
    "mean_Z = np.mean(Z)\n",
    "\n",
    "# Calculate the covariance between X and Z, and Y and Z\n",
    "cov_XZ = np.mean((X - mean_X) * (Z - mean_Z))\n",
    "cov_YZ = np.mean((Y - mean_Y) * (Z - mean_Z))\n",
    "\n",
    "# Calculate the standard deviations of X, Y, and Z\n",
    "std_X = np.std(X, ddof=1)\n",
    "std_Y = np.std(Y, ddof=1)\n",
    "std_Z = np.std(Z, ddof=1)\n",
    "\n",
    "# Calculate the correlation coefficients\n",
    "corr_XZ = cov_XZ / (std_X * std_Z)\n",
    "corr_YZ = cov_YZ / (std_Y * std_Z)\n",
    "\n",
    "corr_XZ, corr_YZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "x_corr_hist = []\n",
    "y_corr_hist = []\n",
    "\n",
    "for ii in range(10000):\n",
    "  dX = np.random.normal(0, 1, 1) # 100 data points from a normal distribution for X\n",
    "  dY = np.random.normal(0, 1, 1) # 100 data points from a normal distribution for Y\n",
    "  dZ = dX**2 + dY**2\n",
    "\n",
    "  naive_est_x = dZ / dX\n",
    "  naive_est_y = dZ / dY\n",
    "\n",
    "  X = np.concatenate((dX, [0]))\n",
    "  Y = np.concatenate((dY, [0]))\n",
    "  Z = X**2 + Y**2\n",
    "\n",
    "  # Calculate the sample means\n",
    "  mean_X = np.sum(X) / len(X)\n",
    "  mean_Y = np.sum(Y) / len(Y)\n",
    "  mean_Z = np.sum(Z) / len(Z)\n",
    "\n",
    "  # Calculate the covariance between X and Z, and Y and Z\n",
    "  cov_XZ = np.mean((X - mean_X) * (Z - mean_Z))\n",
    "  cov_YZ = np.mean((Y - mean_Y) * (Z - mean_Z))\n",
    "\n",
    "  # Calculate the standard deviations of X, Y, and Z\n",
    "  std_X = np.std(X, ddof=1)\n",
    "  std_Y = np.std(Y, ddof=1)\n",
    "  std_Z = np.std(Z, ddof=1)\n",
    "\n",
    "  # Calculate the correlation coefficients\n",
    "  corr_XZ = cov_XZ / (std_X * std_Z)\n",
    "  corr_YZ = cov_YZ / (std_Y * std_Z)\n",
    "\n",
    "  x_corr_hist.append(corr_XZ)\n",
    "  y_corr_hist.append(corr_YZ)\n",
    "  #print(ii)\n",
    "  #print(np.mean(x_hist))\n",
    "  #print(np.mean(y_hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "In a given learning episode one of four cases can occur:\n",
    "\n",
    "1. Prey is present, given the sensory input the organisms strikes, a positive reward is obtained\n",
    "2. Prey is not present, given the sensory input the organism strikes, a negative reward is obtained\n",
    "3. Prey is present, given the sensory input the organism does not strike, no reward is obtained\n",
    "4. Prey is not present, given the sensory input the organism does not strike, no reward is obtained\n",
    "\n",
    "We have the (initial) caveat that from the perspective of the organisms cases 3 and 4 are identical, so the parameter update rule must also be identical in each of those cases.\n",
    "\n",
    "It has been noted and proposed many times that the inherent stochasticity of synaptic firing might provide a kind of natural 'guess' and allow for this kind of learning to be implemented in the brain, [citations]. Empirical evidence for this kind of learning is also begining to accumulate [citations].\n",
    "\n",
    "The basic idea is as follows. Due to the stochastic spiking of neurons, in any given episode of senosry input -> behaviour production -> contingent reward, some but not all of the synaptic weights are causally implicated in the production of the behaviour (and consequent reward). When a good thing happens, synaptic connections that were involved in the generation of the behaviour should be strengthened, and conversely when bad things happen synaptic connections that were involved in the generation of the behaviour should be weakened. Synaptic connections that were not involved in producing the behaviour should be left unchanged. Zooming in to the synaptic level the four cases above are expanded as follows:\n",
    "\n",
    "1. Both input and output neurons spike, a good thing happens\n",
    "2. Both input and output neurons spike, a bad thing happens\n",
    "3. Input neuron spikes but output neuron doesn't spike, a neutral thing happens\n",
    "4. Input neuron doesn't spike, output neuron spikes, a good thing happens\n",
    "5. Input neuron doesn't spike, output neuron spikes, a bad thing happens\n",
    "6. Neither input nor output neuron spikes, a neutral thing happens.\n",
    "\n",
    "In this our first and most simple (from a physiological mechanism perspective, actually a bit complicated from a mathematical description perspective), only in the first two cases, when both the pre- and post- synaptic neurons fire, is the synapse causally implicated in the reward outcome. So our rule needs to prescribe no change, unless both pre- and post- synaptic neurons have fired. If a good thing happens, then the connection should be reinforced, and if a bad thing happens then the connection should be weakened. Our learning rule written to echo and presiage the 'REINFORCE' formulation of Williams (1992), and also Resrola Wagnar rule, and also TD formulations. In some sense this is cannonical form of learning rules is.\n",
    "\n",
    "$$\\Delta w_i = \\alpha \\cdot (r - b) \\cdot s_i \\cdot s_o$$\n",
    "\n",
    "Almost every learning rule for parameters that we look at will be roughly composed of these three factors: learning rate, error, eligibility. The first factor is positive learning rate or step size parameter of the learning process. In this specific case $\\alpha$ is constant, but it could be made contingent on various factors in more complicated models, e.g. learning rates often start high and are lowered as the learning system gets more 'dialed in'. The second part is an 'error' term, in this case a 'reward prediction error', $(r-b)$, gives the way in which the actual received reward $r$ differs from some baseline expectation. Again in more complicated learning rules $b$ could be made contingent on many factors, and this contingency of $b$ on other factors might itself be learned. Here though, we assume a constant expected reward, $b=0$, so we can think of $b$ as an expectation of how good the world is on average, if the organism is performing as well as expected. We imagine that parameters like $\\alpha$ and $b$ are set and tuned by evolution to ensure effective learning. Lastly, in this simple model the $s_i \\cdot s_o$ term is the 'eligibility' of the synapse for change. In general, this will be some measure of how causally implicated the particular parameter $w_i$ is in causing the received reward. Here we use one of the simplest possible eligibility terms, which takes the value one only when both pre- and post- synaptic neurons fire, and is otherwise zero. Again in more complicated learning rules, this eligibility term could be more complicated, e.g. it might incorporate knowledge of the structure of the network producing the behaviour, often in the form of a gradient, to further modulate the strength and direction of the weight change. Note, $w_i$ is the $i^{\\text{th}}$ element of $\\mathbf{W}$ and $s_i$ and $s_o$ are the realized spikes during the episode for the $i^{\\text{th}}$ input neuron and the output neuron respectively. Okay, let's implement and see how well this works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We model this creature's sensory-behaviour system as follows. Let $\\mathbf{x}$ be the raw sensory input vector in a given episode. We imagine that each element of $\\mathbf{x}$, denoted $x_i$ corresponds to the activation level of a single photosensitive neuron. These activation levels might be losely interpreted as firing rates. This input neurons are then connected by synapses to a single output neuron. The activation level of this output neuron is computed as\n",
    "$$y = \\mathbf{Wx}$$\n",
    "Here, $\\mathbf{W}$ is the matrix of synaptic weights between the input neurons and the output neuron (is this case there is only one output so $\\mathbf{W}$ is effectively a row vector). As a quick reminder, in our notation, bold lowercase letters represent column vectors, while bold uppercase letters denote matrices or higher-dimensional tensors. We imagine that the probabilistic spiking of this output neuron determines the strike-no-strike behaviour of the organism, specifically:\n",
    "$$ \\Pr \\{\\text{strike}\\} = \\sigma(y) $$\n",
    "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma(y)$$\n",
    "\n",
    "Here $\\sigma(x): \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}$ is the standard logistic (sigmoid) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "!pip install ucimlrepo > /dev/null 2> /dev/null #google.colab\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "data_set = fetch_ucirepo(id=80)\n",
    "X = data_set.data.features.values\n",
    "# Translate the data to have a minimum of 0\n",
    "X_translated = X - X.min()\n",
    "# Scale the data to have a range from 0 to 12 (which is 6 - (-6))\n",
    "scaling_factor = 12 / (X.max() - X.min())\n",
    "X_scaled = X_translated * scaling_factor\n",
    "# Finally, shift the data to be centered between -6 and 6\n",
    "X_final = X_scaled - 6\n",
    "\n",
    "y = data_set.data.targets.values\n",
    "rng = np.random.default_rng(seed=2021)\n",
    "scramble_permutation = rng.permutation(X.shape[1])\n",
    "Xs = X_final[:, scramble_permutation]\n",
    "y1 = y % 2\n",
    "y2 = np.array(y >= 5, dtype=y.dtype)\n",
    "simple_index = ((y.flatten()==1) | (y.flatten()==0))\n",
    "X_simple = Xs[simple_index]\n",
    "y1_simple = y1[simple_index]\n",
    "# if you only had one feature which would likely be best for discrimination\n",
    "epsilon = 10\n",
    "class_a_sep = np.mean(X_simple[y1_simple.flatten() == 1, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 1, :], axis=0) + epsilon)\n",
    "class_b_sep = np.mean(X_simple[y1_simple.flatten() == 0, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 0, :], axis=0) + epsilon)\n",
    "best_feature = np.argmax(class_a_sep - class_b_sep)\n",
    "X_simple_1_feature = X_simple[:, [best_feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def learning_episode(W, x, target, rng, alpha=0.01, b=0, ):\n",
    "  #forward pass, generate behaviour\n",
    "  sensory_nueron_spike_probs = np_sigmoid(x)\n",
    "  sensory_neuron_spikes = rng.binomial(1, sensory_nueron_spike_probs)\n",
    "  output_neuron_activation = np.dot(W, sensory_neuron_spikes)\n",
    "  output_neuron_spike_prob = np_sigmoid(output_neuron_activation)\n",
    "  output_neuron_spike = rng.binomial(1, output_neuron_spike_prob)\n",
    "  #evaluate behaviour\n",
    "  reward = output_neuron_spike * target\n",
    "  # backward pass,update parameters\n",
    "  W += alpha * (reward - b) * output_neuron_spike * sensory_neuron_spikes\n",
    "  return W, reward, output_neuron_spike, output_neuron_spike_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def np_sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Now embedd those learning episodes in a training loop\n",
    "# pre calculate activation to probability for efficiency\n",
    "train_rng = np.random.default_rng(1234)\n",
    "W = train_rng.standard_normal(size=(1, Xs.shape[1]))\n",
    "\n",
    "epochs = 10  # Number of times to iterate over the dataset\n",
    "performance_tracker = {'cumulative_reward': [], 'accuracy': [],\n",
    "                       'TP':[], 'FP':[], 'TN':[], 'FN':[],}\n",
    "max_possible_reward = np.sum(y1==1)\n",
    "alpha=0.00001\n",
    "b=0\n",
    "pbar = tqdm(range(epochs), desc='Training Progress')\n",
    "for epoch in pbar:\n",
    "  cumulative_reward = 0\n",
    "  TP, FP, TN, FN = 0, 0, 0, 0\n",
    "  shuffled_indices = train_rng.permutation(Xs.shape[0])\n",
    "  for i in shuffled_indices:  # Iterate over each example\n",
    "    x = Xs[i]\n",
    "    target = y1[i]\n",
    "    W, reward, output_neuron_spike, _ = learning_episode(W, x, target,\n",
    "                                                         train_rng, alpha, b)\n",
    "    # Track performance\n",
    "    cumulative_reward += reward\n",
    "    # Update confusion matrix\n",
    "    if output_neuron_spike == 1 and target == 1:\n",
    "      TP += 1\n",
    "    elif output_neuron_spike == 1 and target == -1:\n",
    "      FP += 1\n",
    "    elif output_neuron_spike == 0 and target == 1:\n",
    "      FN += 1\n",
    "    elif output_neuron_spike == 0 and target == -1:\n",
    "      TN += 1\n",
    "  epoch_accuracy = (TP + TN) / Xs.shape[0]\n",
    "  performance_tracker['cumulative_reward'].append(cumulative_reward)\n",
    "  performance_tracker['accuracy'].append(epoch_accuracy)\n",
    "  # Store confusion matrix values\n",
    "  performance_tracker.setdefault('TP', []).append(TP)\n",
    "  performance_tracker.setdefault('FP', []).append(FP)\n",
    "  performance_tracker.setdefault('TN', []).append(TN)\n",
    "  performance_tracker.setdefault('FN', []).append(FN)\n",
    "  pbar.set_postfix({\n",
    "        'Acc': f'{epoch_accuracy:.4f}',\n",
    "        'Cum. Reward': cumulative_reward/max_possible_reward,\n",
    "        'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMyeW9Ol32rwSk+TyX39SR0",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "perturbation_experiments",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
