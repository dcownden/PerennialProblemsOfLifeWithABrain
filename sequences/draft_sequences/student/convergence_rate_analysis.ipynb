{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/draft_sequences/convergence_rate_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/draft_sequences/convergence_rate_analysis.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz ucimlrepo > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import asyncio\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "data_set = fetch_ucirepo(id=80)\n",
    "X = data_set.data.features.values\n",
    "# Translate the data to have a minimum of 0\n",
    "X_translated = X - X.min()\n",
    "# Scale the data to have a range from 0 to 12 (which is 6 - (-6))\n",
    "scaling_factor = 12 / (X.max() - X.min())\n",
    "X_scaled = X_translated * scaling_factor\n",
    "# Finally, shift the data to be centered between -6 and 6\n",
    "X_final = X_scaled - 6\n",
    "\n",
    "y = data_set.data.targets.values\n",
    "rng = np.random.default_rng(seed=2021)\n",
    "scramble_permutation = rng.permutation(X.shape[1])\n",
    "Xs = X_final[:, scramble_permutation]\n",
    "y1 = y % 2\n",
    "y2 = np.array(y >= 5, dtype=y.dtype)\n",
    "simple_index = ((y.flatten()==1) | (y.flatten()==0))\n",
    "X_simple = Xs[simple_index]\n",
    "y1_simple = y1[simple_index]\n",
    "# if you only had one feature which would likely be best for discrimination\n",
    "epsilon = 10\n",
    "class_a_sep = np.mean(X_simple[y1_simple.flatten() == 1, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 1, :], axis=0) + epsilon)\n",
    "class_b_sep = np.mean(X_simple[y1_simple.flatten() == 0, :], axis=0) / (np.std(X_simple[y1_simple.flatten() == 0, :], axis=0) + epsilon)\n",
    "best_feature = np.argmax(class_a_sep - class_b_sep)\n",
    "print(f'Best feature is {best_feature}')\n",
    "X_simple_1_feature = X_simple[:, [best_feature]]\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P2C1_S2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.3.2: Optimizing a Quadratic of Many Variables with Propose-Accept-Reject and Perturb-Measure-Step\n",
    "\n",
    "In the previous sequences we built up our intuitions around the Propose-Accept-Reject algorithm in lower dimensions. Now we're going to try to develop our intuitions around how the number of dimensions, or scale, of the problem affects the rate at which Propose-Accept-Reject can get close to an optimal solution. Systematic comparisons of learning algorithms is notorously fraught, different meta-parameter setting (e.g. step size) can make a huge difference, and in general good meta-parameters for one alogorithm-problem combination, will be very different from the good meta-parameters for a different algorithm-problem combination. All the details kind of matter. That said we can still develop some useful intuitions about scale, by studying a very simple system.\n",
    "\n",
    "We will use the simple quadratic function\n",
    "\n",
    "$$f(\\mathbf{x}) = \\sum_{i=1}^{N}x_i^2 $$\n",
    "\n",
    "and see how the rate of convergence to a good solution changes as we increase the number of dimensions. We will run our algorithms until either 1000 iterations have been completed, or the value of $f(\\mathbf{x})$ is less 0.1. For ease of comparison we will initialize $\\mathbf{x}$ randomly, such that $\\| \\mathbf{x} \\| = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to see how iterations to convergence changes as the number of dimensions increases\n",
    "def simple_quadratic(x):\n",
    "  return np.dot(x,x)\n",
    "\n",
    "def scale_stable_propose(x, step_size=0.1, rng=np.random.default_rng()):\n",
    "  dim = x.shape[0]\n",
    "  step_scale = step_size / np.sqrt(dim)\n",
    "  proposed_x = x + rng.standard_normal(size=x.shape) * step_scale\n",
    "  return proposed_x\n",
    "\n",
    "def fixed_scale_propose(x, step_size=0.1, rng=np.random.default_rng()):\n",
    "  proposed_x = x + rng.standard_normal(size=x.shape) * step_size\n",
    "  return proposed_x\n",
    "\n",
    "# propose accept reject loop\n",
    "step_size = 0.1\n",
    "max_proposals = 2000\n",
    "starting_distance_from_optimal = 5\n",
    "par_rng = np.random.default_rng(42)\n",
    "N_Dim = [1, 10, 100, 1000, 10000] # number of dimensions\n",
    "x_histories = []\n",
    "z_histories = []\n",
    "for N in N_Dim:\n",
    "  x_history = []\n",
    "  z_history = []\n",
    "  num_proposed = 0\n",
    "  num_accepted = 0\n",
    "  # initialize x\n",
    "  x = par_rng.standard_normal(size=N)\n",
    "  x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "  z = simple_quadratic(x)\n",
    "  x_history.append(x)\n",
    "  z_history.append(z)\n",
    "  while z_history[-1] > step_size**2:\n",
    "    proposed_x = scale_stable_propose(x, step_size=step_size, rng=par_rng)\n",
    "    num_proposed += 1\n",
    "    z_proposed = simple_quadratic(proposed_x)\n",
    "    if z_proposed < z:\n",
    "      x = proposed_x\n",
    "      z = z_proposed\n",
    "      num_accepted += 1\n",
    "    x_history.append(x)\n",
    "    z_history.append(z)\n",
    "    if num_proposed > max_proposals:\n",
    "      break\n",
    "  x_histories.append(x_history)\n",
    "  z_histories.append(z_history)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cmap = plt.cm.viridis  # This colormap goes from light (yellow) to dark (blue)\n",
    "colors = cmap(np.linspace(0, 1, len(N_Dim)+1))\n",
    "# Plotting the data\n",
    "for ii, z_history in enumerate(z_histories):\n",
    "  if len(z_history) > max_proposals:\n",
    "    z_history = z_history[:max_proposals]\n",
    "  ax.plot(z_history, label=f'{N_Dim[ii]} Dimensions', color=colors[ii])\n",
    "ax.hlines(step_size**2, 0, len(z_histories[-1]), linestyles='dashed', colors='gray')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Iterations to Convergence by Dimension\\nWith Constant Average (Total) Step Size')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The proposals in propose-accept-reject are generated by adding Gaussian noise to the current $\\mathbf{x}$ value that is\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{\\text{proposed}} = \\mathbf{x}_{\\text{current}} + \\mathbf{\\xi}\n",
    "$$\n",
    "\n",
    "with $\\mathbf{\\xi} \\sim \\sigma \\cdot \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The average size of a perturbation, i.e., its Euclidean length $\\|\\mathbf{\\xi} \\|$, is given by\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\mathbb{E}[\\|\\mathbf{\\xi} \\|] &= \\mathbb{E}\\left[\\sqrt{\\sum_{i=1}^N \\xi_i^2}\\right] = \\sigma \\mathbb{E}[\\chi_N] = \\sigma \\sqrt{2} \\frac{\\Gamma\\left(\\frac{N+1}{2}\\right)}{\\Gamma\\left(\\frac{N}{2}\\right)} \\\\\n",
    "&\\approx \\sigma \\sqrt{N} \\ \\text{for large } N.\n",
    "\\end{align}$$\n",
    "\n",
    "Here $\\chi_N$ denotes a chi distribution with $N$ degrees of freedom, representing the distribution of the norm of a vector of $N$ independent standard normal random variables. ($\\Gamma$ denotes the gamma function which is a generalization of the factorial function on integers to real and complex values.) For high dimensions, i.e., large $N$, the approximation $\\sigma \\sqrt{N}$ becomes increasingly accurate due to the central limit theorem.\n",
    "\n",
    "Thus to have a consistent average proposal step length, $s$, we need to have $\\sigma = \\frac{s}{\\sqrt{N}}$. If instead have a constant $\\sigma$ as dimension increase, the average sizes of the steps will increase as the number of the dimensions increases. This is problematic as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to see how having step size increase with dimension is a problem.\n",
    "# propose accept reject loop\n",
    "step_size = 0.1\n",
    "starting_distance_from_optimal = 5\n",
    "par_rng = np.random.default_rng(42)\n",
    "N_Dim = [1, 10, 100, 1000] # number of dimensions\n",
    "x_histories = []\n",
    "z_histories = []\n",
    "max_proposals = 2000\n",
    "for N in N_Dim:\n",
    "  x_history = []\n",
    "  z_history = []\n",
    "  num_proposed = 0\n",
    "  num_accepted = 0\n",
    "  # initialize x\n",
    "  x = par_rng.standard_normal(size=N)\n",
    "  x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "  z = simple_quadratic(x)\n",
    "  x_history.append(x)\n",
    "  z_history.append(z)\n",
    "  while z_history[-1] > step_size:\n",
    "    proposed_x = fixed_scale_propose(x, step_size=step_size, rng=par_rng)\n",
    "    num_proposed += 1\n",
    "    z_proposed = simple_quadratic(proposed_x)\n",
    "    if z_proposed < z:\n",
    "      x = proposed_x\n",
    "      z = z_proposed\n",
    "      num_accepted += 1\n",
    "    x_history.append(x)\n",
    "    z_history.append(z)\n",
    "    if num_proposed > max_proposals:\n",
    "      break\n",
    "  x_histories.append(x_history)\n",
    "  z_histories.append(z_history)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cmap = plt.cm.viridis  # This colormap goes from light (yellow) to dark (blue)\n",
    "colors = cmap(np.linspace(0, 1, len(N_Dim)+1))\n",
    "# Plotting the data\n",
    "for ii, z_history in enumerate(z_histories):\n",
    "  if len(z_history) > max_proposals:\n",
    "    z_history = z_history[:max_proposals]\n",
    "  ax.plot(z_history, label=f'{N_Dim[ii]} Dimensions', color= colors[ii])\n",
    "ax.hlines(0.1, 0, len(z_histories[-1]), linestyles='dashed', colors='gray')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Iterations to Convergence by Dimension\\n With Constant Per Dimension Average Step Size')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Having step size scale up with dimension can lead to fast convergence at first (bigger step sizes means you get where you're going more quickly) but at some point large step sizes make it more difficult to zero in on the exact location of the optimal value. Larger step sizes limit percision, eventually to the point where no improvment is made.\n",
    "\n",
    "Now that we've seen what increasing the number of dimensions does to convergence rate of propose-accept-reject, let's take a look at perturb-measure-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to look at convergence rates in different dimensions for perturb-measure-step\n",
    "test_perturb = 0.00001\n",
    "step_scale = 0.1\n",
    "starting_distance_from_optimal = 5\n",
    "pms_rng = np.random.default_rng(42)\n",
    "N_Dim = [1, 10, 100, 1000, 10000] # number of dimensions\n",
    "x_histories = []\n",
    "z_histories = []\n",
    "max_steps = 2000\n",
    "for N in N_Dim:\n",
    "  x_history = []\n",
    "  z_history = []\n",
    "  num_steps = 0\n",
    "  num_function_evaluations = 0\n",
    "  # initialize x\n",
    "  x = par_rng.standard_normal(size=N)\n",
    "  x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "  z = simple_quadratic(x)\n",
    "  num_function_evaluations += 1\n",
    "  x_history.append(x)\n",
    "  z_history.append(z)\n",
    "  while z_history[-1] > step_size:\n",
    "    for dim in range(N):\n",
    "      x[dim] += test_perturb #perturb\n",
    "      z_test = simple_quadratic(x) #test\n",
    "      x[dim] -= test_perturb # un-perturb\n",
    "      dim_grad_est = (z_test - z) / test_perturb #measure slope\n",
    "      x[dim] -= step_scale * dim_grad_est #step\n",
    "      num_steps += 1\n",
    "      z = simple_quadratic(x)\n",
    "      num_function_evaluations += 2\n",
    "      x_history.append(x)\n",
    "      z_history.append(z)\n",
    "      if num_steps > max_steps:\n",
    "        break\n",
    "    if num_steps > max_steps:\n",
    "      break\n",
    "  x_histories.append(x_history)\n",
    "  z_histories.append(z_history)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cmap = plt.cm.viridis  # This colormap goes from light (yellow) to dark (blue)\n",
    "colors = cmap(np.linspace(0, 1, len(N_Dim)+1))\n",
    "# Plot each dimensions history\n",
    "for ii, z_history in enumerate(z_histories):\n",
    "  if len(z_history) > max_proposals:\n",
    "    z_history = z_history[:max_proposals]\n",
    "  ax.plot(z_history, label=f'{N_Dim[ii]} Dimensions', color= colors[ii])\n",
    "ax.hlines(0.1, 0, len(z_histories[-1]), linestyles='dashed', colors='gray')\n",
    "ax.set_xlabel('# Steps')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Steps to Convergence\\nWith Fixed Step Scale Factor')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The number of iterations here to convergence increases with dimension, partly because, this version of perturb-measure-step basically treats each dimension in isolation, so $N$ steps are required to make improvements in all $N$ dimensions. In the case above where we have 10,000 dimensions, but only run for 2000 steps, 8000 of the elements of $\\mathbf{x}$ have been improved. Scale is important for the brain. Recall that there are trillions of synapses the brains of brainy animals. Learning in the brain is going to require optimzation algorithms that are still effective in high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 2.1.3.3 Perturb-Meausre-Step and Propose-Accept-Reject Are Always Slow in Higher Dimensions.\n",
    "\n",
    "As mentioned before, comparing algorithms is notoriously difficult, so much can depend upon the specifics of the problems being used for comparison, and the interplay of the meta-parameters of the optimization algorithm with the specifics of the problem. To navigate through this potential noise, we're going to use an analytical approach. Our problem setup is as follows:\n",
    "\n",
    "We consider a function \\(f(\\mathbf{x})\\) that is *analytic*. This has a precise mathematical meaning, but intuitively it means that the function does not exhibit excessive local variability. More formally, an analytic function can be locally expressed by a convergent power series. For our purposes, this implies smoothness and the absence of abrupt changes in behavior, which in turn allows for reliable use of linear approximations in the vicinity of any point within its domain. Specifically, the function can be approximated as:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x} + \\mathbf{\\delta}) \\approx f(\\mathbf{x}) + \\mathbf{\\delta} \\cdot \\mathbf{g})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{g} = \\nabla f(\\mathbf{x}) = \\left. \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)\\right|_{\\mathbf{x}}$ is the gradient of $f$ evalutated at $\\mathbf{x}$, $\\mathbf{\\delta}$ is a small perturbation vector in $\\mathbb{R}^n$, and $\\mathbf{delta} \\cdot \\mathbf{g}$ is the inner product of the two vectors.\n",
    "Note that the perturbation $\\mathbf{\\delta}$ must be sufficiently small. As $\\|\\mathbf{\\delta}\\|$ increases in magnitude, higher-order terms in the Taylor expansion of $f$ at $\\mathbf{x}$ become significant, and the linear approximation becomes less accurate.\n",
    "\n",
    "Okay so now we want to think about our two algorithms, propose-accept-reject and perturb-measure-step, and ask, for any analytic function, at any point $\\mathbf{x}$, with a given step size $s$, what is the average expected improvement from a single interation of the algorithm, and how does this expected improvement compare to the 'best possible' improvement of taking a step in the direction of the gradient?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Propose-accept-reject analysis\n",
    "Let's start, propose-accept-reject goes first.\n",
    "\n",
    "1. The proposals in propose-accept-reject are generated by adding Gaussian noise to the current $\\mathbf{x}$ value that is\n",
    "$$\n",
    "\\mathbf{x}' = \\mathbf{x} + \\mathbf{\\xi}\n",
    "$$\n",
    "with $\\mathbf{\\xi} \\sim \\frac{s}{\\sqrt{n}} \\cdot \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. We scale each perturbation by $\\frac{s}{\\sqrt{n}}$ so that $\\mathbb{E}\\left[\\| \\mathbf{\\xi} \\|\\right] = s$.If we reject a proposal the improvement is zero.\n",
    "2. We assume that $s$ is small enough, that the linear approximation is good, and the expected change in the function is then simply $$ \\xi \\cdot \\mathbf{g}$$\n",
    "3. Now we haven't said what $\\mathbf{g}$ is or what $f$ is, so at first glance it may seem crazy to try and compute this, but we're about to do a sneaky math trick so pay attention. We're going to do a change of coordinates. $\\mathbf{\\xi}$ is basically a little Guassian cloud, and the density of this cloud is isotropic, that means the same in all radial dimensions, so that means that any rigid rotation of coordinates will leave the distribution unchanged. Okay, now $\\mathbf{g}$ can be re-written as the product of a unit direction vector $\\frac{\\mathbf{g}}{\\|\\mathbf{g}\\|}$ and a magnitude $\\|\\mathbf{g}\\|$. Then we can shift our coordinates so that our first dimension is perfectly aligned with the gradient, and all other $n-1$ dimensions are perfectly unaligned with the gradient. In this new coordinate system $\\mathbf{g} = \\|\\mathbf{g}\\|(1, 0, \\cdots, 0)$, and then the inner product of is simply $$(\\xi_1 \\cdot 1 + \\xi_2 \\cdot 0 + \\dots + \\xi_n \\cdot 0) \\|\\mathbf{g}\\| = \\xi_1 \\|\\mathbf{g}\\|$$\n",
    "4. Then $\\mathbb{E}\\left[\\xi_1 | \\text{Proposal Accepted} \\right] = s \\sqrt{\\frac{2}{n \\pi}}$, since the proposal will only be accepted if $\\xi_1 > 0$, so conditional on acceptance $\\xi_1$ follows a half-normal or $\\chi_1$ distribution.\n",
    "5. If a proposal is accepted the expected improvement is $$s \\|\\mathbf{g}\\| \\sqrt{\\frac{2}{n \\pi}}$$\n",
    "5. By the symetry of the normal distribution we also have that on average, half of the time proposals will be rejected and half of the time they will be accepted.\n",
    "6. When a proposal is rejected the improvement is zero\n",
    "7. Putting this all together the expected rate of improvement per iteration of propose-accept-reject is\n",
    "$$ \\frac{s\\|\\mathbf{g}\\|}{\\sqrt{2\\pi n}}$$\n",
    "\n",
    "Now if we were to just know the direction of $\\mathbf{g}$ and step $s$ in that direction, the improvement would be $s\\|\\mathbf{g}\\|$. So we see that the rate of improvement relative to the optimally oriented small step is reduced by a factor of $\\frac{1}{\\sqrt{2 \\pi n }}$. Intuitively as the number of dimensions increases, random step directions are less and less likely to be well aligned with this optimal direction. Note that if we were to use our 'always-step' variant of propose-accept-reject, we could do a little better with\n",
    "$$ \\frac{s\\|\\mathbf{g}\\|\\sqrt{2}}{\\sqrt{\\pi n}}$$ but we would still have our expected improvement per step drop of with $\\frac{1}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Perturb-Measure-Step Analysis\n",
    "\n",
    "Okay now the same for perturb-measure-step. In pertrub-measure-step we iterate through the dimensions generating a test point according to\n",
    "$$\n",
    "\\mathbf{x}' = \\mathbf{x} + s_{\\text{test}}\\mathbf{e}^i\n",
    "$$\n",
    "Where $\\mathbf{e}^i$ is an ortho-normal basis vector, for the $i^{th}$ dimension. By measuring the change caused by this perturbation we can get a good estimate of the $g_i$, the component of the gradient in the $i^{th}$ dimension. Based on this the estimate of $g_i$ we then steps $s \\cdot g_i$ in the $i^{th}$ dimension. Now because we don't know anything about the particular structure of $\\mathbf{g}$ and the function $f$, it hard to say much about what kind of improvement we will get out of any one step in any one dimension, but we do know that after iterating through all $n$ dimensions we will have taken a step roughly in the direction of the gradient, and that the length of this combination of $n$-steps is $\\sqrt{\\sum_{i=1}^{n} s^2 g_i^2} = s \\|\\mathbf{g}\\| $. Now the rate of improvement in the direction of the gradient is $\\|\\mathbf{g}\\|$, so the expected improvement over $n$ steps will be roughly $ s\\|\\mathbf{g}\\|^2$. Then on average, each step will contribute an improvement of\n",
    "\n",
    "$$\\frac{s\\|\\mathbf{g}\\|^2}{n}$$\n",
    "\n",
    "The rate of improvement for perturb-measure-step drops off with $\\frac{1}{n}$, even faster than that of propose-accept-reject. This analysis is a little loose, since $\\mathbf{g}$ will change slightly as steps are taken, but it could be tightened up by analyzing a more tractable \"stochastic\" variant of pertrub-measure-step, in which instead of iterating through the directions systematically, a random direction is chosen for perturbation. This random direction could be chosen from the $n$ orthonormal basis directions, in which case the analysis is much as above, or the direction could be choose uniformly over all possible directions, i.e. using $$\n",
    "\\mathbf{x}' = \\mathbf{x} + s_{\\text{test}}\\frac{\\xi}{\\|\\xi\\|}\n",
    "$$\n",
    "With $\\mathbf{\\xi} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. Then we can do our same coordinate shifting trick to get the expected rate of improvement in the direction of $\\xi$ as\n",
    "$$ \\frac{\\xi}{\\| \\xi \\|} \\cdot \\mathbf{g} = \\frac{\\| \\mathbf{g}\\|}{\\| \\mathbf{\\xi}\\|} (\\xi_1 \\cdot 1 + \\xi_2 \\cdot 0 + \\dots + \\xi_n \\cdot 0) = \\xi_1 \\frac{\\| \\mathbf{g}\\|}{\\| \\mathbf{\\xi}\\|}$$.\n",
    "\n",
    "The algorithm will then take a step of length $s \\xi_1 \\frac{\\| \\mathbf{g}\\|}{\\| \\mathbf{\\xi}\\|}$ in the direction of $\\xi$ and this will result in an improvement of $$ s \\| \\mathbf{g}\\|^2  \\frac{\\xi_1^2}{\\| \\mathbf{\\xi}\\|^2}$$.\n",
    "\n",
    "Now $\\xi_1^2$ \\| follows a $\\chi^2_1$ distribution and $\\|\\mathbf{\\xi}\\|^2$ follows a $\\chi^2_n$ distribution. Now the normalized ratio of $chi^2$ distribution follows an F-distribution, i.e.\n",
    "$$ n \\frac{\\chi^2_1}{\\chi^2_n} \\sim F(1,n)$$\n",
    "\n",
    "Now, $\\xi_1^2$ and $\\|\\mathbf{\\xi}\\|^2$ are not independent at all. Just the opposite $\\xi_1$ is a component of the sum that determines $\\|\\mathbf{\\xi}\\|^2$, but for large $n$ this contribution becomes negligable and can approximate the true distribution using a F-distribution, that is\n",
    "\n",
    "$$\\mathbf{E}\\left[ s \\| \\mathbf{g}\\|^2  \\frac{\\xi_1^2}{\\| \\mathbf{\\xi}\\|^2}\\right] \\approx \\frac{s \\| \\mathbf{g}\\|^2}{n-2}$$\n",
    "\n",
    "In either version of perturb-measure-step the expected improvement per step drops off like $\\frac{1}{n}$ as the number of dimensions increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Run the following cell to see how well our analysis lines up with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def simple_quadratic(x):\n",
    "  return np.sum(x**2)\n",
    "\n",
    "def scale_stable_propose(x, step_size=0.1, rng=np.random.default_rng()):\n",
    "  dim = x.shape[0]\n",
    "  step_scale = step_size / np.sqrt(dim)\n",
    "  proposed_x = x + rng.standard_normal(size=x.shape) * step_scale\n",
    "  return proposed_x\n",
    "\n",
    "# Propose-accept-reject loop\n",
    "def propose_accept_reject(N_Dim, step_size, max_proposals, starting_distance_from_optimal, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(0)\n",
    "  histories = []\n",
    "  for N in N_Dim:\n",
    "    x = rng.standard_normal(size=N)\n",
    "    x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "    z = simple_quadratic(x)\n",
    "    z_history = [z]\n",
    "    num_proposed = 1\n",
    "    while z > step_size**2 and len(z_history) < max_proposals:\n",
    "      proposed_x = scale_stable_propose(x, step_size=step_size, rng=par_rng)\n",
    "      z_proposed = simple_quadratic(proposed_x)\n",
    "      if z_proposed < z:\n",
    "        x = proposed_x\n",
    "        z = z_proposed\n",
    "      z_history.append(z)\n",
    "    histories.append(z_history)\n",
    "  return histories\n",
    "\n",
    "# Perturb-measure-step\n",
    "def perturb_measure_step(N_Dim, step_scale, max_steps, test_perturb, starting_distance_from_optimal, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(42)\n",
    "  histories = []\n",
    "  for N in N_Dim:\n",
    "    x = rng.standard_normal(size=N)\n",
    "    x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "    z = simple_quadratic(x)\n",
    "    z_history = [z]\n",
    "    num_steps = 0\n",
    "    while z_history[-1] > step_size:\n",
    "      for dim in range(N):\n",
    "        x[dim] += test_perturb #perturb\n",
    "        z_test = simple_quadratic(x) #test\n",
    "        x[dim] -= test_perturb # un-perturb\n",
    "        dim_grad_est = (z_test - z) / test_perturb #measure slope\n",
    "        x[dim] -= step_scale * dim_grad_est #step\n",
    "        num_steps += 1\n",
    "        z = simple_quadratic(x)\n",
    "        #num_function_evaluations += 2\n",
    "        x_history.append(x)\n",
    "        z_history.append(z)\n",
    "        if num_steps > max_steps:\n",
    "          break\n",
    "      if num_steps > max_steps:\n",
    "        break\n",
    "    histories.append(z_history)\n",
    "  return histories\n",
    "\n",
    "def good_perturb_measure_step(N_Dim, step_scale, max_steps, perturbation_scale, starting_distance_from_optimal, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(42)\n",
    "  histories = []\n",
    "  for N in N_Dim:\n",
    "    x = rng.standard_normal(size=N)\n",
    "    x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "    z = simple_quadratic(x)\n",
    "    z_history = [z]\n",
    "    while z > step_scale and len(z_history) < max_steps:\n",
    "      raw_test_perturb = rng.standard_normal(size=N)\n",
    "      unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb)\n",
    "      test_perturbation = unit_test_perturb * perturbation_scale\n",
    "      z_test = simple_quadratic(x + test_perturbation)\n",
    "      directional_grad_est = (z_test - z) / perturbation_scale\n",
    "      x -= step_scale * directional_grad_est * unit_test_perturb\n",
    "      z = simple_quadratic(x)\n",
    "      z_history.append(z)\n",
    "    histories.append(z_history)\n",
    "  return histories\n",
    "\n",
    "def scaling_perturb_measure_step(N_Dim, step_scale, max_steps, perturbation_scale, starting_distance_from_optimal, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(42)\n",
    "  histories = []\n",
    "  for N in N_Dim:\n",
    "    x = rng.standard_normal(size=N)\n",
    "    x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "    z = simple_quadratic(x)\n",
    "    z_history = [z]\n",
    "    dimensional_scale_factor = np.sqrt(np.pi * N / 2)\n",
    "    while z > step_scale and len(z_history) < max_steps:\n",
    "      raw_test_perturb = rng.standard_normal(size=N)\n",
    "      unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb)\n",
    "      test_perturbation = unit_test_perturb * perturbation_scale\n",
    "      z_test = simple_quadratic(x + test_perturbation)\n",
    "      directional_grad_est = (z_test - z) / perturbation_scale\n",
    "      x -= step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
    "      z = simple_quadratic(x)\n",
    "      z_history.append(z)\n",
    "    histories.append(z_history)\n",
    "  return histories\n",
    "\n",
    "def over_scaling_perturb_measure_step(N_Dim, step_scale, max_steps, perturbation_scale, starting_distance_from_optimal, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(42)\n",
    "  histories = []\n",
    "  for N in N_Dim:\n",
    "    x = rng.standard_normal(size=N)\n",
    "    x = x / np.linalg.norm(x) * starting_distance_from_optimal\n",
    "    z = simple_quadratic(x)\n",
    "    z_history = [z]\n",
    "    dimensional_scale_factor = N # np.sqrt(np.pi * N / 2)\n",
    "    while z > step_scale and len(z_history) < max_steps:\n",
    "      raw_test_perturb = rng.standard_normal(size=N)\n",
    "      unit_test_perturb = raw_test_perturb / np.linalg.norm(raw_test_perturb)\n",
    "      test_perturbation = unit_test_perturb * perturbation_scale\n",
    "      z_test = simple_quadratic(x + test_perturbation)\n",
    "      directional_grad_est = (z_test - z) / perturbation_scale\n",
    "      x -= step_scale * dimensional_scale_factor * directional_grad_est * unit_test_perturb\n",
    "      z = simple_quadratic(x)\n",
    "      z_history.append(z)\n",
    "    histories.append(z_history)\n",
    "  return histories\n",
    "\n",
    "# Parameters\n",
    "test_rng = np.random.default_rng(0)\n",
    "N_Dim = [10, 100, 10000]\n",
    "step_size = 0.001\n",
    "max_proposals = 20000\n",
    "max_steps = 20000\n",
    "starting_distance_from_optimal = 5\n",
    "test_perturb = 0.00001\n",
    "step_scale = 0.001\n",
    "\n",
    "# Collect data\n",
    "z_par_histories = propose_accept_reject(N_Dim, step_size, max_proposals, starting_distance_from_optimal)\n",
    "#z_pms_histories = perturb_measure_step(N_Dim, step_scale, max_steps, test_perturb, starting_distance_from_optimal)\n",
    "z_gpms_histories = good_perturb_measure_step(N_Dim, step_scale, max_steps, test_perturb, starting_distance_from_optimal)\n",
    "z_spms_histories = scaling_perturb_measure_step(N_Dim, step_scale, max_steps, test_perturb, starting_distance_from_optimal)\n",
    "#z_ospms_histories = over_scaling_perturb_measure_step(N_Dim, step_scale, max_steps, test_perturb, starting_distance_from_optimal)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "cmap = plt.cm.viridis  # This colormap goes from light (yellow) to dark (blue)\n",
    "colors = cmap(np.linspace(0, 1, len(N_Dim)))\n",
    "\n",
    "for ii, N in enumerate(N_Dim):\n",
    "  evals_per_step = N + 1\n",
    "  z_par_vals = z_par_histories[ii]\n",
    "  #z_pms_vals = z_pms_histories[ii]\n",
    "  z_gpms_vals = z_gpms_histories[ii]\n",
    "  z_spms_vals = z_spms_histories[ii]\n",
    "  #z_ospms_vals = z_ospms_histories[ii]\n",
    "  # always just 2 evaluations per step for the chaotic-good perturb-measure-step\n",
    "  ax.plot(z_par_vals, color=colors[ii], linestyle='-', label=f'Propose-Accept-Reject {N_Dim[ii]}-d')\n",
    "  #ax.plot(z_pms_vals, color=colors[ii], linestyle='-.', label=f'Systematic-Perturb-Measure-Step{N_Dim[ii]}-d')\n",
    "  ax.plot(z_gpms_vals, color=colors[ii], linestyle='-.', label=f'Perturb-Measure-Step{N_Dim[ii]}-d')\n",
    "  ax.plot(z_spms_vals, color=colors[ii], linestyle='--', label=f'Scaled-Perturb-Measure-Step{N_Dim[ii]}-d')\n",
    "#  ax.plot(z_ospms_vals, color=colors[ii], linestyle='-', label=f'Over-Scaling-Perturb-Measure-Step{N_Dim[ii]}-d')\n",
    "\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Comparison of Propose-Accept-Reject and Perturb-Measure-Step by Dimension')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In accordance with our analysis above, for lower dimensions perturb-measure-step does better because the squared gradient term compensates for the $\\frac{1}{n}$ drop off, but eventually as we go up to 10,000 dimensions in the figure above, propose-accept-reject becomes more efficient. At what scale one method become better than the other will of course depend on the details of the problem, but as might be expected, because perturb-measure-step uses estimates of the slope to inform its steps it is at an advantage when there are steep gradients to be informed by. The point though isn't really about which of perturb-measure-step or propose-accept-reject is better in higher dimensions, the point is they are both become ineffective as we move into higher-dimensional optimization problems.\n",
    "\n",
    "On the face of it, it might seem like propose-accept-reject scales better to higher dimensions than does perturb-measure-step, and indeed in this simplified analysis it does. However, in practice, step sizes and step scaling factors are critical to the rate of convergence. Higher values allow for rapid traversal of the parameter space, while smaller steps allow for greater percision in dialing the the exact loccation of the optima once the rough neighbourhood in parameter space has been reached. Because the perturb-measure-step algorithm takes steps in proportion to the gradient, and (for analytic functions) the gradient gets close to zero near optima, it is possible to have a higher step-size scaling in perturb-measure-step than average step-sizes in propose-accept-reject.\n",
    "\n",
    "In the version of perturb measure step above, we always take a step proportional to the rate of improvement in the sampled test direction. Now this rate of improvement is the product of two components, one is the magnitude of the gradient, $\\| \\mathbf{g} \\|$ at the point $\\mathbf{x}$, and the other is the alignment of the test direction with the gradient direction, this is given by $\\frac{\\xi_1}{\\| \\xi \\|}$. Now for large $n$ the average value of $\\frac{\\xi_1}{\\| \\xi \\|}$, is roughly $\\sqrt{\\frac{2}{n \\pi}}$, which means that we can safely scale up our step sizes by the inverse of this amount, to (partially) compensate for the increasing lack of alignment with gradient in higher dimensions. This leads to the scaled-perturb-measure-step in the plot above, which clearly outperforms the unscaled-perturb-measure-step and propose-accept-reject, with its expected improvement per step of\n",
    "\n",
    "$$ s\\|\\mathbf{g}\\|^2 \\sqrt{\\frac{\\pi}{2n}}$$\n",
    "\n",
    "So looks like we can't really have scaling any better than $\\frac{1}{\\sqrt{n}}$ with either propose-accept-reject or perturb-measure-step. If it takes $5$ iterations to solve a problem in one dimension it will take $50 = 5 \\cdot \\sqrt{100}$ iterations to solve the $n=100$ dimension equivalent of the problem, $500 = 5 \\cdot \\sqrt{10000}$ to solve the $n=10000$ dimension equivalent of the problem, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Propose-Accept-Reject Method\n",
    "\n",
    "The **propose-accept-reject** method is a stochastic optimization technique that iteratively proposes changes to a parameter vector and accepts these changes based on a simple comparison rule. Here's how this method operates in detail:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the parameter vector $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
    "   - Choose a step size $s$ for the perturbations.\n",
    "\n",
    "2. **Loop Through the Following Steps**:\n",
    "   1. **Function Evaluation**:\n",
    "      - Evaluate $f(\\mathbf{x})$ to establish a baseline for comparison.\n",
    "\n",
    "   2. **Generate a Candidate Point**:\n",
    "      - Sample a candidate point $\\mathbf{x}' = \\mathbf{x} + \\frac{s}{\\sqrt{n}} \\cdot \\mathcal{N}(0, \\mathbf{I})$, where $\\mathcal{N}(0, \\mathbf{I})$ represents a standard, $n$-dimensional multivariate normal distribution. This perturbation adjusts the scale of the step size inversely with the square root of the dimensionality, keeping average step size roughly constant regardless of the number of dimensions.\n",
    "\n",
    "   3. **Candidate Point Evaluation**:\n",
    "      - Evaluate the function at the candidate point $f(\\mathbf{x}')$.\n",
    "\n",
    "   4. **Accept or Reject the Candidate**:\n",
    "      - Compare the function values: if $f(\\mathbf{x}') < f(\\mathbf{x})$, then update $\\mathbf{x} \\leftarrow \\mathbf{x}'$. This step implements the 'accept if better' rule, where only improvements lead to an update. (If we were minimizing we would use $f(\\mathbf{x}') > f(\\mathbf{x})$ as our acceptance criteria instead.)\n",
    "\n",
    "   5. **Convergence Check**:\n",
    "      - Repeat the loop until a convergence criterion is met. Criteria can include a maximum number of function evaluations, a maximum number of consecutive failed attempts at improvement, achievement of a target function value, or a combination of these factors.\n",
    "\n",
    "\n",
    "### Sequential Perturb-Measure-Step Method\n",
    "\n",
    "Sequential perturb-measure-step grew out of the idea of using more information from function evaluations. In one dimensional problems there was only on direction to make a perturbation. In two dimensions we alternated our test perturbations between the two dimensions sequentially.\n",
    "\n",
    "A. **Initialization**:\n",
    "   - Initialize the parameter vector $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
    "   - Choose a test perturbation magnitude $s_{\\text{test}}$ and a parameter step scaling size $s_x$.\n",
    "\n",
    "B. **Loop Through the Following Steps**:\n",
    "   1. **Function Evaluation**:\n",
    "      - Evaluate $f(\\mathbf{x})$ to establish a baseline.\n",
    "   2. **Perturb Each Dimension Separately**:\n",
    "      - For each dimension $i \\in \\{1, 2, \\dots, n\\}$:\n",
    "        1. Generate a test point $\\mathbf{x}' = \\mathbf{x} + \\mathbf{e}^i \\cdot s_{\\text{test}}$. Here, $\\mathbf{e}^i$ is the standard orthonormal unit basis vector for the $i^{\\text{th}}$ dimension.\n",
    "        2. Evaluate the function at $\\mathbf{x}'$.\n",
    "        3. Estimate the rate of change in the $i^{\\text{th}}$ dimension: $$ \\hat{g}_i = \\frac{f(\\mathbf{x}') - f(\\mathbf{x})}{s_{\\text{test}}} $$\n",
    "        4. Update $\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{e}^i \\cdot \\hat{g}_i \\cdot s_x$.\n",
    "        5. $s_{\\text{test}}$\n",
    "   3. **Convergence Check**:\n",
    "      - Repeat the loop until a convergence criterion is met, such as a maximum number of function evaluations, a significant drop in improvement per iteration, achievement of a target function value, or a combination of these factors.\n",
    "\n",
    "### Batch Perturb-Measure-Step Method\n",
    "\n",
    "An alternative approach to the sequential perturb-measure-step method is to estimate changes in all dimensions before applying a single comprehensive update. This method can be more stable as it considers the cumulative gradient vector from all dimensions before making adjustments.\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Similar to the sequential method.\n",
    "\n",
    "2. **Loop Through the Following Steps**:\n",
    "   1. **Function Evaluation**:\n",
    "      - Evaluate $f(\\mathbf{x})$.\n",
    "   2. **Perturb and Measure in Batch**:\n",
    "      - For each dimension $i \\in \\{1, 2, \\dots, n\\}$:\n",
    "        1. Generate a test point $\\mathbf{x}_{\\text{test}} = \\mathbf{x} + \\mathbf{e}^i \\cdot s$        \n",
    "        2. Evaluate $f(\\mathbf{x}_{\\text{test}})$.\n",
    "        3. Estimate the rate of change in the $i^{\\text{th}}$ dimension: $$ \\hat{g}_i = \\frac{f(\\mathbf{x}_{\\text{test}}) - f(\\mathbf{x})}{s_{\\text{test}}} $$\n",
    "   3. **Update Using the Complete Gradient Vector**:\n",
    "      - Construct the gradient vector $\\hat{\\mathbf{g}} = (\\hat{g}_1, \\hat{g}_2, \\dots, \\hat{g}_n)$.\n",
    "      - Update $\\mathbf{x} \\leftarrow \\mathbf{x} + \\hat{\\mathbf{g}} \\cdot s_x$.\n",
    "   4. **Convergence Check**:\n",
    "      - Similar to the sequential method.\n",
    "\n",
    "The sequential and batch flavours of perturb-measure-step are roughly equivalent, though the batch version my help avoid some pathological cases that emerge for the sequential variant.\n",
    "\n",
    "### Reckless-Perturb-Measure-Step Method\n",
    "\n",
    "The perturb-measure-step variants described above, here after the **careful-perturb-measure-step** methods, are overly meticulous, evaluating each dimension separately and systematically. There is a better way which embraces chaotic power inherent in the **propose-accept-reject** method, while still extracting significant information from each function evaluation by measuring the rate of change. A we call the hybrid approach that achieves this **reckless-perturb-measure-step**.\n",
    "\n",
    "The core idea is to randomly select a direction for testing the function's response, then based on the estimated rate of improvement in that direction within the parameter space, take a proportional step. Here's the process in detail:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the parameter vector $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
    "   - Choose a test perturbation magnitude $s_{\\text{test}}$ and a parameter step scaling size $s_x$.\n",
    "\n",
    "2. **Loop Through the Following Steps**:\n",
    "   1. **Function Evaluation**:\n",
    "      - Evaluate $f(\\mathbf{x})$.\n",
    "\n",
    "   2. **Generate a Random Direction**:\n",
    "      - Generate a raw random vector $\\mathbf{v}$ from a standard, $n$-dimensional multivariate normal distribution $\\mathcal{N}(0, \\mathbf{I})$. This point sits in a 'Gaussian Cloud' around the orgin.\n",
    "      - Normalize $\\mathbf{v}$ to obtain a random unit vector $\\mathbf{u} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|}$. This point sits on the surface of a hyper-sphere of radius 1.\n",
    "\n",
    "   3. **Test Point Evaluation**:\n",
    "      - Generate a test point $\\mathbf{x}' = \\mathbf{x} + s_{\\text{test}} \\cdot \\mathbf{u}$.\n",
    "      - Evaluate the function at the test point $f(\\mathbf{x}')$.\n",
    "\n",
    "   4. **Measure Rate of Change**:\n",
    "      - Calculate the rate of change of $f$ in the direction of $\\mathbf{u}$ using the formula:\n",
    "        $$\\Delta f_{\\mathbf{u}} = \\frac{f(\\mathbf{x}') - f(\\mathbf{x})}{s_{\\text{test}}}$$\n",
    "        Because $\\mathbf{u}$ is a unit vector, $s_{\\text{test}}$ gives the 'run' in this 'rise over run' slope calculation.\n",
    "\n",
    "   5. **Update Parameter Vector**:\n",
    "      - Update $\\mathbf{x}$ by moving in the direction of $\\mathbf{u}$, scaled by the estimated rate of improvement and the step scaling size:\n",
    "        $$\\mathbf{x} \\leftarrow \\mathbf{x} + s_x \\cdot \\Delta f_{\\mathbf{u}} \\cdot \\mathbf{u}$$\n",
    "\n",
    "   6. **Convergence Check**:\n",
    "      - Repeat the loop until a convergence criterion is met, such as a maximum number of function evaluations, a significant drop in improvement per iteration, achievement of a target function value, some combination of these factors, or even something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def simple_quadratic(x):\n",
    "  return np.sum(x * x, axis=0)\n",
    "\n",
    "def gradient_simple_quadratic(x):\n",
    "  return 2 * x\n",
    "\n",
    "test_rng = np.random.default_rng(42)\n",
    "\n",
    "def propose_accept_reject_average_improvement(x, step_size, num_samples=400, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(42)\n",
    "  N = x.shape[0]\n",
    "  stable_step_scale = step_size / np.sqrt(N)\n",
    "  proposals = x[..., np.newaxis] + stable_step_scale * rng.standard_normal(size=(N, num_samples,))\n",
    "  f_x = simple_quadratic(x)\n",
    "  f_proposals = simple_quadratic(proposals)\n",
    "  f_diff = f_proposals - f_x\n",
    "  f_diff[f_diff < 0] = 0\n",
    "  empirical_improvement = np.mean(f_diff)\n",
    "  g_x = gradient_simple_quadratic(x)\n",
    "  theoretical_improvement = np.linalg.norm(g_x) * stable_step_scale * np.sqrt(2/np.pi) * 0.5\n",
    "  best_possible_improvement = np.linalg.norm(g_x) * step_size\n",
    "  return empirical_improvement, theoretical_improvement, best_possible_improvement\n",
    "\n",
    "propose_accept_reject_average_improvement(10 * test_rng.standard_normal(size=500), 0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def systematic_perturb_measure_step_average_improvement(x, test_scale, step_scale, num_samples=400, rng=None):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(42)\n",
    "  N = x.shape[0]\n",
    "  perturbs = test_scale * np.eye(N)\n",
    "  test_points = x[..., np.newaxis] + perturbs\n",
    "  f_x = simple_quadratic(x)\n",
    "  f_test_points = simple_quadratic(test_points)\n",
    "  f_diffs = f_test_points - f_x\n",
    "  f_steps = f_diffs / test_scale * step_scale\n",
    "  new_xs = x[..., np.newaxis] + np.diag(f_steps) # N x N\n",
    "  f_new_xs = simple_quadratic(new_xs)\n",
    "  empirical_improvement = np.mean(f_new_xs - f_x)\n",
    "  g_x = gradient_simple_quadratic(x)\n",
    "  theoretical_improvement = (np.linalg.norm(g_x))**2 * step_scale * (1/N)\n",
    "  best_possible_improvement = (np.linalg.norm(g_x))**2 * step_scale\n",
    "  return empirical_improvement, theoretical_improvement, best_possible_improvement\n",
    "\n",
    "systematic_perturb_measure_step_average_improvement(10 * test_rng.standard_normal(size=500), 0.0000001, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def measure_step_improvement(initial_point, test_step_size, step_scale, num_samples=400, rng=None, plot=False):\n",
    "  \"\"\"\n",
    "  Measure the average empirical improvement of a perturbation-based optimization method.\n",
    "  Parameters:\n",
    "  - initial_point (np.ndarray): Starting point in parameter space.\n",
    "  - test_step_size (float): Magnitude of the perturbations applied.\n",
    "  - step_scale (float): Scaling factor for step size based on the optimization feedback.\n",
    "  - num_samples (int, optional): Number of perturbation samples.\n",
    "  - rng (np.random.Generator, optional): Random number generator for reproducibility.\n",
    "  - plot (bool, optional): If True, plot the histogram of empirical improvements.\n",
    "  Returns:\n",
    "  - tuple: Average empirical improvement, theoretical improvement, best possible improvement.\n",
    "  \"\"\"\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng()\n",
    "  N = initial_point.shape[0]\n",
    "  raw_perturbs = rng.standard_normal(size=(N, num_samples))\n",
    "  unit_perturbs = raw_perturbs / np.linalg.norm(raw_perturbs, axis=0, keepdims=True)\n",
    "  test_points = initial_point[:, np.newaxis] + test_step_size * unit_perturbs\n",
    "  value_at_initial = simple_quadratic(initial_point)\n",
    "  values_at_test_points = simple_quadratic(test_points)\n",
    "  diffs = values_at_test_points - value_at_initial\n",
    "  steps = diffs / test_step_size * step_scale\n",
    "  new_points = initial_point[:, np.newaxis] + steps[np.newaxis, :] * unit_perturbs\n",
    "  values_at_new_points = simple_quadratic(new_points)\n",
    "  improvements = values_at_new_points - value_at_initial\n",
    "  average_improvement = np.mean(improvements)\n",
    "  gradient_at_initial = gradient_simple_quadratic(initial_point)\n",
    "  theoretical_improvement = (np.linalg.norm(gradient_at_initial))**2 * step_scale * (1/N)\n",
    "  best_possible_improvement = (np.linalg.norm(gradient_at_initial))**2 * step_scale\n",
    "  if plot:\n",
    "      plot_improvements(improvements, average_improvement, theoretical_improvement)\n",
    "  return average_improvement, theoretical_improvement, best_possible_improvement\n",
    "\n",
    "def plot_improvements(improvements, average_improvement, theoretical_improvement):\n",
    "  fig, ax = plt.subplots(figsize=(12, 8))\n",
    "  remove_ip_clutter(fig)\n",
    "  n, bins, patches = ax.hist(improvements, bins=50, density=True, alpha=0.5, label='Empirical Improvements')\n",
    "  ax.axvline(theoretical_improvement, label='Theoretical Mean Improvement', color='r', linestyle='--')\n",
    "  ax.axvline(average_improvement, label='Actual Mean Improvement', color='g', linestyle='--')\n",
    "  ax.set_xlabel('Improvement')\n",
    "  ax.set_ylabel('Frequency')\n",
    "  ax.legend()\n",
    "  plt.show()\n",
    "\n",
    "measure_step_improvement(10 * test_rng.standard_normal(size=500), 0.0000001, 0.0001, num_samples=1000, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "def reckless_perturb_measure_step_average_improvement(x, test_step_size, step_scale, num_samples=400, rng=None, plot=False):\n",
    "  if rng is None:\n",
    "    rng = np.random.default_rng(42)\n",
    "  N = x.shape[0]\n",
    "  raw_perturbs = rng.standard_normal(size=(N, num_samples))\n",
    "  unit_perturbs = raw_perturbs / np.linalg.norm(raw_perturbs, axis=0, keepdims=True)\n",
    "  test_points = x[:, np.newaxis] + test_step_size * unit_perturbs\n",
    "  f_x = simple_quadratic(x)\n",
    "  f_test_points = simple_quadratic(test_points)\n",
    "  f_diffs = f_test_points - f_x\n",
    "  f_steps = f_diffs / test_step_size * step_scale\n",
    "  new_xs = x[:, np.newaxis] + f_steps[np.newaxis,:] * unit_perturbs\n",
    "  print(new_xs.shape)\n",
    "  f_new_xs = simple_quadratic(new_xs)\n",
    "  empirical_improvements = f_new_xs - f_x\n",
    "  average_empirical_improvement = np.mean(empirical_improvements)\n",
    "  g_x = gradient_simple_quadratic(x)\n",
    "  theoretical_improvement = (np.linalg.norm(g_x))**2 * step_scale * (1/N)\n",
    "  best_possible_improvement = (np.linalg.norm(g_x))**2 * step_scale\n",
    "  if plot:\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    remove_ip_clutter(fig)\n",
    "    ax.hist(empirical_improvements, bins=50, density=True, alpha=0.5, label='Empirical Improvements')\n",
    "    ax.axvline(theoretical_improvement, label='Theoretical Mean Improvement', color='r', linestyle='--')\n",
    "    ax.axvline(average_empirical_improvement, label='Actual Mean Improvement', color='g', linestyle='--')\n",
    "    ax.set_xlabel('Improvement')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "  return average_empirical_improvement, theoretical_improvement, best_possible_improvement\n",
    "\n",
    "reckless_perturb_measure_step_average_improvement(10 * test_rng.standard_normal(size=500), 0.00000001, 0.000001, num_samples=10000, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Another way of coneceptualizing this is as the projection of the gradient of $f$ onto the random direction  $$ \\hat{\\mathbf{g} \\cdot \\mathbf{u}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_Dim = [1, 2, 4, 6]\n",
    "step_size = 0.1\n",
    "max_proposals = 1500\n",
    "max_steps = 200\n",
    "starting_distance_from_optimal = 5\n",
    "test_perturb = 0.00001\n",
    "step_scale = 0.1\n",
    "\n",
    "# Collect data\n",
    "z_par_histories = propose_accept_reject(N_Dim, step_size, max_proposals, starting_distance_from_optimal)\n",
    "z_pms_histories = perturb_measure_step(N_Dim, step_scale, max_steps, test_perturb, starting_distance_from_optimal)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "colors = ['b', 'g', 'r', 'c']  # colors for different dimensions\n",
    "\n",
    "for ii, N in enumerate(N_Dim):\n",
    "  evals_per_step = N + 1\n",
    "  z_par_vals = z_par_histories[ii]\n",
    "  z_pms_vals = z_pms_histories[ii]\n",
    "  n_evaluations = np.arange(0, evals_per_step * (len(z_pms_vals)), evals_per_step)\n",
    "  ax.plot(np.arange(len(z_par_vals)), z_par_vals, f'{colors[ii]}-', label=f'Propose-Accept-Reject {N_Dim[ii]}-d')\n",
    "  ax.plot(n_evaluations, z_pms_vals, f'{colors[ii]}--', label=f'Perturb-Measure-Step{N_Dim[ii]}-d')\n",
    "\n",
    "ax.set_xlabel('# Function Evaluations')\n",
    "ax.set_ylabel('f(x)')\n",
    "ax.set_title('Comparison of Optimization Methods by Dimension')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def plot_samples(N_dimensions, step_size):\n",
    "  stable_samples = [scale_stable_propose(np.zeros(N_dimensions, dtype=float), step_size) for _ in range(10000)]\n",
    "  fixed_scale_samples = [fixed_scale_propose(np.zeros(N_dimensions, dtype=float), step_size) for _ in range(10000)]\n",
    "\n",
    "  norm_stable_samples = [np.sqrt(np.sum(s_**2)) for s_ in stable_samples]\n",
    "  norm_fixed_scale_samples = [np.sqrt(np.sum(s_**2)) for s_ in fixed_scale_samples]\n",
    "\n",
    "  plt.figure(figsize=(6, 4))\n",
    "  plt.hist(norm_stable_samples, bins=100, alpha=0.5, label='Scale-Stable')\n",
    "  plt.hist(norm_fixed_scale_samples, bins=100, alpha=0.5, label='Fixed-Scale')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "# Create interactive widgets\n",
    "interactive_plot = widgets.interactive(plot_samples, N_dimensions=(1, 50), step_size=(0.01, 1.0, 0.01))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '250px' # Adjust the plot height\n",
    "interactive_plot"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "convergence_rate_analysis",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
