{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/draft_sequences/GP_andMDP_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/draft_sequences/GP_andMDP_scratch.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is some rough work on Gaussian Processes and MDP notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run the following cell to setup and install the various dependencies and helper functions for this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "\n",
    "!pip install ipympl vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "!pip install pyDOE GPy gpyopt\n",
    "\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import GPy\n",
    "import GPyOpt\n",
    "from copy import copy\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
    "             'gw_widgets.py', 'gw_NN_RL.py']\n",
    "#filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C2_S1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "# new definition gridworld board to support multiple agents\n",
    "###########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "# refactor Monte Carlo for boards that support multiple critters\n",
    "################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarlo():\n",
    "  \"\"\"\n",
    "  Implementation of Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, nnet, default_depth=5, random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def pis_vs_from_board(self, board, critter):\n",
    "    #helper function, to put board in canonical form that nn was trained on\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    co_pieces = board['pieces'].copy()\n",
    "    this_critter_locs = np.where(co_pieces == critter)\n",
    "    all_critter_locs = np.where(co_pieces >= 1)\n",
    "    # other critters are invisible to this player\n",
    "    co_pieces[all_critter_locs] = 0\n",
    "    # nnet trained to see self as 1\n",
    "    co_pieces[this_critter_locs] = 1\n",
    "    scalar_rounds_left = board['rounds_left'][0]\n",
    "    co_rounds_left = scalar_rounds_left // self.game.num_critters\n",
    "    if critter-1 < scalar_rounds_left % self.game.num_critters:\n",
    "       # add an extra if we haven't had this players turn yet in the round cycle\n",
    "       co_rounds_left = co_rounds_left + 1\n",
    "    co_rounds_left = np.array([co_rounds_left]*batch_size)\n",
    "    pis, vs = self.nnet.predict(co_pieces,\n",
    "                                board['scores'][:,critter-1],\n",
    "                                co_rounds_left)\n",
    "    return pis, vs\n",
    "\n",
    "\n",
    "  def simulate(self, board, actions, action_indexes, critter=1, depth=None):\n",
    "    \"\"\"\n",
    "    Helper function to simulate one Monte Carlo rollout\n",
    "\n",
    "    Args:\n",
    "      board: triple (batch_size x x_size x y_size np.array of board position,\n",
    "                     scalar of current score,\n",
    "                     scalar of rounds left\n",
    "      actions: batch size list/array of integer indexes for moves on each board\n",
    "      these are assumed to be legal, no check for validity of moves\n",
    "    Returns:\n",
    "      temp_v:\n",
    "        Terminal State\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    next_board = self.game.get_next_state(board, critter,\n",
    "                                          actions, action_indexes)\n",
    "    # in this version of the mc player, the existence of other players is\n",
    "    # ignored, in another version of mc other players moves might be simulated\n",
    "    next_board['active_player'] = critter-1\n",
    "\n",
    "    if depth is None:\n",
    "      depth = self.default_depth\n",
    "    # potentially expand the game tree here,\n",
    "    # but just do straight rollouts after this\n",
    "    # doesn't expand to deal with all random food generation possibilities\n",
    "    # just expands based on the actions given\n",
    "    expand_bs, _, _ = next_board['pieces'].shape\n",
    "\n",
    "    for i in range(depth):  # maxDepth\n",
    "      if next_board['rounds_left'][0] <= 0:\n",
    "        # check that game isn't over\n",
    "        # assumes all boards have the same rounds left\n",
    "        # no rounds left return scores as true values\n",
    "        terminal_vs = next_board['scores'][:,critter-1].copy()\n",
    "        return terminal_vs\n",
    "      else:\n",
    "        #pis, vs = self.nnet.predict(next_board['pieces'], next_board['scores'], next_board['rounds_left'])\n",
    "        pis, vs = self.pis_vs_from_board(next_board, critter)\n",
    "        valids = self.game.get_valid_actions(next_board, critter)\n",
    "        masked_pis = pis * valids\n",
    "        sum_pis = np.sum(masked_pis, axis=1)\n",
    "        probs = np.array(\n",
    "            [masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "             else valid / valid.sum()\n",
    "             for valid, masked_pi in zip(valids, masked_pis)])\n",
    "        samp = self.rng.uniform(size = expand_bs).reshape((expand_bs,1))\n",
    "        sampled_actions = np.argmax(probs.cumsum(axis=1) > samp, axis=1)\n",
    "      next_board = self.game.get_next_state(next_board, critter,\n",
    "                                            sampled_actions)\n",
    "      # in this version of the mc player, existence of other players is ignored\n",
    "      # in another better version other players moves might be simulated, either\n",
    "      # as copies of self, or as distinct environmental dynamics\n",
    "      next_board['active_player'] = critter-1\n",
    "\n",
    "\n",
    "    pis, vs = self.pis_vs_from_board(next_board, critter)\n",
    "    #pis, vs = self.nnet.predict(next_board['pieces'], next_board['scores'],\n",
    "    #                            next_board['rounds_left'])\n",
    "    #print(vs.shape)\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title plotting functions\n",
    "#################################################\n",
    "# More plotting functions\n",
    "#################################################\n",
    "\n",
    "\n",
    "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
    "                    name=None):\n",
    "  \"\"\"\n",
    "  Plot vector field indicating critter direction probabilities.\n",
    "\n",
    "  Args:\n",
    "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
    "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
    "      and values as lists of direction probabilities corresponding to the\n",
    "      directions ['right', 'down', 'left', 'up'].\n",
    "    critter (int): Identifier for which critter directions are associated with.\n",
    "    deterministic (bool, optional): If True, the probabilities array is\n",
    "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
    "      and are larger. Defaults to False.\n",
    "  \"\"\"\n",
    "\n",
    "  #looks like direction ignores inverted axis\n",
    "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
    "                       'left': (-1, 0), 'up': (0, 1)}\n",
    "  # but offsets need to be aware of inverted\n",
    "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
    "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
    "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
    "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
    "  # same logic for colors\n",
    "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
    "  # Get the offset and color for this critter\n",
    "  critter_offset = critter_offsets[critter]\n",
    "  critter_color = critter_colors[critter]\n",
    "\n",
    "  # Add legend only if critter is not 0\n",
    "  custom_leg_handles = []\n",
    "  if critter != 0:\n",
    "    if name is None:\n",
    "      name = f'Critter {critter}'\n",
    "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
    "    # Add the legend for this critter\n",
    "    custom_leg_handles.append(legend_patch)\n",
    "\n",
    "  C, R, U, V, A = [], [], [], [], []\n",
    "\n",
    "  for loc in loc_prob_dict.keys():\n",
    "    row, col = loc\n",
    "    probs = loc_prob_dict[loc]\n",
    "    for dir_key, prob in probs.items():\n",
    "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
    "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
    "      U.append(direction_vectors[dir_key][0])\n",
    "      V.append(direction_vectors[dir_key][1])\n",
    "\n",
    "      if deterministic:\n",
    "        A.append(1 if prob == max(probs.values()) else 0)\n",
    "      else:\n",
    "        A.append(prob)\n",
    "\n",
    "  linewidth = 1.5 if deterministic else 0.5\n",
    "  scale = 15 if deterministic else 30\n",
    "\n",
    "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
    "            scale=scale, linewidth=linewidth)\n",
    "  return fig, ax, custom_leg_handles\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for futher use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  # Create a new figure and axes with given figsize\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  # Set width and height padding, remove horizontal and vertical spacing\n",
    "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
    "  # Show right and top borders (spines) of the plot\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  # Set major ticks (where grid lines will be) on x and y axes\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Set labels for major ticks with font size of 8\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "  # Set minor ticks (no grid lines here) to be between major ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "  # Move x-axis ticks to the top of the plot\n",
    "  ax.xaxis.tick_top()\n",
    "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "  # Remove minor ticks (not the grid lines)\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "  # Set limits of x and y axes\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  # Invert y axis direction\n",
    "  ax.invert_yaxis()\n",
    "  # If title is provided, set it as the figure title\n",
    "  if title is not None:\n",
    "    fig.suptitle(title)\n",
    "  # Hide header and footer, disable toolbar and resizing of the figure\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  # Redraw the figure with these settings\n",
    "  fig.canvas.draw()\n",
    "  # Return figure and axes handles for further customization\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=150, marker='o', color='red', label='Food')\n",
    "  rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "  #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]]) -> List[Dict[str, object]]:\n",
    "  \"\"\"\n",
    "  Plots multiple types of \"critters\" on a grid implied by the given\n",
    "  fig, ax arguments.\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects.\n",
    "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
    "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
    "    critter.\n",
    "\n",
    "  Returns:\n",
    "    Updated critter_specs with handles.\n",
    "  \"\"\"\n",
    "  for spec in critter_specs:\n",
    "    # Ensure required keys are present\n",
    "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
    "      if key not in spec:\n",
    "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
    "    handle_ = spec.get('handle')\n",
    "    if handle_ is None:\n",
    "      handle_ = ax.scatter([], [], s=250, marker=spec['marker'],\n",
    "                           color=spec['color'], label=spec['name'])\n",
    "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
    "    spec.update({'handle': handle_})\n",
    "  return critter_specs\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc,\n",
    "                 critter=None, critter_name='Critter'):\n",
    "  \"\"\"\n",
    "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollenction object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h',\n",
    "                         color='blue', label=critter_name)\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov, fov=None):\n",
    "  \"\"\"\n",
    "  Plots a mask on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter: ndarry(int) (row,col) of the critter\n",
    "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib Image object of mask, either new if no handle\n",
    "    was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
    "  mask_array = np.ones((n_rows, n_cols, 4))\n",
    "  mask_array[:, :, :3] = 0.5  # light grey color\n",
    "  if has_fov == True:\n",
    "    mask_array[:, :, 3] = 0.5  # 50% opacity\n",
    "    # Create arrays representing the row and column indices\n",
    "    rows = np.arange(n_rows)[:, np.newaxis]\n",
    "    cols = np.arange(n_cols)[np.newaxis, :]\n",
    "    # Iterate over each critter location\n",
    "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
    "    # Set the region within the specified radius around the critter to transparent\n",
    "    mask_array[dist <= radius, 3] = 0\n",
    "  else:\n",
    "    mask_array[:, :, 3] = 0\n",
    "\n",
    "  if fov is None:\n",
    "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
    "  else:\n",
    "    fov.set_data(mask_array)\n",
    "\n",
    "  return fov\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title GridworldBoard class\n",
    "#######################################################################\n",
    "# extend GridworldGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game board that\n",
    "  define the logic of the game, and allows for multiple critters on the same\n",
    "  board\n",
    "\n",
    "  board state is represented by primarily by pieces, score, and rounds left\n",
    "  pieces is a batch x n_rows x n_cols numpy array positive integers are critter\n",
    "  locations 0's are empty space and -1's are food.\n",
    "\n",
    "  For pieces first dim is batch, second dim row , third is col,\n",
    "  so pieces[0][1][7] is the square in row 2, in column 8 of the first board in\n",
    "  the batch of boards.\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game.\n",
    "\n",
    "  active_player keeps track of which players turn it is\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_critters=2, num_food=10,\n",
    "               lifetime=30, rng = None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_critters = num_critters\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"Set up starting board using game parameters\"\"\"\n",
    "    #set rounds_left and score\n",
    "    self.rounds_left = (np.ones(self.batch_size) *\n",
    "                        self.lifetime * self.num_critters)\n",
    "    # each players move counts down the clock so making this a multiple of the\n",
    "    # number of critters ensures every player gets an equal number of turns\n",
    "    self.scores = np.zeros((self.batch_size, self.num_critters))\n",
    "    # create an empty board array.\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols))\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+num_critter because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(\n",
    "        self.n_rows, self.n_cols, self.num_food+self.num_critters)\n",
    "      # critter random start locations\n",
    "      for c_ in np.arange(self.num_critters):\n",
    "        self.pieces[(ii, rc_idx[0][c_], rc_idx[1][c_])] = c_ + 1\n",
    "      # food random start locations\n",
    "      self.pieces[(ii, rc_idx[0][self.num_critters:],\n",
    "                   rc_idx[1][self.num_critters:])] = -1\n",
    "    self.active_player = 0\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy(),\n",
    "             'active_player': copy(self.active_player)}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\" board is dictionary giving game state a triple of np arrays\n",
    "      pieces:        numpy array (batch_size x n_rows x n_cols),\n",
    "      scores:        numpy array (batch_size x num_critters)\n",
    "      rounds_left:   numpy array (batch_size)\n",
    "      active_player: int\n",
    "    \"\"\"\n",
    "    self.pieces = board['pieces'].copy()\n",
    "    self.scores = board['scores'].copy()\n",
    "    self.rounds_left = board['rounds_left'].copy()\n",
    "    self.active_player = copy(board['active_player'])\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state, which is a triple of np arrays\n",
    "    pieces,       - batch_size x n_rows x n_cols\n",
    "    scores,       - batch_size\n",
    "    rounds_left   - batch_size\n",
    "    \"\"\"\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'rounds_left': self.rounds_left.copy(),\n",
    "             'active_player': copy(self.active_player)}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, critter):\n",
    "    \"\"\"\n",
    "    Updates the state of the board given the moves made.\n",
    "\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord.\n",
    "\n",
    "    Notes:\n",
    "      Assumes that there is exactly one valid move for each board in the\n",
    "      batch of boards for the critter type given. i.e. it does't check for\n",
    "      bounce/reflection on edges or with other critters, or for multiple move\n",
    "      made on the same board. It only checks for eating food and adds new food\n",
    "      when appropriate. Invalid moves could lead to illegal teleporting\n",
    "      behavior, critter duplication, or index out of range errors.\n",
    "\n",
    "      Currently just prints a message if critter making the move is not the\n",
    "      active player, could enforce this more strictly if needed.\n",
    "    \"\"\"\n",
    "    if critter-1 != self.active_player:\n",
    "      # note critter is [1 to num_critter] inclusive so that it can be used\n",
    "      # directly in where statements on pieces but self.active_player is\n",
    "      # [0 to numcritter-1] inclusive so that it can be used directly in\n",
    "      # indexing player lists\n",
    "      raise ValueError(\"Warning! The critter moving is not the expected active player\")\n",
    "    #critters leave their spots\n",
    "    self.pieces[self.pieces==critter] = 0\n",
    "    #which critters have food in their new spots\n",
    "    eats_food = self.pieces[moves] == -1\n",
    "    # some critters eat and their scores go up\n",
    "    # note critter is +int so need to -1 for indexing\n",
    "    self.scores[:,critter-1] = self.scores[:,critter-1] + eats_food\n",
    "\n",
    "    num_empty_after_eat = (self.n_rows*self.n_cols - self.num_food -\n",
    "                           self.num_critters + 1) # +1 for the food just eaten\n",
    "    # which boards in the batch had eating happen\n",
    "    g_eating = np.where(eats_food)[0]\n",
    "    # put critters in new positions\n",
    "    self.pieces[moves] = critter\n",
    "    if np.any(eats_food):\n",
    "      # add random food to replace what is eaten\n",
    "      possible_new_locs = np.where(np.logical_and(\n",
    "          self.pieces == 0, #the spot is empty\n",
    "          eats_food.reshape(self.batch_size, 1, 1))) #food eaten on that board\n",
    "      food_sample_ = self.rng.choice(num_empty_after_eat,\n",
    "                                     size=np.sum(eats_food))\n",
    "      food_sample = food_sample_ + np.arange(len(g_eating))*num_empty_after_eat\n",
    "      assert np.all(self.pieces[(possible_new_locs[0][food_sample],\n",
    "                                 possible_new_locs[1][food_sample],\n",
    "                                 possible_new_locs[2][food_sample])] == 0)\n",
    "      #put new food on the board\n",
    "      self.pieces[(possible_new_locs[0][food_sample],\n",
    "                   possible_new_locs[1][food_sample],\n",
    "                   possible_new_locs[2][food_sample])] = -1\n",
    "    self.rounds_left = self.rounds_left - 1\n",
    "    if not np.all(self.pieces.sum(axis=(1,2)) ==\n",
    "                  ((self.num_food * -1) + np.sum(np.arange(self.num_critters)+1))):\n",
    "      print(self.pieces.sum(axis=(1,2)))\n",
    "      print(((self.num_food * -1) + np.sum(np.arange(self.num_critters)+1)))\n",
    "    assert np.all(self.pieces.sum(axis=(1,2)) ==\n",
    "                  ((self.num_food * -1) + np.sum(np.arange(self.num_critters)+1)))\n",
    "    # next player's turn\n",
    "    self.active_player = (self.active_player + 1) % (self.num_critters)\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, critter):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter, taking into acount\n",
    "    bouncing/reflection at edges,\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offstet on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "    # can only move one cell down, up, right, and left from current location\n",
    "    critter_locs = np.array(np.where(self.pieces == critter))\n",
    "    legal_offsets = np.stack([\n",
    "      critter_locs + np.array([np.array([0,  1, 0])]*self.batch_size).T,\n",
    "      critter_locs + np.array([np.array([0, -1, 0])]*self.batch_size).T,\n",
    "      critter_locs + np.array([np.array([0, 0,  1])]*self.batch_size).T,\n",
    "      critter_locs + np.array([np.array([0, 0, -1])]*self.batch_size).T])\n",
    "    legal_offsets = np.vstack(np.transpose(legal_offsets, (0, 2, 1)))\n",
    "    legal_offsets = set([tuple(m_) for m_ in legal_offsets])\n",
    "    # must land on the board and not on another critter\n",
    "    legal_destinations = np.where(self.pieces <= 0)\n",
    "    legal_destinations = set([(g, r, c) for\n",
    "                              g, r, c in zip(*legal_destinations)])\n",
    "    # legal moves satisfy both these conditions\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_perceptions(self, radius, critter):\n",
    "    \"\"\"\n",
    "    Generates a vector representation of the critter perceptions, oriented\n",
    "    around the critter.\n",
    "\n",
    "    Args:\n",
    "      radius: int, how many grid squared the critter can see around it\n",
    "        using L1  (Manhattan/cityblock) distance\n",
    "\n",
    "    Returns:\n",
    "      A batch_size x 2*radius*(radius+1) + 1, giving the values\n",
    "      of the percept reading left to right, top to bottom over the board,\n",
    "      for each board in the batch\n",
    "    \"\"\"\n",
    "    # define the L1 ball mask\n",
    "    diameter = radius*2+1\n",
    "    mask = np.zeros((diameter, diameter), dtype=bool)\n",
    "    mask_coords = np.array([(i-radius, j-radius)\n",
    "      for i in range(diameter)\n",
    "        for j in range(diameter)])\n",
    "    mask_distances = cdist(mask_coords, [[0, 0]],\n",
    "                           'cityblock').reshape(mask.shape)\n",
    "    mask[mask_distances <= radius] = True\n",
    "    mask[radius,radius] = False  # exclude the center\n",
    "\n",
    "    # pad the array\n",
    "    padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "     (radius, radius)), constant_values=-2)\n",
    "\n",
    "    # get locations of critters\n",
    "    critter_locs = np.argwhere(padded_arr == critter)\n",
    "\n",
    "    percepts = []\n",
    "    for critter_loc in critter_locs:\n",
    "      b, r, c = critter_loc\n",
    "      surrounding = padded_arr[b, r-radius:r+radius+1, c-radius:c+radius+1]\n",
    "      percept = surrounding[mask]\n",
    "      percepts.append(percept)\n",
    "    return(np.array(percepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title GridworldGame class\n",
    "#######################################################################\n",
    "# extend GridworldGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1, n_rows=7, n_cols=7,\n",
    "               num_critters=2, num_food=10,\n",
    "               lifetime=30, rng=None):\n",
    "    \"\"\"\n",
    "    Initializes an instance of the class with the specified parameters.\n",
    "\n",
    "    Args:\n",
    "      batch_size (int, optional): Number of instances in a batch. Default is 1.\n",
    "      n_rows (int, optional): Number of rows in the grid. Default is 7.\n",
    "      n_cols (int, optional): Number of columns in the grid. Default is 7.\n",
    "      num_critters (int, optional): Number of different agents running around\n",
    "        on each board in the batch. Default is 2.\n",
    "      num_food (int, optional): Number of food items. Default is 10.\n",
    "      lifetime (int, optional): Time before critter's life ends, in terms of\n",
    "        time steps. Default is 30.\n",
    "      rng (numpy random number generator, optional): Random number generator\n",
    "        for reproducibility. If None, uses default RNG with a preset seed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for positive integer inputs\n",
    "    assert all(isinstance(i, int) and i >= 0\n",
    "               for i in [batch_size, n_rows, n_cols, num_critters, num_food,\n",
    "                         lifetime]), \"All inputs must be non-negative integers.\"\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_critters = num_critters\n",
    "    # Check for num_food exceeding maximum possible value\n",
    "    max_food = n_rows * n_cols - num_critters\n",
    "    if num_food > max_food:\n",
    "      print(f'num_food is too large, setting it to maximum possible value: {max_food}')\n",
    "      num_food = max_food\n",
    "    self.num_food = num_food\n",
    "    self.lifetime = lifetime\n",
    "    # Set up random number generator\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(batch_size=self.batch_size, n_rows=self.n_rows,\n",
    "                       n_cols=self.n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    return b.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_size(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,r,c coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 0-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to r,c indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g=0):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return (board['pieces'][g].tobytes() + board['scores'][g].tobytes() +\n",
    "            board['rounds_left'][g].tobytes())\n",
    "\n",
    "\n",
    "  def get_square_symbol(self, piece):\n",
    "    \"\"\" Translate integer piece value to symbol for display\"\"\"\n",
    "    if piece == -1:\n",
    "      return \"X\"\n",
    "    elif piece == 0:\n",
    "      return \"-\"\n",
    "    elif piece >= 1:\n",
    "      return \"0\"\n",
    "    else:\n",
    "      return \"???????????????????????????\"\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g=0):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.get_square_symbol(square)\n",
    "                        for row in board['pieces'][g]\n",
    "                          for square in row])\n",
    "    board_s = board_s + '_' + str(board['scores'][g])\n",
    "    board_s = board_s + '_' + str(board['rounds_left'][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board['rounds_left'].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, \"|\", end=\"\")    # Print the row\n",
    "      for r_ in range(self.n_rows):\n",
    "        piece = board['pieces'][g,c_,r_]    # Get the piece to print\n",
    "        #print(piece)\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board['rounds_left'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g, critter_index):\n",
    "    return np.squeeze(np.array(np.where(board['pieces'][g]==critter_index)))\n",
    "\n",
    "\n",
    "  def plot_moves(self, board, player0, g=0, player1=None,\n",
    "                 fig=None, ax=None, p0_name='Player 0', p1_name='Player 1',\n",
    "                 figsize=(6,5), critter_name='Critter', title=None,\n",
    "                 deterministic=False):\n",
    "    \"\"\"\n",
    "    Uses plotting functions to make picture of the current board state, and what\n",
    "    a critter would do at each non-food location in the current board state\n",
    "    \"\"\"\n",
    "    def make_prob_dict(critter_locs, play):\n",
    "      offset_dict = {(0, 1): 'right',\n",
    "                     (0,-1): 'left',\n",
    "                     ( 1, 0): 'down',\n",
    "                     (-1, 0): 'up'}\n",
    "      index_probs = play[2].copy()\n",
    "      loc_prob_dict = {}\n",
    "      # for each non food locations\n",
    "      for g, loc_ in enumerate(critter_locs):\n",
    "        # this is the location as an r, c tuple\n",
    "        rc_tup = tuple((loc_[1], loc_[2]))\n",
    "        # the relevant probabilities\n",
    "        raw_probs = index_probs[g]\n",
    "        probs = raw_probs[raw_probs > 0]\n",
    "        indexes = np.argwhere(raw_probs > 0)\n",
    "        # turn the probability indexes into r, c coords\n",
    "        rows = np.floor_divide(indexes, gwg.n_cols)\n",
    "        cols = np.remainder(indexes, gwg.n_cols)\n",
    "        moves = np.squeeze(np.array([z for z in zip(rows, cols)]), axis=2)\n",
    "        #compute the offsets and turn them to strings\n",
    "        offsets = moves - loc_[1:]\n",
    "        str_offsets = np.array(list(map(offset_dict.get, map(tuple, offsets))))\n",
    "        # update the loc_prob_dict for plotting\n",
    "        prob_dict = dict(zip(str_offsets, probs))\n",
    "        loc_prob_dict.update({rc_tup: prob_dict})\n",
    "      return loc_prob_dict\n",
    "\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] == -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    food = plot_food(fig, ax, rc_food_plotting)\n",
    "\n",
    "    expanded_board = self.critter_everywhere_state_expansion(\n",
    "      board, player0.critter_index, to_expand=g)\n",
    "    critter_locs = np.argwhere(expanded_board['pieces']==player0.critter_index)\n",
    "    #play the expanded state\n",
    "    p0_play = player0.play(expanded_board)\n",
    "    #get the prob dict\n",
    "    p0_loc_prob_dict = make_prob_dict(critter_locs, p0_play)\n",
    "    # same for player1 if there is one\n",
    "    if player1 is not None:\n",
    "      p1_play = player1.play(expanded_board)\n",
    "      p1_loc_prob_dict = make_prob_dict(critter_locs, p1_play)\n",
    "\n",
    "    existing_handels, _ = ax.get_legend_handles_labels()\n",
    "    if player1 is None:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=0, deterministic=deterministic)\n",
    "      leg_handles = existing_handels\n",
    "    else:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=1, deterministic=deterministic, name=p0_name)\n",
    "      fig, ax, leg_handles_1 = plot_directions(fig, ax, p1_loc_prob_dict,\n",
    "        critter=2, deterministic=deterministic, name=p1_name)\n",
    "      leg_handles = existing_handels + leg_handles_0 + leg_handles_1\n",
    "\n",
    "    fig.legend(handles=leg_handles, loc=\"outside right upper\")\n",
    "    fig.canvas.draw()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war feild_of_view\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter'):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(self.num_critters):\n",
    "        critter_name = name if self.num_critters == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == self.num_critters, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(np.where(board['pieces'][g] == spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] == -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # will need to think about how to do this for multiple\n",
    "      # critters, currently just use rc of first critter in the spec list\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[0]['rc_loc'], n_rows, n_cols,\n",
    "                       radius, has_fov)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[0]['rc_loc'], n_rows, n_cols,\n",
    "                       radius, has_fov, fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board, critter):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=batch_size, n_rows=n_rows,\n",
    "                       n_cols=n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves(critter)\n",
    "    valids = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for g, r, c in legal_moves:\n",
    "      valids[g, r * n_cols + c] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, critter=1, g=0):\n",
    "    \"\"\"Displays possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    A=np.reshape(self.get_valid_actions(board, critter)[g],\n",
    "                 (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, \"|\", end=\"\")    # Print the row\n",
    "      for row in range(self.n_rows):\n",
    "        piece = A[col][row]    # Get the piece to print\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, critter):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=batch_size, n_rows=n_rows,\n",
    "                       n_cols=n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    return(b.get_perceptions(radius, critter))\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, critter, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if board['rounds_left'][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      moves = self.actions_to_moves(actions)\n",
    "      b = GridworldBoard(batch_size=len(actions), n_rows=n_rows,\n",
    "                         n_cols=n_cols, num_critters=self.num_critters,\n",
    "                         num_food=self.num_food, lifetime=self.lifetime,\n",
    "                         rng=self.rng)\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        b.set_state(board)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        new_pieces = np.array([board['pieces'][ai].copy() for ai in a_indx])\n",
    "        new_scores = np.array([board['scores'][ai].copy() for ai in a_indx])\n",
    "        new_rounds_left = np.array([board['rounds_left'][ai].copy() for ai in a_indx])\n",
    "        new_active_player = copy(board['active_player'])\n",
    "        new_state = {'pieces': new_pieces,\n",
    "                     'scores': new_scores,\n",
    "                     'rounds_left': new_rounds_left,\n",
    "                     'active_player': new_active_player}\n",
    "        b.set_state(new_state)\n",
    "      b.execute_moves(moves, critter)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    b = GridworldBoard(batch_size=batch_size, n_rows=n_rows,\n",
    "                       n_cols=n_cols, num_critters=self.num_critters,\n",
    "                       num_food=self.num_food, lifetime=self.lifetime,\n",
    "                       rng=self.rng)\n",
    "    b.set_state(board)\n",
    "    moves = self.critter_direction_to_move(board, offsets, critter)\n",
    "    b.execute_moves(moves, critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right'\n",
    "      critter: integer index for the critter we want moves for\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = np.where(board['pieces'] == critter)\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def direction_probs_to_flat_probs(self, board, direction_probs, critter):\n",
    "    \"\"\"\n",
    "    Converts direction probabilities in reference to the critter's location into\n",
    "    probability arrays on the flattened board.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      direction_probs: batch length list of dictionaries with keys\n",
    "        ['up', 'down', 'left', 'right'] and corresponding probabilities.\n",
    "\n",
    "    Returns:\n",
    "      probs_arrays: list of arrays, where each array is of length n_rows*n_cols\n",
    "                    and represents the flattened probability distribution for\n",
    "                    board in the batch.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {\n",
    "        'up': np.array((0, -1, 0)),\n",
    "        'down': np.array((0, 1, 0)),\n",
    "        'left': np.array((0, 0, -1)),\n",
    "        'right': np.array((0, 0, 1))}\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    probs_arrays = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for batch_index in range(batch_size):\n",
    "      prob_array = np.zeros(n_rows * n_cols)\n",
    "      for direction, prob in direction_probs[batch_index].items():\n",
    "          offset = offset_dict[direction]\n",
    "          new_loc = np.array(critter_locs)[:, batch_index] + offset\n",
    "          # Check bounces at boundaries\n",
    "          new_loc[1] = np.where(new_loc[1] >= n_rows, n_rows-2, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] >= n_cols, n_cols-2, new_loc[2])\n",
    "          new_loc[1] = np.where(new_loc[1] < 0, 1, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] < 0, 1, new_loc[2])\n",
    "          # Convert 2D location to flattened index\n",
    "          flattened_index = new_loc[1] * n_cols + new_loc[2]\n",
    "          prob_array[flattened_index] += prob\n",
    "      probs_arrays[batch_index, :] = prob_array\n",
    "    return list(probs_arrays)\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, critter, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right'\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    critter_locs = np.column_stack(np.where(board['pieces'] == critter))\n",
    "    valid_directions = []\n",
    "    for g, batch_valid in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valid==1)[0]\n",
    "      critter_loc = critter_locs[critter_locs[:, 0] == g, 1:]\n",
    "      # critter_loc shape is (1, 2)\n",
    "      critter_loc = np.squeeze(critter_loc)\n",
    "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
    "      offsets = moves - critter_loc\n",
    "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "    Returns a batch size np.array of -1 if not ended, and scores for each game\n",
    "    in the batch if it is ended, note only returns scores if all games in the\n",
    "    batch have ended\n",
    "    \"\"\"\n",
    "    rounds_left = board['rounds_left']\n",
    "    scores = board['scores']\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores\n",
    "\n",
    "\n",
    "  def critter_everywhere_state_expansion(self, board_state,\n",
    "                                         critter=1, to_expand=0):\n",
    "    \"\"\"\n",
    "    Expand a given board state by placing a critter at each non-food location.\n",
    "\n",
    "    The function takes a game state and returns an expanded version of it. For\n",
    "    each board in the state, it creates a new version of the board for every\n",
    "    non-food location, placing a critter at that location. The scores and\n",
    "    remaining rounds are copied for each new board. The result is a new game state\n",
    "    with a larger number of boards, each representing a possible configuration\n",
    "    with a critter at a different location.\n",
    "\n",
    "    Args:\n",
    "      board_state (dict): A dictionary containing the current game state.\n",
    "      It should have the following keys:\n",
    "        - 'pieces': a 3D numpy array (batch x n_col x n_row) representing the game\n",
    "          board. -1 -> food, 0 -> empty cell, and 1 -> critter.\n",
    "        - 'scores': 1D numpyp array of the score for each board in the batch.\n",
    "        - 'rounds_left': a 1D numpy array of the rounds left for\n",
    "          each board in the batch.\n",
    "      critter: integer index to place on the expanded board state\n",
    "      to_expand (list (int)): list of batch indices to have state expanded\n",
    "\n",
    "    Returns:\n",
    "      dict: A dictionary containing the expanded game state with the same keys\n",
    "        as the input. The number of boards will be larger than the input state.\n",
    "    \"\"\"\n",
    "    pieces = board_state['pieces'].copy()\n",
    "    scores = board_state['scores'].copy()\n",
    "    rounds_left = board_state['rounds_left'].copy()\n",
    "    active_player = copy(board_state['active_player'])\n",
    "    # Determine non-food locations\n",
    "    non_food_locs = np.argwhere(pieces[to_expand] != -1)\n",
    "    #scrub all existing critter locations,\n",
    "    # maybe later only scrub specific critter type\n",
    "    pieces[pieces >= 1] = 0\n",
    "    # lists to store expanded states\n",
    "    expanded_pieces = []\n",
    "    expanded_scores = []\n",
    "    expanded_rounds_left = []\n",
    "    # Iterate over each non-food location\n",
    "    for i in range(non_food_locs.shape[0]):\n",
    "      # Create a copy of the board\n",
    "      expanded_board = np.copy(pieces[to_expand])\n",
    "      # Place the critter at the non-food location\n",
    "      # later consider only placing at non-food,\n",
    "      # non-other critter locs\n",
    "      expanded_board[tuple(non_food_locs[i])] = critter\n",
    "      # Add the expanded board to the list along score and rounds_left\n",
    "      expanded_pieces.append(expanded_board)\n",
    "      expanded_scores.append(scores[to_expand])\n",
    "      expanded_rounds_left.append(rounds_left[to_expand])\n",
    "    # Convert to arrays and create expanded board state\n",
    "    expanded_state = {'pieces': np.stack(expanded_pieces),\n",
    "                      'scores': np.array(expanded_scores),\n",
    "                      'rounds_left': np.array(expanded_rounds_left),\n",
    "                      'active_player': active_player}\n",
    "    return expanded_state\n",
    "\n",
    "\n",
    "  def play_game(self, players=[], collect_fov_data=False, fov_radius=2,\n",
    "                visualize = False):\n",
    "    \"\"\"This method takes a list of players the same length as num_critters,\n",
    "        and then plays a batch of games with them and returns the final board\n",
    "        state\"\"\"\n",
    "    if len(players) != self.num_critters:\n",
    "      raise ValueError(\"number of players different than expected\")\n",
    "\n",
    "    board = self.get_init_board()\n",
    "    if visualize == True:\n",
    "      self.display(board, 0)\n",
    "\n",
    "    if collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "      b = GridworldBoard(batch_size=batch_size,\n",
    "                         n_rows=n_rows, n_cols=n_cols,\n",
    "                         num_critters=self.num_critters,\n",
    "                         num_food=self.gwg.num_food,\n",
    "                         lifetime=self.gwg.lifetime,\n",
    "                         rng=self.gwg.rng)\n",
    "\n",
    "\n",
    "    for ii in range(self.lifetime):\n",
    "      for jj, player in enumerate(players):\n",
    "        active_player_index = board['active_player']\n",
    "        old_scores = board['scores']\n",
    "        if collect_fov_data is True:\n",
    "          b.set_state(board)\n",
    "          percepts = b.get_perceptions(fov_radius)\n",
    "\n",
    "        a_player, _, _ = players[active_player_index].play(board)\n",
    "        board = self.get_next_state(board, active_player_index+1, a_player)\n",
    "        if visualize == True:\n",
    "          self.display(board, 0)\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title player zoo\n",
    "###########################################################################\n",
    "# make a separate player zoo\n",
    "###########################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomValidPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    action_size = self.game.get_action_size()\n",
    "\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomDirectionPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, critter_index=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function assigns a uniform probability to going up down left or right\n",
    "    independent of whether it is at an edge or cornor or not. Then because of\n",
    "    bouncing off edges it will have a higher probability of moving away from\n",
    "    edges as opposed to along them than the random valid move player.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    action_probs = {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n",
    "\n",
    "    critter_oriented_moves = self.game.rng.choice(list(action_probs.keys()),\n",
    "                                                  size=(batch_size))\n",
    "    direction_probs = [action_probs] * batch_size\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs,\n",
    "                                                    self.critter_index)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MonteCarloBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Player based on Monte Carlo Algorithm\n",
    "\n",
    "  Note: Has dependencies in the gw_NN_RL.py util, namely a policy/value\n",
    "  network and the Monte Carlo class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet,\n",
    "               critter_index=1,\n",
    "               default_depth=1,\n",
    "               default_rollouts=1,\n",
    "               default_K=4,\n",
    "               default_temp=1.0,\n",
    "               random_seed=None):\n",
    "    \"\"\"\n",
    "    Initialize Monte Carlo Parameters\n",
    "\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      nnet: gridworldNet instance\n",
    "        Instance of the gridworldNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.nnet = nnet\n",
    "    self.default_depth = default_depth\n",
    "    self.default_rollouts = default_rollouts\n",
    "    self.mc = MonteCarlo(self.game, self.nnet, self.default_depth)\n",
    "    self.default_K = default_K\n",
    "    self.default_temp = default_temp\n",
    "    self.rng = np.random.default_rng(seed=random_seed)\n",
    "\n",
    "\n",
    "  def play(self, board,\n",
    "           num_rollouts=None,\n",
    "           rollout_depth=None,\n",
    "           K=None,\n",
    "           softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Simulates a batch Monte Carlo based plays on the given board state.\n",
    "\n",
    "    Computes the probability of each valid move being played using a softmax\n",
    "    activation on the Monte Carlo based value (Q) of each action then selects a\n",
    "    move randomly for each game in the batch based on those probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if num_rollouts is None:\n",
    "      num_rollouts = self.default_rollouts\n",
    "    if rollout_depth is None:\n",
    "      rollout_depth = self.default_depth\n",
    "    if K is None:\n",
    "      K = self.default_K\n",
    "    if softmax_temp is None:\n",
    "      softmax_temp = self.default_temp\n",
    "\n",
    "    # figure out top k actions according to normalize action probability\n",
    "    # given by our policy network prediction\n",
    "    #co_pieces = board['pieces'].copy()\n",
    "    #this_critter_locs = np.where(co_pieces == self.critter_index+1)\n",
    "    #all_critter_locs = np.where(co_pieces >= 1)\n",
    "    # other critters are invisible to this player\n",
    "    #co_pieces[all_critter_locs] = 0\n",
    "    # nnet trained to see self as 1\n",
    "    #co_pieces[this_critter_locs] = 1\n",
    "    #scalar_rounds_left = board['rounds_left'][0]\n",
    "    #co_rounds_left = scalar_rounds_left // self.game.num_critters\n",
    "    #if self.critter_index-1 < scalar_rounds_left % self.game.num_critters:\n",
    "       # add an extra if we haven't had this players turn yet in the round cycle\n",
    "    #   co_rounds_left = co_rounds_left + 1\n",
    "    #co_rounds_left = np.array([co_rounds_left]*batch_size)\n",
    "    #pis, vs = self.nnet.predict(co_pieces,\n",
    "    #                            board['scores'][:,self.critter_index-1],\n",
    "    #                            co_rounds_left)\n",
    "    pis, vs = self.mc.pis_vs_from_board(board, self.critter_index)\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    masked_pis = pis * valids  # Masking invalid moves\n",
    "    sum_pis = np.sum(masked_pis, axis=1)\n",
    "    num_valid_actions = np.sum(valids, axis=1)\n",
    "    effective_topk = np.array(np.minimum(num_valid_actions, K), dtype= int)\n",
    "    probs = np.array([masked_pi / masked_pi.sum() if masked_pi.sum() > 0\n",
    "                      else valid / valid.sum()\n",
    "                      for valid, masked_pi in zip(valids, masked_pis)])\n",
    "    partioned = np.argpartition(probs,-effective_topk)\n",
    "    topk_actions = [partioned[g,-(ii+1)]\n",
    "                      for g in range(batch_size)\n",
    "                        for ii in range(effective_topk[g])]\n",
    "    topk_actions_index = [ii\n",
    "                            for ii, etk in enumerate(effective_topk)\n",
    "                              for _ in range(etk)]\n",
    "    values = np.zeros(len(topk_actions))\n",
    "    # Do some rollouts\n",
    "    for _ in range(num_rollouts):\n",
    "      values = values + self.mc.simulate(board, topk_actions,\n",
    "                                         topk_actions_index,\n",
    "                                         critter=self.critter_index,\n",
    "                                         depth=rollout_depth)\n",
    "    values = values / num_rollouts\n",
    "\n",
    "    value_expand = np.zeros((batch_size, n_rows*n_cols))\n",
    "    value_expand[(topk_actions_index, topk_actions)] = values\n",
    "    value_expand_shift = value_expand - np.max(value_expand, axis=1, keepdims=True)\n",
    "    value_expand_scale = value_expand_shift/softmax_temp\n",
    "    v_probs = np.exp(value_expand_scale) / np.sum(\n",
    "        np.exp(value_expand_scale), axis=1, keepdims=True)\n",
    "    v_probs = v_probs * valids\n",
    "    v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "    samp = self.rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "    sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "    a_1Hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    return sampled_actions, a_1Hots, v_probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleRulePlayer():\n",
    "  \"\"\"\n",
    "  A Player based on the following simple policy:\n",
    "  If there is any food immediately nearby move towards it,\n",
    "  otherwise it move randomly.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, fov_radius=2, critter_index=1):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.fov_radius = fov_radius\n",
    "\n",
    "\n",
    "  def simple_action_from_percept(self, percept):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A 1D array (len 12 if fov_radius = 2)representing the perception\n",
    "        of the organism. Indices correspond to spaces around the organism. The\n",
    "        values in the array can be -2 (out-of-bounds), 0 (empty space), or\n",
    "        -1 (food).\n",
    "\n",
    "    Returns:\n",
    "      action: a str, one of 'up', 'down', 'left', 'right'. If food in one or\n",
    "        more of the spaces immediately beside the organism, the function will\n",
    "        return a random choice among these directions. If there is no food\n",
    "        nearby, the function will return a random direction.\n",
    "    \"\"\"\n",
    "    # a human interpretable overview of the percept structure\n",
    "    percept_struct = [\n",
    "      'far up', 'left up', 'near up', 'right up',\n",
    "      'far left', 'near left', 'near right', 'far right',\n",
    "      'left down', 'near down', 'right down', 'far down']\n",
    "    # Defines directions corresponding to different perception indices\n",
    "    direction_struct = [\n",
    "      'None', 'None', 'up', 'None',\n",
    "      'None', 'left', 'right', 'None',\n",
    "      'None', 'down', 'None', 'None']\n",
    "    # these are what count as nearby in the percept\n",
    "    nearby_directions = ['near up', 'near left', 'near right', 'near down']\n",
    "    # Get the corresponding indices in the percept array\n",
    "    nearby_indices = [percept_struct.index(dir_) for dir_ in nearby_directions]\n",
    "    # Identify the directions where food is located\n",
    "    food_indices = [index for index in nearby_indices if percept[index] == -1]\n",
    "    food_directions = [direction_struct[index] for index in food_indices]\n",
    "\n",
    "    action_probs = {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n",
    "    if len(food_directions) > 0:  # If there is any food nearby\n",
    "      # If there is any food nearby randomly choose a direction with food\n",
    "      action = self.game.rng.choice(food_directions)  # Move towards a random one\n",
    "      for direction in food_directions:\n",
    "        action_probs[direction] = 1.0 /len(food_directions)\n",
    "    else:\n",
    "      # If there is no food nearby, move randomly\n",
    "      action = self.game.rng.choice(['up', 'down', 'left', 'right'])\n",
    "      for direction in ['up', 'down', 'left', 'right']:\n",
    "        action_probs[direction] = 0.25\n",
    "\n",
    "    return action, action_probs\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius,\n",
    "                                            self.critter_index)\n",
    "\n",
    "    critter_oriented_moves = []\n",
    "    direction_probs = []\n",
    "    for g in range(batch_size):\n",
    "      action, action_probs = self.simple_action_from_percept(perceptions[g])\n",
    "      critter_oriented_moves.append(action)\n",
    "      direction_probs.append(action_probs)\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                direction_probs,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PerceptParamPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a parameterized policy defined by the given weights\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, weights=None, fov_radius=2, critter_index=1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      weights: 4 x 12 numpy array (assumes fov_radius = 2), that gives the\n",
    "        connection strengths between the 'perception' neurons and the direction\n",
    "        'neurons'\n",
    "      fov_radius: int how far around itself the critter perceives, weights is\n",
    "        expecting fov_radius = 2\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    if weights is None:\n",
    "      self.W = np.array(\n",
    "      [[1., 1., 4., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 4., 1., 1.],\n",
    "       [0., 1., 0., 0., 1., 4., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 4., 1., 0., 0., 1., 0.]])\n",
    "    else:\n",
    "      self.W = weights\n",
    "    self.fov_radius = fov_radius\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "\n",
    "  def param_action_from_percept(self, percept, valid_directions, W,\n",
    "                                softmax_temp=None):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A 1D len 12 array representing the perception of the organism.\n",
    "        Indices correspond to spaces around the organism. The values in the\n",
    "        array can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "      W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "        between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "    Returns:\n",
    "      direction: a str, one of 'up', 'down', 'left', 'right'. If food in one or\n",
    "        more of the spaces immediately beside the organism, the function will\n",
    "        return a random choice among these directions. If there is no food\n",
    "        nearby, the function will return a random direction.\n",
    "      direction_probs: dictionary with probabilities of taking each action.\n",
    "    \"\"\"\n",
    "    if len(valid_directions) == 0:\n",
    "      # if there is no where legit to move, stay put\n",
    "      return 'still', {direction: 0 for direction in output_struct}\n",
    "\n",
    "    if softmax_temp is None:\n",
    "      # very low temp, basically deterministic for this range of values\n",
    "      softmax_temp = self.default_softmax_temp\n",
    "    # a human interpretable overview of the percept structure\n",
    "    percept_struct = [\n",
    "      'far up', 'left up', 'near up', 'right up',\n",
    "      'far left', 'near left', 'near right', 'far right',\n",
    "      'left down', 'near down', 'right down', 'far down']\n",
    "    # a human interpretable overview of the out structure\n",
    "    output_struct = ['up', 'down', 'left', 'right']\n",
    "    # boolean representation of percept, no edges, just 1's where food is,\n",
    "    # zero otherwise, also means other organisms are invisible\n",
    "    x = np.asarray(percept == -1, int)\n",
    "    output_activations = W @ x\n",
    "\n",
    "    # softmax shift by max, scale by temp\n",
    "    shift_scale_ex = np.exp((output_activations -\n",
    "                             np.max(output_activations))/softmax_temp)\n",
    "    # set invalid direction activations to zero\n",
    "    invalid_directions = [direction for direction in output_struct\n",
    "                           if direction not in valid_directions]\n",
    "    invalid_indices = [output_struct.index(direction)\n",
    "                        for direction in valid_directions]\n",
    "    sm = shift_scale_ex / shift_scale_ex.sum() #normalized\n",
    "    # set invalid direction probabilities to zero\n",
    "    invalid_directions = [direction for direction in output_struct\n",
    "                           if direction not in valid_directions]\n",
    "    invalid_indices = [output_struct.index(direction)\n",
    "                        for direction in invalid_directions]\n",
    "    sm[invalid_indices] = 0\n",
    "    probs_sm = sm / sm.sum(axis=0) #re-normalized again for fp issues\n",
    "    direction = self.game.rng.choice(output_struct, p=probs_sm)\n",
    "    direction_probs = {direction: prob\n",
    "                        for direction, prob in zip(output_struct, probs_sm)}\n",
    "    return direction, direction_probs\n",
    "\n",
    "\n",
    "  def play(self, board, temp=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius,\n",
    "                                            self.critter_index)\n",
    "    critter_oriented_moves = []\n",
    "    direction_probs = []\n",
    "\n",
    "    # Get valid actions for each game in the batch\n",
    "    valid_directions = self.game.get_valid_directions(board, self.critter_index)\n",
    "    for g in range(batch_size):\n",
    "      direction, batch_direction_probs = self.param_action_from_percept(\n",
    "        perceptions[g], valid_directions[g], self.W, softmax_temp=temp)\n",
    "      critter_oriented_moves.append(direction)\n",
    "      direction_probs.append(batch_direction_probs)\n",
    "    moves = self.game.critter_direction_to_move(board, critter_oriented_moves,\n",
    "                                                self.critter_index)\n",
    "    probs = self.game.direction_probs_to_flat_probs(board, direction_probs)\n",
    "    sampled_actions = self.game.moves_to_actions(moves)\n",
    "    a_1hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "    a_1hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "\n",
    "    return sampled_actions, a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BatchOptPerceptParamPlayer():\n",
    "  \"\"\"\n",
    "  A Player playing a parameterized policy defined by the given weights\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, weights=None, fov_radius=2, critter_index=1,\n",
    "               get_probs=False, deterministic=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: Gridworld Game instance\n",
    "        Instance of the gridworldGame class above;\n",
    "      weights: 4 x 12 numpy array (assumes fov_radius = 2), that gives the\n",
    "        connection strengths between the 'perception' neurons and the direction\n",
    "        'neurons'\n",
    "      fov_radius: int how far around itself the critter percieves, weights is\n",
    "        expecting fov_radius = 2\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    # all critters need these things\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "    self.get_probs = get_probs\n",
    "    # these things are specfic to this kind of critter\n",
    "    self.deterministic = deterministic\n",
    "    if weights is None:\n",
    "      self.W = np.array(\n",
    "      [[1., 1., 4., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 4., 1., 1.],\n",
    "       [0., 1., 0., 0., 1., 4., 0., 0., 1., 0., 0., 0.],\n",
    "       [0., 0., 0., 1., 0., 0., 4., 1., 0., 0., 1., 0.]])\n",
    "    else:\n",
    "      self.W = weights\n",
    "    self.fov_radius = fov_radius\n",
    "    self.default_softmax_temp = 0.05\n",
    "\n",
    "\n",
    "  def direction_value_from_percept(self, percepts, W):\n",
    "    \"\"\"\n",
    "    Determine an action based on perception.\n",
    "\n",
    "    Args:\n",
    "      percept: A batch by 1D len 12 array representing the perceptions of the\n",
    "      organism. Indices correspond to spaces around the organism. The values in\n",
    "      the array can be -2 (out-of-bounds), 0 (empty space), or -1 (food).\n",
    "      W: a 4 x 12 weight matrix parameter representing the connection strengths\n",
    "        between the 12 perceptions inputs and the 4 possible output actions.\n",
    "\n",
    "    Returns:\n",
    "      direction_probs: array of probabilities of taking each action.\n",
    "    \"\"\"\n",
    "    # a human interpretable overview of the percept structure\n",
    "    #percept_struct = [\n",
    "    #  'far up', 'left up', 'near up', 'right up',\n",
    "    #  'far left', 'near left', 'near right', 'far right',\n",
    "    #  'left down', 'near down', 'right down', 'far down']\n",
    "    # a human interpretable overview of the out structure\n",
    "    #output_struct = ['up', 'down', 'left', 'right']\n",
    "    # boolean representation of percept, no edges, just 1's where food is,\n",
    "    # zero otherwise, also means other organisms are invisible\n",
    "    # x is batch x 12\n",
    "    x = np.asarray(percepts == -1, int)\n",
    "    # W is 4 x 12\n",
    "    # this does the broadcasting we want\n",
    "    output_activations = (W @ x.T).T\n",
    "    # output activations is batch by 4\n",
    "    return output_activations\n",
    "\n",
    "\n",
    "  def play(self, board, temp=None, W=None):\n",
    "    \"\"\"\n",
    "    Simulate Play on a Board\n",
    "\n",
    "    Args:\n",
    "      board: dict {'pieces':\n",
    "      (batch x num_rows x num_cols) np.ndarray of board position,\n",
    "                  'scores': batch len array of current scores,\n",
    "                  'rounds_left': batch len array of rounds left\n",
    "\n",
    "    Returns:\n",
    "      sampled_actions: a batch, row, col index of the move taken\n",
    "      by each player on each board\n",
    "      a_1hots: a batch nrow*ncol array of 1hot indices of those same moves\n",
    "      v_probs: sampling probabilities for those 1hots (If the policy\n",
    "      is deterministic a_1hots is returned here as well... or if getting the\n",
    "      probs is an un-needed fuss to compute)\n",
    "    \"\"\"\n",
    "    if temp is None:\n",
    "      temp = self.default_softmax_temp\n",
    "    if W is None:\n",
    "      W = self.W\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    perceptions = self.game.get_perceptions(board, self.fov_radius,\n",
    "                                            self.critter_index)\n",
    "    # note the fragile order based dependency on how legal offsets is written,\n",
    "    # and how output activations are interpreted\n",
    "    direction_v = self.direction_value_from_percept(perceptions, W)\n",
    "    flat_ds = direction_v.T.ravel()\n",
    "\n",
    "    critter_locs = np.array(np.where(board['pieces'] == self.critter_index))\n",
    "    legal_offsets = np.stack([\n",
    "    critter_locs + np.array([np.array([0, -1,  0])]*batch_size).T, # up\n",
    "    critter_locs + np.array([np.array([0,  1,  0])]*batch_size).T, # down\n",
    "    critter_locs + np.array([np.array([0,  0, -1])]*batch_size).T, # left\n",
    "    critter_locs + np.array([np.array([0,  0,  1])]*batch_size).T]) #right\n",
    "    legal_offsets = np.vstack(np.transpose(legal_offsets, (0, 2, 1)))\n",
    "\n",
    "    # conditions for offsets on the board\n",
    "    c1 = legal_offsets[:,1] >= 0\n",
    "    c2 = legal_offsets[:,1] <= n_rows-1\n",
    "    c3 = legal_offsets[:,2] >= 0\n",
    "    c4 = legal_offsets[:,2] <= n_cols-1\n",
    "    all_c = np.logical_and.reduce([c1, c2, c3, c4])\n",
    "\n",
    "    batch_indexes = legal_offsets[:,0][all_c]\n",
    "    action_indexes = legal_offsets[:,1][all_c] * n_cols + legal_offsets[:,2][all_c]\n",
    "    direction_values = flat_ds[all_c]\n",
    "\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index)\n",
    "    value_expand = np.zeros((batch_size, n_rows*n_cols))\n",
    "    value_expand[(batch_indexes, action_indexes)] = direction_values\n",
    "    valid_value_expand = value_expand * valids\n",
    "    row_sums = valid_value_expand.sum(axis=1)\n",
    "    zero_rows = (row_sums == 0)\n",
    "    valid_row_sums = valids.sum(axis=1, keepdims=True)\n",
    "    # Handle the case where a row in 'valids' is all zeros to avoid division by zero\n",
    "    valid_row_sums[valid_row_sums == 0] = 1\n",
    "    normalized_valids = valids / valid_row_sums\n",
    "    # Update only those rows where the sum was zero\n",
    "    valid_value_expand[zero_rows] = normalized_valids[zero_rows]\n",
    "    if self.deterministic:\n",
    "      sampled_actions = np.argmax(valid_value_expand, axis=1)\n",
    "      a_1Hots = np.zeros((batch_size, n_rows * n_cols))\n",
    "      a_1Hots[np.arange(batch_size), sampled_actions] = 1.0\n",
    "      v_probs = a_1Hots\n",
    "    else:\n",
    "      value_expand_shift = value_expand - np.max(value_expand, axis=1, keepdims=True)\n",
    "      value_expand_scale = value_expand_shift/temp\n",
    "      v_probs = np.exp(value_expand_scale) / np.sum(\n",
    "        np.exp(value_expand_scale), axis=1, keepdims=True)\n",
    "      v_probs = v_probs / np.sum(v_probs, axis=1, keepdims=True)\n",
    "      samp = self.game.rng.uniform(size = batch_size).reshape((batch_size,1))\n",
    "      sampled_actions = np.argmax(v_probs.cumsum(axis=1) > samp, axis=1)\n",
    "      a_1Hots = np.zeros((batch_size, n_rows*n_cols))\n",
    "      a_1Hots[(range(batch_size), sampled_actions)] = 1.0\n",
    "    return sampled_actions, a_1Hots, v_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# different param selection cells\n",
    "Why does GP suck here compared to propose and reject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# run simple grid search... it sucks\n",
    "sym_dimensions = [np.array([0.0, 0.1, 0.5]) for _ in range(8)]\n",
    "try:\n",
    "  # Generate all possible combinations of symmetry parameters\n",
    "  param_combinations = list(itertools.product(*sym_dimensions))\n",
    "except ValueError as e:\n",
    "  display(f\"Caught an error: {e}\")\n",
    "\n",
    "game = GridworldGame(batch_size=9, n_rows=7, n_cols=7,\n",
    "                     num_critters=1, num_food=10, lifetime=30,\n",
    "                     rng=np.random.default_rng(48))\n",
    "best_avg_score = float('-inf')\n",
    "best_params = None\n",
    "\n",
    "def convert_symmetry_to_weights(symmetry_params):\n",
    "  # Initialize the weight matrix with zeros\n",
    "  weights = np.zeros((4,12))\n",
    "  symmetry_indices = {\n",
    "    'Up':    [0,  1,  2,  1,  3,  4,  4,  3,  5,  6,  5,  7],\n",
    "    'Down':  [7,  5,  6,  5,  3,  4,  4,  3,  1,  2,  1,  0],\n",
    "    'Left':  [3,  1,  4,  5,  0,  2,  6,  7,  1,  4,  5,  3],\n",
    "    'Right': [3,  5,  4,  1,  7,  6,  2,  0,  5,  4,  1,  3]}\n",
    "  # Use the symmetry indices to populate the 48-dimensional weight vector\n",
    "  for i, direction in enumerate(['Up', 'Down', 'Left', 'Right']):\n",
    "    for j, idx in enumerate(symmetry_indices[direction]):\n",
    "      weights[i, j] = symmetry_params[idx]\n",
    "  return weights\n",
    "\n",
    "# Loop through each combination\n",
    "for params in tqdm(param_combinations):\n",
    "  # Convert symmetry parameters to the actual weights\n",
    "  weights = convert_symmetry_to_weights(params)\n",
    "\n",
    "  # Run the game with the weights\n",
    "  boppp = BatchOptPerceptParamPlayer(game, weights=weights, deterministic=True)\n",
    "  final_board = game.play_game(players=[boppp], visualize=False)\n",
    "\n",
    "  # Evaluate the score\n",
    "  scores = final_board['scores'].flatten()\n",
    "  avg_score = np.mean(scores)\n",
    "\n",
    "  # Update best parameters if needed\n",
    "  if avg_score > best_avg_score:\n",
    "    best_avg_score = avg_score\n",
    "    best_params = params\n",
    "\n",
    "print(best_params)\n",
    "print(best_avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# run simple propose and reject algo, it works shockingly well\n",
    "game = GridworldGame(batch_size=256, n_rows=7, n_cols=7,\n",
    "                     num_critters=1, num_food=10, lifetime=30,\n",
    "                     rng=np.random.default_rng(48))\n",
    "\n",
    "\n",
    "# Initialize parameters\n",
    "initial_params = np.zeros(8)\n",
    "best_params = initial_params\n",
    "best_avg_score = float('-inf')\n",
    "max_rejected = 200\n",
    "rejected_count = 0\n",
    "n_iterations = 1000  # Number of iterations\n",
    "std_dev = 0.1  # Standard deviation for Gaussian proposal\n",
    "\n",
    "# Propose-and-test loop\n",
    "while rejected_count < max_rejected:\n",
    "  # Propose new parameters: sample from Gaussian centered at best_params\n",
    "  delta_params = np.random.normal(0, std_dev, best_params.shape)\n",
    "  proposal_params = best_params + delta_params\n",
    "  # Convert symmetry parameters to actual weights\n",
    "  weights = convert_symmetry_to_weights(proposal_params)\n",
    "\n",
    "  # Run the game with the proposed weights\n",
    "  boppp = BatchOptPerceptParamPlayer(game, weights=weights, deterministic=False)\n",
    "  final_board = game.play_game(players=[boppp], visualize=False)\n",
    "  # Evaluate the score\n",
    "  scores = final_board['scores'].flatten()\n",
    "  avg_score = np.mean(scores)\n",
    "\n",
    "  # Update best parameters if needed\n",
    "  if avg_score > best_avg_score:\n",
    "    best_avg_score = avg_score\n",
    "    best_params = proposal_params\n",
    "    print('best params so far:')\n",
    "    display(best_params)\n",
    "    print('best score so far:')\n",
    "    display(best_avg_score)\n",
    "    print(f\"found after {rejected_count} tests\")\n",
    "    rejected_count = 0\n",
    "  else:\n",
    "    rejected_count += 1\n",
    "\n",
    "# Print the best found parameters and score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Average Score:\", best_avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# try to get GPyOpt working\n",
    "\n",
    "def convert_symmetry_to_weights(symmetry_params):\n",
    "  # Initialize the weight matrix with zeros\n",
    "  weights = np.zeros((4,12))\n",
    "  symmetry_indices = {\n",
    "    'Up':    [0,  1,  2,  1,  3,  4,  4,  3,  5,  6,  5,  7],\n",
    "    'Down':  [7,  5,  6,  5,  3,  4,  4,  3,  1,  2,  1,  0],\n",
    "    'Left':  [3,  1,  4,  5,  0,  2,  6,  7,  1,  4,  5,  3],\n",
    "    'Right': [3,  5,  4,  1,  7,  6,  2,  0,  5,  4,  1,  3]}\n",
    "  # Use the symmetry indices to populate the 48-dimensional weight vector\n",
    "  for i, direction in enumerate(['Up', 'Down', 'Left', 'Right']):\n",
    "    for j, idx in enumerate(symmetry_indices[direction]):\n",
    "      weights[i, j] = symmetry_params[idx]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def evaluate(W):\n",
    "  game = GridworldGame(batch_size=256, n_rows=7, n_cols=7,\n",
    "                     num_critters=1, num_food=10, lifetime=30,\n",
    "                     rng=np.random.default_rng(48))\n",
    "  # Run the game with the proposed weights\n",
    "  boppp = BatchOptPerceptParamPlayer(game, weights=W, deterministic=False)\n",
    "  final_board = game.play_game(players=[boppp], visualize=False)\n",
    "  # Evaluate the score\n",
    "  scores = final_board['scores'].flatten()\n",
    "  avg_score = np.mean(scores)\n",
    "  return(avg_score)\n",
    "\n",
    "\n",
    "def objective_function_batch(X):\n",
    "  results = []\n",
    "  for x in X:\n",
    "    weights = convert_symmetry_to_weights(x)\n",
    "    score = evaluate(weights)\n",
    "    results.append(-score)\n",
    "  return np.array(results).reshape(-1, 1)\n",
    "\n",
    "# Define the bounds of the search space\n",
    "bounds = [{'name': 'var_'+str(i),\n",
    "           'type': 'continuous',\n",
    "           'domain': (-1,2)} for i in range(8)]\n",
    "\n",
    "kernel = GPy.kern.RBF(input_dim=8, variance=2.0, lengthscale=0.5)\n",
    "kernel.variance.fix()\n",
    "kernel.lengthscale.fix()\n",
    "# Create a Bayesian optimizer\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=objective_function_batch,\n",
    "                                                domain=bounds,\n",
    "                                                acquisition_type='MPI',\n",
    "                                                kernel=kernel,\n",
    "                                                initial_design_type='latin',  # Using Latin hypercube sampling\n",
    "                                                initial_design_numdata=20)\n",
    "# Update the acquisition function parameters for more exploration\n",
    "#optimizer.acquisition.exploration_weight = 50\n",
    "\n",
    "# Run optimization verbosely\n",
    "max_iter = 20\n",
    "for iteration in range(max_iter):\n",
    "  optimizer.run_optimization(max_iter=1, verbosity=False)  # Run for one iteration\n",
    "  print(f\"\\nIteration {iteration + 1}/{max_iter}:\")\n",
    "  print(f\"  Cumulative Time: {optimizer.cum_time:.2f} seconds\")\n",
    "  print(f\"  Current Best Parameters (x_opt): {optimizer.x_opt}\")\n",
    "  print(f\"  Objective Value of Best Parameters (fx_opt): {optimizer.fx_opt:.4f}\\n\")\n",
    "  print(\"-\" * 50)\n",
    "\n",
    "optimizer.plot_convergence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This out of the box GP kind of sucks, appears to be getting caught in local minima. Bummer that the thing that should be robust, awesome, super cool is kind of fragile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# try to get our own GP implementation going\n",
    "\n",
    "\n",
    "def evaluate(W):\n",
    "  game = GridworldGame(batch_size=256, n_rows=7, n_cols=7,\n",
    "                     num_critters=1, num_food=10, lifetime=30,\n",
    "                     rng=np.random.default_rng(48))\n",
    "  # Run the game with the proposed weights\n",
    "  boppp = BatchOptPerceptParamPlayer(game, weights=W, deterministic=False)\n",
    "  final_board = game.play_game(players=[boppp], visualize=False)\n",
    "  # Evaluate the score\n",
    "  scores = final_board['scores'].flatten()\n",
    "  avg_score = np.mean(scores)\n",
    "  return(avg_score)\n",
    "\n",
    "\n",
    "# Squared Exponential Kernel\n",
    "def se_kernel(x, y, l=1.0, sigma_f=1.0):\n",
    "  \"\"\"Squared Exponential kernel.\"\"\"\n",
    "  sqdist = np.sum(x**2, 1).reshape(-1, 1) + np.sum(y**2, 1) - 2 * np.dot(x, y.T)\n",
    "  return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\n",
    "\n",
    "# Gaussian Process Posterior\n",
    "def gp_posterior(X_train, Y_train, X_new, l=1.0, sigma_f=1.0, sigma_y=1e-8):\n",
    "  \"\"\"Compute the posterior mean and covariance for new data X_new given training data X_train and Y_train.\"\"\"\n",
    "  K = se_kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\n",
    "  K_s = se_kernel(X_train, X_new, l, sigma_f)\n",
    "  K_ss = se_kernel(X_new, X_new, l, sigma_f) + 1e-8 * np.eye(len(X_new))\n",
    "  K_inv = np.linalg.inv(K)\n",
    "  # Posterior mean\n",
    "  mu_s = K_s.T.dot(K_inv).dot(Y_train)\n",
    "  # Posterior covariance\n",
    "  cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\n",
    "  return mu_s, cov_s\n",
    "\n",
    "# Acquisition function: Expected Improvement (EI)\n",
    "def expected_improvement(X_new, X_train, Y_train, l=1.0, sigma_f=1.0, xi=0.01):\n",
    "  \"\"\"Compute the expected improvement at X_new.\"\"\"\n",
    "  mu_s, cov_s = gp_posterior(X_train, Y_train, X_new, l, sigma_f)\n",
    "  sigma_s = np.sqrt(np.diag(cov_s))\n",
    "  mu_sample_opt = np.max(Y_train)\n",
    "  with np.errstate(divide='warn'):\n",
    "    imp = mu_s - mu_sample_opt - xi\n",
    "    Z = imp / sigma_s\n",
    "    ei = imp * norm.cdf(Z) + sigma_s * norm.pdf(Z)\n",
    "    ei[sigma_s == 0.0] = 0.0\n",
    "  return np.sum(ei)\n",
    "\n",
    "\n",
    "def propose_next_sample(acquisition, X_train, Y_train, l=1.0, sigma_f=1.0, bounds=None, n_restarts=25):\n",
    "  \"\"\"Propose the next sampling point by optimizing the acquisition function.\n",
    "  Args:\n",
    "    acquisition: Acquisition function to optimize.\n",
    "    X_train: Training inputs.\n",
    "    Y_train: Training outputs.\n",
    "    l, sigma_f: Hyperparameters for the kernel.\n",
    "    bounds: Bounds on the inputs space.\n",
    "    n_restarts: Number of restarts for the optimizer.\n",
    "  Returns:\n",
    "    x_next: The next sampling point.\n",
    "  \"\"\"\n",
    "  dim = X_train.shape[1]\n",
    "  min_val = 1\n",
    "  x_next = None\n",
    "\n",
    "  # Randomly sample possible starting points for optimizer\n",
    "  starting_points = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, dim))\n",
    "\n",
    "  for x_try in starting_points:\n",
    "    res = minimize(lambda x: -acquisition(x.reshape(1, -1), X_train, Y_train, l, sigma_f),\n",
    "                    x_try,\n",
    "                    bounds=bounds,\n",
    "                    method='L-BFGS-B')\n",
    "\n",
    "    if res.fun < min_val:\n",
    "      min_val = res.fun\n",
    "      x_next = res.x\n",
    "\n",
    "  return x_next.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def gp_optimization(initial_params, n_iterations, bounds):\n",
    "  \"\"\"\n",
    "  Use Gaussian Process to optimize the function evaluate by selecting the best parameters.\n",
    "\n",
    "  Args:\n",
    "  - initial_params: Initial parameters to start the optimization.\n",
    "  - n_iterations: Number of iterations for the GP optimization.\n",
    "  - bounds: Bounds for the parameters.\n",
    "\n",
    "  Returns:\n",
    "  - best_params: Best found parameters.\n",
    "  - best_avg_score: Best average score.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initial data\n",
    "  X_train = initial_params.reshape(1, -1)\n",
    "  weights = convert_symmetry_to_weights(X_train[0])\n",
    "  scores = evaluate(weights)\n",
    "  Y_train = np.array([[np.mean(scores)]])  # Ensure Y_train is 2D\n",
    "  #print(X_train)\n",
    "  #print(Y_train)\n",
    "\n",
    "  best_params = X_train[0]\n",
    "  best_avg_score = Y_train[0, 0]\n",
    "\n",
    "  for iteration in tqdm(range(n_iterations)):\n",
    "    # Propose next sample\n",
    "    X_new = propose_next_sample(expected_improvement, X_train, Y_train, bounds=bounds)\n",
    "    #print(X_new)\n",
    "    # Evaluate the new sample\n",
    "    weights_new = convert_symmetry_to_weights(X_new[0])\n",
    "    scores_new = evaluate(weights_new)\n",
    "    avg_score_new = np.mean(scores_new)\n",
    "\n",
    "    # Add new data to training set\n",
    "    X_train = np.vstack((X_train, X_new))\n",
    "    Y_train = np.vstack((Y_train, avg_score_new))\n",
    "\n",
    "    # Update best parameters if needed\n",
    "    if avg_score_new > best_avg_score:\n",
    "      best_avg_score = avg_score_new\n",
    "      best_params = X_new[0]\n",
    "      print(f\"Iteration {iteration + 1}:\")\n",
    "      print('Best params so far:', best_params)\n",
    "      print('Best score so far:', best_avg_score)\n",
    "\n",
    "  return best_params, best_avg_score\n",
    "\n",
    "# Let's assume some bounds for your parameters. You might need to adjust these.\n",
    "bounds = np.array([[-1, 2] for _ in range(8)])\n",
    "# For simplicity, assuming all parameters are bounded between -1 and 2.\n",
    "\n",
    "# Rerun the GP optimization\n",
    "#best_params_gp, best_avg_score_gp = gp_optimization(np.zeros(8),\n",
    "#                                                    n_iterations=50,\n",
    "#                                                    bounds=bounds)\n",
    "#best_params_gp, best_avg_score_gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Our home brew version seems to be getting less stuck and running quicker than the GPyOpt, but still not as quick as simple propose and reject, note that the propose_next_sample bit takes up more and more time as the number of sample points get's better and better. Ideally for this kind of task, you'd be dropping points that weren't doing much and picking points based on how much info they give about where the true max is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Below is a more simple method that has a bit of mememory, basically just our propose and reject, but fits a linear model based on some fixed number of best test points (what could go wrong!) and then uses that as a gradient estimate to guide a step from the current best guess. Hard part is getting the gradient scale right, and or combining that with noise. Probably don't need to add noise, since the gradient estimate itself is already pretty noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def evaluate(flat_W, game):\n",
    "  # Run the game with the proposed weights\n",
    "  W = flat_W.reshape((4,12))\n",
    "  boppp = BatchOptPerceptParamPlayer(game, weights=W, deterministic=False)\n",
    "  final_board = game.play_game(players=[boppp], visualize=False)\n",
    "  # Evaluate the score\n",
    "  scores = final_board['scores'].flatten()\n",
    "  avg_score = np.mean(scores)\n",
    "  return(avg_score)\n",
    "\n",
    "\n",
    "def memory_propose_and_test(batch_size=25, high_batch_size=400,\n",
    "                            N = 100, # number of test points to remember\n",
    "                            dim = 48, # depends on evaluated function\n",
    "                            max_rejected=100,\n",
    "                            grad_scale=0.2,\n",
    "                            noise_scale = 0.1,\n",
    "                            verbose=True):\n",
    "\n",
    "  game = GridworldGame(batch_size=batch_size, n_rows=7, n_cols=7,\n",
    "                       num_critters=1, num_food=10, lifetime=30,\n",
    "                       rng=np.random.default_rng(48))\n",
    "  high_batch_game = GridworldGame(batch_size=high_batch_size, n_rows=7, n_cols=7,\n",
    "                                  num_critters=1, num_food=10, lifetime=30,\n",
    "                                  rng=np.random.default_rng(48))\n",
    "  # Initialization\n",
    "  test_points = np.random.uniform(-1, 1, (N, dim))\n",
    "  test_values = np.zeros(len(test_points))\n",
    "  start_time = time.time()\n",
    "  print(f\"Initializing {N} test points\")\n",
    "  for i, tp in enumerate(tqdm(test_points)):\n",
    "    test_values[i] = evaluate(tp, game)\n",
    "  rejected_count = 0\n",
    "  tests_to_new_best = 0\n",
    "  total_tests = 0  # Number of iterations\n",
    "\n",
    "  print('Starting propose and test loop')\n",
    "  # Propose-and-test loop\n",
    "  while rejected_count < max_rejected:\n",
    "    model = LinearRegression().fit(test_points, test_values)\n",
    "    # Identify the best point\n",
    "    best_idx = np.argmax(test_values)\n",
    "    best_point = test_points[best_idx]\n",
    "\n",
    "    # Proposal based on model gradient\n",
    "    gradient = model.coef_\n",
    "    #print(np.linalg.norm(gradient))\n",
    "    # to maximize score go in the grad direction, to min go neg grad\n",
    "    delta = np.random.normal(0, noise_scale, dim) + grad_scale * gradient\n",
    "    proposed_point = best_point + delta\n",
    "    proposed_value = evaluate(proposed_point, game)\n",
    "\n",
    "    worst_test_value = np.min(test_values)\n",
    "    if proposed_value > worst_test_value:\n",
    "      lower_var_proposed_value = evaluate(proposed_point, high_batch_game)\n",
    "      if lower_var_proposed_value > worst_test_value:\n",
    "        print(f\"Added new test point after {rejected_count} tests and {time.time() - start_time} seconds\")\n",
    "        worst_idx = np.argmin(test_values)\n",
    "        old_best = np.max(test_values)\n",
    "        best_idx = np.argmax(test_values)\n",
    "        best_params = test_points[best_idx]\n",
    "        test_points[worst_idx] = proposed_point\n",
    "        test_values[worst_idx] = lower_var_proposed_value\n",
    "        if old_best < lower_var_proposed_value:\n",
    "          if verbose:\n",
    "            print(f\"New best test score now: {np.max(test_values)}\")\n",
    "            print(f\"Found after {tests_to_new_best} tests\")\n",
    "            #print(f\"Best params now: {best_params}\")\n",
    "            #print(f\"Params just added: {proposed_point}\")\n",
    "          tests_to_new_best = 0\n",
    "        rejected_count = 0\n",
    "      else:\n",
    "        rejected_count += 1\n",
    "    else:\n",
    "      rejected_count += 1\n",
    "    tests_to_new_best += 1\n",
    "    total_tests +=1\n",
    "  end_time = time.time()\n",
    "  elapsed_time = end_time - start_time\n",
    "  best_score = np.max(test_values)\n",
    "  best_idx = np.argmax(test_values)\n",
    "  best_params = test_points[best_idx]\n",
    "\n",
    "  if verbose:\n",
    "    # Print the best found parameters and score\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best Average Score:\", best_score)\n",
    "    print(\"Parameter combinations tested:\", total_tests)\n",
    "    print(f\"Time taken for the optimization loop: {elapsed_time:.2f} seconds\")\n",
    "  return best_params, best_score\n",
    "#best_params, best_avg_score = memory_propose_and_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# MDP Notation\n",
    "To get precise about what we are trying to optimize we first need to introduce some important notation, and formalize many of the general concepts introduced earlier in the book through our Gridworld example. If you are already farmiliar with these ideas feel free to skip this bit. Similarly if you find mathematical notation a bit overwhelming, you can also skim this section, (don't worry about understanding it all right away) and then use this as glossary as needed. A shortened version of these definitions also appear in the glossary/notation reference section found at the end of each notebook.\n",
    "\n",
    "* $\\pi_{\\theta}(a|s)$: **Policy Function** - A policy is the behavioural blueprint for the organism. It's a function that takes (some representation or filtered down aspect of) the environmental state $s$ as input, and guided by its parameters $\\theta$, gives the probability of taking action $a$, where $a$ is in the set $\\mathcal{A}(s)$ of possible actions given state $s$. The organism can then sample an actio from this set according to these probabilities. Sometimes the explicit reference to $\\theta$ is dropped when it is clear from context or does not need to be emphasized as in $\\pi(a|s)$, other times the the reference to the parameters is made more explicit by writing $\\pi(a | s, \\theta)$. In our Gridworld example each of the organisms we defined, 'Random Valid', 'Parameterized Weights', 'Eat When Near' all had a policy function at their core.\n",
    "\n",
    "* $s$: **A State** - The state represents a complete snapshot of what the environment looks like at a given moment. In our Gridworld example this is primarily the positions of food pieces and the organism, but also the number of rounds left in the simulation. The set of all possible states is denoted $\\mathcal{S}$.\n",
    "\n",
    "* $a$: **An Action** - The action an organism takes. Depending on how things are set up in our Gridworld example this might be represented as a direction or as a (row, columns) coordinate of the organism's new position, or as a flattened boolean index of the organism's new position. The set of all possible actions is denoted $\\mathcal{A}$, and the set of possible actions in a given state as $\\mathcal{A}(s)$.\n",
    "\n",
    "* $r$ : **A Reward** - The immediate reward (feedback, score, points etc.) an organism recieves after taking an action $a$ in state $s$ and transitioning to new state $s'$. In our Gridworld example $r = 1$ if the organism eats a food piece as a result of its move and $r = 0$ otherwise.\n",
    "\n",
    "* $\\theta$: **Parameters** - The aspects of an organism's policy function that can be represented by numbers. Note that these do not describe the overall structure of the policy function, but rather determine a particular instance of the policy functions possible *given* the structure (archietecture) of a policy function. In our 'Parameterized Weights' policy from our Gridworld example, the connective weight strengths $W$ are the paramweters, i.e. $\\theta = W$ in for this particular policy. For a more complicated policy with many layers of connective weights we might write $\\theta = \\{W_1, W_2,\\dots, W_N \\}$. We use $\\theta$ as a generic term so that we can make general statements about parameterized policies without having to worry about the particular archiectecure or functional form of the policy.   \n",
    "\n",
    "Given the stochastic nature of the environment (and often the policy as well), at any given time $t$ over the course of a simulation run, each of states, actions and rewards can be thought of as random variables specifically:\n",
    "\n",
    "* $S_t$: **State at Time $t$** - A random variable that denotes the state of the environment at a specific time $t$. For example, $S_t = s$ means that at time $t$, in a particular simulation run, the environment was in state $s$, or in other words that $s$ is the realization of the random variable $S_t$.\n",
    "\n",
    "* $A_t$: **Action at Time $t$** - A random variable denoting the action taken by the organism at time $t$. $A_t = a$ indicates that the action $a$ is taken at time $t$, or that $a$ is the realization of the random variable $A_t$ in a particular simulation run.\n",
    "\n",
    "* $R_t$: **Reward at Time $t$** - A random variable indicating the immediate reward received by the organism at time $t$. $R_t = r$ indicates that the reward $r$ is obtained at time $t$, or in that $r$ is the realization of the random variable $R_t$ in a particular simulation run.\n",
    "\n",
    "* $T$: **Total Simulation Time** - The total number of time steps in a given simulation. There are cases where having an infinite time horizon, $T=\\infty$, is a mathematical convenience, but since our focus is on evolved, living and learning systems, and few things live forever, we will typically work with a finite time horizons.\n",
    "\n",
    "* $t$: **Time-Step Index** - We typically subscript with $t$ to denote the value of a state, reward, action, etc. at a given specific time $t$.\n",
    "\n",
    "We can then think of simulation run as sequence of random variables:\n",
    "$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots, S_{T-1}, A_{T-1}, R_{T}, {S_T}$$\n",
    "\n",
    "The dynamics, or equations of motion, that generate this sequence of random variables are primarily encapsulated in a *transition function*, together with an *initial state distribution*, both defined as follows.\n",
    "\n",
    "* $p(s', r | s, a)$: **Transition Function** - Sometimes called the *State Transition Function*, or the *Transition Kernel* (kernel is more common when dealing with continuous state spaces) this function give the probability of transitioning from state $s$ to $s'$ and recieving reward $r$ from time-step $t$ to $t+1$, given that action $a$ is taken at time $t$. In terms of our previous notation this is defined as:\n",
    "$$ p(s', r | s, a) := \\Pr \\{S_{t+1} = s' , R_{t+1} = r | S_t = s, A_t = a \\}$$  \n",
    "\n",
    "* $p_0(s)$: **Initial State Distribution** - This is the probability distribution (density function) over the set of possible states, \\mathcal{S}, so $p_0(s) := \\Pr \\{S_0 = s\\}$. Sometimes we write $S_0 \\sim p_0$, which is read as 'The random variable $S_0$ is distributed according to the probability density function $p_0$'.\n",
    "\n",
    "This random variable notation also allows us to make uur definition of a policy function more precise: $$\\pi_{\\theta}(a | s) := \\Pr \\{A_{t} = a | S_t = s\\}.$$\n",
    "\n",
    "Then, if a policy is fixed it can simply be folded into the dynamics of the environment, creating what is refered to as the *policy-induced dynamics*.\n",
    "\n",
    "* $p_\\pi(s', r | s)$: **Policy-Induced Dynamics** - This is also called the 'dynamics under policy $\\pi$ and is defined as:\n",
    "$$p_\\pi(s', r | s)$:= \\Pr \\{S_{t+1} = s' , R_{t+1} = r | S_t = s, \\pi \\} = \\sum_{a\\in\\mathcal{A}(s)} \\pi_\\theta (a | s) \\ p(s', r | s, a).$$\n",
    "Sometimes the depedence on a specfic policy, $\\pi$, is taken as implicit and we simply write $p(s', r | s)$.\n",
    "\n",
    "The takeaway here is that for a fixed $\\pi$ and a given transition function $p$ (and initial state distribution $p_0$) the stochastic dynamics of the system are completely determined.\n",
    "\n",
    "With all that defined we can start to formally describe how rewards should be added up over time to define our goals. We just need to introduce the idea of a *Return* and a *Value* function.\n",
    "\n",
    "* $G_t$: **Return following time $t$** - Sometimes called the reward to go, or simply the return, this a random variable that indicates the total reward yet to be realized after time $t$, i.e. $G_t := \\sum_{k=t+1}^T R_k$.\n",
    "\n",
    "* $v_{\\pi}(s,t)$: **Value Function** - A function giving the *expected* return conditional on being in state $s$ at time $t$ and following a given policy $\\pi$, specifically:\n",
    "$$v_{\\pi}(s,t) := \\mathbb{E}_\\pi \\left[G_t | S_t = s \\right].$$\n",
    "In a slight stretch of notation $t$ can be treated as part of $s$ and we can write $v_{\\pi}(s)$. The dependence on a specific policy is sometimes treated as implicit and we write $v(s)$ or $v(s,t)$.\n",
    "\n",
    "In this context then our goal is to maximize the *Expectation* of a simulation run, or equivalently the average value from playing through many simulations (in the limit as many --> $\\infty$). We call this formalization of our goal objective function and define our particular objective function in this context as\n",
    "\n",
    "* $J(\\theta)$: **Objective Function** - The function that we are trying to maximize, emphasizing the dependence on the parameters, $\\theta$.\n",
    "\n",
    "The objective function is in some ways the most subjective thing in this whole set up. It's what defines the \"problem to be solved\". In our particular case we we are going to use the following as our objective.\n",
    "\n",
    "$$J(\\theta):= \\mathbb{E}\\left[ v_{\\pi_\\theta}(S_0) \\right] = \\sum_{s \\in \\mathcal{S}} p_0(s) \\cdot v_{\\pi_{\\theta}}(s)$$\n",
    "\n",
    "Then the formalization of our problem is choosing parameters $\\theta$ such that $J(\\theta)$ is as high as possible. In general this goal is written as:\n",
    "$$ \\max_{\\theta} J(\\theta),$$\n",
    "\n",
    "and in our particular case of maximizing the expected value, given a finite and discrete state space, our goal is written as:\n",
    "\n",
    "$$ \\max_\\theta\\sum_{s \\in \\mathcal{S}} p_0(s) \\cdot v_{\\pi_{\\theta}}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's get formal and introduce some notation. Our policy can be thought of as a function $\\pi(x|\\theta)$ which takes some environmental input $x$, and using its parameters $\\theta$, it returns probabilites over the set of possible actions(and a sample of the actions according to those probabilites). We use $s$ to represent the *state* of the board, specifically the locations of the food and the organism, and $s_t$ to indicate the state at a particular time step *t* in the simulation. $R_t$ is the reward at time-step $t$, in our Gridworld this will be a $R_t=1$ if the organism eats food at time-step $t$ and $R_t=0$ if it does not."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "GP_andMDP_scratch",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
