{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/draft_sequences/intro_RL_value_P1C4_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/draft_sequences/intro_RL_value_P1C4_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "___\n",
    "# **1.4.1: Is Life Just One Big Markov Decision Process? Yeah, kind of.**\n",
    "## Objective:\n",
    "In this sequence we explore and \"solve\" a simplified version of our Gridworld within the Markov Decision Process (MDP) framework. MDPs offer a robust and flexible method for rigorously defining and identifying optimal policies across various scenarios. Although the formalism of MDPs may seem complex, its components have straightforward and intuitive meanings. We are already acquainted with the key elements of an MDP:\n",
    "\n",
    "* Policy: The behavioural rule that maps stimuli to an organism's actions.\n",
    "* Organism (Agent): The entity that reacts to stimuli and performs actions.\n",
    "* Actions: The specific responses an organism makes at any moment, guided by its policy.\n",
    "* Environment: The context in which an organism operates, including the source of stimuli, the rules for state changes in response to actions, and the nature of rewards based on actions and state transitions. Other organisms' policies can influence environmental conditions.\n",
    "* Reward: An evaluation of the outcomes of an organism's actions, typically considered in evolutionary terms such as survival, reproduction, and offspring survival.\n",
    "* Markov Process: A framework for understanding stochastic dynamics by dividing the world into possible states and defining transition probabilities between these states. Crucially, these probabilities depend solely on the current state, embodying the Markov property and simplifying analysis. Historical relevance to state dynamics is integrated into the state definition, ensuring a comprehensive state concept.\n",
    "\n",
    "In the sequence we will introduce the crucial notion of **Value**, which integrates these elements and allows for rigorous optimization. Value reflects the expected total future reward from a specific state under a particular policy. Our focus will be on utilizing Value to determine the optimal policy within an MDP. We will also see how, this approach becomes impractical for complex problems due to scalability and computational limits. Despite these practical limitations, Value provides the theoretical foundation for more scalable and hence practical solutions which can be understood as approximations to the an ideal but intractable solution method.\n",
    "\n",
    "Previously we have also touched upon Partial Observability, i.e. situations where the full state of the environment is not known to the organism. Most organisms are not omniscient, so in some sense partial observability is always the case. For now though we are going to leave Partial Observability aside and focus on the simple case where the state of the world is perfectly known to the organism, to streamline the presentation of Value and Backward Induction.\n",
    "\n",
    "This all seems a bit much, but as we walk through our simplifed Gridworld example, these concepts will become more accessible and less daunting than they might initially appear. We promise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image, IFrame\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
    "             'gw_widgets.py', 'gw_NN_RL.py']\n",
    "#filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "feedback_prefix = \"P1C2_S3\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title plotting functions\n",
    "#################################################\n",
    "# More plotting functions\n",
    "#################################################\n",
    "\n",
    "\n",
    "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
    "                    name=None):\n",
    "  \"\"\"\n",
    "  Plot vector field indicating critter direction probabilities.\n",
    "\n",
    "  Args:\n",
    "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
    "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
    "      and values as lists of direction probabilities corresponding to the\n",
    "      directions ['right', 'down', 'left', 'up'].\n",
    "    critter (int): Identifier for which critter directions are associated with.\n",
    "    deterministic (bool, optional): If True, the probabilities array is\n",
    "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
    "      and are larger. Defaults to False.\n",
    "  \"\"\"\n",
    "\n",
    "  #looks like direction ignores inverted axis\n",
    "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
    "                       'left': (-1, 0), 'up': (0, 1)}\n",
    "  # but offsets need to be aware of inverted\n",
    "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
    "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
    "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
    "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
    "  # same logic for colors\n",
    "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
    "  # Get the offset and color for this critter\n",
    "  critter_offset = critter_offsets[critter]\n",
    "  critter_color = critter_colors[critter]\n",
    "\n",
    "  # Add legend only if critter is not 0\n",
    "  custom_leg_handles = []\n",
    "  if critter != 0:\n",
    "    if name is None:\n",
    "      name = f'Critter {critter}'\n",
    "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
    "    # Add the legend for this critter\n",
    "    custom_leg_handles.append(legend_patch)\n",
    "\n",
    "  C, R, U, V, A = [], [], [], [], []\n",
    "\n",
    "  for loc in loc_prob_dict.keys():\n",
    "    row, col = loc\n",
    "    probs = loc_prob_dict[loc]\n",
    "    for dir_key, prob in probs.items():\n",
    "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
    "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
    "      U.append(direction_vectors[dir_key][0])\n",
    "      V.append(direction_vectors[dir_key][1])\n",
    "\n",
    "      if deterministic:\n",
    "        A.append(1 if prob == max(probs.values()) else 0)\n",
    "      else:\n",
    "        A.append(prob)\n",
    "\n",
    "  linewidth = 1.5 if deterministic else 0.5\n",
    "  scale = 15 if deterministic else 30\n",
    "\n",
    "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
    "            scale=scale, linewidth=linewidth)\n",
    "  return fig, ax, custom_leg_handles\n",
    "\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for further use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  # Create a new figure and axes with given figsize\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  # Set width and height padding, remove horizontal and vertical spacing\n",
    "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
    "  # Show right and top borders (spines) of the plot\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  # Set major ticks (where grid lines will be) on x and y axes\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Set labels for major ticks with font size of 8\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "  # Set minor ticks (no grid lines here) to be between major ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "  # Move x-axis ticks to the top of the plot\n",
    "  ax.xaxis.tick_top()\n",
    "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "  # Remove minor ticks (not the grid lines)\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "  # Set limits of x and y axes\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  # Invert y axis direction\n",
    "  ax.invert_yaxis()\n",
    "  # If title is provided, set it as the figure title\n",
    "  if title is not None:\n",
    "    fig.suptitle(title)\n",
    "  # Hide header and footer, disable toolbar and resizing of the figure\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  # Redraw the figure with these settings\n",
    "  fig.canvas.draw()\n",
    "  # Return figure and axes handles for further customization\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None, size=None,\n",
    "              show_food=True):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if size is None:\n",
    "    size=150\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=size, marker='o',\n",
    "                      color='red', label='Food')\n",
    "  if show_food:\n",
    "    rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "    #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "    #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "    food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]],\n",
    "                  size=None) -> List[Dict[str, object]]:\n",
    "  \"\"\"\n",
    "  Plots multiple types of \"critters\" on a grid implied by the given\n",
    "  fig, ax arguments.\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects.\n",
    "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
    "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
    "    critter.\n",
    "\n",
    "  Returns:\n",
    "    Updated critter_specs with handles.\n",
    "  \"\"\"\n",
    "  if size is None:\n",
    "    size=250\n",
    "  for spec in critter_specs:\n",
    "    # Ensure required keys are present\n",
    "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
    "      if key not in spec:\n",
    "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
    "    handle_ = spec.get('handle')\n",
    "    if handle_ is None:\n",
    "      handle_ = ax.scatter([], [], s=size, marker=spec['marker'],\n",
    "                           color=spec['color'], label=spec['name'],\n",
    "                           edgecolors='white', linewidths=1)\n",
    "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
    "    spec.update({'handle': handle_})\n",
    "  return critter_specs\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc,\n",
    "                 critter=None, critter_name='Critter'):\n",
    "  \"\"\"\n",
    "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h',\n",
    "                         color='blue', label=critter_name)\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov,\n",
    "             opaque=False, fov=None):\n",
    "  \"\"\"\n",
    "  Plots a mask on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter: ndarry(int) (row,col) of the critter\n",
    "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib Image object of mask, either new if no handle\n",
    "    was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
    "  mask_array = np.ones((n_rows, n_cols, 4))\n",
    "  mask_array[:, :, :3] = 0.5  # light grey color\n",
    "  if has_fov == True:\n",
    "    if opaque:\n",
    "      mask_array[:, :, 3] = 1.0  # 50% opacity\n",
    "    else:\n",
    "      mask_array[:, :, 3] = 0.5  # 50% opacity\n",
    "    # Create arrays representing the row and column indices\n",
    "    rows = np.arange(n_rows)[:, np.newaxis]\n",
    "    cols = np.arange(n_cols)[np.newaxis, :]\n",
    "    # Iterate over each critter location\n",
    "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
    "    # Set the region within the specified radius around the critter to transparent\n",
    "    mask_array[dist <= radius, 3] = 0\n",
    "  else:\n",
    "    mask_array[:, :, 3] = 0\n",
    "\n",
    "  if fov is None:\n",
    "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
    "  else:\n",
    "    fov.set_data(mask_array)\n",
    "\n",
    "  return fov\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld Board Class\n",
    "# Local definition to be put in utils later\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters for our Gridworld game.\n",
    "\n",
    "  board state is represented by primarily by pieces, scores, rounds_left and is_over\n",
    "\n",
    "  pieces is a batch x n_rows x n_cols numpy array positive integers are critter\n",
    "  locations 0's are empty space and negative integers are food. Each critter is\n",
    "  unique and executing it's own policy so they are non-fungible, whereas food\n",
    "  (of the same type) is always the same, so there can and typically will be\n",
    "  duplicates of negative integers in the pieces array, but never of positive\n",
    "  integers\n",
    "\n",
    "  For pieces first dim is batch, second dim row , third is col,\n",
    "  so pieces[0][1][7] is the square in row 2, in column 8 of the first board in\n",
    "  the batch of boards.\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game. Each critter gets one\n",
    "  move per round so this will be the same for every critter in every batch.\n",
    "\n",
    "  is_over just tracks whether each game in each batch has concluded, this allows\n",
    "  for probabalistic end times, not just deterministic end times based on moves left\n",
    "\n",
    "  Note: In this version the game class handles the end conditions, without any\n",
    "      input from this board class. Even though they are not used, max_rounds_taken\n",
    "      and end_prob are passed in to the constructor for completeness.\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization is aligned with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical, so we use invert y-axis when plotting\n",
    "    with matplotlib\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  class CritterFoodType(Enum):\n",
    "    FOOD = \"food\"\n",
    "    PREY = \"prey\"\n",
    "    PREDATOR = \"predator\"\n",
    "\n",
    "  ARRAY_PAD_VALUE = -200\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=2,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_foragers=1,\n",
    "               num_predators=0,\n",
    "               max_rounds_taken=30,\n",
    "               end_prob=0.00,\n",
    "               food_num_deterministic = True,\n",
    "               food_patch_prob=10.0/49.0,\n",
    "               food_forager_regen = True,\n",
    "               rng=None,\n",
    "               state_elements = ['pieces', 'scores', 'is_over', 'rounds_left'],\n",
    "               init_board_state = None\n",
    "               ):\n",
    "\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    # size of the board/world\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    #number and type of critters on the board\n",
    "    self.num_foragers = num_foragers\n",
    "    self.num_predators = num_predators\n",
    "    # foragers will be indicated by lower valued positive integers, predators\n",
    "    # by higher valued intagers\n",
    "    self.forager_predator_threshold = self.num_foragers\n",
    "    self.num_critters = num_foragers + num_predators\n",
    "\n",
    "    # end conditions can be deterministic or stochastic\n",
    "    # one of moving, or eating or both might take time, e.g. eating might be\n",
    "    # automatic and free after moving, conversely, moving might be free, but\n",
    "    # eating count towards the session/episode ending, or both might\n",
    "    self.max_rounds_taken = max_rounds_taken\n",
    "    self.end_prob = end_prob\n",
    "\n",
    "    # what proportion of the (non-critter occupied) patches contain food.\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.food_num_deterministic = food_num_deterministic\n",
    "    if self.food_num_deterministic:\n",
    "      self.num_food = int((self.n_rows * self.n_cols - self.num_critters)\n",
    "                          * self.food_patch_prob)\n",
    "    self.food_forager_regen = food_forager_regen\n",
    "\n",
    "    # reproducible stochasticity\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "    self.state_elements = state_elements\n",
    "\n",
    "    # initialize the board\n",
    "    if init_board_state is None:\n",
    "      init_board_state = self.get_init_board_state()\n",
    "\n",
    "    self.set_state(init_board_state)\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement, useful for placing\n",
    "    critters and food on the board.\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"\n",
    "    Set up starting board using game parameters\n",
    "    \"\"\"\n",
    "    state = {}\n",
    "    state['rounds_left'] = (np.ones(self.batch_size) *\n",
    "                           self.max_rounds_taken)\n",
    "    state['is_over'] = np.zeros(self.batch_size, dtype=bool)\n",
    "    state['scores'] = np.zeros((self.batch_size, self.num_critters))\n",
    "\n",
    "    # create an empty board array.\n",
    "    pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols),\n",
    "                       dtype=int)\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    if self.food_num_deterministic:\n",
    "      init_food_nums = [self.num_food] * self.batch_size\n",
    "    else:\n",
    "      init_food_nums = self.rng.binomial(self.n_rows * self.n_cols - self.num_critters,\n",
    "                                         self.food_patch_prob, size=self.batch_size)\n",
    "    # place food and critters randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+num_critter because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(\n",
    "        self.n_rows, self.n_cols, init_food_nums[ii]+self.num_critters)\n",
    "      # critter random start locations\n",
    "      for c_ in np.arange(self.num_critters):\n",
    "        pieces[(ii, rc_idx[0][c_], rc_idx[1][c_])] = c_ + 1\n",
    "      # food random start locations\n",
    "      for f_ in np.arange(init_food_nums[ii]):\n",
    "        pieces[(ii, rc_idx[0][self.num_critters + f_],\n",
    "                    rc_idx[1][self.num_critters + f_])] = -f_ - 1\n",
    "    state['pieces'] = pieces\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board, check=False):\n",
    "    \"\"\" board is dictionary giving game state \"\"\"\n",
    "    if check:\n",
    "      if board['pieces'].shape != (self.batch_size, self.n_rows, self.n_cols):\n",
    "        raise ValueError(\"Invalid shape for 'pieces'\")\n",
    "      if board['scores'].shape != (self.batch_size, self.num_crititters):\n",
    "        raise ValueError(\"Invalid shape for 'scores'\")\n",
    "      if board['rounds_left'].shape != (self.batch_size,):\n",
    "        raise ValueError(\"Invalid shape for 'rounds_left'\")\n",
    "      if board['is_over'].shape != (self.batch_size,):\n",
    "        raise ValueError(\"Invalid shape for 'is_over'\")\n",
    "    for key in self.state_elements:\n",
    "      if key in board:\n",
    "        setattr(self, key, board[key].copy())\n",
    "      else:\n",
    "        raise ValueError(f\"Key '{key}' not found in the provided board state.\")\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state dictionary\"\"\"\n",
    "    state = {key: getattr(self, key).copy() for key in self.state_elements}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  def get_critter_food_type(self, critter_food):\n",
    "    if critter_food <= -1:\n",
    "        critter_food_type = self.CritterFoodType.FOOD\n",
    "    elif critter_food > self.forager_predator_threshold:\n",
    "        critter_food_type = self.CritterFoodType.PREDATOR\n",
    "    else:\n",
    "        critter_food_type = self.CritterFoodType.PREY\n",
    "    return critter_food_type\n",
    "\n",
    "\n",
    "  def get_type_masks(self):\n",
    "    \"\"\"\n",
    "    Returns masks indicating the position types on the board.\n",
    "    Returns:\n",
    "        tuple: Tuple containing masks for empty spaces, food, prey, and predator.\n",
    "    \"\"\"\n",
    "    empt_mask = self.pieces == 0\n",
    "    food_mask = self.pieces <= -1\n",
    "    prey_mask = (1 <= self.pieces) & (self.pieces <= self.forager_predator_threshold)\n",
    "    pred_mask = self.forager_predator_threshold < self.pieces\n",
    "    return empt_mask, food_mask, prey_mask, pred_mask\n",
    "\n",
    "\n",
    "  def get_collisions(self, moves, critter_food, critter_food_type):\n",
    "    \"\"\"\n",
    "    Determine the collision results and update scores accordingly.\n",
    "    Args:\n",
    "        moves (tuple): Tuple of arrays indicating the moves.\n",
    "        critter_food (int): Index to identify the critter or food.\n",
    "        critter_food_type (enum): Type of the critter or food\n",
    "    Returns:\n",
    "        tuple: Tuple containing move collision messages and separates out the\n",
    "        moves by where they land i.e., empty spaces, food, prey, and predator.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    move_mask = np.zeros(self.pieces.shape, dtype=bool)\n",
    "    move_mask[moves] = True\n",
    "    (empt_mask, food_mask,\n",
    "     prey_mask, pred_mask) = self.get_type_masks()\n",
    "\n",
    "    move_coll_msg = np.zeros(batch_size)\n",
    "    empt_moves = np.where(empt_mask & move_mask)\n",
    "    food_moves = np.where(food_mask & move_mask)\n",
    "    prey_moves = np.where(prey_mask & move_mask)\n",
    "    pred_moves = np.where(pred_mask & move_mask)\n",
    "    move_coll_msg[empt_moves[0]] = 1\n",
    "\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      move_coll_msg[food_moves[0]] = 2\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      move_coll_msg[food_moves[0]] = 3\n",
    "      move_coll_msg[prey_moves[0]] = 4\n",
    "    # all collision types are blocking for food types\n",
    "\n",
    "    return (move_coll_msg, empt_moves, food_moves, prey_moves, pred_moves)\n",
    "\n",
    "\n",
    "  def update_scores(self, move_coll_msg, critter_food,\n",
    "                    critter_food_type, prey_moves):\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      self.scores[:, critter_food-1] += (move_coll_msg == 2)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      # predators that eat get a point\n",
    "      self.scores[:, critter_food-1] += (move_coll_msg == 4)\n",
    "      # prey that are eaten lose 10 points\n",
    "      who_eaten = self.pieces[prey_moves]\n",
    "      self.scores[prey_moves[0], who_eaten-1] -= 10\n",
    "    # food types don't get a score, it's a neuro book\n",
    "\n",
    "\n",
    "  def move_pieces(self, critter_food, move_coll_msg, moves):\n",
    "    \"\"\"\n",
    "    Move the pieces on the board based on the collision messages.\n",
    "\n",
    "    Args:\n",
    "        critter_food (int): Index to identify the critter or food.\n",
    "        move_coll_msg (np.array): Array of collision messages.\n",
    "        moves (tuple): Tuple of arrays indicating the moves.\n",
    "    \"\"\"\n",
    "    old_locs = np.where(self.pieces == critter_food)\n",
    "    vacated_old_locs = np.column_stack(old_locs)[np.where(move_coll_msg > 0)]\n",
    "    vacated_old_locs_idx = (vacated_old_locs[:,0],\n",
    "                            vacated_old_locs[:,1],\n",
    "                            vacated_old_locs[:,2])\n",
    "    self.pieces[vacated_old_locs_idx] = 0\n",
    "    new_locs = np.column_stack(moves)[np.where(move_coll_msg > 0)]\n",
    "    new_locs_idx = (new_locs[:,0], new_locs[:,1], new_locs[:,2])\n",
    "    self.pieces[new_locs_idx] = critter_food\n",
    "\n",
    "\n",
    "  def replace_destroyed(self, destroying_moves, old_pieces):\n",
    "    \"\"\"\n",
    "    Replace the destroyed pieces on the board.\n",
    "\n",
    "    Args:\n",
    "        destroying_moves (tuple): Tuple of arrays indicating the moves that\n",
    "        resulted in destruction.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = old_pieces.shape\n",
    "    g_gone = np.zeros(batch_size)\n",
    "    g_gone[destroying_moves[0]] = 1\n",
    "    which_gone = old_pieces[destroying_moves]\n",
    "    if np.sum(g_gone) > 0:\n",
    "      num_empty_after = (n_rows*n_cols - self.num_food - self.num_critters + 1)\n",
    "      p_new_locs = np.where(np.logical_and(\n",
    "        self.pieces == 0, g_gone.reshape(batch_size, 1, 1)))\n",
    "      food_sample_ = self.rng.choice(num_empty_after, size=int(np.sum(g_gone)))\n",
    "      food_sample = food_sample_ + np.arange(int(np.sum(g_gone)))*num_empty_after\n",
    "      new_loc_vals = self.pieces[(p_new_locs[0][food_sample],\n",
    "                   p_new_locs[1][food_sample],\n",
    "                   p_new_locs[2][food_sample])]\n",
    "      # this requires that p_new_locs and destroying moves are both\n",
    "      # lexographically sorted... but they are not always\n",
    "      self.pieces[(p_new_locs[0][food_sample],\n",
    "                   p_new_locs[1][food_sample],\n",
    "                   p_new_locs[2][food_sample])] = which_gone\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, critter_food):\n",
    "    \"\"\"\n",
    "    Execute the moves on the board, handle collisions, update scores,\n",
    "    and replace destroyed/eaten pieces.\n",
    "\n",
    "    Args:\n",
    "      moves (tuple): Tuple of arrays indicating the moves.\n",
    "      critter_food (int): Index to identify the critter or food.\n",
    "    \"\"\"\n",
    "    # what type of critter is moving\n",
    "    critter_food_type = self.get_critter_food_type(critter_food)\n",
    "    # what do they land on when they move\n",
    "    (move_coll_msg, empt_moves, food_moves,\n",
    "     prey_moves, pred_moves) = self.get_collisions(\n",
    "        moves, critter_food, critter_food_type)\n",
    "    # based on what they move onto increment/decrement scores\n",
    "    self.update_scores(move_coll_msg, critter_food,\n",
    "                       critter_food_type, prey_moves)\n",
    "    # move the pieces\n",
    "    old_pieces = self.pieces.copy()\n",
    "    self.move_pieces(critter_food, move_coll_msg, moves)\n",
    "    # eaten/destroyed food and prey respawn in some variants\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      if self.food_forager_regen:\n",
    "        self.replace_destroyed(food_moves, old_pieces)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      if self.food_forager_regen:\n",
    "        self.replace_destroyed(food_moves, old_pieces)\n",
    "        self.replace_destroyed(prey_moves, old_pieces)\n",
    "\n",
    "    if self.food_forager_regen:\n",
    "      check_sum = np.sum(np.arange(start=-self.num_food,\n",
    "                                   stop=self.num_critters+1))\n",
    "      if np.any(np.sum(self.pieces, axis=(1,2)) != check_sum):\n",
    "        print('something went terribly wrong')\n",
    "        print(old_pieces)\n",
    "        print(critter_food)\n",
    "        print(moves)\n",
    "        print(self.pieces)\n",
    "\n",
    "\n",
    "  def get_neighbor_grc_indices(self, critter_food, radius, pad=False):\n",
    "    \"\"\"\n",
    "    Returns all grid positions within a certain cityblock distance radius from\n",
    "    the place corresponding to critter_food.\n",
    "\n",
    "    Args:\n",
    "        critter_food (int): The idex of the focal critter_food.\n",
    "        radius (int): The cityblock distance.\n",
    "        pad (bool): whether or not to pad the array, if padded all row, col\n",
    "          indexes are valid for the padded array, useful for getting percept\n",
    "          if not all indexes are correct for the original array, useful for\n",
    "          figuring out legal moves.\n",
    "\n",
    "    Returns:\n",
    "        an array of indices, each row is a g, r, c index for the neighborhoods\n",
    "        around the critters, can use the g value to know which board you are in.\n",
    "        if pad=True also returns the padded array (the indices in that case) are\n",
    "        for the padded array, so won't work on self.pieces, whereas if pad is\n",
    "        False the indices will be for the offsets in reference to the original\n",
    "        self.pieces, but note that some of these will be invalid, and will\n",
    "        need to be filtered out (as we do in get_legal)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    # Create meshgrid for offsets\n",
    "    if pad is True:\n",
    "      padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "        (radius, radius)), constant_values=self.ARRAY_PAD_VALUE)\n",
    "      batch, rows, cols = np.where(padded_arr == critter_food)\n",
    "    else:\n",
    "      batch, rows, cols = np.where(self.pieces == critter_food)\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    neighbors_rows = extended_rows + valid_row_offsets\n",
    "    neighbors_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    indices = np.column_stack((np.repeat(np.arange(batch_size),\n",
    "                                         neighbors_rows.shape[1]),\n",
    "                               neighbors_rows.ravel(),\n",
    "                               neighbors_cols.ravel()))\n",
    "    if pad is False:\n",
    "      return indices\n",
    "    elif pad is True:\n",
    "      return indices, padded_arr\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, critter_food, radius=1):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter, taking into acount which moves\n",
    "    are blocking based on type.\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    critter_locs = np.array(np.where(self.pieces == critter_food))\n",
    "    # turn those row, col offsets into a set of legal offsets\n",
    "    legal_offsets = self.get_neighbor_grc_indices(critter_food, radius)\n",
    "    legal_offsets = {tuple(m_) for m_ in legal_offsets}\n",
    "\n",
    "    # Apply logic of where a successful move can be made, by which\n",
    "    # type of critter, be they food, prey, predator or something else\n",
    "    empt_mask, food_mask, prey_mask, pred_mask = self.get_type_masks()\n",
    "    critter_food_type = self.get_critter_food_type(critter_food)\n",
    "    #print(critter_food_type)\n",
    "    if critter_food_type == self.CritterFoodType.FOOD:\n",
    "      #food only drifts into empty places\n",
    "      legal_destinations = np.where(empt_mask)\n",
    "    elif critter_food_type == self.CritterFoodType.PREY:\n",
    "      legal_destinations = np.where(empt_mask | food_mask)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      legal_destinations = np.where(empt_mask | food_mask | prey_mask)\n",
    "    else:\n",
    "      raise ValueError(\"Unexpected value for critter_food_type.\")\n",
    "    legal_destinations = {tuple(coords) for coords in zip(*legal_destinations)}\n",
    "    # Add the current locations of the critters to legal_destinations\n",
    "    current_locations = {tuple(loc) for loc in critter_locs.T}\n",
    "    legal_destinations = legal_destinations.union(current_locations)\n",
    "\n",
    "    # legal moves are both legal offsets and legal destinations\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_legal_offsets(self, critter_food, radius):\n",
    "    \"\"\"\n",
    "    Identifies all legal offsets for a critter or food, so filter out moves\n",
    "    that are off the board, but does not filter out collisions that would be\n",
    "    blocking. For a random valid player likely better to use get_legal_moves,\n",
    "    but this is much quicker, because it doesn't check collision types, for\n",
    "    use by RL agents in training loops\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    batch, rows, cols = np.where(self.pieces == critter_food)\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    potential_moves_rows = extended_rows + valid_row_offsets\n",
    "    potential_moves_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    # Filter offsets that would take the critter outside the board\n",
    "    c1 = potential_moves_rows >= 0\n",
    "    c2 = potential_moves_rows <= n_rows-1\n",
    "    c3 = potential_moves_cols >= 0\n",
    "    c4 = potential_moves_cols <= n_cols-1\n",
    "    valid_move_mask = np.logical_and.reduce([c1, c2, c3, c4])\n",
    "\n",
    "    legal_offsets_rows = potential_moves_rows[valid_move_mask]\n",
    "    legal_offsets_cols = potential_moves_cols[valid_move_mask]\n",
    "    batch_indexes = np.repeat(batch, valid_row_offsets.shape[0])\n",
    "    legal_offsets = np.column_stack((batch_indexes[valid_move_mask.ravel()],\n",
    "                                     legal_offsets_rows.ravel(),\n",
    "                                     legal_offsets_cols.ravel()))\n",
    "    return legal_offsets, valid_move_mask\n",
    "\n",
    "\n",
    "  def get_perceptions(self, critter_food, radius):\n",
    "    idx, pad_pieces = self.get_neighbor_grc_indices(critter_food,\n",
    "                                                    radius, pad=True)\n",
    "    #percept_mask = np.zeros(pad_pieces.shape, dtype=bool)\n",
    "    #percept_mask[idx[:,0], idx[:,1]], idx[:,2]] = True\n",
    "    percept = pad_pieces[idx[:,0], idx[:,1], idx[:,2]]\n",
    "    return(percept.reshape(self.batch_size, -1))\n",
    "\n",
    "\n",
    "  def execute_drift(self, offset_probs, wrapping=False):\n",
    "    \"\"\"\n",
    "    Drift the food on the board based on the given offsets probabilities.\n",
    "    Collisions handled by checking possible new locations in a random order and\n",
    "    cancelling moves that result in a collision.\n",
    "\n",
    "    Parameters:\n",
    "    - offset_probs: Probabilities corresponding to each offset, note implicit\n",
    "    order dependence here\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - nothing, just updates self.pieces\n",
    "    \"\"\"\n",
    "    # Check the length of offset_probs\n",
    "    #if len(offset_probs) != 5:\n",
    "    #    raise ValueError(\"offset_probs should be of length 5.\")\n",
    "    # Check if values are non-negative\n",
    "    #if any(p < 0 for p in offset_probs):\n",
    "    #    raise ValueError(\"All probabilities in offset_probs should be non-negative.\")\n",
    "    # Normalize the probabilities\n",
    "    #offset_probs = np.array(offset_probs) / np.sum(offset_probs)\n",
    "    # Convert offsets to a 2D numpy array\n",
    "    possible_offsets = np.array([[ 0, -1,  0], # up\n",
    "                                 [ 0,  1,  0], # down\n",
    "                                 [ 0,  0, -1], # left\n",
    "                                 [ 0,  0,  1], # right\n",
    "                                 [ 0,  0,  0]]) # still\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    # original food locations\n",
    "    food_locations = np.argwhere(self.pieces == -1)\n",
    "    # Sample offsets for each food location\n",
    "    num_food = food_locations.shape[0]\n",
    "    sampled_offsets = possible_offsets[self.rng.choice(\n",
    "        np.arange(possible_offsets.shape[0]),\n",
    "        size=num_food, replace=True, p=offset_probs)]\n",
    "    # Possible new food locations\n",
    "    possible_new_locations = food_locations + sampled_offsets\n",
    "    possible_wrap_row_indexes = self.rng.choice(np.arange(n_rows),\n",
    "                                                size=num_food)\n",
    "    possible_wrap_col_indexes = self.rng.choice(np.arange(n_cols),\n",
    "                                                size=num_food)\n",
    "\n",
    "    # Randomly iterate through the possible new locations\n",
    "    random_order = np.random.permutation(num_food)\n",
    "    for idx in random_order:\n",
    "      g, r, c = possible_new_locations[idx]\n",
    "      # Check if the new location is inside the boundaries of the board\n",
    "      if 0 <= r < self.pieces.shape[1] and 0 <= c < self.pieces.shape[2]:\n",
    "        # Check if the new location is empty or contains a critter\n",
    "        if self.pieces[g, r, c] == 0:\n",
    "          # Update the board\n",
    "          old_g, old_r, old_c = food_locations[idx]\n",
    "          self.pieces[g, r, c] = -1\n",
    "          self.pieces[old_g, old_r, old_c] = 0\n",
    "      elif wrapping == True:\n",
    "        # If wrapping is on then food can drift off the edge of the board and\n",
    "        # 'new' food will appear in a random loc on the opposite side\n",
    "        # Determine the opposite edge\n",
    "        if r < 0:  # Top edge\n",
    "          opposite_r = n_rows - 1\n",
    "          opposite_c = possible_wrap_col_indexes[idx]\n",
    "        elif r >= n_rows:  # Bottom edge\n",
    "          opposite_r = 0\n",
    "          opposite_c = possible_wrap_col_indexes[idx]\n",
    "        elif c < 0:  # Left edge\n",
    "          opposite_c = n_cols - 1\n",
    "          opposite_r = possible_wrap_row_indexes[idx]\n",
    "        elif c >= n_cols:  # Right edge\n",
    "          opposite_c = 0\n",
    "          opposite_r = possible_wrap_row_indexes[idx]\n",
    "\n",
    "        # Check if the opposite location is unoccupied\n",
    "        if self.pieces[g, opposite_r, opposite_c] == 0:\n",
    "          old_g, old_r, old_c = food_locations[idx]\n",
    "          self.pieces[g, opposite_r, opposite_c] = -1\n",
    "          self.pieces[old_g, old_r, old_c] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title GridworldGame class\n",
    "#######################################################################\n",
    "# extend GridworldGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=2,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_foragers=1,\n",
    "               num_predators=0,\n",
    "               max_rounds_taken=30,\n",
    "               end_prob=0.00,\n",
    "               food_num_deterministic = True,\n",
    "               food_patch_prob=10.0/48.0,\n",
    "               food_forager_regen = True,\n",
    "               rng=None,\n",
    "               state_elements = ['pieces', 'scores', 'is_over', 'rounds_left'],\n",
    "               init_board_state = None,\n",
    "               drift_player = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes an instance of the class with the specified parameters.\n",
    "    Args:\n",
    "      batch_size (int, optional): Number of instances in a batch. Default is 1.\n",
    "      n_rows (int, optional): Number of rows in the grid. Default is 7.\n",
    "      n_cols (int, optional): Number of columns in the grid. Default is 7.\n",
    "      num_foragers (int, optional): Number of different agents running around\n",
    "        on each board in the batch eating food. Default is 1.\n",
    "      num_predators (int, optional): Number of different agents running around\n",
    "        on each board in the batch eating foragers. Default is 0.\n",
    "      max_rounds_taken (int, optional): Time before critter's foraging session\n",
    "        ends, in terms of moves taken. Default is 30.\n",
    "      end_prob (float, optional): Probability of ending the game before max\n",
    "        moves are taken, on a given round. Default is 0.00.\n",
    "      food_num_deterministic (bool, optional): Whether or not the number of food\n",
    "        items on each board is deterministic. Default is True.\n",
    "      food_patch_prob (float, optional): Probability of food appearing on each\n",
    "        non-critter-occupied grid cell. Default is 10.0/49.\n",
    "        If food_num_determinisitc is true we use the expected value for each\n",
    "        game in the batch\n",
    "      food_forager_regen (bool, optional): Whether or not foragers and food\n",
    "        respawn/regenerate after they are eaten/destroyed. Default is True.\n",
    "      rng (numpy random number generator, optional): Random number generator\n",
    "        for reproducibility. If None, uses default RNG with a preset seed.\n",
    "      state_elements (list of strings, optional): Elements of the state\n",
    "        passed to players to determine moves. Default is ['pieces', 'scores',\n",
    "        'is_over', 'rounds_left'].\n",
    "      init_board_state (dict, optional): Allows for manual game state\n",
    "        initilization. Default is None, resulting in a random initialization.\n",
    "      drift_player (player object, optional): a 'player' who moves the food\n",
    "        pieces around (drifting) if none, skip food movement\n",
    "\n",
    "    Note: In this version game class handles the end conditions, without any\n",
    "      input from the board class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for positive integer inputs\n",
    "    assert all(isinstance(i, int) and i >= 0\n",
    "               for i in [batch_size, n_rows, n_cols, num_foragers,\n",
    "                         num_predators, max_rounds_taken]), \"These inputs must be non-negative integers.\"\n",
    "\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_foragers = num_foragers\n",
    "    self.num_predators = num_predators\n",
    "    self.num_critters = num_predators + num_foragers\n",
    "    self.pred_prey_threshold = self.num_foragers\n",
    "    self.max_rounds_taken = max_rounds_taken\n",
    "    self.end_prob = end_prob\n",
    "    self.food_num_deterministic = food_num_deterministic\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.food_forager_regen = food_forager_regen\n",
    "    self.drift_player = drift_player\n",
    "    self.init_board_state = init_board_state\n",
    "    self.state_elements = state_elements\n",
    "\n",
    "    # convience wrapper for passing parameters to board class constructor\n",
    "    self.board_params = {\n",
    "      'batch_size': self.batch_size,\n",
    "      'n_rows': self.n_rows,\n",
    "      'n_cols': self.n_cols,\n",
    "      'num_foragers': self.num_foragers,\n",
    "      'num_predators': self.num_predators,\n",
    "      'max_rounds_taken': self.max_rounds_taken,\n",
    "      'end_prob': self.end_prob,\n",
    "      'food_num_deterministic': self.food_num_deterministic,\n",
    "      'food_patch_prob': self.food_patch_prob,\n",
    "      'food_forager_regen': self.food_forager_regen,\n",
    "      'rng': self.rng,\n",
    "      'state_elements': self.state_elements\n",
    "    }\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(**self.board_params,\n",
    "                       init_board_state=self.init_board_state)\n",
    "    return b.get_state()\n",
    "\n",
    "\n",
    "  def get_board_shape(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,r,c coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 0-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to r,c indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g=0):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return (board['pieces'][g].tobytes() + board['scores'][g].tobytes() +\n",
    "            board['rounds_left'][g].tobytes())\n",
    "\n",
    "\n",
    "  def get_square_symbol(self, piece):\n",
    "    \"\"\" Translate integer piece value to symbol for display\"\"\"\n",
    "    if piece <= -1:\n",
    "      return \"X\"\n",
    "    elif piece == 0:\n",
    "      return \"-\"\n",
    "    elif piece >= 1:\n",
    "      return \"0\"\n",
    "    else:\n",
    "      return \"???????????????????????????\"\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g=0):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.get_square_symbol(square)\n",
    "                        for row in board['pieces'][g]\n",
    "                          for square in row])\n",
    "    board_s = board_s + '_' + str(board['scores'][g])\n",
    "    board_s = board_s + '_' + str(board['rounds_left'][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board['rounds_left'].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for r_ in range(self.n_rows):\n",
    "      print(r_, \"|\", end=\"\")    # Print the row\n",
    "      for c_ in range(self.n_cols):\n",
    "        piece = board['pieces'][g,r_,c_]    # Get the piece to print\n",
    "        #print(piece)\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board['rounds_left'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g, critter_index):\n",
    "    return np.squeeze(np.array(np.where(board['pieces'][g]==critter_index)))\n",
    "\n",
    "\n",
    "  def plot_moves(self, board, player0, g=0, player1=None,\n",
    "                 fig=None, ax=None, p0_name='Player 0', p1_name='Player 1',\n",
    "                 figsize=(6,5), critter_name='Critter', title=None,\n",
    "                 deterministic=False):\n",
    "    \"\"\"\n",
    "    Uses plotting functions to make picture of the current board state, and what\n",
    "    a critter would do at each non-food location in the current board state\n",
    "    \"\"\"\n",
    "    def make_prob_dict(critter_locs, play):\n",
    "      offset_dict = {(0, 1): 'right',\n",
    "                     (0,-1): 'left',\n",
    "                     ( 1, 0): 'down',\n",
    "                     (-1, 0): 'up'}\n",
    "      index_probs = play[2].copy()\n",
    "      loc_prob_dict = {}\n",
    "      # for each non food locations\n",
    "      for g, loc_ in enumerate(critter_locs):\n",
    "        # this is the location as an r, c tuple\n",
    "        rc_tup = tuple((loc_[1], loc_[2]))\n",
    "        # the relevant probabilities\n",
    "        raw_probs = index_probs[g]\n",
    "        probs = raw_probs[raw_probs > 0]\n",
    "        indexes = np.argwhere(raw_probs > 0)\n",
    "        # turn the probability indexes into r, c coords\n",
    "        rows = np.floor_divide(indexes, gwg.n_cols)\n",
    "        cols = np.remainder(indexes, gwg.n_cols)\n",
    "        moves = np.squeeze(np.array([z for z in zip(rows, cols)]), axis=2)\n",
    "        #compute the offsets and turn them to strings\n",
    "        offsets = moves - loc_[1:]\n",
    "        str_offsets = np.array(list(map(offset_dict.get, map(tuple, offsets))))\n",
    "        # update the loc_prob_dict for plotting\n",
    "        prob_dict = dict(zip(str_offsets, probs))\n",
    "        loc_prob_dict.update({rc_tup: prob_dict})\n",
    "      return loc_prob_dict\n",
    "\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    food = plot_food(fig, ax, rc_food_plotting)\n",
    "\n",
    "    expanded_board = self.critter_everywhere_state_expansion(\n",
    "      board, player0.critter_index, to_expand=g)\n",
    "    critter_locs = np.argwhere(expanded_board['pieces']==player0.critter_index)\n",
    "    #play the expanded state\n",
    "    p0_play = player0.play(expanded_board)\n",
    "    #get the prob dict\n",
    "    p0_loc_prob_dict = make_prob_dict(critter_locs, p0_play)\n",
    "    # same for player1 if there is one\n",
    "    if player1 is not None:\n",
    "      p1_play = player1.play(expanded_board)\n",
    "      p1_loc_prob_dict = make_prob_dict(critter_locs, p1_play)\n",
    "\n",
    "    existing_handels, _ = ax.get_legend_handles_labels()\n",
    "    if player1 is None:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=0, deterministic=deterministic)\n",
    "      leg_handles = existing_handels\n",
    "    else:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=1, deterministic=deterministic, name=p0_name)\n",
    "      fig, ax, leg_handles_1 = plot_directions(fig, ax, p1_loc_prob_dict,\n",
    "        critter=2, deterministic=deterministic, name=p1_name)\n",
    "      leg_handles = existing_handels + leg_handles_0 + leg_handles_1\n",
    "\n",
    "    fig.legend(handles=leg_handles, loc=\"outside right upper\")\n",
    "    fig.canvas.draw()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war feild_of_view\n",
    "                 fov_opaque=False, #let human see trhough fog of war or not\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter',\n",
    "                 focal_critter_index = 0):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(self.num_critters):\n",
    "        critter_name = name if self.num_critters == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == self.num_critters, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(np.where(board['pieces'][g] == spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # plot field of view around the 'active player'\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque, fov=fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to get the legal moves, as set of batch, row, col triples\n",
    "    giving for the given board. Does return moves that are technically legal\n",
    "    but that will result in a blocking move, this is good for a random valid\n",
    "    player, so that the don't have a high probability of staying still if\n",
    "    there are lots of blocking moves.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    legal_moves =  b.get_legal_moves(critter, radius)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_legal_offsets(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to the legal moves, as an array where each row is\n",
    "    a batch, row, col index giving legal moves on a given board. Includes\n",
    "    blocking moves, but excludes offsets that will take the critter off the\n",
    "    board\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    legal_offsets, valid_moves_mask =  b.get_legal_offsets(critter, radius)\n",
    "    return legal_offsets, valid_moves_mask\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    legal_moves =  self.get_legal_moves(board, critter, radius)\n",
    "    g, r, c = zip(*legal_moves)\n",
    "    valids = np.zeros((self.batch_size, self.n_rows * self.n_cols))\n",
    "    valids[g, np.array(r) * self.n_cols + np.array(c)] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, critter=1, g=0):\n",
    "    \"\"\"Displays possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    A=np.reshape(self.get_valid_actions(board, critter)[g],\n",
    "                 (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, \"|\", end=\"\")    # Print the row\n",
    "      for row in range(self.n_rows):\n",
    "        piece = A[col][row]    # Get the piece to print\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, critter):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    perceptions = b.get_perceptions(radius, critter)\n",
    "    return perceptions\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, critter, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if board['rounds_left'][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      adapted_board_params = self.board_params.copy()\n",
    "      adapted_board_params.update({'batch_size': len(actions)})\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        adapted_board_params.update({'init_board_state': board})\n",
    "        b = GridworldBoard(**adapted_board_params)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        new_pieces = np.array([board['pieces'][ai].copy() for ai in a_indx])\n",
    "        new_scores = np.array([board['scores'][ai].copy() for ai in a_indx])\n",
    "        new_rounds_left = np.array([board['rounds_left'][ai].copy() for ai in a_indx])\n",
    "        new_active_player = copy(board['active_player'])\n",
    "        new_state = {'pieces': new_pieces,\n",
    "                     'scores': new_scores,\n",
    "                     'rounds_left': new_rounds_left,\n",
    "                     'active_player': new_active_player}\n",
    "        adapted_board_params.update({'init_board_state': new_state})\n",
    "        b = GridworldBoard(**adapted_board_params)\n",
    "      moves = self.actions_to_moves(actions)\n",
    "      b.execute_moves(moves, critter)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    moves = self.critter_direction_to_move(board, offsets, critter)\n",
    "    b.execute_moves(moves, critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right'\n",
    "      critter: integer index for the critter we want moves for\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = np.where(board['pieces'] == critter)\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def direction_probs_to_flat_probs(self, board, direction_probs, critter):\n",
    "    \"\"\"\n",
    "    Converts direction probabilities in reference to the critter's location into\n",
    "    probability arrays on the flattened board.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      direction_probs: batch length list of dictionaries with keys\n",
    "        ['up', 'down', 'left', 'right'] and corresponding probabilities.\n",
    "\n",
    "    Returns:\n",
    "      probs_arrays: list of arrays, where each array is of length n_rows*n_cols\n",
    "                    and represents the flattened probability distribution for\n",
    "                    board in the batch.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {\n",
    "        'up': np.array((0, -1, 0)),\n",
    "        'down': np.array((0, 1, 0)),\n",
    "        'left': np.array((0, 0, -1)),\n",
    "        'right': np.array((0, 0, 1))}\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    probs_arrays = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for batch_index in range(batch_size):\n",
    "      prob_array = np.zeros(n_rows * n_cols)\n",
    "      for direction, prob in direction_probs[batch_index].items():\n",
    "          offset = offset_dict[direction]\n",
    "          new_loc = np.array(critter_locs)[:, batch_index] + offset\n",
    "          # Check bounces at boundaries\n",
    "          new_loc[1] = np.where(new_loc[1] >= n_rows, n_rows-2, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] >= n_cols, n_cols-2, new_loc[2])\n",
    "          new_loc[1] = np.where(new_loc[1] < 0, 1, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] < 0, 1, new_loc[2])\n",
    "          # Convert 2D location to flattened index\n",
    "          flattened_index = new_loc[1] * n_cols + new_loc[2]\n",
    "          prob_array[flattened_index] += prob\n",
    "      probs_arrays[batch_index, :] = prob_array\n",
    "    return list(probs_arrays)\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, critter, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right', 'still'\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 0): 'still',\n",
    "                   (0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up',\n",
    "                   ( 0, 0): 'still'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    critter_locs = np.column_stack(np.where(board['pieces'] == critter))\n",
    "    valid_directions = []\n",
    "    for g, batch_valid in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valid==1)[0]\n",
    "      critter_loc = critter_locs[critter_locs[:, 0] == g, 1:]\n",
    "      # critter_loc shape is (1, 2)\n",
    "      critter_loc = np.squeeze(critter_loc)\n",
    "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
    "      offsets = moves - critter_loc\n",
    "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "    Returns a batch size np.array of -1 if not ended, and scores for each game\n",
    "    in the batch if it is ended, note only returns scores if all games in the\n",
    "    batch have ended\n",
    "    \"\"\"\n",
    "    rounds_left = board['rounds_left']\n",
    "    scores = board['scores']\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores\n",
    "\n",
    "\n",
    "  def critter_everywhere_state_expansion(self, board_state,\n",
    "                                         critter=1, to_expand=0):\n",
    "    \"\"\"\n",
    "    Expand a given board state by placing a critter at each non-food location.\n",
    "\n",
    "    The function takes a game state and returns an expanded version of it. For\n",
    "    each board in the state, it creates a new version of the board for every\n",
    "    non-food location, placing a critter at that location. The scores and\n",
    "    remaining rounds are copied for each new board. The result is a new game state\n",
    "    with a larger number of boards, each representing a possible configuration\n",
    "    with a critter at a different location.\n",
    "\n",
    "    Args:\n",
    "      board_state (dict): A dictionary containing the current game state.\n",
    "      It should have the following keys:\n",
    "        - 'pieces': a 3D numpy array (batch x n_col x n_row) representing the game\n",
    "          board. -1 -> food, 0 -> empty cell, and 1 -> critter.\n",
    "        - 'scores': 1D numpy array of the score for each board in the batch.\n",
    "        - 'rounds_left': a 1D numpy array of the rounds left for\n",
    "          each board in the batch.\n",
    "      critter: integer index to place on the expanded board state\n",
    "      to_expand (list (int)): list of batch indices to have state expanded\n",
    "\n",
    "    Returns:\n",
    "      dict: A dictionary containing the expanded game state with the same keys\n",
    "        as the input. The number of boards will be larger than the input state.\n",
    "    \"\"\"\n",
    "    pieces = board_state['pieces'].copy()\n",
    "    scores = board_state['scores'].copy()\n",
    "    rounds_left = board_state['rounds_left'].copy()\n",
    "    active_player = copy(board_state['active_player'])\n",
    "    # Determine non-food locations\n",
    "    non_food_locs = np.argwhere(pieces[to_expand] != -1)\n",
    "    #scrub all existing critter locations,\n",
    "    # maybe later only scrub specific critter type\n",
    "    pieces[pieces >= 1] = 0\n",
    "    # lists to store expanded states\n",
    "    expanded_pieces = []\n",
    "    expanded_scores = []\n",
    "    expanded_rounds_left = []\n",
    "    # Iterate over each non-food location\n",
    "    for i in range(non_food_locs.shape[0]):\n",
    "      # Create a copy of the board\n",
    "      expanded_board = np.copy(pieces[to_expand])\n",
    "      # Place the critter at the non-food location\n",
    "      # later consider only placing at non-food,\n",
    "      # non-other critter locs\n",
    "      expanded_board[tuple(non_food_locs[i])] = critter\n",
    "      # Add the expanded board to the list along score and rounds_left\n",
    "      expanded_pieces.append(expanded_board)\n",
    "      expanded_scores.append(scores[to_expand])\n",
    "      expanded_rounds_left.append(rounds_left[to_expand])\n",
    "    # Convert to arrays and create expanded board state\n",
    "    expanded_state = {'pieces': np.stack(expanded_pieces),\n",
    "                      'scores': np.array(expanded_scores),\n",
    "                      'rounds_left': np.array(expanded_rounds_left),\n",
    "                      'active_player': active_player}\n",
    "    return expanded_state\n",
    "\n",
    "\n",
    "  def play_game(self, players=[], collect_fov_data=False, fov_radius=2,\n",
    "                visualize = False):\n",
    "    \"\"\"This method takes a list of players the same length as num_critters,\n",
    "        and then plays a batch of games with them and returns the final board\n",
    "        state\"\"\"\n",
    "    if len(players) != self.num_critters:\n",
    "      raise ValueError(\"number of players different than expected\")\n",
    "\n",
    "    board = self.get_init_board()\n",
    "    if visualize == True:\n",
    "      self.display(board, 0)\n",
    "\n",
    "    if collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "      adapted_board_params = self.board_params.copy()\n",
    "      adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "      b = GridworldBoard(**adapted_board_params)\n",
    "    for p_idx, player_ in enumerate(players):\n",
    "      if player_.critter_index != p_idx+1:\n",
    "        print(player_.critter_index)\n",
    "        print(p_idx + 1)\n",
    "        raise ValueError(\"player order does not match assigned critter index\")\n",
    "\n",
    "    for ii in range(self.max_rounds_taken):\n",
    "      for player_ in players:\n",
    "        old_scores = board['scores']\n",
    "        if collect_fov_data is True:\n",
    "          b.set_state(board)\n",
    "          percepts = b.get_perceptions(fov_radius)\n",
    "\n",
    "        a_player, _, _ = player_.play(board)\n",
    "        board = self.get_next_state(board, player_.critter_index, a_player)\n",
    "        if visualize == True:\n",
    "          self.display(board, 0)\n",
    "      if self.end_prob > 0:\n",
    "        if np.random.rand() < self.end_prob:\n",
    "          print('game ended stochastically before max rounds taken')\n",
    "          break\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Interactive Gridworld Widget\n",
    "\n",
    "########################################\n",
    "# widgets refactor for multi-critter\n",
    "#########################################\n",
    "# Interactive Gridworld Game Widgets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomValidPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld, could be prey or pred... or even food\n",
    "  It leans hard on the game's get valid method and then just samples from there\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1, speed=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.speed = speed\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index, self.speed)\n",
    "    action_size = self.game.get_action_size()\n",
    "\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None, has_fov=False,\n",
    "               radius=2, fov_opaque=False, collect_fov_data=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human'],\n",
    "               final_score_type='raw'):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        InteractiveGridworld expects the GridworldGame to have batchsize 1\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the gridworld_game.num_critters\n",
    "        attribute. Order should align with critter_name.\n",
    "      score_type: magic string specifying whether score displayed should be\n",
    "        a 'raw' score or a per move 'normalized' score\n",
    "\n",
    "      Note: fov is going to look pretty janky with more than one player, maybe\n",
    "      we get fov to only turn on for the 'active' player?\n",
    "      Note: Specific initialization state is handled by the GridworldGame object\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.active_player_index = 0\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.gwg.num_critters):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] #implict check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        # player objects expected to have a critter_index attribute\n",
    "        # we set it appropriately here so it aligns with the players list\n",
    "        # used to create the widget\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    self.final_score_type = final_score_type # 'raw' or 'normalized'\n",
    "    self.board_state = self.gwg.get_init_board()\n",
    "    if self.collect_fov_data is True:\n",
    "      # keep raw records of percept and eating for manipulation later\n",
    "      self.percept_eat_records = []\n",
    "      # keep data in contingency table of how many food items were in\n",
    "      # the percept, and whether or not food was eaten\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.fov_eat_table_display = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='25.0em', min_height='18.8em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if self.collect_fov_data and self.any_human_players:\n",
    "      self.legend_type = None # don't keep regenerating the legend\n",
    "      # do legend separately if showing observations and no human player\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          legend_type='separate', figsize=self.figsize, has_fov=self.has_fov,\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque)\n",
    "    elif len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                fov_opaque=self.fov_opaque,\n",
    "                                radius=self.radius, figsize=self.figsize)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "      if len(self.players) == 1:\n",
    "        #one human player\n",
    "        self.output_and_score = widgets.HBox([self.scoreboard, self.output])\n",
    "        self.no_table_final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                                  self.output_and_score])\n",
    "        if self.collect_fov_data == True:\n",
    "          # a single human player collecting data\n",
    "          self.final_display = widgets.HBox([self.no_table_final_display,\n",
    "                                           self.fov_eat_table_display])\n",
    "        else: # self.collect_fov_data == False:\n",
    "          # a single human player not collecting data\n",
    "          self.final_display = self.no_table_final_display\n",
    "      else:\n",
    "        # more than one player, one of them human\n",
    "        self.V_board_outbput = widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        self.final_display = widgets.HBox([self.V_board_outbput,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # player is some kind of ai\n",
    "      if self.collect_fov_data == True:\n",
    "        # an ai player with recording\n",
    "        # in this case legend is separate\n",
    "        self.V_score_start_output_legend = widgets.VBox([self.scoreboard,\n",
    "          self.start_button,  self.output, self.b_fig_legend.canvas])\n",
    "        self.V_board_table = widgets.VBox([self.b_fig.canvas,\n",
    "                                           self.fov_eat_table_display])\n",
    "        self.final_display = widgets.HBox([self.V_board_table,\n",
    "                                           self.V_score_start_output_legend])\n",
    "      else:\n",
    "        if len(self.players) == 1:\n",
    "          # an ai player without recording\n",
    "          self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "          self.final_display = widgets.VBox([\n",
    "            self.b_fig.canvas, self.H_score_output_start])\n",
    "        else:\n",
    "          # more than one ai player\n",
    "          self.V_board_outbput = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "          self.V_scoreboard_start_legend = widgets.VBox([\n",
    "              self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "          self.final_display = widgets.HBox([self.V_board_outbput,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "        print('There are {} rounds in this game'.format(self.board_state['rounds_left'][0]))\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.fov_eat_table_display:\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # fussy off-by-one adjustement\n",
    "    self.board_state['rounds_left'] -= 1\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "    if self.gwg.n_cols == 1:  # If the board only has one column\n",
    "      self.left_button.disabled = True\n",
    "      self.right_button.disabled = True\n",
    "\n",
    "    if self.gwg.n_rows == 1:  # If the board only has one row\n",
    "      self.up_button.disabled = True\n",
    "      self.down_button.disabled = True\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    # index of players is 0 through num_critter-1,\n",
    "    # same player represented by value of index + 1 in\n",
    "    old_scores = old_board['scores'][0]\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      adapted_board_params = self.gwg.board_params.copy()\n",
    "      adapted_board_params['init_state'] = old_board\n",
    "      b = GridworldBoard(**adapted_board_params)\n",
    "      percept = b.get_perceptions(self.radius)[0]\n",
    "\n",
    "    if (isinstance(self.players[self.active_player_index], str) and\n",
    "        'human' in self.players[self.active_player_index]):\n",
    "      direction = which_button\n",
    "    else:\n",
    "      a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
    "      # print(a_player)\n",
    "      a_player = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                      self.active_player_index+1,\n",
    "                                                      a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = a_player[0]\n",
    "\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "          self.board_state, self.active_player_index+1, [direction])\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = np.floor(self.gwg.max_rounds_taken -\n",
    "                         rounds_left / self.gwg.num_critters)\n",
    "    if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
    "      #eating happened\n",
    "      eating_string = \"They ate the food/prey there!\"\n",
    "      did_eat = 1\n",
    "    else: #eating didn't happen\n",
    "      eating_string = \"There's no food/prey there.\"\n",
    "      did_eat = 0\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0,\n",
    "                                       self.active_player_index+1)\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             radius=self.radius, legend_type=self.legend_type)\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = {'perception': percept.copy(),\n",
    "                  'state': old_board,\n",
    "                  'did_eat': bool(did_eat)}\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1) # number of food items in FoV\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        print(\"The critter (tried) to move \" + direction +\n",
    "              \" and is now at ({}, {}).\".format(row,col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\\nFood Per Move: {:.2f}\".format(\n",
    "            rounds_left, new_scores[self.active_player_index],\n",
    "            new_scores[self.active_player_index] / num_moves))\n",
    "      else: # more than one players\n",
    "        print(\"Critter {} (tried) to move \".format(self.active_player_index+1) +\n",
    "              direction +\n",
    "              \" and is now at ({}, {}).\".format(row, col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\".format(\n",
    "            rounds_left, new_scores))\n",
    "    end_sample = self.gwg.rng.random()\n",
    "    if rounds_left == 0:\n",
    "      game_over_msg = 'Game Over.'\n",
    "    elif end_sample < self.gwg.end_prob:\n",
    "      game_over_msg = 'Game Over.\\nEnded stochastically before max moves taken'\n",
    "\n",
    "\n",
    "    if rounds_left == 0 or end_sample < self.gwg.end_prob: # game is over!\n",
    "      if self.final_score_type == 'raw':\n",
    "        self.final_scores.append(new_scores)\n",
    "      elif self.final_score_type == 'normalized':\n",
    "        self.final_scores.append(new_scores/num_moves)\n",
    "      else:\n",
    "        raise ValueError(f\"Invalid final_score_type: {self.final_score_type}. Expected 'raw' or 'normalized'.\")\n",
    "\n",
    "      game_over_msg = game_over_msg + '\\nResetting board for a new game.'\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print(game_over_msg)\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                               self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                               has_fov=self.has_fov, radius=self.radius,\n",
    "                               fov_opaque=self.fov_opaque,\n",
    "                               legend_type=self.legend_type)\n",
    "      with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.players) == 1:\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = [\n",
    "              ['High Score:', str(np.max(np.array(self.final_scores)))],\n",
    "              ['Last Score:', str(self.final_scores[-1][0])],\n",
    "              ['Average Score',\n",
    "              '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "          else:\n",
    "            table = [['High Score:', '--'],\n",
    "                     ['Last Score:', '--'],\n",
    "                     ['Average Score:', '--']]\n",
    "          print(tabulate(table))\n",
    "        else: # len(self.players) > 1\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = []\n",
    "            # Assuming the batch size is 1 for now\n",
    "            current_scores = self.final_scores[-1]\n",
    "            max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "            average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "            table.append(['High Scores:'] + [str(score) for score in max_scores])\n",
    "            table.append(['Last Scores:'] + [str(score) for score in current_scores])\n",
    "            table.append(['Average Scores:'] + ['{:.2f}'.format(score) for score in average_scores])\n",
    "          else:\n",
    "            table = [\n",
    "              ['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "          print(tabulate(table, headers=headers))\n",
    "      if self.collect_fov_data is True:\n",
    "        with self.fov_eat_table_display:\n",
    "          clear_output()\n",
    "          printmd(\"**Observations**\")\n",
    "          table_data = [[str(ii),\n",
    "                         str(self.fov_eat_table_data[0,ii]),\n",
    "                         str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "          table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "                   table_data)\n",
    "          print(tabulate(table))\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "\n",
    "  def human_ai_player_loop(self, direction):\n",
    "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
    "    # Execute the move of the human who clicked the button\n",
    "    self.button_output_update(direction)\n",
    "    # Move to the next player\n",
    "    def update_player_and_rounds():\n",
    "      \"\"\"Update the player index and decrement rounds if a full loop is completed.\"\"\"\n",
    "      self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "      if self.active_player_index == 0:\n",
    "        self.board_state['rounds_left'] -= 1\n",
    "    update_player_and_rounds()\n",
    "    # Do AI moves if there are any\n",
    "    while self.players[self.active_player_index] != 'human':\n",
    "      self.button_output_update('tbd')\n",
    "      # Move to the next player\n",
    "      update_player_and_rounds()\n",
    "    # Next player is human turn buttons on for them\n",
    "    self.enable_direction_buttons()\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    for ii in range(self.gwg.max_rounds_taken*self.gwg.num_critters):\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.2)\n",
    "    self.start_button.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Graph Viz Helper Functions\n",
    "################################################################\n",
    "# @title Graphviz Helper Functions\n",
    "\n",
    "\n",
    "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
    "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
    "  ax.axis(\"off\")\n",
    "  #plt.tight_layout()\n",
    "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "def make_gw_png(gwg, state, state_num, file_path, dpi, figsize, fontsize, plot_score):\n",
    "  if plot_score:\n",
    "    if state_num is None:\n",
    "      title = \"Score: \"+str(state['scores'][0][0])\n",
    "    else:\n",
    "      title = \"State: \"+ state_num + \"\\nScore: \"+str(state['scores'][0][0])\n",
    "  else:\n",
    "    if state_num is None:\n",
    "      title = \"\"\n",
    "    else:\n",
    "      title = \"State: \"+ state_num\n",
    "  fig, ax, critter_specs, food, fov = gwg.plot_board(state, legend_type=None,\n",
    "                                                     figsize=figsize)\n",
    "  ax.set_title(title, fontsize=fontsize)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
    "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
    "  for edge in edge_labels:\n",
    "    src, dest, latex_str = edge\n",
    "    if graph.has_edge(src, dest):\n",
    "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
    "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
    "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
    "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\",\n",
    "                     fixedsize=\"true\", width=\"1.5\", height=\"1\")\n",
    "      graph.delete_edge(src, dest)\n",
    "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
    "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
    "  return graph\n",
    "\n",
    "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
    "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
    "  for node in graph.nodes():\n",
    "    if not node.startswith(\"dummy\"):\n",
    "      node.attr['width'] = width\n",
    "      node.attr['height'] = height\n",
    "  return graph\n",
    "\n",
    "def collapse_intermediate_states(graph, intermediate_nodes):\n",
    "  for node in intermediate_nodes:\n",
    "    incoming_edges = graph.in_edges(node)\n",
    "    outgoing_edges = graph.out_edges(node)\n",
    "    for in_edge in incoming_edges:\n",
    "      for out_edge in outgoing_edges:\n",
    "        graph.add_edge(in_edge[0], out_edge[1])\n",
    "        # Remove the intermediate node\n",
    "    graph.remove_node(node)\n",
    "  return graph\n",
    "\n",
    "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                            action_nodes = [],\n",
    "                            state_visualization = {},\n",
    "                            intermediate_nodes = [],\n",
    "                            state_nums = {},\n",
    "                            gwg=None,\n",
    "                            node_colors = {},\n",
    "                            node_labels = {},\n",
    "                            output_path=\"graphviz_output.png\", dpi=600,\n",
    "                            latex_figsize=(1.0, 1.0), latex_fontsize=20,\n",
    "                            gwg_figsize=(1.0, 1.0), gwg_fontsize=20,\n",
    "                            plot_score=True,\n",
    "                            rankdir='LR'):\n",
    "  \"\"\"\n",
    "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
    "\n",
    "  Parameters:\n",
    "    nodes_list (list): List of nodes in the graph.\n",
    "    edges_list (list): List of edges in the graph.\n",
    "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
    "    output_path (str): Path to save the rendered graph.\n",
    "    dpi (int): DPI for rendering the graph.\n",
    "    figsize (tuple): Figure size for the LaTeX labels.\n",
    "\n",
    "  Returns:\n",
    "    str: Path to the saved graph image.\n",
    "  \"\"\"\n",
    "  # Graph Creation and Configuration\n",
    "  G = pgv.AGraph(directed=True, strict=False, rankdir='UD', ranksep=0.5, nodesep=0.5)\n",
    "\n",
    "  # Add state and decision nodes\n",
    "  for node in nodes_list:\n",
    "    shape = \"box\" if node in action_nodes else \"ellipse\"  # Use 'box' for decision nodes\n",
    "    color = node_colors.get(node, \"black\")\n",
    "    label = node_labels.get(node, node)\n",
    "    if node in state_visualization.keys():\n",
    "      state = state_visualization[node]\n",
    "      if node in state_nums.keys():\n",
    "        state_num = state_nums[node]\n",
    "      else:\n",
    "        state_num = None\n",
    "      img_path = str(node) + \".png\"\n",
    "      make_gw_png(gwg, state, state_num, img_path, dpi, gwg_figsize, gwg_fontsize, plot_score)\n",
    "      label=\"\"\n",
    "    else:\n",
    "      img_path = \"\"\n",
    "    G.add_node(node, color=color, label=label, shape=shape, image=img_path,\n",
    "               fixedsize=\"true\", width=\"2\", height=\"2\")\n",
    "\n",
    "  for edge in edges_list:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
    "  # G = set_regular_node_sizes(G, width=1, height=1)\n",
    "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=latex_figsize, fontsize=latex_fontsize)\n",
    "\n",
    "  # Remove any itermediate nodes\n",
    "  G = collapse_intermediate_states(G, intermediate_nodes)\n",
    "\n",
    "  # Additional graph attributes\n",
    "  # G.graph_attr['size'] = \"8,8\"\n",
    "  G.graph_attr['dpi'] = str(dpi)\n",
    "\n",
    "  # Render and save the graph\n",
    "  G.layout(prog='dot')\n",
    "  G.draw(output_path)\n",
    "\n",
    "  return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.1 Being the very best in a simple Gridworld\n",
    "\n",
    "The Gridworld problem we initially explored earlier in this book had some complexity, making the absolutely optimal policy difficult to determine. Here we will look at a simplified version of the problem that is readily tractable. Run the code cell below to try out this highly simplified version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld, but shorter, smaller, and food doesn't regenerate\n",
    "# @markdown Run this cell to try and eat as much food as possible in relatively few moves\n",
    "init_state = {\n",
    "  'pieces': np.array([[[ 0, 0, 0, -1,],\n",
    "                       [-2, 1, 0, -3,]]], dtype=int),\n",
    "  'scores': np.array([[0]]),\n",
    "  'rounds_left': np.array([3]),\n",
    "  'is_over': np.array([0])\n",
    "}\n",
    "gwg1 = GridworldGame(batch_size=1,\n",
    "                    n_rows=2, n_cols=4,\n",
    "                    num_foragers=1,\n",
    "                    num_predators=0,\n",
    "                    max_rounds_taken=5,\n",
    "                    end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=init_state)\n",
    "\n",
    "igwg1 = InteractiveGridworld(\n",
    "    gwg1, players=['human'], critter_names=['Critter (You)'],\n",
    "    figsize=(5,4), has_fov=False)\n",
    "display(igwg1.b_fig.canvas)\n",
    "clear_output()\n",
    "display(igwg1.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully you were able to score 2 points in this variant of the game.\n",
    "Thinking Exercises:\n",
    "\n",
    "Is this the best that can be done?(Answer: Yes)\n",
    "\n",
    "How do you know? (Answer: With only three moves and three pieces of food the highest possible score is at most 3. However, the three pieces of food aren't contiguous, so the highest possible score is at most 2. We were able to achieve a score of 2, so the highest possible score is 2. Other lines of reasoning also work.)\n",
    "\n",
    "How did you figure out what the best policy was? Did you try different things and see what the result was? Carefully plan out three moves and calculate the result in your head? Or some combination of these two approaches?\n",
    "\n",
    "This simplified version of the Gridworld problem is only slightly different from the original problem we investigated, but these slight differences make it relatively easy to figure out the best policy. One obvious difference is that this problem is smaller in every way. The board is 2x4 instead of 7x7, there are 3 food items instead of 10, and the number of rounds is 3 instead of 30. Slighltly more subtle, is that the food does not regenerate, and the starting position is not randomized, effectively removing any uncertainty and stochasticity from the game. Eliminating this stochasticity, both in the initial board position, and in terms of where food will respawn, makes it much easier to plan out an optimal tragjectory of moves. Because the game is so small we can even write out every possible course of action in a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the fully expanded decision tree for this simple scenario\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([3]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LU\": {\n",
    "        'pieces': np.array([[[1, 0, 0, -1], [0, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LUR\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [0, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LUD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [0, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LRL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LRU\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [0, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LRR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [0, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_U\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UL\": {\n",
    "        'pieces': np.array([[[1, 0, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_ULR\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_ULD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UR\": {\n",
    "        'pieces': np.array([[[0, 0, 1, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_URR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, 1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_URL\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_URD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UDL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UDU\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UDR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RLL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RLU\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RLR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RU\": {\n",
    "        'pieces': np.array([[[0, 0, 1, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RUL\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RUR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, 1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RUD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RRL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RRU\": {\n",
    "        'pieces': np.array([[[0, 0, 0, 1], [-2, 0, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      dpi=600)\n",
    "Image(output_path, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Scanning over the decion tree above (the single initial state appears at the top). We see that actions (Left, Right, Up, Down) change the board state and when the forager moves onto food, the score increases. The processes ends after three actions. There are 21 different paths here. Scanning the scores along the final (leaf) nodes of the decision tree we see that most of these trajectories result in a score of one or zero. However, there is a single trajectory, Right-Right-Up, that results in a score of two. This method, where we write out everything that can possibly happen, and then figure out the best possible trajectory is akin to a brute force grid search over a parameter space for a policy. It is not very compuationally effecient, but it will always work given enough computational resources.\n",
    "\n",
    "In this kind of problem, where everything is fully observable and determinisitic, the optimal policy and the optimal trajectory are kind of the same thing. However, as soon as stochasticity is introduced, either in how the organism chooses their actions, or how those actions result in state transitions, there ceases to be certainty about what path through the game tree will be followed, and a policy must be able to choose actions along any of these probable trajectories. In general a policy (together with the stocastic dynmaics of the environment) can be thought of as inducing a distribution over all the possible trajectories through the decision process. In this simple case because there is was no stochasticity at all, the optimal policy collapsed into simply following the optimal trajectory. Next, we will take a look at how this game tree approach becomes more interesting and difficult when uncertainty is introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.2 Being the very best in a simple but uncertain Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In our earlier Gridworld problems from chapter 1.1, food items regenerated in a random empty location after eating events, adding a level of unpredicatability to the world. In the simple Gridworld problem above, where food did not regenerate, there was no stochasticity, which made the decision tree of the process much easier to expand. Let's look at an even smaller, shorter gridworld game, but where food does regenerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld, even smaller and shorter, but food does regenerate\n",
    "# @markdown Run this cell to try and eat as much food as possible in two moves.\n",
    "init_state = {\n",
    "  'pieces': np.array([[[ -1, 1, -2, 0,]]], dtype=int),\n",
    "  'scores': np.array([[0]]),\n",
    "  'rounds_left': np.array([2]),\n",
    "  'is_over': np.array([0])\n",
    "}\n",
    "gwg2 = GridworldGame(batch_size=1,\n",
    "                    n_rows=1, n_cols=4,\n",
    "                    num_foragers=1,\n",
    "                    num_predators=0,\n",
    "                    max_rounds_taken=2,\n",
    "                    end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=2/3,\n",
    "                    food_forager_regen=True,\n",
    "                    init_board_state=init_state)\n",
    "\n",
    "igwg2 = InteractiveGridworld(\n",
    "    gwg2, players=['human'], critter_names=['Critter (You)'],\n",
    "    figsize=(5,4), has_fov=False)\n",
    "display(igwg2.b_fig.canvas)\n",
    "clear_output()\n",
    "display(igwg2.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Are you able to always get a score of two, or do you sometimes get a score of one? Let's expand the game tree for this simple game as well and think about the optimal trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the fully expanded decision tree for this simple scenario\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down',\n",
    "        '0': '0.5',\n",
    "        '1': '0.5',\n",
    "        '2': '0.5',\n",
    "        '3': '0.5'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[1, 0, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L3\": {\n",
    "        'pieces': np.array([[[1, 0, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L3R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1R\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R3L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R\": {\n",
    "        'pieces': np.array([[[-1, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      dpi=600)\n",
    "Image(output_path, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Naively, on the first move, it might seem like both going left and going right are equally good moves. They both result in eating food right away, and in both cases there is a chance of eating more food, depending on where the food regenerates. Looking at the going left first, we see that there is an even chance of food spawning in a place where the forager will be able to eat it. So in addition to the immediate reward from eating the future expected reward of moving left is one half. In contrast when we look at going right we see that no matter where the food spawns there is a move that will allow the forager to eat (also a move where the forager will not eat).  If we assume that the forager will eat the food adjacent to it on the second round, then the expected future reward when moving right is one.\n",
    "\n",
    "In the decision tree above we have broken up the state transition into two parts, one where the forager moves, and another where the environment reacts and regenerates the food. From the perspective of the organism these things can be collapsed, since it doesn't choose actions at these 'intermediate' states. Then our game tree looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the decision tree without 'intermediate' states.\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down',\n",
    "        '0': '0.5',\n",
    "        '1': '0.5',\n",
    "        '2': '0.5',\n",
    "        '3': '0.5'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[1, 0, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R0\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R3\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L3\": {\n",
    "        'pieces': np.array([[[1, 0, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L3R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L2\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L3\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1R\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R3L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R\": {\n",
    "        'pieces': np.array([[[-1, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R2\": {\n",
    "        'pieces': np.array([[[-1, 0, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "state_nums = {\"init_state\": '0',\n",
    "              \"state_L1\": '1',\n",
    "              \"state_L3\": '2',\n",
    "              \"state_R1\": '3',\n",
    "              \"state_R3\": '4',\n",
    "              \"state_L1R0\": '5',\n",
    "              \"state_L1R3\": '6',\n",
    "              \"state_L3R\": '7',\n",
    "              \"state_R1L2\": '8',\n",
    "              \"state_R1L3\": '9',\n",
    "              \"state_R1R\": '10',\n",
    "              \"state_R3L\": '11',\n",
    "              \"state_R3R1\": '12',\n",
    "              \"state_R3R2\": '13',\n",
    "             }\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      intermediate_nodes=[\"state_L\", \"state_R\",\n",
    "                                                          \"state_L1R\",\n",
    "                                                          \"state_R1L\",\n",
    "                                                          \"state_R3R\"],\n",
    "                                      state_nums=state_nums,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      dpi=600,\n",
    "                                      gwg_fontsize=16,\n",
    "                                      gwg_figsize=(1.5,1.5))\n",
    "Image(output_path, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Squinting at these diagrams, we can see that taking a 'right' on the first move is the way to go. This is not because going right gives food immediately, both moves do that, but because it gives the forager more flexibility in responding to the where new food regenerates. After moving right, regardless of where food regenerates the forager has an action that allows it to eat again. This illustrates a core principle in many Markov Decision Processes (MDPs): an action can be good both because it secures immediate reward, but also because it sets the organism up for future gains. Often there is a tradeoff to be made between immediate and future gains. The concept of *value* encapsulates the potential for future rewards from any given state under a specific policy, and is used.\n",
    "\n",
    "Note when using using the term *value* it will typically be clear from context whether this is meant in the specific and technical meaning of Markov Decision Processes and Reinforcement learning or in the more standard general usage, e.g. \"The value of $x$ in this case is three.\" If there is some ambiguity we will use italics to indicate that the precise MDP meaning applies.\n",
    "\n",
    "To get a sense of what *value* is, let's compute the *value* of each of the states in this game tree. Since the game is over, the *value* of the leaf nodes (states 5 through 13) is zero, there are no more rewards coming! In math symbols, $v_\\pi(s_i) = 0$ for $i \\in \\{5,6,7,8,9,10,11,12,13\\}$. Note that this is true for all terminal states of a process and any policy, $\\pi$.\n",
    "\n",
    "The *value* of state $s_1$ is one, since the forager can only do one thing (move right), and the outcome always results in a reward being obtained, and ending up in either state $s_5$ or $s_6$, both of which have *value* zero, as terminal states.\n",
    "\n",
    "More formally, the *value* of a state is the expectation of the immediate reward that will be obtained combined with the expected *value* of the the next state that is transitioned to, given the probability distribution over the actions that the organism takes in that state according to their policy. That is\n",
    "$$\n",
    "v_\\pi(s)= \\sum_{a \\in \\mathcal{A}(s)}\\left[\\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s, a) \\right]\n",
    "$$\n",
    "Here $\\pi(a|s)$ gives the probability of taking a given action, $a$, in a given state, $s$, and $p(s',r|s,a)$ gives the probability of transitioning from state $s$ to state $s'$ and recieving reward $r$, given that action $a$ is chosen.\n",
    "\n",
    "Working from this definition of value we have\n",
    "$$\\begin{align}\n",
    "v_\\pi(s_1) &= \\sum_{a \\in \\mathcal{A}(s_1)}\\left[\\pi(a|s_1) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s_1, a) \\right] \\\\\n",
    " &= \\pi(\\text{Right}|s_1) \\left[ (1 + v_\\pi(s_5))0.5 + (1+v_\\pi(s_6))0.5) \\right]\n",
    " \\end{align}$$\n",
    "\n",
    "The various transition probabilities can be looked up in the decision tree above. Regardless of what the policy is, 'Right' is the only possible move in state $s_1$ so $\\pi(\\text{Right}|s_1)=1$. Then $v_\\pi(s_1) = 1$\n",
    "\n",
    "The *value* of state 2 is zero since again the forager can only do one thing (move right) but from this state they will not obtain any reward, and the next state after this, $s_7$, has a value of zero. See if you can work from the definition of value, to obtain $v_\\pi(s_2)=0$.\n",
    "$$v_\\pi(s_2)= \\sum_{a \\in \\mathcal{A}(s_1)}\\left[\\pi(a|s_2) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s, a) \\right]$$\n",
    "\n",
    "The Value of state 3 is a little more complex, because it depends on what the forager does. In states $s_1$ and $s_2$ the forager could only move right, but here in state $s_3$ the forager can go both right or left, so the value of the state depends on what the forager will do. It depends on the foragers' policy. Working from the definition\n",
    "$$ \\begin{align}\n",
    "v_\\pi(s_3) = & \\ \\pi(\\text{Left} \\mid s_3) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s_3, \\text{Left}) \\ + \\\\\n",
    "& \\ \\pi(\\text{Right} \\mid s_3) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s_3, \\text{Right})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If this forager is using the 'Random Valid' policy, $\\pi_\\text{RV}$, i.e. it moves with left or right with equal probability, then:\n",
    "$$\n",
    "v_{\\pi_\\text{RV}}(s_3) = 0.5 (0.5(1+0) + 0.5(1+0)) + 0.5 (0 + 0) = 0.5\n",
    "$$\n",
    "\n",
    "But, if the forager is playing the 'Eat Nearby Food' policy, $\\pi_\\text{EN}$, i.e. it moves to immediately adjacent food if possible, chooses between multiple adjacent food options with uniform probability, and chooses from multiple non-food options, when no food is immediately adjacent, with uniform probability. In this case the value will is.\n",
    "\n",
    "$$ v_{\\pi_\\text{EN}}(s_3) = 1 (0.5(1 + 0) + 0.5(1+0)) + 0 (0+0)) = 1$$\n",
    "\n",
    "The value of state 4, like that of state 3 is dependent on the policy, and can be calculated in much the same way with $ v_{\\pi_\\text{EN}}(s_4) = 1$, but $ v_{\\pi_\\text{RV}}(s_4) = 0.5$.\n",
    "\n",
    "This is a key aspect of value, it depends both on the **state**, $s$ and the **policy**, $\\pi$. This is because it is measure of how well the policy will exploit the opportunities for future reward afforded by that state going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We use $v_*(s)$ to denote the *value* of a state under the (an) optimal policy, and $\\pi_*$ to denote the (an) optimal policy.\n",
    "\n",
    "If the goal is to maximize expected reward, and $v_*(s)$ is the expected reward going forward from a state, then an optimal policy $\\pi_*$ is one that will choose an action at each state that produces the greatest combined expected reward and value, i.e.\n",
    "$$\\pi_*(s) = \\underset{a \\in \\mathcal{A}(s)}{\\arg\\max} \\; \\mathbb{E} \\left[ R(s,a) \\mid s, a \\right] + \\mathbb{E} \\left[ v_*(s')  \\mid s, a\\right]$$\n",
    "\n",
    "Given this definition of the optimal policy, and our short duration (2 rounds!) foraging episode, with it's fixed starting positions, we can describe the optimal policy as follows.\n",
    "\n",
    "\"Go right on the first move, then move to where the food is on the second move.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.3 Being the very best when the world is uncertain, and duration is uncertain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the problem above, even though there was some uncertainty about where new food would respawn, because the duration of the foraging session was so short (2 rounds!), it was relatively easy to enumerate all possible trajectories in the decision tree, and reason about what the optimal policy was. We used the concept of *value* to quantify and formalize our intuitions about why moving right at first was better than moving left. Right was better, not because the immediate reward was greater, but because the expected future reward was greater. However, *value* wasn't really necessary to find the optimal policy. Now, we're going to play almost the same game again, but with slightly different rules. Although there is a finite maximum number of rounds $50$, there is also a fixed probability, $0.2$, that the session will end on each round. So on average there will be five rounds per session though the actual number will be variable from run to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Small Gridworld, food regenerates and end time is unpredictable.\n",
    "# @markdown Run this cell to try and eat as much food as possible before the foraging session ends.\n",
    "init_state = {\n",
    "  'pieces': np.array([[[ -1, 1, -2, 0,]]], dtype=int),\n",
    "  'scores': np.array([[0]]),\n",
    "  'rounds_left': np.array([50]),\n",
    "  'is_over': np.array([0])\n",
    "}\n",
    "gwg3 = GridworldGame(batch_size=1,\n",
    "                    n_rows=1, n_cols=4,\n",
    "                    num_foragers=1,\n",
    "                    num_predators=0,\n",
    "                    max_rounds_taken=50,\n",
    "                    end_prob=0.2,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=2/3,\n",
    "                    food_forager_regen=True,\n",
    "                    init_board_state=init_state,)\n",
    "\n",
    "igwg3 = InteractiveGridworld(\n",
    "    gwg3, players=['human'], critter_names=['Critter (You)'],\n",
    "    figsize=(5,4), has_fov=False, final_score_type='normalized')\n",
    "display(igwg3.b_fig.canvas)\n",
    "clear_output()\n",
    "display(igwg3.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's be clear, even in this very simple game, drawing out the game tree is not a practical solution. Even though there is only a small probability of the game lasting the full 50 rounds (roughly a 1 in 70 thousand chance), these unlikely possibilities still need to be considered by the truly optimal strategy. Ironically, assuming that the game has no hard end (i.e. has an infinite decision tree), allows us to simplify and find the optimal solution. This is a bit like magic so watch carefully.\n",
    "\n",
    "First we'll take a look at the game tree from our last problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the decision tree from our short problem.\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down',\n",
    "        '0': '0.5',\n",
    "        '1': '0.5',\n",
    "        '2': '0.5',\n",
    "        '3': '0.5'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[1, 0, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R0\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R3\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L3\": {\n",
    "        'pieces': np.array([[[1, 0, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L3R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L2\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L3\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1R\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R3L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R\": {\n",
    "        'pieces': np.array([[[-1, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R2\": {\n",
    "        'pieces': np.array([[[-1, 0, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "state_nums = {\"init_state\": '0',\n",
    "              \"state_L1\": '1',\n",
    "              \"state_L3\": '2',\n",
    "              \"state_R1\": '3',\n",
    "              \"state_R3\": '4',\n",
    "              \"state_L1R0\": '5',\n",
    "              \"state_L1R3\": '6',\n",
    "              \"state_L3R\": '7',\n",
    "              \"state_R1L2\": '8',\n",
    "              \"state_R1L3\": '9',\n",
    "              \"state_R1R\": '10',\n",
    "              \"state_R3L\": '11',\n",
    "              \"state_R3R1\": '12',\n",
    "              \"state_R3R2\": '13',\n",
    "             }\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      intermediate_nodes=[\"state_L\", \"state_R\",\n",
    "                                                          \"state_L1R\",\n",
    "                                                          \"state_R1L\",\n",
    "                                                          \"state_R3R\"],\n",
    "                                      state_nums=state_nums,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      dpi=600,\n",
    "                                      gwg_fontsize=16,\n",
    "                                      gwg_figsize=(1.5,1.5))\n",
    "Image(output_path, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The first thing to notice here, is that states $s_5$ and $s_8$ are identical, so if we were to expand this game tree further, whatever expansion we figured out for $s_5$ could simply be copied and 'grafted' onto $s_8$.\n",
    "\n",
    "The second thing to note is that $s_0$ is almost identical to $s_5$ and $s_8$, the only difference being that in $s_0$ there are still two rounds of play left, whereas in $s_5$ and $s_8$ there are no rounds of play left. Note that 'score' is not a part of state per se, but something that is tracked seperately. This is very much on purpose, because score (or more formely accumulated reward in this case), should not have any bearing on current decisions, current decisions are focused purely on how to get more reward going forward.\n",
    "\n",
    "(Comment Footnote: Not doing this is such an important fallacy in decision making that it has a name, \"The Concord Fallacy\", which is the idea that just because you started on a course of action you should stick with it, \"We'll we've already spent 10 Billion on this project we wouldn't want that to be wasted so let's spend another 30 Billion to make sure that that 10 Billion isn't a waste\" This notion is also captured in the idion \"Throwing good money after bad\". Though it should be noted that this fallacy stems from an often very useful heuristic, that it is often better to \"stay the course\" and commit to a chosen course of action for a time, as constantly waffling and changing strategy will almost certainly lead to wasted effort.)\n",
    "(Comment Box about score versus reward here, like the winner of a game is the one with the highest score at the end, then scores of the players is something that should be incorporated into state, and reward, is likely either a simple +/- 1 at the end of the game)\n",
    "\n",
    "Simiarly $s_9$ and $s_{11}$ are identical (except for score) as are $s_{10}$ and $s_{12}$. So again, if we were going to expand the game tree further, the expansion from $s_9$ could be applied to $s_{11}$ (less one score point) and the expansion from $s_{10}$ could be applied to $s_{12}$ (with one score point added). Now because choice of action doesn't depend on past score, this means that these states have the same *value*, i.e. $v_\\pi(s_5) = v_\\pi(s_8)$ and $v_\\pi(s_9) = v_\\pi(s_{11})$ and $v_\\pi(s_{10}) = v_\\pi(s_{12})$. This will be true no matter how far we expand the game tree out, and no matter what policy is being used.\n",
    "\n",
    "We aren't going to keep expanding this game tree, instead, we're going to do something a little different.\n",
    "\n",
    "Let's focus on the difference between $s_0$ and $s_5$ in the tree above. The scores are different, but we have already decided that that is not important for further expansion of the tree. However what is important is that in $s_0$ there are two rounds of play left whereas in $s_5$ there are none. This is because we are playing with a fixed, finite duration, which will always make states that are identical in terms of board position, distinct in terms of how many rounds are left to play. However, in a Markov Decision Process where there is no fixed end time, and instead the processes ends with some fixed, constant probability, the number of rounds that have passed since the start of the episode cease to be a factor. All states become identical with regard to their probability of ending. In an infinite tree, the expansion and play from $s_0$ becomes identical to the expansion and play from $s_5$. Having an infinite time horizon (and constant end probability) allows us to ignore time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Freed from the constraints of time, in this simple scenario there are 12 possible board states, and the transition dynamics between them can be visualized like this\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the decision tree from our short problem.\n",
    "\n",
    "def parse_transitions(state_names):\n",
    "  intermediate_states = []\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  # Directions a critter can move\n",
    "  directions = {'Left': -1, 'Right': 1}\n",
    "\n",
    "  for state in state_names:\n",
    "    board = state[5:]  # Extract board configuration from state name\n",
    "    critter_pos = board.find('C')\n",
    "\n",
    "    for move, offset in directions.items():\n",
    "      # Calculate new position after move\n",
    "      new_pos = critter_pos + offset\n",
    "      # Ensure the new position is within bounds\n",
    "      if 0 <= new_pos < len(board):\n",
    "        intermediate_board = list(board)\n",
    "        # Move critter\n",
    "        intermediate_board[critter_pos] = 'N'\n",
    "        intermediate_board[new_pos] = 'C'\n",
    "        intermediate_state = 'state' + ''.join(intermediate_board)\n",
    "        # Append edges and labels\n",
    "        if intermediate_state in state_names:\n",
    "          edges.append((state, intermediate_state))\n",
    "          edge_labels.append((state, intermediate_state, move))\n",
    "        else:\n",
    "          intermediate_state = state + '_' + intermediate_state\n",
    "          edges.append((state, intermediate_state))\n",
    "          edge_labels.append((state, intermediate_state, move))\n",
    "          if board[new_pos] == 'F':\n",
    "            if intermediate_state not in intermediate_states:\n",
    "              intermediate_states.append(intermediate_state)\n",
    "            for i, cell in enumerate(intermediate_board):\n",
    "              if cell == 'N':\n",
    "                regen_board = copy.deepcopy(intermediate_board)\n",
    "                regen_board[i] = 'F'\n",
    "                final_state = 'state' + ''.join(regen_board)\n",
    "                edges.append((intermediate_state, final_state))\n",
    "                edge_labels.append((intermediate_state, final_state, '0.5'))\n",
    "\n",
    "  return list(set(intermediate_states)), list(set(edges)), list(set(edge_labels))\n",
    "\n",
    "state_nodes = {\n",
    "    \"stateCFFN\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateCFNF\": {\n",
    "        'pieces': np.array([[[1, -1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateCNFF\": {\n",
    "        'pieces': np.array([[[1, 0, -1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFCFN\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFCNF\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateNCFF\": {\n",
    "        'pieces': np.array([[[0, 1, -1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFFCN\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFNCF\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateNFCF\": {\n",
    "        'pieces': np.array([[[0, -1, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFFNC\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFNFC\": {\n",
    "        'pieces': np.array([[[-1, 0, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateNFFC\": {\n",
    "        'pieces': np.array([[[0, -1, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "}\n",
    "\n",
    "# Parse transitions based on game logic\n",
    "intermediate_states, edges, edge_labels = parse_transitions(state_nodes)\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "state_nums = {\"stateFCFN\": '0',\n",
    "              \"stateCFFN\": '2',\n",
    "              \"stateCNFF\": '3',\n",
    "              \"state_R3\": '4',\n",
    "              \"state_L1R0\": '5',\n",
    "              \"state_L1R3\": '6',\n",
    "              \"state_L3R\": '7',\n",
    "              \"state_R1L2\": '8',\n",
    "              \"state_R1L3\": '9',\n",
    "              \"state_R1R\": '10',\n",
    "              \"state_R3L\": '11',\n",
    "              \"state_R3R1\": '12',\n",
    "             }\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "intermediate_nodes, edges, edge_labels = parse_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=2/4,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['stateFCFN'])\n",
    "\n",
    "def make_gw_png(gwg, state, state_num, file_path, dpi, figsize, fontsize, plot_score):\n",
    "  if plot_score:\n",
    "    if state_num is None:\n",
    "      title = \"Score: \"+str(state['scores'][0][0])\n",
    "    else:\n",
    "      title = \"State: \"+ state_num + \"\\nScore: \"+str(state['scores'][0][0])\n",
    "  else:\n",
    "    if state_num is None:\n",
    "      title = \"\"\n",
    "    else:\n",
    "      title = \"State: \"+ state_num\n",
    "  fig, ax, critter_specs, food, fov = gwg.plot_board(state, legend_type=None,\n",
    "                                                     figsize=figsize)\n",
    "  ax.set_title(title, fontsize=fontsize)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "That's a bit of a visual salad. The transition probabilities can also be visualized as a matrix, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize transition probabilities as a matrix.\n",
    "\n",
    "\n",
    "def make_transition_matrix(state_names):\n",
    "  transition_probs = np.empty((num_states, num_states), dtype=object)\n",
    "  transition_probs.fill('0')\n",
    "  transition_rewards = np.empty((num_states, num_states), dtype=object)\n",
    "  transition_rewards.fill('0')\n",
    "  # Directions a critter can move\n",
    "  directions = {'L': -1, 'R': 1}\n",
    "\n",
    "  for ii, state in enumerate(state_names):\n",
    "    board = state[5:]  # Extract board configuration from state name\n",
    "    critter_pos = board.find('C')\n",
    "\n",
    "    for move, offset in directions.items():\n",
    "      # Calculate new position after move\n",
    "      new_pos = critter_pos + offset\n",
    "      # Ensure the new position is within bounds\n",
    "      if 0 <= new_pos < len(board):\n",
    "        intermediate_board = list(board)\n",
    "        # Move critter\n",
    "        intermediate_board[critter_pos] = 'N'\n",
    "        intermediate_board[new_pos] = 'C'\n",
    "        intermediate_state = 'state' + ''.join(intermediate_board)\n",
    "        # Append edges and labels\n",
    "        if intermediate_state in state_names:\n",
    "          prob_str = f\"$\\\\pi({move})$\"\n",
    "          to_index = state_names.index(intermediate_state)\n",
    "          transition_probs[ii, to_index] = prob_str\n",
    "        else:\n",
    "          intermediate_state = state + '_' + intermediate_state\n",
    "          if board[new_pos] == 'F':\n",
    "            # eating happened\n",
    "            for jj, cell in enumerate(intermediate_board):\n",
    "              if cell == 'N':\n",
    "                regen_board = copy.deepcopy(intermediate_board)\n",
    "                regen_board[jj] = 'F'\n",
    "                next_state = 'state' + ''.join(regen_board)\n",
    "                prob_str = f\"$\\\\frac{{\\\\pi({move})}}{{2}}$\"\n",
    "                to_index = state_names.index(next_state)\n",
    "                transition_probs[ii, to_index] =  prob_str\n",
    "                transition_rewards[ii, to_index] = '1'\n",
    "\n",
    "  return transition_probs, transition_rewards\n",
    "\n",
    "state_nodes = {\n",
    "    \"stateCFFN\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateCFNF\": {\n",
    "        'pieces': np.array([[[1, -1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateCNFF\": {\n",
    "        'pieces': np.array([[[1, 0, -1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFCFN\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFCNF\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateNCFF\": {\n",
    "        'pieces': np.array([[[0, 1, -1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFFCN\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFNCF\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateNFCF\": {\n",
    "        'pieces': np.array([[[0, -1, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFFNC\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateFNFC\": {\n",
    "        'pieces': np.array([[[-1, 0, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"stateNFFC\": {\n",
    "        'pieces': np.array([[[0, -1, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "num_states = len(state_names)\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "transition_probs, transition_rewards = make_transition_matrix(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=2/4,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['stateFCFN'])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.suptitle('State Transition Probabilities')\n",
    "# Grid\n",
    "\n",
    "gs = gridspec.GridSpec(num_states + 1, num_states + 1)\n",
    "\n",
    "def make_image_state_label(fig, ax, gwg, state):\n",
    "  fig, ax, critter_specs, food, fov = gwg.plot_board(state, legend_type=None,\n",
    "                                                       fig=fig, ax=ax)\n",
    "  ax.axis('off')\n",
    "  #ax.set_aspect('equal')\n",
    "def make_string_state_label(ax, state):\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig.add_subplot(gs[0, idx + 1])\n",
    "  #make_image_state_label(fig, ax, gwg, state_nodes[state])  # Column labels\n",
    "  make_string_state_label(ax, state)\n",
    "  ax = fig.add_subplot(gs[idx + 1, 0])\n",
    "  make_string_state_label(ax, state)\n",
    "  #make_image_state_label(fig, ax, gwg, state_nodes[state])  # Row labels\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col in range(num_states):\n",
    "    ax = fig.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, transition_probs[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This is potentially overwhelming in a different way. This chart is read as follows. It gives all the probabilites of transitioning from one state to another, given the policy (the dependence of $\\pi(R)$ and $\\pi(L)$ is left implicit to keep the chart legible. The states are represented by strings where N means the cell is empty, F means the cell has food in it, and C means the critter is in that grid cell. The element in the first row (CFFN) and the fourth column (FCFN) gives the probability of transitioning from the row state (CFFN) to the row state (FCFN) as $\\frac{\\pi(R)}{2}$, i.e. the probability that the organism goes right given its policy and that the food regenerates in the grid cell the organism just left. Because some kind of transition must happen each round, the value of the rows must always sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize immediate reward associated with each transition.\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.suptitle('State Transition Rewards')\n",
    "# Grid\n",
    "\n",
    "gs = gridspec.GridSpec(num_states + 1, num_states + 1)\n",
    "\n",
    "def make_image_state_label(fig, ax, gwg, state):\n",
    "  fig, ax, critter_specs, food, fov = gwg.plot_board(state, legend_type=None,\n",
    "                                                       fig=fig, ax=ax)\n",
    "  ax.axis('off')\n",
    "  #ax.set_aspect('equal')\n",
    "def make_string_state_label(ax, state):\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig.add_subplot(gs[0, idx + 1])\n",
    "  #make_image_state_label(fig, ax, gwg, state_nodes[state])  # Column labels\n",
    "  make_string_state_label(ax, state)\n",
    "  ax = fig.add_subplot(gs[idx + 1, 0])\n",
    "  make_string_state_label(ax, state)\n",
    "  #make_image_state_label(fig, ax, gwg, state_nodes[state])  # Row labels\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col in range(num_states):\n",
    "    ax = fig.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, transition_rewards[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Great, we have two big grids of numbers. How does this help us determine the opitmal policy? It's doesn't directly, but it get's us close to setting up a recurrence relation between the *value* the states. Recall our definition of value\n",
    "\n",
    "$$\n",
    "v_\\pi(s)= \\sum_{a \\in \\mathcal{A}(s)}\\left[\\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s, a) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's call our grid of transition probabilities $\\mathbf{P}_\\pi$\n",
    "$$ \\mathbf{P}_\\pi =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\frac{\\pi(R)}{2} & 0 & \\frac{\\pi(R)}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\frac{\\pi(R)}{2} & \\frac{\\pi(R)}{2} & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & \\pi(R) & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "\\frac{\\pi(L)}{2} & 0 & \\frac{\\pi(L)}{2} & 0 & 0 & 0 & \\frac{\\pi(R)}{2} & \\frac{\\pi(R)}{2} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & \\frac{\\pi(L)}{2} & \\frac{\\pi(L)}{2} & 0 & 0 & 0 & 0 & \\pi(R) & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & \\pi(L) & 0 & 0 & 0 & 0 & \\frac{\\pi(R)}{2} & \\frac{\\pi(R)}{2} & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & \\frac{\\pi(L)}{2} & \\frac{\\pi(L)}{2} & 0 & 0 & 0 & 0 & \\pi(R) & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\pi(L) & 0 & 0 & 0 & 0 & \\frac{\\pi(R)}{2} & \\frac{\\pi(R)}{2} & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\frac{\\pi(L)}{2} & \\frac{\\pi(L)}{2} & 0 & 0 & 0 & \\frac{\\pi(R)}{2} & 0 & \\frac{\\pi(R)}{2} \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\pi(L) & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\frac{\\pi(L)}{2} & \\frac{\\pi(L)}{2} & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & \\frac{\\pi(L)}{2} & 0 & \\frac{\\pi(L)}{2} & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "And, let's call our grid of transition rewards $\\mathbf{R}$\n",
    "$$ \\mathbf{R} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Then the immediate expected reward in a given state, when using policy $\\pi$ is given by\n",
    "\n",
    "$$ \\mathbf{r}_\\pi = \\left(\\mathbf{R} \\circ \\mathbf{P}_\\pi\\right)\\mathbf{1}$$\n",
    "\n",
    "Here $\\circ$ is elementwise multiplication (Hadamard product), and \\mathbf{1}, is column vector of ones, (so that multiplication by $\\mathbf{1}$ causes a summation over the elementwise matrix product).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Then letting $\\mathbf{v}_\\pi$ denote the vector of state values under the policy $\\pi$ we have by the definition of *value* that\n",
    "\n",
    "$$ \\mathbf{v}_\\pi = \\mathbf{r} + \\mathbf{P}_\\pi \\mathbf{v}_\\pi $$\n",
    "\n",
    "Trying to solve this equation will result in the value of each state being infinite (for all but the most pathological policies). This makes sense. If the process is allowed to continue indefinately, and negative rewards are never possible, and postive (non-vanishing) rewards are always possible, the expected total reward from any state will diverge to positive inifinity. However, in our problem, even though the process can continue for a long time, there is a probability on each round that the process will end. If the process ends, then the future rewards will have no value, this means that the value of the next state should be *discounted* by the probability of the process continuing, since the future looking *value* of a state is only relevant if the process continues. This gives us the slightly modified equation\n",
    "\n",
    "$$ \\mathbf{v}_\\pi = \\mathbf{r} + \\gamma \\mathbf{P}_\\pi \\mathbf{v}_\\pi $$\n",
    "\n",
    "For our particular problem $\\gamma=0.8$ which is the probability of the process not stopping after a given round.\n",
    "\n",
    "In words this equation is saying that the *value* of a state is equal to the immediate expected rewards given the policy plus the expected *value* of the nexts state discounted by the probability that the process continues.\n",
    "\n",
    "This gives a system of 12 equations with 12 unknowns which can then be solved using linear algebra, for any given policy. But what does a given policy look like. For a small problem like this, with only 12 states and 2 actions, we can completely describe the policy with 12 x 2 table of values, giving the probability of taking each action in each state. For the random valid policy this would look like this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the Random Valid policy as a table.\n",
    "\n",
    "pi_random_valid_tab = np.array([\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "])\n",
    "\n",
    "num_actions = pi_random_valid_tab.shape[1]\n",
    "fig_pi_0 = plt.figure(figsize=(4, 8))\n",
    "fig_pi_0.suptitle('Random Valid Policy - Tabular')\n",
    "# Grid\n",
    "\n",
    "gs = gridspec.GridSpec(num_states + 1, num_actions + 1)\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_pi_0.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "for idx, action in enumerate(['Left', 'Right']):\n",
    "  ax = fig_pi_0.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, action, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col in range(num_actions):\n",
    "    ax = fig_pi_0.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, pi_random_valid_tab[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "So given this policy we can compute the value of each state, under the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "pi_random_valid_tab = np.array([\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [0.5, 0.5],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "])\n",
    "\n",
    "def make_transition_probs(pi):\n",
    "  pi_L = pi[:, 0]  # Probabilities of going left for all states\n",
    "  pi_R = pi[:, 1]  # Probabilities of going right for all states\n",
    "\n",
    "  # Transition Probabilities give pi\n",
    "  P_pi = np.array([\n",
    "    [0, 0, 0, pi_R[0]/2, 0, pi_R[0]/2, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, pi_R[1]/2, pi_R[1]/2, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, pi_R[2], 0, 0, 0, 0, 0, 0],\n",
    "    [pi_L[3]/2, 0, pi_L[3]/2, 0, 0, 0, pi_R[3]/2, pi_R[3]/2, 0, 0, 0, 0],\n",
    "    [0, pi_L[4]/2, pi_L[4]/2, 0, 0, 0, 0, pi_R[4], 0, 0, 0, 0],\n",
    "    [0, 0, pi_L[5], 0, 0, 0, 0, pi_R[5]/2, pi_R[5]/2, 0, 0, 0],\n",
    "    [0, 0, 0, pi_L[6]/2, pi_L[6]/2, 0, 0, 0, 0, pi_R[6], 0, 0],\n",
    "    [0, 0, 0, 0, pi_L[7], 0, 0, 0, 0, pi_R[7]/2, pi_R[7]/2, 0],\n",
    "    [0, 0, 0, 0, pi_L[8]/2, pi_L[8]/2, 0, 0, 0, pi_R[8]/2, 0, pi_R[8]/2],\n",
    "    [0, 0, 0, 0, 0, 0, pi_L[9], 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, pi_L[10]/2, pi_L[10]/2, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, pi_L[11]/2, 0, pi_L[11]/2, 0, 0, 0],\n",
    "  ])\n",
    "  return P_pi\n",
    "\n",
    "P_pi = make_transition_probs(pi_random_valid_tab)\n",
    "\n",
    "R = np.array([\n",
    "  [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "  [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "  [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "  [0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1],\n",
    "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
    "  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
    "], dtype=float)\n",
    "\n",
    "gamma = 0.8\n",
    "\n",
    "def compute_v_pi(P_pi, R, gamma):\n",
    "  # Calculate expected immediate rewards given policy Ï€\n",
    "  r_pi = np.sum(R * P_pi, axis=1, keepdims=True)\n",
    "  I = np.eye(P_pi.shape[0])\n",
    "  # v_pi = r + gamma * P_pi @ v_pi\n",
    "  # (subtract gamma * P_pi @ v_pi from both sides)\n",
    "  # (I - gamma * P_pi) @ v_pi = r\n",
    "  # left multiply by (I - gamma * P_pi)^{-1} on both sides\n",
    "  # v_pi = (I - gamma * P_pi)^{-1} @ r\n",
    "  # use numpy's linear algegra to get the inverse\n",
    "  v_pi = np.linalg.inv(I - gamma * P_pi) @ r_pi\n",
    "  return v_pi\n",
    "\n",
    "v_pi = compute_v_pi(P_pi, R, gamma)\n",
    "fig_v0 = plt.figure(figsize=(4, 8))\n",
    "fig_v0.suptitle('Random Valid Policy - Tabular')\n",
    "# Grid\n",
    "gs = gridspec.GridSpec(num_states + 1, 2)\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_v0.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "col_names = ['Value']\n",
    "for idx, col_name in enumerate(col_names):\n",
    "  ax = fig_v0.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, col_name, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col, name in enumerate(col_names):\n",
    "    ax = fig_v0.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, v_pi[row, 0], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(v_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMfe9Qehb6dk"
   },
   "source": [
    "We can use these state *values* under the random variable policy to develop an improved policy. We do this by having a policy which takes the action with greatest combined expected immediate reward plus expected $v_{\\pi_{RV}}$ value. Note that the values we are using to inform this new policy, call it $\\pi_{RV'}$, are not the true values of this new policy, they are the values of the previous policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "pi_left_when_possible = np.array([\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "])\n",
    "\n",
    "pi_right_when_possible = np.array([\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [0.0, 1.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "  [1.0, 0.0],\n",
    "])\n",
    "\n",
    "P_left = make_transition_probs(pi_left_when_possible)\n",
    "P_right = make_transition_probs(pi_right_when_possible)\n",
    "P_as = [P_left, P_right]\n",
    "\n",
    "r_left = np.sum((R * P_left), axis=1, keepdims=True)\n",
    "r_right = np.sum((R * P_right), axis=1, keepdims=True)\n",
    "rs = np.hstack([r_left, r_right])\n",
    "\n",
    "fig_rewards = plt.figure(figsize=(12, 8))\n",
    "fig_rewards.suptitle('Random Valid Policy - Expected Reward')\n",
    "\n",
    "# Grid\n",
    "gs = gridspec.GridSpec(num_states + 1, 3)\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_rewards.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "col_names = ['R(Left)', 'R(Right)']\n",
    "for idx, col_name in enumerate(col_names):\n",
    "  ax = fig_rewards.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, col_name, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "# Fill in each cell\n",
    "for row in range(num_states):\n",
    "  for col, name in enumerate(col_names):\n",
    "    ax = fig_rewards.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, rs[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "fig_ev = plt.figure(figsize=(12, 8))\n",
    "fig_ev.suptitle('Random Valid Policy - Expected Value')\n",
    "\n",
    "ev = np.hstack([P_a @ v_pi * gamma for P_a in P_as])\n",
    "\n",
    "# Grid\n",
    "gs = gridspec.GridSpec(num_states + 1, 3)\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_ev.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "col_names = ['Discounted Expected Value (Left)', 'Discounted Expected Value (Right)']\n",
    "for idx, col_name in enumerate(col_names):\n",
    "  ax = fig_ev.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, col_name, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "# Fill in each cell\n",
    "for row in range(num_states):\n",
    "  for col, name in enumerate(col_names):\n",
    "    ax = fig_ev.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, ev[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def compute_Q_approx(P_as, rs, V_approx, gamma):\n",
    "  # P_a is a list of the transition matrices under different actions,\n",
    "  # length of this list corresponds to the number of actions\n",
    "  # in some states only one action is legal, this is assumed to be incorporated\n",
    "  # already into the transition matrices in P_a\n",
    "  num_states = V_approx.shape[0]\n",
    "  num_actions = len(P_as)\n",
    "\n",
    "  # Initialize Q matrix\n",
    "  Q_approx = np.zeros((num_states, num_actions))\n",
    "\n",
    "  for a, P_a in enumerate(P_as):\n",
    "    Q_approx[:, a] = (rs[:,a].reshape(-1, 1) + (P_a @ V_approx) * gamma).flatten()\n",
    "  return Q_approx\n",
    "\n",
    "Q_approx = compute_Q_approx(P_as, rs, v_pi, 0.8)\n",
    "\n",
    "fig_Q = plt.figure(figsize=(12, 8))\n",
    "fig_Q.suptitle('Random Valid Policy - Q function')\n",
    "# Grid\n",
    "gs = gridspec.GridSpec(num_states + 1, 3)\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_Q.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "col_names = ['Q(Left)', 'Q(Right)']\n",
    "for idx, col_name in enumerate(col_names):\n",
    "  ax = fig_Q.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, col_name, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col, name in enumerate(col_names):\n",
    "    ax = fig_Q.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, Q_approx[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def make_greedy_Q_policy(Q):\n",
    "  # For each state, find the action with the maximum Q value\n",
    "  # np.argmax will return the index of the highest value in each row (across columns)\n",
    "  best_actions = np.argmax(Q, axis=1)\n",
    "  # Initialize the greedy policy matrix\n",
    "  num_states, num_actions = Q.shape\n",
    "  pi_greedy = np.zeros((num_states, num_actions))\n",
    "\n",
    "  # Set the action with the maximum Q value to 1 in the policy matrix\n",
    "  for s, a in enumerate(best_actions):\n",
    "    pi_greedy[s, a] = 1.0\n",
    "  #enforce legal moves\n",
    "  pi_greedy[:3, 0] = 0.0\n",
    "  pi_greedy[:3, 1] = 1.0\n",
    "  pi_greedy[-3:, 0] = 1.0\n",
    "  pi_greedy[-3:, 1] = 0.0\n",
    "  return pi_greedy\n",
    "pi_greedy1 = make_greedy_Q_policy(Q_approx)\n",
    "\n",
    "num_actions = pi_greedy1.shape[1]\n",
    "fig_pi_1 = plt.figure(figsize=(4, 8))\n",
    "fig_pi_1.suptitle('Greedy Q1 Policy - Tabular')\n",
    "# Grid\n",
    "\n",
    "gs = gridspec.GridSpec(num_states + 1, num_actions + 1)\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_pi_1.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "for idx, action in enumerate(['Left', 'Right']):\n",
    "  ax = fig_pi_1.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, action, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col in range(num_actions):\n",
    "    ax = fig_pi_1.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, pi_greedy1[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "P_greedy1 = make_transition_probs(pi_greedy1)\n",
    "\n",
    "v_pi_greedy1 = compute_v_pi(P_greedy1, R, gamma)\n",
    "fig_v1 = plt.figure(figsize=(4, 8))\n",
    "fig_v1.suptitle('Greedy 1 Policy - Values')\n",
    "# Grid\n",
    "gs = gridspec.GridSpec(num_states + 1, 2)\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_v1.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "col_names = ['Value']\n",
    "for idx, col_name in enumerate(col_names):\n",
    "  ax = fig_v1.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, col_name, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col, name in enumerate(col_names):\n",
    "    ax = fig_v1.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, v_pi_greedy1[row, 0], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(v_pi_greedy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "Q2_approx = compute_Q_approx(P_as, rs, v_pi_greedy1, 0.8)\n",
    "\n",
    "fig_Q2 = plt.figure(figsize=(12, 8))\n",
    "fig_Q2.suptitle('Random Valid Policy - Q function')\n",
    "# Grid\n",
    "gs = gridspec.GridSpec(num_states + 1, 3)\n",
    "\n",
    "# Place state labels as row and column headers\n",
    "for idx, state in enumerate(state_names):\n",
    "  ax = fig_Q2.add_subplot(gs[idx + 1, 0])\n",
    "  ax.text(0.5, 0.5, state[5:], ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "col_names = ['Q(Left)', 'Q(Right)']\n",
    "for idx, col_name in enumerate(col_names):\n",
    "  ax = fig_Q2.add_subplot(gs[0, idx+1])\n",
    "  ax.text(0.5, 0.5, col_name, ha='center', va='center')\n",
    "  ax.axis('off')\n",
    "\n",
    "# Fill in each cell with LaTeX strings\n",
    "for row in range(num_states):\n",
    "  for col, name in enumerate(col_names):\n",
    "    ax = fig_Q2.add_subplot(gs[row + 1, col + 1])  # Offset for labels\n",
    "    ax.text(0.5, 0.5, Q2_approx[row, col], ha='center', va='center')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "P_greedy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "P_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "print(pi_greedy1)\n",
    "P_greedy1 = make_transition_probs(pi_greedy1) **bold text**\n",
    "v_pi_greedy = compute_v_pi(P_greedy1, R, gamma)\n",
    "print(\"Value function v_pi_greedy:\\n\", v_pi_greedy)\n",
    "print(\"Value of init state:\", v_pi_greedy[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "transition_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Wisdom from Sutton and Barto to be included somehow\n",
    "\n",
    "\"\"\" We have defined optimal value functions and optimal policies. Clearly, an\n",
    "agent that learns an optimal policy has done very well, but in practice this\n",
    "rarely happens. For the kinds of tasks in which we are interested, optimal\n",
    "policies can be generated only with extreme computational cost. A well-defined\n",
    "notion of optimality organizes the approach to learning we describe in this book\n",
    "and provides a way to understand the theoretical properties of various learning\n",
    "algorithms, but it is an ideal that agents can only approximate to varying\n",
    "degrees. As we discussed above, even if we have a complete and accurate\n",
    "model of the environment's dynamics, it is usually not possible to simply\n",
    "compute an optimal policy by solving the Bellman optimality equation. For\n",
    "example, board games such as chess are a tiny fraction of human experience,\n",
    "yet large, custom-designed computers still cannot compute the optimal moves.\n",
    "A critical aspect of the problem facing the agent is always the computational\n",
    "power available to it, in particular, the amount of computation it can perform\n",
    "in a single time step.\n",
    "\n",
    "The memory available is also an important constraint. A large amount\n",
    "of memory is often required to build up approximations of value functions,\n",
    "policies, and models. In tasks with small, finite state sets, it is possible to\n",
    "form these approximations using arrays or tables with one entry for each state\n",
    "(or state-action pair). This we call the tabular case, and the corresponding\n",
    "methods we call tabular methods. In many cases of practical interest, however,\n",
    "there are far more states than could possibly be entries in a table. In these\n",
    "cases the functions must be approximated, using some sort of more compact\n",
    "parameterized function representation. (Like an ANN!!!)\n",
    "\n",
    "Our framing of the reinforcement learning problem forces us to settle for\n",
    "approximations. However, it also presents us with some unique opportunities\n",
    "for achieving useful approximations. For example, in approximating optimal behavior, there may be many states that the agent faces with such a low\n",
    "probability that selecting suboptimal actions for them has little impact on the\n",
    "amount of reward the agent receives. Tesauro's backgammon player, for exam-\n",
    "ple, plays with exceptional skill even though it might make very bad decisions\n",
    "on board configurations that never occur in games against experts. In fact, it\n",
    "is possible that TD-Gammon makes bad decisions for a large fraction of the\n",
    "game's state set. The on-line nature of reinforcement learning makes it possi-\n",
    "ble to approximate optimal policies in ways that put more effort into learning\n",
    "to make good decisions for frequently encountered states, at the expense of\n",
    "less effort for infrequently encountered states. This is one key property that\n",
    "distinguishes reinforcement learning from other approaches to approximately\n",
    "solving MDP\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# MDP Notation and Glossary\n",
    "\n",
    "### States, Actions, Rewards:\n",
    "- $\\mathcal{S}$: Set of all non-terminal states.\n",
    "- $s$: A particular state in $\\mathcal{S}$\n",
    "- $S_t$: State at time $t$. A random variable because it depends on the stochastic dynamics of the environment and policy.\n",
    "- $\\mathcal{S}^+$: Set of all states, extend to include terminal states.\n",
    "- $\\mathcal{A}(s)$: Set of actions possible in state $s$. This is written as a function of the to indicate that the available actions may depend on the current state.\n",
    "- $a$: A particular action\n",
    "- $A_t$: Action at time $t$. A random variable due to the potentially stochastic policy, and the stochastic environmental dynamics that brought about $S_t = s$ and hence constrained and influenced the possible realizations of $A_t$\n",
    "- $\\mathcal{R}$: Set of possible rewards. Typically real valued scalars.\n",
    "- $r$: A particular reward.\n",
    "- $R_t$: Reward at time $t$, dependent on $A_{t-1}$ and $S_{t-1}$. A random variable reflecting the immediate outcome of actions.\n",
    "\n",
    "### Policy, Transition and Value Functions:\n",
    "- $\\pi$: Policy, a decision-making rule, can be deterministic or stochastic.\n",
    "  - **Deterministic Policy** $\\pi(s)$: Maps from state space $\\mathcal{S}$ to action space $\\mathcal{A}(s)$.\n",
    "  - **Stochastic Policy** $\\pi(a|s)$: Maps from state-action pairs, $\\mathcal{S} \\times \\mathcal{A}(s)$, to probabilities, i.e. real values on the interval $[0, 1]$.\n",
    "- $p(s', r|s, a)$: State-transition probability function, mapping from state-action pairs together with the state transition and reward outcomes, $\\mathcal{S}^+ \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A}(s)$, to probabilities, real values on the interval $[0, 1]$. This gives the distribution of $R_t$ and $S_t$ conditional on $S_{t-1}$ and $A_{t-1}$. Together with the policy this completely determines the dynamics of the process.\n",
    "- $v_\\pi(s)$: Value function under policy $\\pi$, expected return (always a real value) from state $s$.\n",
    "- $q_\\pi(s, a)$: Action-value function under policy $\\pi$, expected return (always a real value) from taking action $a$ in state $s$.\n",
    "\n",
    "### State Transition Probabilities and Condiational Expected Reward:\n",
    "- $p(s'|s, a)$: State transition probabilities can be expressed by marginalizing the over the rewards using the full joint transition dynamics, that is $$p(s'|s, a) = \\Pr\\{S_{t+1} = s' | S_t = s, A_t = a\\} = \\sum_{r \\in \\mathcal{R}} p(s', r|s, a)$$\n",
    "- $r(s, a, s')$: The expected reward when transitioning from state $s$ to $s'$ taking action $a$. This can expressed by taking the expectation of reward conditional on a particular state transition again using the full joint transition dynamics, that is $$r(s, a, s') = \\mathbb{E}\\left[R_{t+1} \\mid S_t = s, A_t = a, S_{t+1} = s'\\right] = \\frac{\\sum_{r\\in\\mathcal{R}}rp(s',r \\mid s, a)}{p(s'\\mid s, a)}$$\n",
    "\n",
    "### Complexity in State and Action Spaces, and Representations:\n",
    "The complexity of states ($s$) and actions ($a$) can vary significantly across different MDPs. States could range from simple configurations, like the position of a few game pieces on a board, to complex representations, such as the pattern of activation strengthes of photoreceptive cells on a pair of retina. Similarly, actions can vary from discrete choices to complex continuous valued tensors, for example representing patterns of motor neuron activation strengths. This variability influences the choice of algorithms and representations in reinforcement learning.\n",
    "\n",
    "The complexity of policies ($\\pi$) and value functions ($v_\\pi$, $q_\\pi$) also varies. Policies can be represented using simple lookup tables or sophisticated neural networks, and may be deterministic or stochastic. Value functions often require approximation techniques to estimate accurately over large and/or continuous spaces. The choice of representation and estimation method is crucial for effective learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Patchy Forage Board Class\n",
    "#######################################################################\n",
    "# make PatchyForageBoard class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchyForageBoard():\n",
    "  \"\"\"\n",
    "  A collection of methods and parameters of a patchy foraging game board that\n",
    "  define the logic of the game, and allows for multiple critters on the same\n",
    "  board\n",
    "\n",
    "  game state is represented by primarily by food locations, forager locations,\n",
    "  predator locations, scores, and rounds left\n",
    "  food patch locations are stored on a batch x n_rows x n_cols numpy array,\n",
    "  forager and predator(when we have them) locations are stored as dictionaries\n",
    "  with integer keys corresponding to a forager/predatore 1, 2, 3 etc, and then\n",
    "  np.argwhere style tuples of arrays of (batch_array, row_array, col_array)\n",
    "  giving the locations\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game.\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization inline with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical\n",
    "  \"\"\"\n",
    "\n",
    "  ARRAY_PAD_VALUE = -200\n",
    "\n",
    "  def __init__(self, batch_size=1,\n",
    "               n_rows=10, n_cols=5,\n",
    "               num_foragers=1,\n",
    "               max_foraging_attempts=20, # set very high for stochastic end time\n",
    "               end_prob=0.05, #set to zero for strictly finite time\n",
    "               moves_cost=False,\n",
    "               food_patch_prob = 0.4,\n",
    "               food_regen_prob=0.0,\n",
    "               forage_success_prob = 0.7,\n",
    "               food_extinct_prob = 0.1, rng=None):\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "    self.num_foragers = num_foragers\n",
    "    self.max_foraging_attempts = max_foraging_attempts\n",
    "    self.end_prob = end_prob\n",
    "    self.moves_cost = moves_cost\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.forage_success_prob = forage_success_prob\n",
    "    self.food_extinct_prob = food_extinct_prob\n",
    "    self.food_regen_prob = food_regen_prob\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"\n",
    "    Set up starting board using game parameters\n",
    "\n",
    "    Returns:\n",
    "      state (dict):\n",
    "      The state dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new\n",
    "    \"\"\"\n",
    "    # note that is_over applies at the batch level not the batch x forager level\n",
    "    self.is_over = np.zeros(self.batch_size, dtype=bool)\n",
    "    self.foraging_attempts = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.scores = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    # create an empty board array for food locs\n",
    "    self.pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols),\n",
    "                           dtype=int)\n",
    "    # Place critters in top left corner of the board\n",
    "    self.forager_locs = {}\n",
    "    for c in (np.arange(self.num_foragers)+1):\n",
    "      self.forager_locs[c] = (np.arange(self.batch_size, dtype=int),\n",
    "                              np.zeros(self.batch_size, dtype=int),\n",
    "                              np.zeros(self.batch_size, dtype=int))\n",
    "    # Initial food patches on the board randomly\n",
    "    # each grid has an independent prob of being a pathc (to make the math\n",
    "    # easier later) so total number of patches on a board is binomially\n",
    "    # distributed\n",
    "    num_foods = self.rng.binomial(n=self.n_rows * self.n_cols,\n",
    "                                  p=self.food_patch_prob,\n",
    "                                  size=self.batch_size)\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(self.n_rows, self.n_cols,\n",
    "                                               num_foods[ii])\n",
    "      # food patch start locations (do each patch separate in case we\n",
    "      # want to have different kinds of patches)\n",
    "      for f_ in np.arange(num_foods[ii]):\n",
    "        self.pieces[(ii, rc_idx[0][f_],\n",
    "                         rc_idx[1][f_])] = - 1\n",
    "    # keep track of which foragers have missed how many times\n",
    "    # at what kind of patch\n",
    "    self.misses_new_patch = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.misses_known_patch = np.zeros((self.batch_size, self.num_foragers), dtype=int)\n",
    "    self.at_new_patch = np.ones((self.batch_size, self.num_foragers), dtype=bool)\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'foraging_attempts': self.foraging_attempts.copy(),\n",
    "             'is_over': self.is_over.copy(),\n",
    "             'forager_locs': copy.deepcopy(self.forager_locs),\n",
    "             'misses_new_patch': self.misses_new_patch.copy(),\n",
    "             'misses_known_patch': self.misses_known_patch.copy(),\n",
    "             'at_new_patch': self.at_new_patch.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board):\n",
    "    \"\"\"\n",
    "    Sets the state given a board dictionary.\n",
    "\n",
    "    Args:\n",
    "      board (dict):\n",
    "      The board dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new patch.\n",
    "    \"\"\"\n",
    "    self.pieces = board['pieces'].copy()\n",
    "    self.forager_locs = copy.deepcopy(board['forager_locs'])\n",
    "    self.foraging_attempts = board['foraging_attempts'].copy()\n",
    "    self.scores = board['scores'].copy()\n",
    "    self.is_over = board['is_over'].copy()\n",
    "    self.misses_new_patch = board['misses_new_patch'].copy()\n",
    "    self.misses_known_patch = board['misses_known_patch'].copy()\n",
    "    self.at_new_patch = board['at_new_patch'].copy()\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\"\n",
    "    Returns the current board state.\n",
    "\n",
    "    Returns:\n",
    "      state (dict):\n",
    "      The state dictionary contains:\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': Dictionary of current locations of the foragers on the board.\n",
    "        - 'misses_new_patch': List of counts for missed attempts at new patches for each critter.\n",
    "        - 'misses_known_patch': List of counts for missed attempts at known patches for each critter.\n",
    "        - 'at_new_patch': List of booleans indicating if each critter is at a new patch.\n",
    "    \"\"\"\n",
    "    state = {'pieces': self.pieces.copy(),\n",
    "             'scores': self.scores.copy(),\n",
    "             'foraging_attempts': self.foraging_attempts.copy(),\n",
    "             'is_over': self.is_over.copy(),\n",
    "             'forager_locs': copy.deepcopy(self.forager_locs),\n",
    "             'misses_new_patch': self.misses_new_patch.copy(),\n",
    "             'misses_known_patch': self.misses_known_patch.copy(),\n",
    "             'at_new_patch': self.at_new_patch.copy()}\n",
    "    return state\n",
    "\n",
    "\n",
    "  ################# CORE GAME STATE UPDATE LOGIC ##############################\n",
    "  ################# execute_moves is main, uses these helper functions ########\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, which_critter):\n",
    "    \"\"\"\n",
    "    Execute the moves on the board. A move to the current location implies\n",
    "    foraging. If foraging, check if foraging is successful, update scores,\n",
    "    and check if the food goes extinct. If moving to a new location, simply\n",
    "    update the critter's location.\n",
    "\n",
    "    Args:\n",
    "      moves (tuple): A tuple of three arrays:\n",
    "        - batch_array: Specifies which board in the batch the move corresponds to.\n",
    "        - row_array: Specifies the target row for each move.\n",
    "        - col_array: Specifies the target column for each move.\n",
    "        Each array in the tuple has the same length. A move is represented by\n",
    "        the combination of a batch index, row index, and column index at the\n",
    "        same position in their respective arrays.\n",
    "      which_critter (int): Index to identify the critter. Starts from 1.\n",
    "\n",
    "    Returns: Nothing, just updates state related attributes of the board object\n",
    "\n",
    "    \"\"\"\n",
    "    #expand moves tuple\n",
    "    batch_moves, row_moves, col_moves = moves\n",
    "\n",
    "    # Get current locations of the critter\n",
    "    current_locs = self.forager_locs[which_critter]\n",
    "\n",
    "    # Iterate over each board in the batch\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # If the game is over for this board, skip\n",
    "      if self.is_over[ii]:\n",
    "        continue\n",
    "\n",
    "      # Get new location directly from the moves\n",
    "      new_row = int(row_moves[ii])\n",
    "      new_col = int(col_moves[ii])\n",
    "\n",
    "      # Check if the critter has moved to a new patch\n",
    "      if (new_row, new_col) != (current_locs[1][ii], current_locs[2][ii]):\n",
    "        # Moved to a new patch\n",
    "        self.misses_new_patch[ii, which_critter - 1] = 0\n",
    "        self.misses_known_patch[ii, which_critter - 1] = 0\n",
    "        self.at_new_patch[ii, which_critter - 1] = True\n",
    "        if self.moves_cost:\n",
    "          # in this variant moving also ticks down the clock\n",
    "          self.foraging_attempts[ii, which_critter - 1] += 1\n",
    "      # If the critter's position has not changed, it's trying to forage\n",
    "      elif (new_row, new_col) == (current_locs[1][ii], current_locs[2][ii]):\n",
    "        # always increment foraging attempt if foraging\n",
    "        self.foraging_attempts[ii, which_critter - 1] += 1\n",
    "        # Check if there's food at the location\n",
    "        if self.pieces[ii, new_row, new_col] < 0:\n",
    "          # Check if foraging is successful\n",
    "          if self.rng.random() < self.forage_success_prob:\n",
    "            # Successful foraging, increase critter's score\n",
    "            self.scores[ii, which_critter - 1] += 1\n",
    "            # misses are zeroed and no longer at new patch\n",
    "            self.misses_new_patch[ii, which_critter - 1] = 0\n",
    "            self.misses_known_patch[ii, which_critter - 1] = 0\n",
    "            self.at_new_patch[ii, which_critter - 1] = False\n",
    "            # Check if food goes extinct (only on success)\n",
    "            if self.rng.random() < self.food_extinct_prob:\n",
    "              self.pieces[ii, new_row, new_col] = 0  # Set it to empty\n",
    "          else:\n",
    "            #unsuccessful foraging at patch with food\n",
    "            if self.at_new_patch[ii, which_critter - 1]:\n",
    "              # at a new patch\n",
    "              self.misses_new_patch[ii, which_critter - 1] += 1\n",
    "            else:\n",
    "              # at a known patch\n",
    "              self.misses_known_patch[ii, which_critter - 1] += 1\n",
    "        else:\n",
    "          #unsuccessful foraging at patch without food\n",
    "            if self.at_new_patch[ii, which_critter - 1]:\n",
    "              # at a new patch\n",
    "              self.misses_new_patch[ii, which_critter - 1] += 1\n",
    "            else:\n",
    "              # at a known patch\n",
    "              self.misses_known_patch[ii, which_critter - 1] += 1\n",
    "\n",
    "      # Always check if session is over, can end by hitting a fixed\n",
    "      # horizon or by\n",
    "      if self.foraging_attempts[ii] >= self.max_foraging_attempts:\n",
    "        self.is_over[ii] = True\n",
    "      elif self.rng.random() < self.end_prob:\n",
    "        self.is_over[ii] = True\n",
    "\n",
    "    # assume moves are legal and update locs for whole batch at once\n",
    "    self.forager_locs[which_critter] = (batch_moves, row_moves, col_moves)\n",
    "\n",
    "  ###### Getting Legal Moves and Perceptions #########################\n",
    "  ####################################################################\n",
    "  def get_neighbor_grc_indices(self, which_critter, radius, pad=False):\n",
    "    \"\"\"\n",
    "    Returns all grid positions within a certain cityblock distance radius from\n",
    "    the place corresponding to which_critter.\n",
    "\n",
    "    Args:\n",
    "        which_critter (int): The idex of the focal critter_food.\n",
    "        radius (int): The cityblock distance.\n",
    "        pad (bool): whether or not to pad the array, if padded all row, col\n",
    "          indexes are valid for the padded array, useful for getting percept\n",
    "          if not all indexes are correct for the original array, useful for\n",
    "          figuring out legal moves.\n",
    "\n",
    "    Returns:\n",
    "        an array of indices, each row is a g, r, c index for the neighborhoods\n",
    "        around the critters, can use the g value to know which board you are in.\n",
    "        if pad=True also returns the padded array (the indices in that case) are\n",
    "        for the padded array, so won't work on self.pieces, whereas if pad is\n",
    "        False the indices will be for the offsets in reference to the original\n",
    "        self.pieces, but note that some of these will be invalid, and will\n",
    "        need to be filtered out (as we do in get_legal)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    batch, rows, cols = self.forager_locs[which_critter]\n",
    "    # Create meshgrid for offsets\n",
    "    if pad is True:\n",
    "      padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "        (radius, radius)), constant_values=self.ARRAY_PAD_VALUE)\n",
    "      rows = rows + radius\n",
    "      cols = cols + radius\n",
    "\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    neighbors_rows = extended_rows + valid_row_offsets\n",
    "    neighbors_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    indices = np.column_stack((np.repeat(np.arange(batch_size),\n",
    "                                         neighbors_rows.shape[1]),\n",
    "                               neighbors_rows.ravel(),\n",
    "                               neighbors_cols.ravel()))\n",
    "    if pad is False:\n",
    "      return indices\n",
    "    elif pad is True:\n",
    "      return indices, padded_arr\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, which_critter, radius=1):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter.\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    critter_locs = np.array(self.forager_locs[which_critter])\n",
    "    # turn those row, col offsets into a set of legal offsets\n",
    "    legal_offsets = self.get_neighbor_grc_indices(which_critter, radius)\n",
    "    legal_offsets = {tuple(m_) for m_ in legal_offsets}\n",
    "\n",
    "    legal_destinations = np.where(np.ones(self.pieces.shape, dtype=bool))\n",
    "    legal_destinations = {tuple(coords) for coords in zip(*legal_destinations)}\n",
    "    # Add the current locations of the critters to legal_destinations\n",
    "    current_locations = {tuple(loc) for loc in critter_locs.T}\n",
    "    legal_destinations = legal_destinations.union(current_locations)\n",
    "\n",
    "    # legal moves are both legal offsets and legal destinations\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_perceptions(self, critter_food, radius):\n",
    "    idx, pad_pieces = self.get_neighbor_grc_indices(critter_food,\n",
    "                                                    radius, pad=True)\n",
    "    #percept_mask = np.zeros(pad_pieces.shape, dtype=bool)\n",
    "    #percept_mask[idx[:,0], idx[:,1]], idx[:,2]] = True\n",
    "    percept = pad_pieces[idx[:,0], idx[:,1], idx[:,2]]\n",
    "    return(percept.reshape(self.batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# make PatchyForageGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "# @title PatchyForageGame class\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PatchyForagingGame():\n",
    "  \"\"\"\n",
    "  A collection of methods and parameters of a patchy foraging game that allow\n",
    "  for interaction with and display of PatchyForageBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=1, n_rows=10, n_cols=5, num_foragers=1,\n",
    "               max_foraging_attempts=20,\n",
    "               end_prob = 0.05,\n",
    "               moves_cost=False,\n",
    "               food_patch_prob=0.3, food_regen_prob=0.0,\n",
    "               forage_success_prob=0.6, food_extinct_prob=0.2, rng=None):\n",
    "    \"\"\"\n",
    "    Initializes an instance of the PatchyForagingGame with the specified parameters.\n",
    "    Args:\n",
    "    ... [same as in PatchyForageBoard]\n",
    "    \"\"\"\n",
    "    self.board_params = {\n",
    "      'batch_size': batch_size,\n",
    "      'n_rows': n_rows,\n",
    "      'n_cols': n_cols,\n",
    "      'num_foragers': num_foragers,\n",
    "      'max_foraging_attempts': max_foraging_attempts,\n",
    "      'end_prob': end_prob,\n",
    "      'moves_cost': moves_cost,\n",
    "      'food_patch_prob': food_patch_prob,\n",
    "      'forage_success_prob': forage_success_prob,\n",
    "      'food_extinct_prob': food_extinct_prob,\n",
    "      'food_regen_prob': food_regen_prob,\n",
    "      'rng': rng if rng is not None else np.random.default_rng(seed=SEED)\n",
    "    }\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns the initial state of the game.\n",
    "    \"\"\"\n",
    "    board = PatchyForageBoard(**self.board_params)\n",
    "    return board.get_init_board_state()\n",
    "\n",
    "\n",
    "  def get_board_shape(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.board_params['n_rows'], self.board_params['n_cols'])\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only a subset\n",
    "    of these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to (batch,) row and column coordinate indexes of board locations.\n",
    "    \"\"\"\n",
    "    return self.board_params['n_rows'] * self.board_params['n_cols']\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    return self.board_params['batch_size']\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "  def get_foraging_attempts(self, board):\n",
    "    return board['foraging_attempts'].copy()\n",
    "\n",
    "  def get_square_symbol(self, piece, has_forager):\n",
    "    \"\"\"Returns the symbol representation of a board square.\"\"\"\n",
    "    if has_forager and piece < 0: return 'C'  # Critter on food patch\n",
    "    if has_forager: return 'P'  # Forager on an empty square\n",
    "    if piece == 0: return '.'  # Empty square\n",
    "    if piece < 0: return 'F'  # Food patch\n",
    "    return '?'  # Unknown piece type, for debugging\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th game in the batch of boards.\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for r_ in range(self.n_rows):\n",
    "      print(r_, \"|\", end=\"\")  # Print the row number\n",
    "      for c_ in range(self.n_cols):\n",
    "        piece = board['pieces'][g, r_, c_]  # Get the piece to print\n",
    "        # Check if the square is occupied by a forager\n",
    "        has_forager = False\n",
    "        for forager_num, locs in board['forager_locs'].items():\n",
    "          if g in locs[0] and r_ in locs[1] and c_ in locs[2]:\n",
    "            has_forager = True\n",
    "            break\n",
    "\n",
    "        print(self.get_square_symbol(piece, has_forager), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Foraging Attempts: \" + str(board['foraging_attempts'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "  def get_critter_rc(self, board, g, which_critter):\n",
    "    critter_locs = board['forager_locs'][which_critter]\n",
    "    return critter_locs[1][g], critter_locs[2][g]\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war field_of_view\n",
    "                 fov_opaque=False, #let human see through fog of war or not\n",
    "                 show_food=True,\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter',\n",
    "                 focal_critter_index = 0):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    num_foragers = self.board_params['num_foragers']\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, size=550, show_food=show_food)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food, show_food=show_food)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(num_foragers):\n",
    "        critter_name = name if num_foragers == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == num_foragers, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(self.get_critter_rc(board, g, spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # plot field of view around the 'active player'\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque, fov=fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, board, which_critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to get the legal moves, as a set of batch, row, col triples\n",
    "    for the given board. Does return moves that are technically legal\n",
    "    but that will result in a blocking move\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "\n",
    "      which_critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    b = PatchyForageBoard(**self.board_params)\n",
    "    b.set_state(board)\n",
    "    legal_moves =  b.get_legal_moves(which_critter, radius)\n",
    "    return legal_moves\n",
    "\n",
    "  def get_valid_actions(self, board, which_critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    legal_moves =  self.get_legal_moves(board, which_critter, radius)\n",
    "    g, r, c = zip(*legal_moves)\n",
    "    valids = np.zeros((self.batch_size, self.n_rows * self.n_cols))\n",
    "    valids[g, np.array(r) * self.n_cols + np.array(c)] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, which_critter, actions):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the game tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    assert self.board_params['batch_size'] == len(actions)\n",
    "    b = PatchyForageBoard(**self.board_params)\n",
    "    b.set_state(board)\n",
    "    moves = self.actions_to_moves(actions)\n",
    "    b.execute_moves(moves, which_critter)\n",
    "    return b.get_state()\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, which_critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right' 'still'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the game tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == self.board_params['batch_size']\n",
    "    b = PatchyForageBoard(**self.board_params)\n",
    "    b.set_state(board)\n",
    "    moves = self.critter_direction_to_move(board, offsets, which_critter)\n",
    "    b.execute_moves(moves, which_critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      which_critter: integer index of the critter type\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right', 'still'\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = board['forager_locs'][critter]\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "  def critter_directions_to_actions(self, board, directions, critter):\n",
    "    \"\"\"\n",
    "    Converts a list of direction strings to a list of action indices for the given board state and critter.\n",
    "\n",
    "    Args:\n",
    "      board (dict): The current state of the game.\n",
    "      directions (list of str): List of directions, where each direction is one of 'up', 'down', 'left', 'right', 'still'.\n",
    "      critter (int): The critter index.\n",
    "\n",
    "    Returns:\n",
    "      list of int: List of action indices corresponding to the directions.\n",
    "    \"\"\"\n",
    "    # Ensure the length of directions matches the batch size\n",
    "    assert len(directions) == board['pieces'].shape[0], \"Mismatch between directions length and batch size\"\n",
    "\n",
    "    # Convert directions to moves\n",
    "    moves = self.critter_direction_to_move(board, directions, critter)\n",
    "\n",
    "    # Convert moves to actions\n",
    "    actions = self.moves_to_actions(moves)\n",
    "\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, which_critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up',\n",
    "                   ( 0, 0): 'still'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, which_critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    critter_locs = board['forager_locs'][which_critter]\n",
    "    valid_directions = []\n",
    "    for g, batch_valid in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valid==1)[0]\n",
    "      critter_loc = np.array([[critter_locs[1][g],critter_locs[2][g]]])\n",
    "      # critter_loc shape is (1, 2)\n",
    "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
    "      offsets = moves - critter_loc\n",
    "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, which_critter):\n",
    "    b = PatchyForageBoard(**self.board_params)\n",
    "    b.set_state(board)\n",
    "    return(b.get_perceptions(radius, which_critter))\n",
    "\n",
    "\n",
    "  def play_game(self, players=[], visualize = False):\n",
    "    \"\"\"This method takes a list of players the same length as num_foragers,\n",
    "        and then plays a batch of games with them and returns the final board\n",
    "        states of each game\"\"\"\n",
    "    if len(players) != self.num_foragers:\n",
    "      raise ValueError(\"number of players different than expected\")\n",
    "\n",
    "    board = self.get_init_board()\n",
    "    if visualize == True:\n",
    "      self.display(board, 0)\n",
    "\n",
    "    for p_idx, player_ in enumerate(players):\n",
    "      if player_.critter_index != p_idx+1:\n",
    "        print(player_.critter_index)\n",
    "        print(p_idx + 1)\n",
    "        raise ValueError(\"player order does not match assigned critter index\")\n",
    "\n",
    "    while np.any(board['is_over'] == False):\n",
    "      for player_ in players:\n",
    "        old_scores = board['scores']\n",
    "        if player_.return_direction:\n",
    "          directions = player_.play(board)\n",
    "          a_player = self.critter_directions_to_actions(board, directions, player_.critter_index)\n",
    "        else: # player returns actions directly\n",
    "          a_player, _, _ = player_.play(board)\n",
    "        board = self.get_next_state(board, player_.critter_index, a_player)\n",
    "        if visualize == True:\n",
    "          self.display(board, 0)\n",
    "    return board\n",
    "\n",
    "\n",
    "  def plot_visualizations(board):\n",
    "    # Extracting scores and foraging_attempts for all batches\n",
    "    scores = board['scores']\n",
    "    foraging_attempts = board['foraging_attempts']\n",
    "\n",
    "    # Calculating average scores per round for each batch\n",
    "    avg_scores_per_round = scores / foraging_attempts\n",
    "\n",
    "    # Histogram of Average Score Per Round\n",
    "    plt.figure()\n",
    "    plt.hist(avg_scores_per_round, bins=30, edgecolor='black')\n",
    "    plt.xlabel('Average Score Per Round')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Average Score Per Round')\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter Plot of Averages vs. Foraging Attempts\n",
    "    plt.figure()\n",
    "    plt.scatter(foraging_attempts, avg_scores_per_round, c='blue', alpha=0.5)\n",
    "    plt.xlabel('Foraging Attempts')\n",
    "    plt.ylabel('Average Score Per Round')\n",
    "    plt.title('Scatter Plot of Average Scores vs. Foraging Attempts')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# make InteractivePatchyForage class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "# @title Interactive Patchy Foraging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractivePatchyForage():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, PatchyForage_game, init_board=None, has_fov=False,\n",
    "               radius=2, fov_opaque=False, show_food=True, show_misses=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human']):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a patchy foraging game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of PatchyForageGame object\n",
    "        expects this to have batchsize 1\n",
    "      init_board: (optional) a dictionary containing\n",
    "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
    "        - 'scores': The current scores of the critters.\n",
    "        - 'foraging_attempts': The number of foraging attempts each critter has made.\n",
    "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
    "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the PatchyForage_game.num_foragers\n",
    "        attribute. Order should align with critter_name.\n",
    "\n",
    "      Note: fov only turns on for the 'active' player.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.pfg = PatchyForage_game\n",
    "    self.num_foragers = self.pfg.board_params['num_foragers']\n",
    "    self.moves_cost = self.pfg.board_params['moves_cost']\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.show_food = show_food\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.active_player_index = 0\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.num_foragers):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] #implicit check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        # player objects expected to have a critter_index attribute\n",
    "        # we set it appropriately here so it aligns with the players list\n",
    "        # used to create the widget\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    # Initialize the sidebar for displaying misses if needed\n",
    "    self.show_misses = show_misses\n",
    "    if self.show_misses:\n",
    "      self.misses_sidebar = widgets.Output(layout=widgets.Layout(\n",
    "          min_width='12.5em', max_width='18.8em',\n",
    "          min_height='6.3em', overflow='auto'))\n",
    "      self.misses_new_patch = [0] * self.num_foragers\n",
    "      self.misses_known_patch = ['--'] * self.num_foragers\n",
    "      self.at_new_patch = [True] * self.num_foragers\n",
    "\n",
    "    if init_board is None:\n",
    "      self.board_state = self.pfg.get_init_board()\n",
    "    else:\n",
    "      self.board_state = init_board\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.forage_button = widgets.Button(description=\"Forage\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.empty_space = widgets.Box(layout=widgets.Layout(height='2.5em'))\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.pfg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize,\n",
    "          show_food=self.show_food)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.pfg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                fov_opaque=self.fov_opaque,\n",
    "                                show_food=self.show_food,\n",
    "                                radius=self.radius, figsize=self.figsize)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([widgets.VBox([self.forage_button, self.left_button]),\n",
    "                                 widgets.VBox([self.up_button, self.down_button]),\n",
    "                                 widgets.VBox([self.empty_space, self.right_button])])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "      if len(self.players) == 1:\n",
    "        #one human player\n",
    "        self.output_and_score = widgets.VBox([self.scoreboard, self.output])\n",
    "        if self.show_misses:\n",
    "            self.final_display = widgets.HBox([self.board_and_buttons,\n",
    "                widgets.VBox([self.misses_sidebar, self.output_and_score])])\n",
    "        else:\n",
    "            self.final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                               self.output_and_score])\n",
    "      else:\n",
    "        # more than one player, one of them human\n",
    "        self.V_board_output= widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend,\n",
    "                                             self.misses_sidebar])\n",
    "        else:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # all players are ai\n",
    "      if len(self.players) == 1:\n",
    "        # one ai player\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox(\n",
    "              [widgets.VBox([self.b_fig.canvas, self.scoreboard]),\n",
    "               widgets.VBox([self.misses_sidebar, self.output,\n",
    "                             self.start_button])])\n",
    "        else:\n",
    "          self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "          self.final_display = self.HBox(\n",
    "              [widgets.VBox([self.b_fig.canvas, self.H_score_output_start])])\n",
    "      else:\n",
    "        # more than one ai player\n",
    "        self.V_board_output = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "          self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        if self.show_misses:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend,\n",
    "                                             self.misses_sidebar])\n",
    "        else:\n",
    "          self.final_display = widgets.HBox([self.V_board_output,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['Best Eating Rate:'] + ['--'] * self.num_foragers,\n",
    "               ['Last Eating Rate:'] + ['--'] * self.num_foragers,\n",
    "               ['Average Eating Rate:'] + ['--'] * self.num_foragers,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    # If show_misses is enabled, initialize the misses_sidebar content\n",
    "    if self.show_misses:\n",
    "      with self.misses_sidebar:\n",
    "        table = [['Misses (New Patch):'] + ['0'] * self.num_foragers,\n",
    "                 ['Misses (Known Patch):'] + ['--'] * self.num_foragers]\n",
    "        if len(self.players) > 1:\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
    "          print(tabulate(table, headers=headers))\n",
    "        else: # len(self.players) == 1\n",
    "            print(tabulate(table))\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.forage_button.on_click(self.on_forage_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "\n",
    "  def update_state_based_on_move(self, direction):\n",
    "    old_board = self.board_state.copy()\n",
    "    # index of players is 0 through num_critter-1,\n",
    "    # same player represented by value of index + 1 in\n",
    "    if (isinstance(self.players[self.active_player_index], str) and\n",
    "        'human' in self.players[self.active_player_index]):\n",
    "      direction = direction\n",
    "    else:\n",
    "      if self.players[self.active_player_index].return_direction:\n",
    "        directions = self.players[self.active_player_index].play(old_board)\n",
    "      else:\n",
    "        a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
    "        # print(a_player)\n",
    "        directions = self.pfg.action_to_critter_direction(old_board,\n",
    "                                                        self.active_player_index+1,\n",
    "                                                        a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = directions[0]\n",
    "    self.board_state = self.pfg.critter_oriented_get_next_state(\n",
    "          self.board_state, self.active_player_index+1, [direction])\n",
    "    return direction\n",
    "\n",
    "\n",
    "  def update_output_and_scores(self, direction, old_board):\n",
    "    old_scores = old_board['scores'][0]\n",
    "    old_row, old_col = self.pfg.get_critter_rc(old_board, 0,\n",
    "                                               self.active_player_index+1)\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    foraging_attempts = self.board_state['foraging_attempts'][0]\n",
    "    row, col = self.pfg.get_critter_rc(self.board_state, 0,\n",
    "                                       self.active_player_index+1)\n",
    "\n",
    "    did_eat = False\n",
    "    # Check if the forager moved or tried to forage and what happened\n",
    "    if (row, col) != (old_row, old_col):\n",
    "      # Moved to a new patch\n",
    "      self.misses_new_patch[self.active_player_index] = 0\n",
    "      self.misses_known_patch[self.active_player_index] = '--'\n",
    "      self.at_new_patch[self.active_player_index] = True\n",
    "      action_string = \"tried to move \" + direction + \" to ({}, {})\".format(row, col)\n",
    "      eating_string = \"They were too busy moving to look for food.\"\n",
    "    elif (row, col) == (old_row, old_col):\n",
    "      # they didn't move, tried to forage\n",
    "      action_string = \"tried to forage.\"\n",
    "      if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
    "        # They found food\n",
    "        eating_string = \"They found some food at the patch!\"\n",
    "        did_eat = True\n",
    "        if self.at_new_patch[self.active_player_index]:\n",
    "          # They found food at a new patch\n",
    "          self.misses_new_patch[self.active_player_index] = '--'\n",
    "          self.misses_known_patch[self.active_player_index] = 0\n",
    "          self.at_new_patch[self.active_player_index] = False\n",
    "        else:\n",
    "          # They found food at a known patch\n",
    "          # Reset count as they found food\n",
    "          self.misses_known_patch[self.active_player_index] = 0\n",
    "      else:\n",
    "        # They didn't find food\n",
    "        eating_string = \"They didn't find any food at the patch.\"\n",
    "        if self.at_new_patch[self.active_player_index]:\n",
    "          # They are at a new patch\n",
    "          self.misses_new_patch[self.active_player_index] += 1\n",
    "        else:\n",
    "          # They are at a known patch\n",
    "          self.misses_known_patch[self.active_player_index] += 1\n",
    "\n",
    "    #make the picture of the new board position\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.pfg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             show_food=self.show_food,\n",
    "                             radius=self.radius, legend_type=self.legend_type)\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        print(\"The critter {}\".format(action_string))\n",
    "        print(eating_string)\n",
    "        foraging_attempts_count = foraging_attempts[self.active_player_index]\n",
    "        new_score = new_scores[self.active_player_index]\n",
    "        food_per_attempt = \"-\" if foraging_attempts_count == 0 else \"{:.2f}\".format(new_score / foraging_attempts_count)\n",
    "        if self.moves_cost:\n",
    "          leading_string1 = \"Moves Taken\"\n",
    "          mid_string1 = \"Move\"\n",
    "          leading_string2 = \"Moves Left\"\n",
    "        else:\n",
    "          leading_string1 = \"Foraging Attempts\"\n",
    "          mid_string1 = \"Attempt\"\n",
    "          leading_string2 = \"Foraging Attempts Left\"\n",
    "\n",
    "        print(f\"{leading_string1}: {foraging_attempts_count}\\n\"\n",
    "              f\"Food Eaten: {new_score}\\n\"\n",
    "              f\"Food Per {mid_string1}: {food_per_attempt}\")\n",
    "\n",
    "        print(f\"{leading_string2}: \"\n",
    "              f\"{self.pfg.board_params['max_foraging_attempts'] - foraging_attempts_count}\")\n",
    "      else:  # more than one player\n",
    "        print(f\"Critter {self.active_player_index + 1} {action_string}\")\n",
    "        print(eating_string)\n",
    "        # Assuming foraging_attempts and new_scores are aggregated lists; adjust as necessary.\n",
    "        print(f\"{leading_string1}: {foraging_attempts}\\nFood Eaten: {new_scores}\")\n",
    "\n",
    "\n",
    "    if self.show_misses:\n",
    "      with self.misses_sidebar:\n",
    "        clear_output()\n",
    "        table = [['Misses (New Patch):'] + [str(miss) for miss in self.misses_new_patch],\n",
    "                 ['Misses (Known Patch):'] + [str(miss) for miss in self.misses_known_patch]]\n",
    "        if len(self.players) > 1:\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
    "          print(tabulate(table, headers=headers))\n",
    "        else: # len(self.players) == 1\n",
    "            print(tabulate(table))\n",
    "\n",
    "\n",
    "  def handle_game_end(self):\n",
    "    \"\"\"Handle the logic when the game is over.\"\"\"\n",
    "    self.final_scores.append(self.board_state['scores'][0] / self.board_state['foraging_attempts'][0])\n",
    "    self.board_state = self.pfg.get_init_board()\n",
    "    for player in self.players:\n",
    "      if hasattr(player, 'last_direction'):\n",
    "        player.last_direction = ['right'] * self.pfg.board_params['batch_size']\n",
    "    if self.show_misses:\n",
    "      self.misses_new_patch = [0] * self.num_foragers\n",
    "      self.misses_known_patch = ['--'] * self.num_foragers\n",
    "      self.at_new_patch = [True] * self.num_foragers\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      print('Game Over. Final Food per Attempt {}'.format(self.final_scores[-1]))\n",
    "      print('Resetting the board for another game')\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.pfg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                             self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                             has_fov=self.has_fov, radius=self.radius,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             show_food=self.show_food,\n",
    "                             legend_type=self.legend_type)\n",
    "    with self.scoreboard:\n",
    "      clear_output()\n",
    "      print('Games Played: ' + str(len(self.final_scores)))\n",
    "      if len(self.players) == 1:\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = [\n",
    "            ['Best Eating Rate: ', '{:.2f}'.format(np.max(np.array(self.final_scores)))],\n",
    "            ['Last Eating Rate: ', '{:.2f}'.format(self.final_scores[-1][0])],\n",
    "            ['Average Eating Rate', '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "        else:\n",
    "          table = [['Best Eating Rate:', '--'],\n",
    "                   ['Last Eating Rate:', '--'],\n",
    "                   ['Average Eating Rate:', '--']]\n",
    "        print(tabulate(table))\n",
    "      else: # len(self.players) > 1\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
    "        if len(self.final_scores) > 0:\n",
    "          table = []\n",
    "          # Assuming the batch size is 1 for now\n",
    "          current_scores = self.final_scores[-1]\n",
    "          max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "          average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "          table.append(['Besat Rates:'] +\n",
    "          [str(score) for score in max_scores])\n",
    "          table.append(['Last Rates:'] +\n",
    "            [str(score) for score in current_scores])\n",
    "          table.append(['Average Rates:'] +\n",
    "              ['{:.2f}'.format(score) for score in average_scores])\n",
    "        else:\n",
    "          table = [\n",
    "            ['High Score:'] + ['--'] * self.num_foragers,\n",
    "            ['Last Score:'] + ['--'] * self.num_foragers,\n",
    "            ['Average Score:'] + ['--'] * self.num_foragers,]\n",
    "        print(tabulate(table, headers=headers))\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "    self.forage_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "    self.forage_button.disabled = False\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('right')\n",
    "\n",
    "  def on_forage_button_clicked(self, *args):\n",
    "    self.on_direction_button_click('still')\n",
    "\n",
    "  def execute_moves(self, human_direction=None):\n",
    "    ai_direction = None\n",
    "    while not self.board_state['is_over'][0]:\n",
    "      old_board = self.board_state.copy()\n",
    "      # Check if the current player is human\n",
    "      if self.players[self.active_player_index] == 'human':\n",
    "        if human_direction is None:\n",
    "          # If the human direction is not provided,\n",
    "          # it means the human has not yet made a move\n",
    "          # Break out and wait for one\n",
    "          break\n",
    "        else:\n",
    "          # The human made a move, so execute it and reset the human_direction\n",
    "          self.update_state_based_on_move(human_direction)\n",
    "          self.update_output_and_scores(human_direction, old_board)\n",
    "          human_direction = None  # Reset for next loop iteration\n",
    "      else:\n",
    "        # AI player\n",
    "        ai_direction = self.update_state_based_on_move('tbd')\n",
    "        self.update_output_and_scores(ai_direction, old_board)\n",
    "\n",
    "      # Move to the next player\n",
    "      self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "\n",
    "  def on_direction_button_click(self, direction):\n",
    "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
    "    self.execute_moves(human_direction=direction)\n",
    "    if self.board_state['is_over'][0]:\n",
    "        self.handle_game_end()\n",
    "    self.enable_direction_buttons()  # Re-enable buttons\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    self.execute_moves()\n",
    "    if self.board_state['is_over'][0]:\n",
    "        self.handle_game_end()\n",
    "    self.start_button.disabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.1 Foraging in a patchy environment, two flavours.\n",
    "\n",
    "In Seqeunce 1.2.3 on Normative thinking we introduced a patchy foraging game. The game from that sequence (vanilla here) is playable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Omniscient Patchy Foraging - Vanilla\n",
    "# @markdown **Run this cell** to play the patchy foraging game.\n",
    "rng = np.random.default_rng(1)\n",
    "pfg = PatchyForagingGame(max_foraging_attempts=20, food_patch_prob=0.3,\n",
    "                         forage_success_prob=0.6, food_extinct_prob=0.2,\n",
    "                         moves_cost=False, end_prob=0, rng=rng)\n",
    "omni_ipfg = InteractivePatchyForage(pfg, show_food=True, show_misses=True,\n",
    "                                    figsize=(4,5))\n",
    "display(omni_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(omni_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Omniscient Patchy Foraging - Chocolate\n",
    "# @markdown **Run this cell** to play the a slight variation on the previous patchy foraging game.\n",
    "rng = np.random.default_rng(1)\n",
    "pfg = PatchyForagingGame(max_foraging_attempts=20, food_patch_prob=0.3,\n",
    "                         forage_success_prob=0.6, food_extinct_prob=0.2,\n",
    "                         moves_cost=True, end_prob=0, rng=rng)\n",
    "omni_ipfg = InteractivePatchyForage(pfg, show_food=True, show_misses=True,\n",
    "                                    figsize=(4,5))\n",
    "display(omni_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(omni_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Can you spot the difference between the two variants? Hint: Look at what happens to the number of rounds left when you make a move (up, down, left, right) in each variant.\n",
    "\n",
    "When we can see the food patches and tell when they have been exhausted, the optimal policy is similar in both scenarios. Move to a patch with food forager there until food is exhausted, then move on to the next patch with food. In the variant where moves are costly, some care needs to be taken so that patches with food are navagated to efficiently, much like in our earliest Gridworld foraging problems. But other than this requirement on efficient movement between patches, the decision about when to move on is identical when the state of food patches is known. However, in sequence 1.2.3 we focused on a more complex situation, where the presence or absence of food at a location was not immediately detectable, but rather could only be inferred from the recent history of foraging successes and failures at a given patch.\n",
    "\n",
    "You can try out that variant of the game below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Cryptic Patchy Foraging - Moves Cost\n",
    "# @markdown **Run this cell** to play the patchy foraging game with cryptic patches and movement has an opportunity cost.\n",
    "rng = np.random.default_rng(3)\n",
    "pfg = PatchyForagingGame(max_foraging_attempts=20, food_patch_prob=0.3,\n",
    "                         forage_success_prob=0.6, food_extinct_prob=0.2,\n",
    "                         end_prob=0, moves_cost=True, rng=rng)\n",
    "cryptic_ipfg = InteractivePatchyForage(pfg, show_food=False,\n",
    "                                       show_misses=True,\n",
    "                                       figsize=(4,5))\n",
    "display(cryptic_ipfg.b_fig.canvas)\n",
    "clear_output()\n",
    "display(cryptic_ipfg.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "While the variant where moves are free and the variant where moves have an opportunity cost are very similar in many ways, this slight difference, means that the approach we used to determine an optimal policy (behaviour rule) for one variant will not work for the other.\n",
    "\n",
    "Consider, when movement between patches has no opportunity cost, all an optimal organism needs to worry about is foraging at a patch that has the highest possible probability of having food present. If that happens to be the patch the organism currently occupies, great, forage there, but if not, no problem, movement is free in some sense, so just move on to a fresh patch if the foraging odds are better there. In contrast, more nuance is required when time spent moving between patches uses up time that could have been spent foraging, i.e. when movement has an opportunity cost. To see this think about what happens when there is a single round left in the foraging episode. When movement is costly is there any situation where movement is preferable to foraging at the current patch on this last round? No, there is always some chance of success (from the forager's perspective) at the current patch, but there is zero chance of foraging success when moving, so an optimal forager would never move on the last round.\n",
    "\n",
    "Things are certainly more complicated things, but the kind of thinking we applied in sequence 1.2.3 can be extended to find an optimal policy for this new, trickier problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.2 Reasoning About The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Defining the Problem**\n",
    "First we need define our model problem precisely.\n",
    "\n",
    "* **Patchy Environment**: The foraging environment consists of discrete patches (represented as grid cells). At the start of the simulation each patch has a probability $p_e \\in (0,1)$ of containing food. The forager starts at a fresh patch.\n",
    "\n",
    "* **Possible Actions**: In each turn, the organism has two options:\n",
    "  - Try to forage at its current patch.\n",
    "  - Move to a new patch.\n",
    "\n",
    "* **Foraging Success**: When a patch contains food, foraging is often successful but not always guaranteed. In this model, foraging at a patch with food is successful with probability $p_s \\in (0,1)$. Conversely, foraging on a food-less patch is certain to be unsuccessful.\n",
    "\n",
    "* **Patch Exhaustion**: After each foraging success, there is a probability $p_x \\in (0,1)$ that the patch becomes exhausted. In such cases, the patch won't provide any more food.\n",
    "\n",
    "* **Session Limit**: The foragers can take a fixed number of actions, $T$, before the session end. Both move actions and foraging attemp actions count towards this limit.\n",
    "\n",
    "* **Rewards**: Every successful foraging attempt gives the organism a reward of 1 point. If the foraging attempt is unsuccessful, no points are awarded for that round. Similarly, if the organism moves, no points are awarded. We denote the reward received on round $t$ as $R_t$.\n",
    "\n",
    "* **Goal**: The overarching objective for the organism is to maximize its *expected cumulative reward* over the entire session. Formally, the forager aims to maximize:\n",
    "$$\n",
    "\\mathbb{E}\\left[ \\sum_{t=1}^{T} R_t \\right] = \\sum_{t=1}^{T} \\mathbb{E}\\left[ R_t \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Previously we kind of glossed over where exactly the actions of the forager were applied in the decision making process. We made simplifying assumptions (implicitly! Yikes!) that of course the forager would forage at a newly arrived patch (why else would it have moved these), but for a moment we're going to leave that aside and be as totally verbose and explicit as possible about all the different things that can happen. Just as a result of the foragers very first action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
    "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
    "  ax.axis(\"off\")\n",
    "  #plt.tight_layout()\n",
    "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
    "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
    "  for edge in edge_labels:\n",
    "    src, dest, latex_str = edge\n",
    "    if graph.has_edge(src, dest):\n",
    "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
    "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
    "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
    "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\", color=\"green\")\n",
    "      graph.delete_edge(src, dest)\n",
    "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
    "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
    "  return graph\n",
    "\n",
    "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
    "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
    "  for node in graph.nodes():\n",
    "    if not node.startswith(\"dummy\"):\n",
    "      node.attr['width'] = width\n",
    "      node.attr['height'] = height\n",
    "  return graph\n",
    "\n",
    "\n",
    "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                            action_nodes = [],\n",
    "                            node_colors = {},\n",
    "                            node_labels = {},\n",
    "                            output_path=\"graphviz_output.png\", dpi=300,\n",
    "                            figsize=(0.6, 0.3), fontsize=16):\n",
    "  \"\"\"\n",
    "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
    "\n",
    "  Parameters:\n",
    "    nodes_list (list): List of nodes in the graph.\n",
    "    edges_list (list): List of edges in the graph.\n",
    "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
    "    output_path (str): Path to save the rendered graph.\n",
    "    dpi (int): DPI for rendering the graph.\n",
    "    figsize (tuple): Figure size for the LaTeX labels.\n",
    "\n",
    "  Returns:\n",
    "    str: Path to the saved graph image.\n",
    "  \"\"\"\n",
    "  # Graph Creation and Configuration\n",
    "  G = pgv.AGraph(directed=True, strict=False, rankdir='LR', ranksep=0.5, nodesep=0.5)\n",
    "\n",
    "  # Add state and decision nodes\n",
    "  for node in nodes_list:\n",
    "    shape = \"box\" if node in action_nodes else \"ellipse\"  # Use 'box' for decision nodes\n",
    "    if node in action_nodes:\n",
    "      # action nodes are square and default to blue colour\n",
    "      color = node_colors.get(node, \"blue\")\n",
    "      shape = \"box\"\n",
    "    else:\n",
    "      # state nodes are round and default to black colour\n",
    "      shape = \"ellipse\"\n",
    "      color = node_colors.get(node, \"black\")\n",
    "    label = node_labels.get(node, node)\n",
    "    G.add_node(node, color=color, label=label, shape=shape)\n",
    "\n",
    "  for edge in edges_list:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
    "  G = set_regular_node_sizes(G, width=1, height=1)\n",
    "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=figsize, fontsize=fontsize)\n",
    "\n",
    "  # Additional graph attributes\n",
    "  G.graph_attr['size'] = \"8,8\"\n",
    "  G.graph_attr['dpi'] = str(dpi)\n",
    "\n",
    "  # Render and save the graph\n",
    "  G.layout(prog='dot')\n",
    "  G.draw(output_path)\n",
    "\n",
    "  return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize the decision tree\n",
    "\n",
    "\n",
    "nodes_list = [\n",
    "    \"New Patch\", \"Has Food0\", \"No Food0\", \"Didn't Find Food0\", \"Found the Food0\",\n",
    "    \"No Food to Find0\", \"Found Impossible Food0\",\n",
    "    \"Search (has food)0\", \"Leave (has food)0\",\n",
    "    \"Search (no food)0\", \"Leave (no food)0\",\n",
    "    \"Has Food1\", \"No Food1\", \"Has Food2\", \"No Food2\"\n",
    "]\n",
    "\n",
    "edges_list = [\n",
    "    (\"New Patch\", \"Has Food0\"), (\"New Patch\", \"No Food0\"),\n",
    "    (\"Has Food0\", \"Search (has food)0\"), (\"Has Food0\", \"Leave (has food)0\"),\n",
    "    (\"No Food0\", \"Search (no food)0\"), (\"No Food0\", \"Leave (no food)0\"),\n",
    "    (\"Search (has food)0\", \"Found the Food0\"), (\"Search (has food)0\", \"Didn't Find Food0\"),\n",
    "    (\"Search (no food)0\", \"Found Impossible Food0\"), (\"Search (no food)0\", \"No Food to Find0\"),\n",
    "    (\"Leave (has food)0\", \"Has Food1\"), (\"Leave (has food)0\", \"No Food1\"),\n",
    "    (\"Leave (no food)0\", \"Has Food2\"), (\"Leave (no food)0\", \"No Food2\"),\n",
    "]\n",
    "\n",
    "latex_edge_labels = [\n",
    "    (\"New Patch\", \"Has Food0\", \"p_e\"),\n",
    "    (\"New Patch\", \"No Food0\", \"1-p_e\"),\n",
    "    (\"Search (has food)0\", \"Didn't Find Food0\", \"1-p_s\"),\n",
    "    (\"Search (has food)0\", \"Found the Food0\", \"p_s\"),\n",
    "    (\"Search (no food)0\", \"No Food to Find0\", \"1\"),\n",
    "    (\"Search (no food)0\", \"Found Impossible Food0\", \"0\"),\n",
    "    (\"Leave (has food)0\", \"No Food1\", \"p_e\"),\n",
    "    (\"Leave (has food)0\", \"Has Food1\", \"1-p_e\"),\n",
    "    (\"Leave (no food)0\", \"No Food2\", \"p_e\"),\n",
    "    (\"Leave (no food)0\", \"Has Food2\", \"1-p_e\")\n",
    "]\n",
    "\n",
    "action_nodes = [\n",
    "    \"Search (has food)0\", \"Search (no food)0\", \"Leave (has food)0\", \"Leave (no food)0\"\n",
    "]\n",
    "\n",
    "node_colors = {\n",
    "    \"New Patch\": \"red\",\n",
    "    \"Has Food0\": \"red\",\n",
    "    \"No Food0\": \"red\",\n",
    "}\n",
    "\n",
    "node_labels = {\n",
    "    \"Has Food0\": \"New Patch\\nHas Food\",\n",
    "    \"No Food0\": \"New Patch Has\\nNo Food\",\n",
    "    \"Has Food1\": \"New Patch\\nHas Food\",\n",
    "    \"No Food1\": \"New Patch Has\\nNo Food\",\n",
    "    \"Has Food2\": \"New Patch\\nHas Food\",\n",
    "    \"No Food2\": \"New Patch Has\\nNo Food\",\n",
    "    \"Search (has food)0\": \"Search\",\n",
    "    \"Leave (has food)0\": \"Leave\",\n",
    "    \"Search (no food)0\": \"Search\",\n",
    "    \"Leave (no food)0\": \"Leave\",\n",
    "    \"Didn't Find Food0\": \"No Food Found\",\n",
    "    \"No Food to Find0\": \"No Food Found\",\n",
    "    \"Found the Food0\": \"Food Found\\n+1 Reward Point\",\n",
    "    \"Found Impossible Food0\": \"Food Found\\n+1 Reward Point\"\n",
    "}\n",
    "\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                                      action_nodes=action_nodes,\n",
    "                                      node_colors=node_colors,\n",
    "                                      node_labels=node_labels)\n",
    "Image(output_path, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this diagram rounded nodes represent states of this process, i.e. the situation the organism is in with respect to the environment. Blue squares represent actions taken by the organism, and yellow squares give the probability of transitioning from the previous state, to the next state, given the action the organism took. These transitions can also be thought of as actions taken by the environment. This the full expansion, but looking at this we can see that if the organism leaves a patch, it doesn't matter whether or not there was food there, the state of the new patch is unaffected by this so we can already simplify this slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run This Cell** to visualize the decision tree\n",
    "\n",
    "\n",
    "nodes_list = [\n",
    "  \"New Patch\", \"Has Food0\", \"No Food0\", \"Didn't Find Food0\", \"Found the Food0\",\n",
    "  \"No Food to Find0\", \"Found Impossible Food0\",\n",
    "  \"Search (has food)0\", \"Leave0\",\n",
    "  \"Search (no food)0\",\n",
    "  \"Has Food1\", \"No Food1\"\n",
    "]\n",
    "\n",
    "edges_list = [\n",
    "  (\"New Patch\", \"Has Food0\"), (\"New Patch\", \"No Food0\"),\n",
    "  (\"Has Food0\", \"Search (has food)0\"), (\"Has Food0\", \"Leave0\"),\n",
    "  (\"No Food0\", \"Search (no food)0\"), (\"No Food0\", \"Leave0\"),\n",
    "  (\"Search (has food)0\", \"Found the Food0\"), (\"Search (has food)0\", \"Didn't Find Food0\"),\n",
    "  (\"Search (no food)0\", \"Found Impossible Food0\"), (\"Search (no food)0\", \"No Food to Find0\"),\n",
    "  (\"Leave0\", \"Has Food1\"), (\"Leave0\", \"No Food1\"),\n",
    "]\n",
    "\n",
    "latex_edge_labels = [\n",
    "  (\"New Patch\", \"Has Food0\", \"p_e\"),\n",
    "  (\"New Patch\", \"No Food0\", \"1-p_e\"),\n",
    "  (\"Search (has food)0\", \"Didn't Find Food0\", \"1-p_s\"),\n",
    "  (\"Search (has food)0\", \"Found the Food0\", \"p_s\"),\n",
    "  (\"Search (no food)0\", \"No Food to Find0\", \"1\"),\n",
    "  (\"Search (no food)0\", \"Found Impossible Food0\", \"0\"),\n",
    "  (\"Leave0\", \"No Food1\", \"p_e\"),\n",
    "  (\"Leave0\", \"Has Food1\", \"1-p_e\"),\n",
    "]\n",
    "\n",
    "action_nodes = [\n",
    "  \"Search (has food)0\", \"Search (no food)0\", \"Leave0\"\n",
    "]\n",
    "\n",
    "node_colors = {\n",
    "    \"New Patch\": \"red\",\n",
    "    \"Has Food0\": \"red\",\n",
    "    \"No Food0\": \"red\",\n",
    "}\n",
    "\n",
    "node_labels = {\n",
    "    \"Has Food0\": \"New Patch\\nHas Food\",\n",
    "    \"No Food0\": \"New Patch Has\\nNo Food\",\n",
    "    \"Has Food1\": \"New Patch\\nHas Food\",\n",
    "    \"No Food1\": \"New Patch Has\\nNo Food\",\n",
    "    \"Search (has food)0\": \"Search\",\n",
    "    \"Leave0\": \"Leave\",\n",
    "    \"Search (no food)0\": \"Search\",\n",
    "    \"Didn't Find Food0\": \"No Food Found\",\n",
    "    \"No Food to Find0\": \"No Food Found\",\n",
    "    \"Found the Food0\": \"Food Found\\n+1 Reward Point\",\n",
    "    \"Found Impossible Food0\": \"Food Found\\n+1 Reward Point\"\n",
    "}\n",
    "\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                                      action_nodes=action_nodes,\n",
    "                                      node_colors=node_colors,\n",
    "                                      node_labels=node_labels)\n",
    "Image(output_path, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Similarly we can remove the zero probability event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "nodes_list = [\n",
    "  \"New Patch\", \"Has Food0\", \"No Food0\", \"Didn't Find Food0\", \"Found the Food0\",\n",
    "  \"No Food to Find0\",\n",
    "  \"Search (has food)0\",\n",
    "  \"Search (has food)1\"\n",
    "  \"Leave0\",\n",
    "  \"Leave1\"\n",
    "  \"Search (no food)0\",\n",
    "  \"Search (no food)1\",\n",
    "  \"Has Food1\", \"No Food1\",\n",
    "  \"Search (didn't find food)0\",\n",
    "  \"Search (found the food)0\",\n",
    "  \"Search (no food to find)0\",\n",
    "  \"Leave (didn't find food)0\",\n",
    "  \"Leave (found the food)0\",\n",
    "  \"Leave (no food to find)0\",\n",
    "]\n",
    "\n",
    "edges_list = [\n",
    "  (\"New Patch\", \"Has Food0\"), (\"New Patch\", \"No Food0\"),\n",
    "  (\"Has Food0\", \"Search (has food)0\"), (\"Has Food0\", \"Leave0\"),\n",
    "  (\"No Food0\", \"Search (no food)0\"), (\"No Food0\", \"Leave0\"),\n",
    "  (\"Search (has food)0\", \"Found the Food0\"), (\"Search (has food)0\", \"Didn't Find Food0\"),\n",
    "  (\"Search (no food)0\", \"No Food to Find0\"),\n",
    "  (\"Leave0\", \"Has Food1\"), (\"Leave0\", \"No Food1\"),\n",
    "]\n",
    "\n",
    "latex_edge_labels = [\n",
    "  (\"New Patch\", \"Has Food0\", \"p_e\"),\n",
    "  (\"New Patch\", \"No Food0\", \"1-p_e\"),\n",
    "  (\"Search (has food)0\", \"Didn't Find Food0\", \"1-p_s\"),\n",
    "  (\"Search (has food)0\", \"Found the Food0\", \"p_s\"),\n",
    "  (\"Search (no food)0\", \"No Food to Find0\", \"1\"),\n",
    "  (\"Leave0\", \"No Food1\", \"p_e\"),\n",
    "  (\"Leave0\", \"Has Food1\", \"1-p_e\"),\n",
    "]\n",
    "\n",
    "action_nodes = [\n",
    "  \"Search (has food)0\", \"Search (no food)0\", \"Leave0\"\n",
    "]\n",
    "\n",
    "node_colors = {}\n",
    "\n",
    "node_labels = {\n",
    "    \"Has Food0\": \"New Patch\\nHas Food\",\n",
    "    \"No Food0\": \"New Patch Has\\nNo Food\",\n",
    "    \"Has Food1\": \"New Patch\\nHas Food\",\n",
    "    \"No Food1\": \"New Patch Has\\nNo Food\",\n",
    "    \"Search (has food)0\": \"Search\",\n",
    "    \"Leave0\": \"Leave\",\n",
    "    \"Search (no food)0\": \"Search\",\n",
    "    \"Didn't Find Food0\": \"No Food Found\",\n",
    "    \"No Food to Find0\": \"No Food Found\",\n",
    "    \"Found the Food0\": \"Food Found\\n+1 Reward Point\",\n",
    "}\n",
    "\n",
    "\n",
    "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                                      action_nodes=action_nodes,\n",
    "                                      node_colors=node_colors,\n",
    "                                      node_labels=node_labels)\n",
    "Image(output_path, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# MDP Notation\n",
    "To get precise about what we are trying to optimize we first need to introduce some important notation, and formalize many of the general concepts introduced earlier in the book through our Gridworld example. If you are already farmiliar with these ideas feel free to skip this bit. Similarly if you find mathematical notation a bit overwhelming, you can also skim this section, (don't worry about understanding it all right away) and then use this as glossary as needed. A shortened version of these definitions also appear in the glossary/notation reference section found at the end of each notebook.\n",
    "\n",
    "* $\\pi_{\\theta}(a|s)$: **Policy Function** - A policy is the behavioural blueprint for the organism. It's a function that takes (some representation or filtered down aspect of) the environmental state $s$ as input, and guided by its parameters $\\theta$, gives the probability of taking action $a$, where $a$ is in the set $\\mathcal{A}(s)$ of possible actions given state $s$. The organism can then sample an actio from this set according to these probabilities. Sometimes the explicit reference to $\\theta$ is dropped when it is clear from context or does not need to be emphasized as in $\\pi(a|s)$, other times the the reference to the parameters is made more explicit by writing $\\pi(a | s, \\theta)$. In our Gridworld example each of the organisms we defined, 'Random Valid', 'Parameterized Weights', 'Eat When Near' all had a policy function at their core.\n",
    "\n",
    "* $s$: **A State** - The state represents a complete snapshot of what the environment looks like at a given moment. In our Gridworld example this is primarily the positions of food pieces and the organism, but also the number of rounds left in the simulation. The set of all possible states is denoted $\\mathcal{S}$.\n",
    "\n",
    "* $a$: **An Action** - The action an organism takes. Depending on how things are set up in our Gridworld example this might be represented as a direction or as a (row, columns) coordinate of the organism's new position, or as a flattened boolean index of the organism's new position. The set of all possible actions is denoted $\\mathcal{A}$, and the set of possible actions in a given state as $\\mathcal{A}(s)$.\n",
    "\n",
    "* $r$ : **A Reward** - The immediate reward (feedback, score, points etc.) an organism recieves after taking an action $a$ in state $s$ and transitioning to new state $s'$. In our Gridworld example $r = 1$ if the organism eats a food piece as a result of its move and $r = 0$ otherwise.\n",
    "\n",
    "* $\\theta$: **Parameters** - The aspects of an organism's policy function that can be represented by numbers. Note that these do not describe the overall structure of the policy function, but rather determine a particular instance of the policy functions possible *given* the structure (archietecture) of a policy function. In our 'Parameterized Weights' policy from our Gridworld example, the connective weight strengths $W$ are the paramweters, i.e. $\\theta = W$ in for this particular policy. For a more complicated policy with many layers of connective weights we might write $\\theta = \\{W_1, W_2,\\dots, W_N \\}$. We use $\\theta$ as a generic term so that we can make general statements about parameterized policies without having to worry about the particular archiectecure or functional form of the policy.   \n",
    "\n",
    "Given the stochastic nature of the environment (and often the policy as well), at any given time $t$ over the course of a simulation run, each of states, actions and rewards can be thought of as random variables specifically:\n",
    "\n",
    "* $S_t$: **State at Time $t$** - A random variable that denotes the state of the environment at a specific time $t$. For example, $S_t = s$ means that at time $t$, in a particular simulation run, the environment was in state $s$, or in other words that $s$ is the realization of the random variable $S_t$.\n",
    "\n",
    "* $A_t$: **Action at Time $t$** - A random variable denoting the action taken by the organism at time $t$. $A_t = a$ indicates that the action $a$ is taken at time $t$, or that $a$ is the realization of the random variable $A_t$ in a particular simulation run.\n",
    "\n",
    "* $R_t$: **Reward at Time $t$** - A random variable indicating the immediate reward received by the organism at time $t$. $R_t = r$ indicates that the reward $r$ is obtained at time $t$, or in that $r$ is the realization of the random variable $R_t$ in a particular simulation run.\n",
    "\n",
    "* $T$: **Total Simulation Time** - The total number of time steps in a given simulation. There are cases where having an infinite time horizon, $T=\\infty$, is a mathematical convenience, but since our focus is on evolved, living and learning systems, and few things live forever, we will typically work with a finite time horizons.\n",
    "\n",
    "* $t$: **Time-Step Index** - We typically subscript with $t$ to denote the value of a state, reward, action, etc. at a given specific time $t$.\n",
    "\n",
    "We can then think of simulation run as sequence of random variables:\n",
    "$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots, S_{T-1}, A_{T-1}, R_{T}, {S_T}$$\n",
    "\n",
    "The dynamics, or equations of motion, that generate this sequence of random variables are primarily encapsulated in a *transition function*, together with an *initial state distribution*, both defined as follows.\n",
    "\n",
    "* $p(s', r | s, a)$: **Transition Function** - Sometimes called the *State Transition Function*, or the *Transition Kernel* (kernel is more common when dealing with continuous state spaces) this function give the probability of transitioning from state $s$ to $s'$ and recieving reward $r$ from time-step $t$ to $t+1$, given that action $a$ is taken at time $t$. In terms of our previous notation this is defined as:\n",
    "$$ p(s', r | s, a) := \\Pr \\{S_{t+1} = s' , R_{t+1} = r | S_t = s, A_t = a \\}$$  \n",
    "\n",
    "* $p_0(s)$: **Initial State Distribution** - This is the probability distribution (density function) over the set of possible states, \\mathcal{S}, so $p_0(s) := \\Pr \\{S_0 = s\\}$. Sometimes we write $S_0 \\sim p_0$, which is read as 'The random variable $S_0$ is distributed according to the probability density function $p_0$'.\n",
    "\n",
    "This random variable notation also allows us to make uur definition of a policy function more precise: $$\\pi_{\\theta}(a | s) := \\Pr \\{A_{t} = a | S_t = s\\}.$$\n",
    "\n",
    "Then, if a policy is fixed it can simply be folded into the dynamics of the environment, creating what is refered to as the *policy-induced dynamics*.\n",
    "\n",
    "* $p_\\pi(s', r | s)$: **Policy-Induced Dynamics** - This is also called the 'dynamics under policy $\\pi$' and is defined as:\n",
    "$$p_\\pi(s', r | s):= \\Pr \\{S_{t+1} = s' , R_{t+1} = r | S_t = s, \\pi \\} = \\sum_{a\\in\\mathcal{A}(s)} \\pi_\\theta (a | s) \\ p(s', r | s, a).$$\n",
    "Sometimes the depedence on a specific policy, $\\pi$, is taken as implicit and we simply write $p(s', r | s)$.\n",
    "\n",
    "The takeaway here is that for a fixed $\\pi$ and a given transition function $p$ (and initial state distribution $p_0$) the stochastic dynamics of the system are completely determined.\n",
    "\n",
    "With all that defined we can start to formally describe how rewards should be added up over time to define our goals. We just need to introduce the idea of a *Return* and a *Value* function.\n",
    "\n",
    "* $G_t$: **Return following time $t$** - Sometimes called the reward to go, or simply the return, this a random variable that indicates the total reward yet to be realized after time $t$, i.e. $G_t := \\sum_{k=t+1}^T R_k$.\n",
    "\n",
    "* $v_{\\pi}(s,t)$: **Value Function** - A function giving the *expected* return conditional on being in state $s$ at time $t$ and following a given policy $\\pi$, specifically:\n",
    "$$v_{\\pi}(s,t) := \\mathbb{E}_\\pi \\left[G_t | S_t = s \\right].$$\n",
    "In a slight stretch of notation $t$ can be treated as part of $s$ and we can write $v_{\\pi}(s)$. The dependence on a specific policy is sometimes treated as implicit and we write $v(s)$ or $v(s,t)$.\n",
    "\n",
    "In this context then our goal is to maximize the *Expectation* of a simulation run, or equivalently the average value from playing through many simulations (in the limit as many --> $\\infty$). We call this formalization of our goal objective function and define our particular objective function in this context as\n",
    "\n",
    "* $J(\\theta)$: **Objective Function** - The function that we are trying to maximize, emphasizing the dependence on the parameters, $\\theta$.\n",
    "\n",
    "The objective function is in some ways the most subjective thing in this whole set up. It's what defines the \"problem to be solved\". In our particular case we we are going to use the following as our objective.\n",
    "\n",
    "$$J(\\theta):= \\mathbb{E}\\left[ v_{\\pi_\\theta}(S_0) \\right] = \\sum_{s \\in \\mathcal{S}} p_0(s) \\cdot v_{\\pi_{\\theta}}(s)$$\n",
    "\n",
    "Then the formalization of our problem is choosing parameters $\\theta$ such that $J(\\theta)$ is as high as possible. In general this goal is written as:\n",
    "$$ \\max_{\\theta} J(\\theta),$$\n",
    "\n",
    "and in our particular case of maximizing the expected value, given a finite and discrete state space, our goal is written as:\n",
    "\n",
    "$$ \\max_\\theta\\sum_{s \\in \\mathcal{S}} p_0(s) \\cdot v_{\\pi_{\\theta}}(s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "intro_RL_value_P1C4_Sequence1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
