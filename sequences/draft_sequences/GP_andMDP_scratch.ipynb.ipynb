{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOleROiWsIkzWmzivXYgZHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/super-opt-perturb/sequences/draft_sequences/GP_andMDP_scratch.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In a given learning episode one of four cases can occur:\n",
        "\n",
        "1. Prey is present, given the sensory input the organisms strikes, a positive reward is obtained\n",
        "2. Prey is not present, given the sensory input the organism strikes, a negative reward is obtained\n",
        "3. Prey is present, given the sensory input the organism does not strike, no reward is obtained\n",
        "4. Prey is not present, given the sensory input the organism does not strike, no reward is obtained\n",
        "\n",
        "We have the (initial) caveat that from the perspective of the organisms cases 3 and 4 are identical, so the parameter update rule must also be identical in each of those cases.\n",
        "\n",
        "It has been noted and proposed many times that the inherent stochasticity of synaptic firing might provide a kind of natural 'guess' and allow for this kind of learning to be implemented in the brain, [citations]. Empirical evidence for this kind of learning is also begining to accumulate [citations].\n",
        "\n",
        "The basic idea is as follows. Due to the stochastic spiking of neurons, in any given episode of senosry input -> behaviour production -> contingent reward, some but not all of the synaptic weights are causally implicated in the production of the behaviour (and consequent reward). When a good thing happens, synaptic connections that were involved in the generation of the behaviour should be strengthened, and conversely when bad things happen synaptic connections that were involved in the generation of the behaviour should be weakened. Synaptic connections that were not involved in producing the behaviour should be left unchanged. Zooming in to the synaptic level the four cases above are expanded as follows:\n",
        "\n",
        "1. Both input and output neurons spike, a good thing happens\n",
        "2. Both input and output neurons spike, a bad thing happens\n",
        "3. Input neuron spikes but output neuron doesn't spike, a neutral thing happens\n",
        "4. Input neuron doesn't spike, output neuron spikes, a good thing happens\n",
        "5. Input neuron doesn't spike, output neuron spikes, a bad thing happens\n",
        "6. Neither input nor output neuron spikes, a neutral thing happens.\n",
        "\n",
        "In this our first and most simple (from a physiological mechanism perspective, actually a bit complicated from a mathematical description perspective), only in the first two cases, when both the pre- and post- synaptic neurons fire, is the synapse causally implicated in the reward outcome. So our rule needs to prescribe no change, unless both pre- and post- synaptic neurons have fired. If a good thing happens, then the connection should be reinforced, and if a bad thing happens then the connection should be weakened. Our learning rule written to echo and presiage the 'REINFORCE' formulation of Williams (1992), and also Resrola Wagnar rule, and also TD formulations. In some sense this is cannonical form of learning rules is.\n",
        "\n",
        "$$\\Delta w_i = \\alpha \\cdot (r - b) \\cdot s_i \\cdot s_o$$\n",
        "\n",
        "Almost every learning rule for parameters that we look at will be roughly composed of these three factors: learning rate, error, eligibility. The first factor is positive learning rate or step size parameter of the learning process. In this specific case $\\alpha$ is constant, but it could be made contingent on various factors in more complicated models, e.g. learning rates often start high and are lowered as the learning system gets more 'dialed in'. The second part is an 'error' term, in this case a 'reward prediction error', $(r-b)$, gives the way in which the actual received reward $r$ differs from some baseline expectation. Again in more complicated learning rules $b$ could be made contingent on many factors, and this contingency of $b$ on other factors might itself be learned. Here though, we assume a constant expected reward, $b=0$, so we can think of $b$ as an expectation of how good the world is on average, if the organism is performing as well as expected. We imagine that parameters like $\\alpha$ and $b$ are set and tuned by evolution to ensure effective learning. Lastly, in this simple model the $s_i \\cdot s_o$ term is the 'eligibility' of the synapse for change. In general, this will be some measure of how causally implicated the particular parameter $w_i$ is in causing the received reward. Here we use one of the simplest possible eligibility terms, which takes the value one only when both pre- and post- synaptic neurons fire, and is otherwise zero. Again in more complicated learning rules, this eligibility term could be more complicated, e.g. it might incorporate knowledge of the structure of the network producing the behaviour, often in the form of a gradient, to further modulate the strength and direction of the weight change. Note, $w_i$ is the $i^{\\text{th}}$ element of $\\mathbf{W}$ and $s_i$ and $s_o$ are the realized spikes during the episode for the $i^{\\text{th}}$ input neuron and the output neuron respectively. Okay, let's implement and see how well this works!"
      ],
      "metadata": {
        "id": "vgLrLL-fs3pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We model this creature's sensory-behaviour system as follows. Let $\\mathbf{x}$ be the raw sensory input vector in a given episode. We imagine that each element of $\\mathbf{x}$, denoted $x_i$ corresponds to the activation level of a single photosensitive neuron. These activation levels might be losely interpreted as firing rates. This input neurons are then connected by synapses to a single output neuron. The activation level of this output neuron is computed as\n",
        "$$y = \\mathbf{Wx}$$\n",
        "Here, $\\mathbf{W}$ is the matrix of synaptic weights between the input neurons and the output neuron (is this case there is only one output so $\\mathbf{W}$ is effectively a row vector). As a quick reminder, in our notation, bold lowercase letters represent column vectors, while bold uppercase letters denote matrices or higher-dimensional tensors. We imagine that the probabilistic spiking of this output neuron determines the strike-no-strike behaviour of the organism, specifically:\n",
        "$$ \\Pr \\{\\text{strike}\\} = \\sigma(y) $$\n",
        "$$ \\Pr \\{\\text{no strike}\\} = 1 - \\sigma(y)$$\n",
        "\n",
        "Here $\\sigma(x): \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}$ is the standard logistic (sigmoid) function."
      ],
      "metadata": {
        "id": "Ol1hpBxbpi9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_episode(W, x, target, rng, alpha=0.01, b=0, ):\n",
        "  #forward pass, generate behaviour\n",
        "  sensory_nueron_spike_probs = np_sigmoid(x)\n",
        "  sensory_neuron_spikes = rng.binomial(1, sensory_nueron_spike_probs)\n",
        "  output_neuron_activation = np.dot(W, sensory_neuron_spikes)\n",
        "  output_neuron_spike_prob = np_sigmoid(output_neuron_activation)\n",
        "  output_neuron_spike = rng.binomial(1, output_neuron_spike_prob)\n",
        "  #evaluate behaviour\n",
        "  reward = output_neuron_spike * target\n",
        "  # backward pass,update parameters\n",
        "  W += alpha * (reward - b) * output_neuron_spike * sensory_neuron_spikes\n",
        "  return W, reward, output_neuron_spike, output_neuron_spike_prob"
      ],
      "metadata": {
        "id": "dLxma3Exf7fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now embedd those learning episodes in a training loop\n",
        "# pre calculate activation to probability for efficiency\n",
        "train_rng = np.random.default_rng(1234)\n",
        "W = train_rng.standard_normal(size=(1, Xs.shape[1]))\n",
        "\n",
        "epochs = 500  # Number of times to iterate over the dataset\n",
        "performance_tracker = {'cumulative_reward': [], 'accuracy': [],\n",
        "                       'TP':[], 'FP':[], 'TN':[], 'FN':[],}\n",
        "max_possible_reward = np.sum(y1==1)\n",
        "alpha=0.00001\n",
        "b=0\n",
        "pbar = tqdm(range(epochs), desc='Training Progress')\n",
        "for epoch in pbar:\n",
        "  cumulative_reward = 0\n",
        "  TP, FP, TN, FN = 0, 0, 0, 0\n",
        "  shuffled_indices = train_rng.permutation(Xs.shape[0])\n",
        "  for i in shuffled_indices:  # Iterate over each example\n",
        "    x = Xs[i]\n",
        "    target = y1[i]\n",
        "    W, reward, output_neuron_spike, _ = learning_episode(W, x, target,\n",
        "                                                         train_rng, alpha, b)\n",
        "    # Track performance\n",
        "    cumulative_reward += reward\n",
        "    # Update confusion matrix\n",
        "    if output_neuron_spike == 1 and target == 1:\n",
        "      TP += 1\n",
        "    elif output_neuron_spike == 1 and target == -1:\n",
        "      FP += 1\n",
        "    elif output_neuron_spike == 0 and target == 1:\n",
        "      FN += 1\n",
        "    elif output_neuron_spike == 0 and target == -1:\n",
        "      TN += 1\n",
        "  epoch_accuracy = (TP + TN) / Xs.shape[0]\n",
        "  performance_tracker['cumulative_reward'].append(cumulative_reward)\n",
        "  performance_tracker['accuracy'].append(epoch_accuracy)\n",
        "  # Store confusion matrix values\n",
        "  performance_tracker.setdefault('TP', []).append(TP)\n",
        "  performance_tracker.setdefault('FP', []).append(FP)\n",
        "  performance_tracker.setdefault('TN', []).append(TN)\n",
        "  performance_tracker.setdefault('FN', []).append(FN)\n",
        "  pbar.set_postfix({\n",
        "        'Acc': f'{epoch_accuracy:.4f}',\n",
        "        'Cum. Reward': cumulative_reward/max_possible_reward,\n",
        "        'TP': TP, 'FP': FP, 'TN': TN, 'FN': FN\n",
        "    })"
      ],
      "metadata": {
        "id": "t9SNxgT2uzYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}