{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/main/sequences/P1C4_LifeIsRL/student/P1C4_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/sequences/P1C4_LifeIsRL/student/P1C4_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Can you help me review this for clarity and correctness, please don't write a new version just call out mistakes or specific sentences for obvious improvement (along with concrete re-writes of those sentences that address the issue).\n",
    "\n",
    "___\n",
    "# **1.4.1: Thinking of Life as a Markov Decision Process**\n",
    "## Objective:\n",
    "In this sequence we explore and \"solve\" a simplified version of our Gridworld within the Markov Decision Process (MDP) framework. MDPs offer a robust, flexible, and unified approach for rigorously defining and identifying optimal policies across various scenarios. Of particular interest to us is the way that MDPs can be used to frame the general learning problem faced by animals with brains. Although MDPs may initially seem complex, their components have straightforward and intuitive interpretations. We are already acquainted with the key elements of an MDP:\n",
    "\n",
    "* Policy: The behavioural rule that maps stimuli to an organism's actions.\n",
    "* Organism (Agent): The entity that reacts to stimuli and performs actions.\n",
    "* Actions: The specific responses an organism makes at any moment, guided by its policy.\n",
    "* Environment: The context in which an organism operates, including the source of stimuli, the rules for state changes in response to actions, and the nature of rewards based on actions and state transitions. (Other organisms' and their policies are often an important part of the environment of the focal organism.)\n",
    "* Reward: The immediate outcomes of an organism's actions. In evolutionary contexts, rewards typically include food, predator avoidance, and other signals crucial for homeostasis.\n",
    "* Markov Process: A framework for modeling stochastic dynamics. A Markov Process diveds the world into a set of possible states defines a transition probabilities between these states. Crucially, these probabilities depend solely on the current state, embodying the Markov property and simplifying analysis. Historical relevance to state dynamics is integrated into the state definition, ensuring a comprehensive state concept.\n",
    "\n",
    "In the sequence we will introduce the crucial notions of **Returns** and **Value**, which integrates these elements and allows for rigorous optimization. Returns are the total accumulated rewards over a given episode. In an evolutionary context, fitness is more closely linked to  *returns* than to immediate rewards. While returns focus on the total success during an episode, value is purely forward looking, and is defined as total expected future reward, from a specific state under a particular policy. We will focus on using value to determine the optimal policy within a simple MDP. We will also see how this approach becomes impractical for complex problems due to scalability and computational limits. Despite these practical limitations, returns and value provide the theoretical foundation for more scalable and hence practical solutions which can be understood as approximations to an ideal but intractable solution method.\n",
    "\n",
    "Previously we have also touched upon \"partial observability\", i.e. situations where the full state of the environment is not known to the organism. Most organisms are not omniscient, so in some sense partial observability is always the case. For simplicity, we'll set aside partial observability for now and focus on cases where the relevant state of the world is perfectly known to the organism. This will streamline our formal introduction to MDPs and the use of value to identify an optimal policy.\n",
    "\n",
    "We will conclude with a brief overview of how MDPs can serve as a model for brain mediated behaviour and learning.\n",
    "\n",
    "While this may all seem a bit much, our simplified Gridworld example will make these concepts more accessible and less intimidating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dependencies, Imports and Setup\n",
    "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
    "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
    "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import pygraphviz as pgv\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from io import BytesIO\n",
    "from enum import Enum\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, clear_output, Markdown, HTML, Image, IFrame\n",
    "from jupyterquiz import display_quiz\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "# random seed settings and\n",
    "# getting torch to use gpu if it's there\n",
    "\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook but not needed.\")\n",
    "    print(\"If possible, in the menu under `Runtime` -> \")\n",
    "    print(\"`Change runtime type.`  select `CPU`\")\n",
    "\n",
    "  return device\n",
    "\n",
    "\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()\n",
    "\n",
    "\n",
    "def printmd(string):\n",
    "  display(Markdown(string))\n",
    "\n",
    "\n",
    "# the different utility .py files used in this notebook\n",
    "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
    "             'gw_widgets.py', 'gw_NN_RL.py']\n",
    "#filenames = []\n",
    "# just run the code straight out of the response, no local copies needed!\n",
    "for filename in filenames:\n",
    "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
    "  response = requests.get(url)\n",
    "  # Check that we got a valid response\n",
    "  if response.status_code == 200:\n",
    "    code = response.content.decode()\n",
    "    exec(code)\n",
    "  else:\n",
    "    print(f'Failed to download {url}')\n",
    "\n",
    "# environment contingent imports\n",
    "try:\n",
    "  print('Running in colab')\n",
    "  from google.colab import output\n",
    "  output.enable_custom_widget_manager()\n",
    "  from google.colab import data_table\n",
    "  data_table.disable_dataframe_formatter()\n",
    "  #from google.colab import output as colab_output\n",
    "  #colab_output.enable_custom_widget_manager()\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print('Not running in colab')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib widget\n",
    "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
    "plt.ioff() #need to use plt.show() or display explicitly\n",
    "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def content_review(notebook_section: str):\n",
    "  return DatatopsContentReviewContainer(\n",
    "    \"\",  # No text prompt\n",
    "    notebook_section,\n",
    "    {\n",
    "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "      \"name\": \"neuro_book\",\n",
    "      \"user_key\": \"xuk960xj\",\n",
    "    },\n",
    "  ).render()\n",
    "\n",
    "feedback_prefix = \"P1C4_S1\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @title plotting functions\n",
    "#################################################\n",
    "# More plotting functions\n",
    "#################################################\n",
    "\n",
    "\n",
    "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
    "                    name=None):\n",
    "  \"\"\"\n",
    "  Plot vector field indicating critter direction probabilities.\n",
    "\n",
    "  Args:\n",
    "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
    "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
    "      and values as lists of direction probabilities corresponding to the\n",
    "      directions ['right', 'down', 'left', 'up'].\n",
    "    critter (int): Identifier for which critter directions are associated with.\n",
    "    deterministic (bool, optional): If True, the probabilities array is\n",
    "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
    "      and are larger. Defaults to False.\n",
    "  \"\"\"\n",
    "\n",
    "  #looks like direction ignores inverted axis\n",
    "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
    "                       'left': (-1, 0), 'up': (0, 1)}\n",
    "  # but offsets need to be aware of inverted\n",
    "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
    "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
    "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
    "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
    "  # same logic for colors\n",
    "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
    "  # Get the offset and color for this critter\n",
    "  critter_offset = critter_offsets[critter]\n",
    "  critter_color = critter_colors[critter]\n",
    "\n",
    "  # Add legend only if critter is not 0\n",
    "  custom_leg_handles = []\n",
    "  if critter != 0:\n",
    "    if name is None:\n",
    "      name = f'Critter {critter}'\n",
    "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
    "    # Add the legend for this critter\n",
    "    custom_leg_handles.append(legend_patch)\n",
    "\n",
    "  C, R, U, V, A = [], [], [], [], []\n",
    "\n",
    "  for loc in loc_prob_dict.keys():\n",
    "    row, col = loc\n",
    "    probs = loc_prob_dict[loc]\n",
    "    for dir_key, prob in probs.items():\n",
    "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
    "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
    "      U.append(direction_vectors[dir_key][0])\n",
    "      V.append(direction_vectors[dir_key][1])\n",
    "\n",
    "      if deterministic:\n",
    "        A.append(1 if prob == max(probs.values()) else 0)\n",
    "      else:\n",
    "        A.append(prob)\n",
    "\n",
    "  linewidth = 1.5 if deterministic else 0.5\n",
    "  scale = 15 if deterministic else 30\n",
    "\n",
    "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
    "            scale=scale, linewidth=linewidth)\n",
    "  return fig, ax, custom_leg_handles\n",
    "\n",
    "\n",
    "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
    "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
    "  returns fig and ax handles for further use\n",
    "  Args:\n",
    "    num_rows (int): number of rows in the grid (vertical dimension)\n",
    "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
    "\n",
    "  Returns:\n",
    "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
    "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
    "  \"\"\"\n",
    "  # Create a new figure and axes with given figsize\n",
    "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
    "  # Set width and height padding, remove horizontal and vertical spacing\n",
    "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
    "  # Show right and top borders (spines) of the plot\n",
    "  ax.spines[['right', 'top']].set_visible(True)\n",
    "  # Set major ticks (where grid lines will be) on x and y axes\n",
    "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
    "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
    "  # Set labels for major ticks with font size of 8\n",
    "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
    "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
    "  # Set minor ticks (no grid lines here) to be between major ticks\n",
    "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
    "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
    "  # Move x-axis ticks to the top of the plot\n",
    "  ax.xaxis.tick_top()\n",
    "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
    "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
    "  # Remove minor ticks (not the grid lines)\n",
    "  ax.tick_params(which='minor', bottom=False, left=False)\n",
    "  # Set limits of x and y axes\n",
    "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
    "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
    "  # Invert y axis direction\n",
    "  ax.invert_yaxis()\n",
    "  # If title is provided, set it as the figure title\n",
    "  if title is not None:\n",
    "    fig.suptitle(title)\n",
    "  # Hide header and footer, disable toolbar and resizing of the figure\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  # Redraw the figure with these settings\n",
    "  fig.canvas.draw()\n",
    "  # Return figure and axes handles for further customization\n",
    "  return fig, ax\n",
    "\n",
    "\n",
    "def plot_food(fig, ax, rc_food_loc, food=None, size=None,\n",
    "              show_food=True):\n",
    "  \"\"\"\n",
    "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
    "    food: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
    "    new if no handle was passed or updated if it was\n",
    "  \"\"\"\n",
    "  # if no PathCollection handle passed in:\n",
    "  if size is None:\n",
    "    size=150\n",
    "  if food is None:\n",
    "    food = ax.scatter([], [], s=size, marker='o',\n",
    "                      color='red', label='Food')\n",
    "  if show_food:\n",
    "    rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
    "    #matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "    #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "    food.set_offsets(np.fliplr(rc_food_loc))\n",
    "  return food\n",
    "\n",
    "\n",
    "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]],\n",
    "                  size=None) -> List[Dict[str, object]]:\n",
    "  \"\"\"\n",
    "  Plots multiple types of \"critters\" on a grid implied by the given\n",
    "  fig, ax arguments.\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects.\n",
    "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
    "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
    "    critter.\n",
    "\n",
    "  Returns:\n",
    "    Updated critter_specs with handles.\n",
    "  \"\"\"\n",
    "  if size is None:\n",
    "    size=250\n",
    "  for spec in critter_specs:\n",
    "    # Ensure required keys are present\n",
    "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
    "      if key not in spec:\n",
    "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
    "    handle_ = spec.get('handle')\n",
    "    if handle_ is None:\n",
    "      handle_ = ax.scatter([], [], s=size, marker=spec['marker'],\n",
    "                           color=spec['color'], label=spec['name'],\n",
    "                           edgecolors='white', linewidths=1)\n",
    "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
    "    spec.update({'handle': handle_})\n",
    "  return critter_specs\n",
    "\n",
    "\n",
    "def plot_critter(fig, ax, rc_critter_loc,\n",
    "                 critter=None, critter_name='Critter'):\n",
    "  \"\"\"\n",
    "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
    "    critter: a handle for the existing food matplotlib PatchCollection object\n",
    "    if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
    "    either new if no handle was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "  if critter is None:\n",
    "    critter = ax.scatter([], [], s=250, marker='h',\n",
    "                         color='blue', label=critter_name)\n",
    "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
    "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
    "  critter.set_offsets(np.flip(rc_critter_loc))\n",
    "  return critter\n",
    "\n",
    "\n",
    "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov,\n",
    "             opaque=False, fov=None):\n",
    "  \"\"\"\n",
    "  Plots a mask on a grid implied by the given fig, ax arguments\n",
    "\n",
    "  Args:\n",
    "    fig, ax: matplotlib figure and axes objects\n",
    "    rc_critter: ndarry(int) (row,col) of the critter\n",
    "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
    "  Returns:\n",
    "    a handle for matplotlib Image object of mask, either new if no handle\n",
    "    was passed in or updated if it was.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
    "  mask_array = np.ones((n_rows, n_cols, 4))\n",
    "  mask_array[:, :, :3] = 0.5  # light grey color\n",
    "  if has_fov == True:\n",
    "    if opaque:\n",
    "      mask_array[:, :, 3] = 1.0  # 50% opacity\n",
    "    else:\n",
    "      mask_array[:, :, 3] = 0.5  # 50% opacity\n",
    "    # Create arrays representing the row and column indices\n",
    "    rows = np.arange(n_rows)[:, np.newaxis]\n",
    "    cols = np.arange(n_cols)[np.newaxis, :]\n",
    "    # Iterate over each critter location\n",
    "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
    "    # Set the region within the specified radius around the critter to transparent\n",
    "    mask_array[dist <= radius, 3] = 0\n",
    "  else:\n",
    "    mask_array[:, :, 3] = 0\n",
    "\n",
    "  if fov is None:\n",
    "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
    "  else:\n",
    "    fov.set_data(mask_array)\n",
    "\n",
    "  return fov\n",
    "\n",
    "\n",
    "def remove_ip_clutter(fig):\n",
    "  fig.canvas.header_visible = False\n",
    "  fig.canvas.toolbar_visible = False\n",
    "  fig.canvas.resizable = False\n",
    "  fig.canvas.footer_visible = False\n",
    "  fig.canvas.draw()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld Board Class\n",
    "# Local definition to be put in utils later\n",
    "\n",
    "\n",
    "class GridworldBoard():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters for our Gridworld game.\n",
    "\n",
    "  board state is represented by primarily by pieces, scores, rounds_left and is_over\n",
    "\n",
    "  pieces is a batch x n_rows x n_cols numpy array positive integers are critter\n",
    "  locations 0's are empty space and negative integers are food. Each critter is\n",
    "  unique and executing it's own policy so they are non-fungible, whereas food\n",
    "  (of the same type) is always the same, so there can and typically will be\n",
    "  duplicates of negative integers in the pieces array, but never of positive\n",
    "  integers\n",
    "\n",
    "  For pieces first dim is batch, second dim row , third is col,\n",
    "  so pieces[0][1][7] is the square in row 2, in column 8 of the first board in\n",
    "  the batch of boards.\n",
    "\n",
    "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
    "  critter on each board in the batch (note off by one indexing)\n",
    "\n",
    "  rounds_left is how many rounds are left in the game. Each critter gets one\n",
    "  move per round so this will be the same for every critter in every batch.\n",
    "\n",
    "  is_over just tracks whether each game in each batch has concluded, this allows\n",
    "  for probabalistic end times, not just deterministic end times based on moves left\n",
    "\n",
    "  Note: In this version the game class handles the end conditions, without any\n",
    "      input from this board class. Even though they are not used, max_rounds_taken\n",
    "      and end_prob are passed in to the constructor for completeness.\n",
    "\n",
    "  Note:\n",
    "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
    "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
    "    as np.array visualization is aligned with typical tensor notation but at odds\n",
    "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
    "    horizontal, second dim, y, is vertical, so we use invert y-axis when plotting\n",
    "    with matplotlib\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  class CritterFoodType(Enum):\n",
    "    FOOD = \"food\"\n",
    "    PREY = \"prey\"\n",
    "    PREDATOR = \"predator\"\n",
    "\n",
    "  ARRAY_PAD_VALUE = -200\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=2,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_foragers=1,\n",
    "               num_predators=0,\n",
    "               max_rounds_taken=30,\n",
    "               end_prob=0.00,\n",
    "               food_num_deterministic = True,\n",
    "               food_patch_prob=10.0/49.0,\n",
    "               food_forager_regen = True,\n",
    "               rng=None,\n",
    "               state_elements = ['pieces', 'scores', 'is_over', 'rounds_left'],\n",
    "               init_board_state = None\n",
    "               ):\n",
    "\n",
    "    \"\"\"Set the parameters of the game.\"\"\"\n",
    "    # size of the board/world\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    #number and type of critters on the board\n",
    "    self.num_foragers = num_foragers\n",
    "    self.num_predators = num_predators\n",
    "    # foragers will be indicated by lower valued positive integers, predators\n",
    "    # by higher valued intagers\n",
    "    self.forager_predator_threshold = self.num_foragers\n",
    "    self.num_critters = num_foragers + num_predators\n",
    "\n",
    "    # end conditions can be deterministic or stochastic\n",
    "    # one of moving, or eating or both might take time, e.g. eating might be\n",
    "    # automatic and free after moving, conversely, moving might be free, but\n",
    "    # eating count towards the session/episode ending, or both might\n",
    "    self.max_rounds_taken = max_rounds_taken\n",
    "    self.end_prob = end_prob\n",
    "\n",
    "    # what proportion of the (non-critter occupied) patches contain food.\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.food_num_deterministic = food_num_deterministic\n",
    "    if self.food_num_deterministic:\n",
    "      self.num_food = int((self.n_rows * self.n_cols - self.num_critters)\n",
    "                          * self.food_patch_prob)\n",
    "    self.food_forager_regen = food_forager_regen\n",
    "\n",
    "    # reproducible stochasticity\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "    self.state_elements = state_elements\n",
    "\n",
    "    # initialize the board\n",
    "    if init_board_state is None:\n",
    "      init_board_state = self.get_init_board_state()\n",
    "\n",
    "    self.set_state(init_board_state)\n",
    "\n",
    "\n",
    "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
    "    \"\"\"\n",
    "    Samples random 2d grid locations without replacement, useful for placing\n",
    "    critters and food on the board.\n",
    "\n",
    "    Args:\n",
    "      n_rows: int, number of rows in the grid\n",
    "      n_cols: int, number of columns in the grid\n",
    "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
    "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
    "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
    "        the row indices and the second giving the col indices. Useful for indexing into\n",
    "        an n_rows by n_cols numpy array.\n",
    "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up default random generator, use the boards default if none explicitly given\n",
    "    if rng is None:\n",
    "      rng = self.rng\n",
    "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
    "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
    "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
    "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
    "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
    "    rc_plotting = np.array(rc_index).T\n",
    "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
    "    return int_loc, rc_index, rc_plotting\n",
    "\n",
    "\n",
    "  def get_init_board_state(self):\n",
    "    \"\"\"\n",
    "    Set up starting board using game parameters\n",
    "    \"\"\"\n",
    "    state = {}\n",
    "    state['rounds_left'] = (np.ones(self.batch_size) *\n",
    "                           self.max_rounds_taken)\n",
    "    state['is_over'] = np.zeros(self.batch_size, dtype=bool)\n",
    "    state['scores'] = np.zeros((self.batch_size, self.num_critters))\n",
    "\n",
    "    # create an empty board array.\n",
    "    pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols),\n",
    "                       dtype=int)\n",
    "    # Place critter and initial food items on the board randomly\n",
    "    if self.food_num_deterministic:\n",
    "      init_food_nums = [self.num_food] * self.batch_size\n",
    "    else:\n",
    "      init_food_nums = self.rng.binomial(self.n_rows * self.n_cols - self.num_critters,\n",
    "                                         self.food_patch_prob, size=self.batch_size)\n",
    "    # place food and critters randomly\n",
    "    for ii in np.arange(self.batch_size):\n",
    "      # num_food+num_critter because we want critter and food locations\n",
    "      int_loc, rc_idx, rc_plot = self.init_loc(\n",
    "        self.n_rows, self.n_cols, init_food_nums[ii]+self.num_critters)\n",
    "      # critter random start locations\n",
    "      for c_ in np.arange(self.num_critters):\n",
    "        pieces[(ii, rc_idx[0][c_], rc_idx[1][c_])] = c_ + 1\n",
    "      # food random start locations\n",
    "      for f_ in np.arange(init_food_nums[ii]):\n",
    "        pieces[(ii, rc_idx[0][self.num_critters + f_],\n",
    "                    rc_idx[1][self.num_critters + f_])] = -f_ - 1\n",
    "    state['pieces'] = pieces\n",
    "    return state\n",
    "\n",
    "\n",
    "  def set_state(self, board, check=False):\n",
    "    \"\"\" board is dictionary giving game state \"\"\"\n",
    "    if check:\n",
    "      if board['pieces'].shape != (self.batch_size, self.n_rows, self.n_cols):\n",
    "        raise ValueError(\"Invalid shape for 'pieces'\")\n",
    "      if board['scores'].shape != (self.batch_size, self.num_crititters):\n",
    "        raise ValueError(\"Invalid shape for 'scores'\")\n",
    "      if board['rounds_left'].shape != (self.batch_size,):\n",
    "        raise ValueError(\"Invalid shape for 'rounds_left'\")\n",
    "      if board['is_over'].shape != (self.batch_size,):\n",
    "        raise ValueError(\"Invalid shape for 'is_over'\")\n",
    "    for key in self.state_elements:\n",
    "      if key in board:\n",
    "        setattr(self, key, board[key].copy())\n",
    "      else:\n",
    "        raise ValueError(f\"Key '{key}' not found in the provided board state.\")\n",
    "\n",
    "\n",
    "  def get_state(self):\n",
    "    \"\"\" returns a board state dictionary\"\"\"\n",
    "    state = {key: getattr(self, key).copy() for key in self.state_elements}\n",
    "    return state\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.pieces[index]\n",
    "\n",
    "\n",
    "  def get_critter_food_type(self, critter_food):\n",
    "    if critter_food <= -1:\n",
    "        critter_food_type = self.CritterFoodType.FOOD\n",
    "    elif critter_food > self.forager_predator_threshold:\n",
    "        critter_food_type = self.CritterFoodType.PREDATOR\n",
    "    else:\n",
    "        critter_food_type = self.CritterFoodType.PREY\n",
    "    return critter_food_type\n",
    "\n",
    "\n",
    "  def get_type_masks(self):\n",
    "    \"\"\"\n",
    "    Returns masks indicating the position types on the board.\n",
    "    Returns:\n",
    "        tuple: Tuple containing masks for empty spaces, food, prey, and predator.\n",
    "    \"\"\"\n",
    "    empt_mask = self.pieces == 0\n",
    "    food_mask = self.pieces <= -1\n",
    "    prey_mask = (1 <= self.pieces) & (self.pieces <= self.forager_predator_threshold)\n",
    "    pred_mask = self.forager_predator_threshold < self.pieces\n",
    "    return empt_mask, food_mask, prey_mask, pred_mask\n",
    "\n",
    "\n",
    "  def get_collisions(self, moves, critter_food, critter_food_type):\n",
    "    \"\"\"\n",
    "    Determine the collision results and update scores accordingly.\n",
    "    Args:\n",
    "        moves (tuple): Tuple of arrays indicating the moves.\n",
    "        critter_food (int): Index to identify the critter or food.\n",
    "        critter_food_type (enum): Type of the critter or food\n",
    "    Returns:\n",
    "        tuple: Tuple containing move collision messages and separates out the\n",
    "        moves by where they land i.e., empty spaces, food, prey, and predator.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    move_mask = np.zeros(self.pieces.shape, dtype=bool)\n",
    "    move_mask[moves] = True\n",
    "    (empt_mask, food_mask,\n",
    "     prey_mask, pred_mask) = self.get_type_masks()\n",
    "\n",
    "    move_coll_msg = np.zeros(batch_size)\n",
    "    empt_moves = np.where(empt_mask & move_mask)\n",
    "    food_moves = np.where(food_mask & move_mask)\n",
    "    prey_moves = np.where(prey_mask & move_mask)\n",
    "    pred_moves = np.where(pred_mask & move_mask)\n",
    "    move_coll_msg[empt_moves[0]] = 1\n",
    "\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      move_coll_msg[food_moves[0]] = 2\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      move_coll_msg[food_moves[0]] = 3\n",
    "      move_coll_msg[prey_moves[0]] = 4\n",
    "    # all collision types are blocking for food types\n",
    "\n",
    "    return (move_coll_msg, empt_moves, food_moves, prey_moves, pred_moves)\n",
    "\n",
    "\n",
    "  def update_scores(self, move_coll_msg, critter_food,\n",
    "                    critter_food_type, prey_moves):\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      self.scores[:, critter_food-1] += (move_coll_msg == 2)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      # predators that eat get a point\n",
    "      self.scores[:, critter_food-1] += (move_coll_msg == 4)\n",
    "      # prey that are eaten lose 10 points\n",
    "      who_eaten = self.pieces[prey_moves]\n",
    "      self.scores[prey_moves[0], who_eaten-1] -= 10\n",
    "    # food types don't get a score, it's a neuro book\n",
    "\n",
    "\n",
    "  def move_pieces(self, critter_food, move_coll_msg, moves):\n",
    "    \"\"\"\n",
    "    Move the pieces on the board based on the collision messages.\n",
    "\n",
    "    Args:\n",
    "        critter_food (int): Index to identify the critter or food.\n",
    "        move_coll_msg (np.array): Array of collision messages.\n",
    "        moves (tuple): Tuple of arrays indicating the moves.\n",
    "    \"\"\"\n",
    "    old_locs = np.where(self.pieces == critter_food)\n",
    "    vacated_old_locs = np.column_stack(old_locs)[np.where(move_coll_msg > 0)]\n",
    "    vacated_old_locs_idx = (vacated_old_locs[:,0],\n",
    "                            vacated_old_locs[:,1],\n",
    "                            vacated_old_locs[:,2])\n",
    "    self.pieces[vacated_old_locs_idx] = 0\n",
    "    new_locs = np.column_stack(moves)[np.where(move_coll_msg > 0)]\n",
    "    new_locs_idx = (new_locs[:,0], new_locs[:,1], new_locs[:,2])\n",
    "    self.pieces[new_locs_idx] = critter_food\n",
    "\n",
    "\n",
    "  def replace_destroyed(self, destroying_moves, old_pieces):\n",
    "    \"\"\"\n",
    "    Replace the destroyed pieces on the board.\n",
    "\n",
    "    Args:\n",
    "        destroying_moves (tuple): Tuple of arrays indicating the moves that\n",
    "        resulted in destruction.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = old_pieces.shape\n",
    "    g_gone = np.zeros(batch_size)\n",
    "    g_gone[destroying_moves[0]] = 1\n",
    "    which_gone = old_pieces[destroying_moves]\n",
    "    if np.sum(g_gone) > 0:\n",
    "      num_empty_after = (n_rows*n_cols - self.num_food - self.num_critters + 1)\n",
    "      p_new_locs = np.where(np.logical_and(\n",
    "        self.pieces == 0, g_gone.reshape(batch_size, 1, 1)))\n",
    "      food_sample_ = self.rng.choice(num_empty_after, size=int(np.sum(g_gone)))\n",
    "      food_sample = food_sample_ + np.arange(int(np.sum(g_gone)))*num_empty_after\n",
    "      new_loc_vals = self.pieces[(p_new_locs[0][food_sample],\n",
    "                   p_new_locs[1][food_sample],\n",
    "                   p_new_locs[2][food_sample])]\n",
    "      # this requires that p_new_locs and destroying moves are both\n",
    "      # lexographically sorted... but they are not always\n",
    "      self.pieces[(p_new_locs[0][food_sample],\n",
    "                   p_new_locs[1][food_sample],\n",
    "                   p_new_locs[2][food_sample])] = which_gone\n",
    "\n",
    "\n",
    "  def execute_moves(self, moves, critter_food):\n",
    "    \"\"\"\n",
    "    Execute the moves on the board, handle collisions, update scores,\n",
    "    and replace destroyed/eaten pieces.\n",
    "\n",
    "    Args:\n",
    "      moves (tuple): Tuple of arrays indicating the moves.\n",
    "      critter_food (int): Index to identify the critter or food.\n",
    "    \"\"\"\n",
    "    # what type of critter is moving\n",
    "    critter_food_type = self.get_critter_food_type(critter_food)\n",
    "    # what do they land on when they move\n",
    "    (move_coll_msg, empt_moves, food_moves,\n",
    "     prey_moves, pred_moves) = self.get_collisions(\n",
    "        moves, critter_food, critter_food_type)\n",
    "    # based on what they move onto increment/decrement scores\n",
    "    self.update_scores(move_coll_msg, critter_food,\n",
    "                       critter_food_type, prey_moves)\n",
    "    # move the pieces\n",
    "    old_pieces = self.pieces.copy()\n",
    "    self.move_pieces(critter_food, move_coll_msg, moves)\n",
    "    # eaten/destroyed food and prey respawn in some variants\n",
    "    if critter_food_type == self.CritterFoodType.PREY:\n",
    "      if self.food_forager_regen:\n",
    "        self.replace_destroyed(food_moves, old_pieces)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      if self.food_forager_regen:\n",
    "        self.replace_destroyed(food_moves, old_pieces)\n",
    "        self.replace_destroyed(prey_moves, old_pieces)\n",
    "\n",
    "    if self.food_forager_regen:\n",
    "      check_sum = np.sum(np.arange(start=-self.num_food,\n",
    "                                   stop=self.num_critters+1))\n",
    "      if np.any(np.sum(self.pieces, axis=(1,2)) != check_sum):\n",
    "        print('something went terribly wrong')\n",
    "        print(old_pieces)\n",
    "        print(critter_food)\n",
    "        print(moves)\n",
    "        print(self.pieces)\n",
    "\n",
    "\n",
    "  def get_neighbor_grc_indices(self, critter_food, radius, pad=False):\n",
    "    \"\"\"\n",
    "    Returns all grid positions within a certain cityblock distance radius from\n",
    "    the place corresponding to critter_food.\n",
    "\n",
    "    Args:\n",
    "        critter_food (int): The idex of the focal critter_food.\n",
    "        radius (int): The cityblock distance.\n",
    "        pad (bool): whether or not to pad the array, if padded all row, col\n",
    "          indexes are valid for the padded array, useful for getting percept\n",
    "          if not all indexes are correct for the original array, useful for\n",
    "          figuring out legal moves.\n",
    "\n",
    "    Returns:\n",
    "        an array of indices, each row is a g, r, c index for the neighborhoods\n",
    "        around the critters, can use the g value to know which board you are in.\n",
    "        if pad=True also returns the padded array (the indices in that case) are\n",
    "        for the padded array, so won't work on self.pieces, whereas if pad is\n",
    "        False the indices will be for the offsets in reference to the original\n",
    "        self.pieces, but note that some of these will be invalid, and will\n",
    "        need to be filtered out (as we do in get_legal)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    # Create meshgrid for offsets\n",
    "    if pad is True:\n",
    "      padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
    "        (radius, radius)), constant_values=self.ARRAY_PAD_VALUE)\n",
    "      batch, rows, cols = np.where(padded_arr == critter_food)\n",
    "    else:\n",
    "      batch, rows, cols = np.where(self.pieces == critter_food)\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    neighbors_rows = extended_rows + valid_row_offsets\n",
    "    neighbors_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    indices = np.column_stack((np.repeat(np.arange(batch_size),\n",
    "                                         neighbors_rows.shape[1]),\n",
    "                               neighbors_rows.ravel(),\n",
    "                               neighbors_cols.ravel()))\n",
    "    if pad is False:\n",
    "      return indices\n",
    "    elif pad is True:\n",
    "      return indices, padded_arr\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, critter_food, radius=1):\n",
    "    \"\"\"\n",
    "    Identifies all legal moves for the critter, taking into acount which moves\n",
    "    are blocking based on type.\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "\n",
    "    critter_locs = np.array(np.where(self.pieces == critter_food))\n",
    "    # turn those row, col offsets into a set of legal offsets\n",
    "    legal_offsets = self.get_neighbor_grc_indices(critter_food, radius)\n",
    "    legal_offsets = {tuple(m_) for m_ in legal_offsets}\n",
    "\n",
    "    # Apply logic of where a successful move can be made, by which\n",
    "    # type of critter, be they food, prey, predator or something else\n",
    "    empt_mask, food_mask, prey_mask, pred_mask = self.get_type_masks()\n",
    "    critter_food_type = self.get_critter_food_type(critter_food)\n",
    "    #print(critter_food_type)\n",
    "    if critter_food_type == self.CritterFoodType.FOOD:\n",
    "      #food only drifts into empty places\n",
    "      legal_destinations = np.where(empt_mask)\n",
    "    elif critter_food_type == self.CritterFoodType.PREY:\n",
    "      legal_destinations = np.where(empt_mask | food_mask)\n",
    "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
    "      legal_destinations = np.where(empt_mask | food_mask | prey_mask)\n",
    "    else:\n",
    "      raise ValueError(\"Unexpected value for critter_food_type.\")\n",
    "    legal_destinations = {tuple(coords) for coords in zip(*legal_destinations)}\n",
    "    # Add the current locations of the critters to legal_destinations\n",
    "    current_locations = {tuple(loc) for loc in critter_locs.T}\n",
    "    legal_destinations = legal_destinations.union(current_locations)\n",
    "\n",
    "    # legal moves are both legal offsets and legal destinations\n",
    "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_legal_offsets(self, critter_food, radius):\n",
    "    \"\"\"\n",
    "    Identifies all legal offsets for a critter or food, so filter out moves\n",
    "    that are off the board, but does not filter out collisions that would be\n",
    "    blocking. For a random valid player likely better to use get_legal_moves,\n",
    "    but this is much quicker, because it doesn't check collision types, for\n",
    "    use by RL agents in training loops\n",
    "\n",
    "    Returns:\n",
    "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
    "\n",
    "    Note:\n",
    "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
    "      fourth offset on the first board.\n",
    "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
    "      move on the second board and the second offset, actions are integers\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    batch, rows, cols = np.where(self.pieces == critter_food)\n",
    "    row_offsets, col_offsets = np.meshgrid(\n",
    "        np.arange(-radius, radius + 1),\n",
    "        np.arange(-radius, radius + 1),\n",
    "        indexing='ij')\n",
    "    # Filter for valid cityblock distances\n",
    "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
    "    valid_row_offsets = row_offsets[mask]\n",
    "    valid_col_offsets = col_offsets[mask]\n",
    "    # Extend rows and cols dimensions for broadcasting\n",
    "    extended_rows = rows[:, np.newaxis]\n",
    "    extended_cols = cols[:, np.newaxis]\n",
    "    # Compute all neighbors for each position in the batch\n",
    "    potential_moves_rows = extended_rows + valid_row_offsets\n",
    "    potential_moves_cols = extended_cols + valid_col_offsets\n",
    "\n",
    "    # Filter offsets that would take the critter outside the board\n",
    "    c1 = potential_moves_rows >= 0\n",
    "    c2 = potential_moves_rows <= n_rows-1\n",
    "    c3 = potential_moves_cols >= 0\n",
    "    c4 = potential_moves_cols <= n_cols-1\n",
    "    valid_move_mask = np.logical_and.reduce([c1, c2, c3, c4])\n",
    "\n",
    "    legal_offsets_rows = potential_moves_rows[valid_move_mask]\n",
    "    legal_offsets_cols = potential_moves_cols[valid_move_mask]\n",
    "    batch_indexes = np.repeat(batch, valid_row_offsets.shape[0])\n",
    "    legal_offsets = np.column_stack((batch_indexes[valid_move_mask.ravel()],\n",
    "                                     legal_offsets_rows.ravel(),\n",
    "                                     legal_offsets_cols.ravel()))\n",
    "    return legal_offsets, valid_move_mask\n",
    "\n",
    "\n",
    "  def get_perceptions(self, critter_food, radius):\n",
    "    idx, pad_pieces = self.get_neighbor_grc_indices(critter_food,\n",
    "                                                    radius, pad=True)\n",
    "    #percept_mask = np.zeros(pad_pieces.shape, dtype=bool)\n",
    "    #percept_mask[idx[:,0], idx[:,1]], idx[:,2]] = True\n",
    "    percept = pad_pieces[idx[:,0], idx[:,1], idx[:,2]]\n",
    "    return(percept.reshape(self.batch_size, -1))\n",
    "\n",
    "\n",
    "  def execute_drift(self, offset_probs, wrapping=False):\n",
    "    \"\"\"\n",
    "    Drift the food on the board based on the given offsets probabilities.\n",
    "    Collisions handled by checking possible new locations in a random order and\n",
    "    cancelling moves that result in a collision.\n",
    "\n",
    "    Parameters:\n",
    "    - offset_probs: Probabilities corresponding to each offset, note implicit\n",
    "    order dependence here\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - nothing, just updates self.pieces\n",
    "    \"\"\"\n",
    "    # Check the length of offset_probs\n",
    "    #if len(offset_probs) != 5:\n",
    "    #    raise ValueError(\"offset_probs should be of length 5.\")\n",
    "    # Check if values are non-negative\n",
    "    #if any(p < 0 for p in offset_probs):\n",
    "    #    raise ValueError(\"All probabilities in offset_probs should be non-negative.\")\n",
    "    # Normalize the probabilities\n",
    "    #offset_probs = np.array(offset_probs) / np.sum(offset_probs)\n",
    "    # Convert offsets to a 2D numpy array\n",
    "    possible_offsets = np.array([[ 0, -1,  0], # up\n",
    "                                 [ 0,  1,  0], # down\n",
    "                                 [ 0,  0, -1], # left\n",
    "                                 [ 0,  0,  1], # right\n",
    "                                 [ 0,  0,  0]]) # still\n",
    "    batch_size, n_rows, n_cols = self.pieces.shape\n",
    "    # original food locations\n",
    "    food_locations = np.argwhere(self.pieces == -1)\n",
    "    # Sample offsets for each food location\n",
    "    num_food = food_locations.shape[0]\n",
    "    sampled_offsets = possible_offsets[self.rng.choice(\n",
    "        np.arange(possible_offsets.shape[0]),\n",
    "        size=num_food, replace=True, p=offset_probs)]\n",
    "    # Possible new food locations\n",
    "    possible_new_locations = food_locations + sampled_offsets\n",
    "    possible_wrap_row_indexes = self.rng.choice(np.arange(n_rows),\n",
    "                                                size=num_food)\n",
    "    possible_wrap_col_indexes = self.rng.choice(np.arange(n_cols),\n",
    "                                                size=num_food)\n",
    "\n",
    "    # Randomly iterate through the possible new locations\n",
    "    random_order = np.random.permutation(num_food)\n",
    "    for idx in random_order:\n",
    "      g, r, c = possible_new_locations[idx]\n",
    "      # Check if the new location is inside the boundaries of the board\n",
    "      if 0 <= r < self.pieces.shape[1] and 0 <= c < self.pieces.shape[2]:\n",
    "        # Check if the new location is empty or contains a critter\n",
    "        if self.pieces[g, r, c] == 0:\n",
    "          # Update the board\n",
    "          old_g, old_r, old_c = food_locations[idx]\n",
    "          self.pieces[g, r, c] = -1\n",
    "          self.pieces[old_g, old_r, old_c] = 0\n",
    "      elif wrapping == True:\n",
    "        # If wrapping is on then food can drift off the edge of the board and\n",
    "        # 'new' food will appear in a random loc on the opposite side\n",
    "        # Determine the opposite edge\n",
    "        if r < 0:  # Top edge\n",
    "          opposite_r = n_rows - 1\n",
    "          opposite_c = possible_wrap_col_indexes[idx]\n",
    "        elif r >= n_rows:  # Bottom edge\n",
    "          opposite_r = 0\n",
    "          opposite_c = possible_wrap_col_indexes[idx]\n",
    "        elif c < 0:  # Left edge\n",
    "          opposite_c = n_cols - 1\n",
    "          opposite_r = possible_wrap_row_indexes[idx]\n",
    "        elif c >= n_cols:  # Right edge\n",
    "          opposite_c = 0\n",
    "          opposite_r = possible_wrap_row_indexes[idx]\n",
    "\n",
    "        # Check if the opposite location is unoccupied\n",
    "        if self.pieces[g, opposite_r, opposite_c] == 0:\n",
    "          old_g, old_r, old_c = food_locations[idx]\n",
    "          self.pieces[g, opposite_r, opposite_c] = -1\n",
    "          self.pieces[old_g, old_r, old_c] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title GridworldGame class\n",
    "#######################################################################\n",
    "# extend GridworldGame class locally before integrating in shared utils\n",
    "#######################################################################\n",
    "\n",
    "\n",
    "\n",
    "class GridworldGame():\n",
    "  \"\"\"\n",
    "  A collection methods and parameters of a gridworld game that allow\n",
    "  for interaction with and display of GridwordlBoard objects.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, batch_size=2,\n",
    "               n_rows=7, n_cols=7,\n",
    "               num_foragers=1,\n",
    "               num_predators=0,\n",
    "               max_rounds_taken=30,\n",
    "               end_prob=0.00,\n",
    "               food_num_deterministic = True,\n",
    "               food_patch_prob=10.0/48.0,\n",
    "               food_forager_regen = True,\n",
    "               rng=None,\n",
    "               state_elements = ['pieces', 'scores', 'is_over', 'rounds_left'],\n",
    "               init_board_state = None,\n",
    "               drift_player = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Initializes an instance of the class with the specified parameters.\n",
    "    Args:\n",
    "      batch_size (int, optional): Number of instances in a batch. Default is 1.\n",
    "      n_rows (int, optional): Number of rows in the grid. Default is 7.\n",
    "      n_cols (int, optional): Number of columns in the grid. Default is 7.\n",
    "      num_foragers (int, optional): Number of different agents running around\n",
    "        on each board in the batch eating food. Default is 1.\n",
    "      num_predators (int, optional): Number of different agents running around\n",
    "        on each board in the batch eating foragers. Default is 0.\n",
    "      max_rounds_taken (int, optional): Time before critter's foraging session\n",
    "        ends, in terms of moves taken. Default is 30.\n",
    "      end_prob (float, optional): Probability of ending the game before max\n",
    "        moves are taken, on a given round. Default is 0.00.\n",
    "      food_num_deterministic (bool, optional): Whether or not the number of food\n",
    "        items on each board is deterministic. Default is True.\n",
    "      food_patch_prob (float, optional): Probability of food appearing on each\n",
    "        non-critter-occupied grid cell. Default is 10.0/49.\n",
    "        If food_num_determinisitc is true we use the expected value for each\n",
    "        game in the batch\n",
    "      food_forager_regen (bool, optional): Whether or not foragers and food\n",
    "        respawn/regenerate after they are eaten/destroyed. Default is True.\n",
    "      rng (numpy random number generator, optional): Random number generator\n",
    "        for reproducibility. If None, uses default RNG with a preset seed.\n",
    "      state_elements (list of strings, optional): Elements of the state\n",
    "        passed to players to determine moves. Default is ['pieces', 'scores',\n",
    "        'is_over', 'rounds_left'].\n",
    "      init_board_state (dict, optional): Allows for manual game state\n",
    "        initilization. Default is None, resulting in a random initialization.\n",
    "      drift_player (player object, optional): a 'player' who moves the food\n",
    "        pieces around (drifting) if none, skip food movement\n",
    "\n",
    "    Note: In this version game class handles the end conditions, without any\n",
    "      input from the board class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for positive integer inputs\n",
    "    assert all(isinstance(i, int) and i >= 0\n",
    "               for i in [batch_size, n_rows, n_cols, num_foragers,\n",
    "                         num_predators, max_rounds_taken]), \"These inputs must be non-negative integers.\"\n",
    "\n",
    "    if rng is None:\n",
    "      self.rng = np.random.default_rng(seed=SEED)\n",
    "    else:\n",
    "      self.rng = rng\n",
    "\n",
    "    self.batch_size = batch_size\n",
    "    self.n_rows = n_rows\n",
    "    self.n_cols = n_cols\n",
    "    self.num_foragers = num_foragers\n",
    "    self.num_predators = num_predators\n",
    "    self.num_critters = num_predators + num_foragers\n",
    "    self.pred_prey_threshold = self.num_foragers\n",
    "    self.max_rounds_taken = max_rounds_taken\n",
    "    self.end_prob = end_prob\n",
    "    self.food_num_deterministic = food_num_deterministic\n",
    "    self.food_patch_prob = food_patch_prob\n",
    "    self.food_forager_regen = food_forager_regen\n",
    "    self.drift_player = drift_player\n",
    "    self.init_board_state = init_board_state\n",
    "    self.state_elements = state_elements\n",
    "\n",
    "    # convience wrapper for passing parameters to board class constructor\n",
    "    self.board_params = {\n",
    "      'batch_size': self.batch_size,\n",
    "      'n_rows': self.n_rows,\n",
    "      'n_cols': self.n_cols,\n",
    "      'num_foragers': self.num_foragers,\n",
    "      'num_predators': self.num_predators,\n",
    "      'max_rounds_taken': self.max_rounds_taken,\n",
    "      'end_prob': self.end_prob,\n",
    "      'food_num_deterministic': self.food_num_deterministic,\n",
    "      'food_patch_prob': self.food_patch_prob,\n",
    "      'food_forager_regen': self.food_forager_regen,\n",
    "      'rng': self.rng,\n",
    "      'state_elements': self.state_elements\n",
    "    }\n",
    "\n",
    "  def get_init_board(self):\n",
    "    \"\"\"\n",
    "    Generates a starting board given the parameters of the game.\n",
    "    Returns a tuple giving current state of the game\n",
    "    \"\"\"\n",
    "    # current score, and rounds left in the episode\n",
    "    b = GridworldBoard(**self.board_params,\n",
    "                       init_board_state=self.init_board_state)\n",
    "    return b.get_state()\n",
    "\n",
    "\n",
    "  def get_board_shape(self):\n",
    "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
    "    return (self.n_rows, self.n_cols)\n",
    "\n",
    "\n",
    "  def get_action_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of all possible actions, even though only  2-4 of\n",
    "    these will ever be valid on a given turn.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to g,r,c coordinate indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.n_rows * self.n_cols\n",
    "\n",
    "\n",
    "  def get_batch_size(self):\n",
    "    \"\"\"\n",
    "    Returns the number of actions, only 0-4 of these will ever be valid.\n",
    "    Actions correspond to integer indexes of board locations,\n",
    "    moves to r,c indexes of board locations\n",
    "    \"\"\"\n",
    "    return self.batch_size\n",
    "\n",
    "\n",
    "  def string_rep(self, board, g=0):\n",
    "    \"\"\" A bytestring representation board g's state in the batch of boards\"\"\"\n",
    "    return (board['pieces'][g].tobytes() + board['scores'][g].tobytes() +\n",
    "            board['rounds_left'][g].tobytes())\n",
    "\n",
    "\n",
    "  def get_square_symbol(self, piece):\n",
    "    \"\"\" Translate integer piece value to symbol for display\"\"\"\n",
    "    if piece <= -1:\n",
    "      return \"X\"\n",
    "    elif piece == 0:\n",
    "      return \"-\"\n",
    "    elif piece >= 1:\n",
    "      return \"0\"\n",
    "    else:\n",
    "      return \"???????????????????????????\"\n",
    "\n",
    "\n",
    "  def string_rep_readable(self, board, g=0):\n",
    "    \"\"\" A human readable representation of g-th board's state in the batch\"\"\"\n",
    "    board_s = \"\".join([self.get_square_symbol(square)\n",
    "                        for row in board['pieces'][g]\n",
    "                          for square in row])\n",
    "    board_s = board_s + '_' + str(board['scores'][g])\n",
    "    board_s = board_s + '_' + str(board['rounds_left'][g])\n",
    "    return board_s\n",
    "\n",
    "\n",
    "  def get_scores(self, board):\n",
    "    return board['scores'].copy()\n",
    "\n",
    "\n",
    "  def get_rounds_left(self, board):\n",
    "    return board['rounds_left'].copy()\n",
    "\n",
    "\n",
    "  def display(self, board, g=0):\n",
    "    \"\"\"Displays the g-th games in the batch of boards\"\"\"\n",
    "    print(\"   \", end=\"\")\n",
    "    for c_ in range(self.n_cols):\n",
    "      print(c_, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for r_ in range(self.n_rows):\n",
    "      print(r_, \"|\", end=\"\")    # Print the row\n",
    "      for c_ in range(self.n_cols):\n",
    "        piece = board['pieces'][g,r_,c_]    # Get the piece to print\n",
    "        #print(piece)\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "    print(\"Rounds Left: \" + str(board['rounds_left'][g]))\n",
    "    print(\"Score: \" + str(board['scores'][g]))\n",
    "\n",
    "\n",
    "  def get_critter_rc(self, board, g, critter_index):\n",
    "    return np.squeeze(np.array(np.where(board['pieces'][g]==critter_index)))\n",
    "\n",
    "\n",
    "  def plot_moves(self, board, player0, g=0, player1=None,\n",
    "                 fig=None, ax=None, p0_name='Player 0', p1_name='Player 1',\n",
    "                 figsize=(6,5), critter_name='Critter', title=None,\n",
    "                 deterministic=False):\n",
    "    \"\"\"\n",
    "    Uses plotting functions to make picture of the current board state, and what\n",
    "    a critter would do at each non-food location in the current board state\n",
    "    \"\"\"\n",
    "    def make_prob_dict(critter_locs, play):\n",
    "      offset_dict = {(0, 1): 'right',\n",
    "                     (0,-1): 'left',\n",
    "                     ( 1, 0): 'down',\n",
    "                     (-1, 0): 'up'}\n",
    "      index_probs = play[2].copy()\n",
    "      loc_prob_dict = {}\n",
    "      # for each non food locations\n",
    "      for g, loc_ in enumerate(critter_locs):\n",
    "        # this is the location as an r, c tuple\n",
    "        rc_tup = tuple((loc_[1], loc_[2]))\n",
    "        # the relevant probabilities\n",
    "        raw_probs = index_probs[g]\n",
    "        probs = raw_probs[raw_probs > 0]\n",
    "        indexes = np.argwhere(raw_probs > 0)\n",
    "        # turn the probability indexes into r, c coords\n",
    "        rows = np.floor_divide(indexes, gwg.n_cols)\n",
    "        cols = np.remainder(indexes, gwg.n_cols)\n",
    "        moves = np.squeeze(np.array([z for z in zip(rows, cols)]), axis=2)\n",
    "        #compute the offsets and turn them to strings\n",
    "        offsets = moves - loc_[1:]\n",
    "        str_offsets = np.array(list(map(offset_dict.get, map(tuple, offsets))))\n",
    "        # update the loc_prob_dict for plotting\n",
    "        prob_dict = dict(zip(str_offsets, probs))\n",
    "        loc_prob_dict.update({rc_tup: prob_dict})\n",
    "      return loc_prob_dict\n",
    "\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    food = plot_food(fig, ax, rc_food_plotting)\n",
    "\n",
    "    expanded_board = self.critter_everywhere_state_expansion(\n",
    "      board, player0.critter_index, to_expand=g)\n",
    "    critter_locs = np.argwhere(expanded_board['pieces']==player0.critter_index)\n",
    "    #play the expanded state\n",
    "    p0_play = player0.play(expanded_board)\n",
    "    #get the prob dict\n",
    "    p0_loc_prob_dict = make_prob_dict(critter_locs, p0_play)\n",
    "    # same for player1 if there is one\n",
    "    if player1 is not None:\n",
    "      p1_play = player1.play(expanded_board)\n",
    "      p1_loc_prob_dict = make_prob_dict(critter_locs, p1_play)\n",
    "\n",
    "    existing_handels, _ = ax.get_legend_handles_labels()\n",
    "    if player1 is None:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=0, deterministic=deterministic)\n",
    "      leg_handles = existing_handels\n",
    "    else:\n",
    "      fig, ax, leg_handles_0 = plot_directions(fig, ax, p0_loc_prob_dict,\n",
    "        critter=1, deterministic=deterministic, name=p0_name)\n",
    "      fig, ax, leg_handles_1 = plot_directions(fig, ax, p1_loc_prob_dict,\n",
    "        critter=2, deterministic=deterministic, name=p1_name)\n",
    "      leg_handles = existing_handels + leg_handles_0 + leg_handles_1\n",
    "\n",
    "    fig.legend(handles=leg_handles, loc=\"outside right upper\")\n",
    "    fig.canvas.draw()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "  def plot_board(self, board, g=0,\n",
    "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
    "                 legend_type='included',\n",
    "                 has_fov=False, #fog_of_war feild_of_view\n",
    "                 fov_opaque=False, #let human see trhough fog of war or not\n",
    "                 radius=2, figsize=(6,5), title=None,\n",
    "                 name='Critter',\n",
    "                 focal_critter_index = 0):\n",
    "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    plt.ioff()\n",
    "    if fig is None and ax is None:\n",
    "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
    "\n",
    "    # generate critter plotting specs if we don't already have them\n",
    "    if critter_specs is None:\n",
    "      critter_specs = []\n",
    "      markers = ['h', 'd']  # hexagon and diamond\n",
    "      colors = sns.color_palette(\"colorblind\")\n",
    "      for i in range(self.num_critters):\n",
    "        critter_name = name if self.num_critters == 1 else f'{name} {i+1}'\n",
    "        spec = {'marker': markers[i % len(markers)],\n",
    "                'color': colors[i // len(markers) % len(colors)],\n",
    "                'name': critter_name,\n",
    "                'int_id': i+1}\n",
    "        critter_specs.append(spec)\n",
    "    # get critter locs and plot them\n",
    "    assert len(critter_specs) == self.num_critters, \"More/fewer specs than critters\"\n",
    "    for spec in critter_specs:\n",
    "      rc_loc = np.array(np.where(board['pieces'][g] == spec['int_id'])).T\n",
    "      spec.update({'rc_loc': rc_loc})\n",
    "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
    "\n",
    "    # get food locs and plot them\n",
    "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
    "    rc_food_plotting = np.array(rc_food_index).T\n",
    "    if food is None:\n",
    "      food = plot_food(fig, ax, rc_food_plotting)\n",
    "    else:\n",
    "      food = plot_food(fig, ax, rc_food_plotting, food)\n",
    "\n",
    "    #plot field of view if doing that\n",
    "    if has_fov:\n",
    "      # plot field of view around the 'active player'\n",
    "      if fov is None:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque)\n",
    "      else:\n",
    "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
    "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque, fov=fov)\n",
    "    # make legend and draw and return figure\n",
    "    if legend_type == 'included':\n",
    "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "    elif legend_type == 'separate':\n",
    "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
    "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "      handles, labels = ax.get_legend_handles_labels()\n",
    "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
    "      ax_legend.axis('off')\n",
    "      fig_legend.canvas.header_visible = False\n",
    "      fig_legend.canvas.toolbar_visible = False\n",
    "      fig_legend.canvas.resizable = False\n",
    "      fig_legend.canvas.footer_visible = False\n",
    "      fig_legend.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
    "    else: #no legend\n",
    "      fig.canvas.draw()\n",
    "      return fig, ax, critter_specs, food, fov\n",
    "\n",
    "\n",
    "  def get_legal_moves(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to get the legal moves, as set of batch, row, col triples\n",
    "    giving for the given board. Does return moves that are technically legal\n",
    "    but that will result in a blocking move, this is good for a random valid\n",
    "    player, so that the don't have a high probability of staying still if\n",
    "    there are lots of blocking moves.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    legal_moves =  b.get_legal_moves(critter, radius)\n",
    "    return legal_moves\n",
    "\n",
    "\n",
    "  def get_legal_offsets(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to the legal moves, as an array where each row is\n",
    "    a batch, row, col index giving legal moves on a given board. Includes\n",
    "    blocking moves, but excludes offsets that will take the critter off the\n",
    "    board\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      moves: set or tuples (g, r, c)\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    legal_offsets, valid_moves_mask =  b.get_legal_offsets(critter, radius)\n",
    "    return legal_offsets, valid_moves_mask\n",
    "\n",
    "\n",
    "  def get_valid_actions(self, board, critter=1, radius=1):\n",
    "    \"\"\"\n",
    "    A Helper function to translate the g,x,y, tuples provided the\n",
    "    GridworldBoard.get_legal_moves method into valid actions, represented\n",
    "    as binary vectors of len num_actions.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter (int): value of critter we are getting the valid actions for\n",
    "      radius (int): how far, in city block distance the critter can move\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
    "              valid moves\n",
    "    \"\"\"\n",
    "    legal_moves =  self.get_legal_moves(board, critter, radius)\n",
    "    g, r, c = zip(*legal_moves)\n",
    "    valids = np.zeros((self.batch_size, self.n_rows * self.n_cols))\n",
    "    valids[g, np.array(r) * self.n_cols + np.array(c)] = 1\n",
    "    return valids\n",
    "\n",
    "\n",
    "  def display_moves(self, board, critter=1, g=0):\n",
    "    \"\"\"Displays possible moves for the g-th games in the batch of boards\"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    A=np.reshape(self.get_valid_actions(board, critter)[g],\n",
    "                 (n_rows, n_cols))\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for col in range(self.n_cols):\n",
    "      print(col, \"|\", end=\"\")    # Print the row\n",
    "      for row in range(self.n_rows):\n",
    "        piece = A[col][row]    # Get the piece to print\n",
    "        print(self.get_square_symbol(piece), end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "\n",
    "  def get_perceptions(self, board, radius, critter):\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    perceptions = b.get_perceptions(radius, critter)\n",
    "    return perceptions\n",
    "\n",
    "\n",
    "  def get_next_state(self, board, critter, actions, a_indx=None):\n",
    "    \"\"\"\n",
    "    Helper function using GridworldBoard.execute_moves to update board state\n",
    "    given actions on a batch of boards, for a given critter\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      critter: integer index of the critter type\n",
    "      actions: list of flat integer indexes of critter's new board positions\n",
    "      a_indx: list of integer indexes indicating which actions are being taken\n",
    "        on which boards in the batch\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      if len(actions) > batch_size of board the returned board state will have\n",
    "      an expanded batch size, allowing multiple paths in the decision tree to be\n",
    "      explored in parallel\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    if board['rounds_left'][0] <= 0:\n",
    "      # assumes all boards in the batch have the same rounds left\n",
    "      # no rounds left return the board unchanged\n",
    "      return board\n",
    "    else:\n",
    "      adapted_board_params = self.board_params.copy()\n",
    "      adapted_board_params.update({'batch_size': len(actions)})\n",
    "      if a_indx is None:\n",
    "        # just one move on each board in the batch\n",
    "        assert batch_size == len(actions)\n",
    "        adapted_board_params.update({'init_board_state': board})\n",
    "        b = GridworldBoard(**adapted_board_params)\n",
    "      else:\n",
    "        # potentially multiple moves on each board, expand the batch\n",
    "        assert len(actions) == len(a_indx)\n",
    "        new_pieces = np.array([board['pieces'][ai].copy() for ai in a_indx])\n",
    "        new_scores = np.array([board['scores'][ai].copy() for ai in a_indx])\n",
    "        new_rounds_left = np.array([board['rounds_left'][ai].copy() for ai in a_indx])\n",
    "        new_active_player = copy(board['active_player'])\n",
    "        new_state = {'pieces': new_pieces,\n",
    "                     'scores': new_scores,\n",
    "                     'rounds_left': new_rounds_left,\n",
    "                     'active_player': new_active_player}\n",
    "        adapted_board_params.update({'init_board_state': new_state})\n",
    "        b = GridworldBoard(**adapted_board_params)\n",
    "      moves = self.actions_to_moves(actions)\n",
    "      b.execute_moves(moves, critter)\n",
    "      return b.get_state()\n",
    "\n",
    "\n",
    "  def actions_to_moves(self, actions):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    Returns\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    \"\"\"\n",
    "    moves = (np.arange(len(actions)),\n",
    "             np.floor_divide(actions, self.n_cols),\n",
    "             np.remainder(actions, self.n_cols))\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def moves_to_actions(self, moves):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for quick numpy operations.\n",
    "    Returns:\n",
    "      actions: a batch length list of integer indexes for the flattened boards,\n",
    "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
    "      as training targets for NN based RL agents.\n",
    "    \"\"\"\n",
    "    _, rows, cols = moves\n",
    "    actions = rows * self.n_cols + cols\n",
    "    return actions\n",
    "\n",
    "\n",
    "  def critter_oriented_get_next_state(self, board, critter, offsets):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then get's the next state.\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      offsets: batch length list of strings one 'up', 'down', 'left', 'right'\n",
    "\n",
    "    Returns:\n",
    "      a board triple signifying next state\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the decision tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    adapted_board_params = self.board_params.copy()\n",
    "    adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "    b = GridworldBoard(**adapted_board_params)\n",
    "    moves = self.critter_direction_to_move(board, offsets, critter)\n",
    "    b.execute_moves(moves, critter)\n",
    "    return(b.get_state())\n",
    "\n",
    "\n",
    "  def critter_direction_to_move(self, board, offsets, critter):\n",
    "    \"\"\"\n",
    "    Translates directions in reference to the critter's location into\n",
    "    moves on the board in absolute terms, while checking for\n",
    "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
    "    other critters though. In general player's move methods should be checking\n",
    "    valid moves and only making legal ones.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      offsets: batch length list of strings,\n",
    "        one of 'up', 'down', 'left', 'right'\n",
    "      critter: integer index for the critter we want moves for\n",
    "\n",
    "    Returns:\n",
    "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
    "        the first array gives the specific board within the batch,\n",
    "        the second array in the tuple gives the new row coord for each critter\n",
    "        on each board and the third gives the new col coord. Note this is\n",
    "        exactly the format expected by GridworldBoard.execute_moves, and\n",
    "        is a canonical way of indexing arrays for numpy.\n",
    "\n",
    "    Note:\n",
    "      Unlike get_next_state, this method does not allow for expansion\n",
    "      of the decision tree, i.e. len(offsets)==batch_size required\n",
    "    \"\"\"\n",
    "    assert len(offsets) == board['pieces'].shape[0]\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {'up': (0, -1, 0),\n",
    "                   'down': (0, 1, 0),\n",
    "                   'left': (0, 0, -1),\n",
    "                   'right': (0, 0, 1),\n",
    "                   'still': (0, 0, 0)}\n",
    "    this_critter_locs = np.where(board['pieces'] == critter)\n",
    "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
    "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
    "                           for ost_ in offsets])\n",
    "    new_locs = np.array(this_critter_locs) + offsets_array\n",
    "    #check bounces at boundaries\n",
    "    new_locs[1,:] = np.where(new_locs[1] >=\n",
    "                               n_rows, n_rows-2, new_locs[1])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
    "                               n_cols, n_cols-2, new_locs[2,:])\n",
    "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
    "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
    "    moves = tuple(new_locs)\n",
    "    return moves\n",
    "\n",
    "\n",
    "  def direction_probs_to_flat_probs(self, board, direction_probs, critter):\n",
    "    \"\"\"\n",
    "    Converts direction probabilities in reference to the critter's location into\n",
    "    probability arrays on the flattened board.\n",
    "\n",
    "    Args:\n",
    "      board: dict of np arrays representing board state\n",
    "        'pieces':       batch_size x n_rows x n_cols\n",
    "        'scores':       batch_size\n",
    "        'rounds_left':  batch_size\n",
    "      direction_probs: batch length list of dictionaries with keys\n",
    "        ['up', 'down', 'left', 'right'] and corresponding probabilities.\n",
    "\n",
    "    Returns:\n",
    "      probs_arrays: list of arrays, where each array is of length n_rows*n_cols\n",
    "                    and represents the flattened probability distribution for\n",
    "                    board in the batch.\n",
    "    \"\"\"\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    offset_dict = {\n",
    "        'up': np.array((0, -1, 0)),\n",
    "        'down': np.array((0, 1, 0)),\n",
    "        'left': np.array((0, 0, -1)),\n",
    "        'right': np.array((0, 0, 1))}\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    probs_arrays = np.zeros((batch_size, n_rows * n_cols))\n",
    "    for batch_index in range(batch_size):\n",
    "      prob_array = np.zeros(n_rows * n_cols)\n",
    "      for direction, prob in direction_probs[batch_index].items():\n",
    "          offset = offset_dict[direction]\n",
    "          new_loc = np.array(critter_locs)[:, batch_index] + offset\n",
    "          # Check bounces at boundaries\n",
    "          new_loc[1] = np.where(new_loc[1] >= n_rows, n_rows-2, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] >= n_cols, n_cols-2, new_loc[2])\n",
    "          new_loc[1] = np.where(new_loc[1] < 0, 1, new_loc[1])\n",
    "          new_loc[2] = np.where(new_loc[2] < 0, 1, new_loc[2])\n",
    "          # Convert 2D location to flattened index\n",
    "          flattened_index = new_loc[1] * n_cols + new_loc[2]\n",
    "          prob_array[flattened_index] += prob\n",
    "      probs_arrays[batch_index, :] = prob_array\n",
    "    return list(probs_arrays)\n",
    "\n",
    "\n",
    "  def action_to_critter_direction(self, board, critter, actions):\n",
    "    \"\"\"\n",
    "    Translates an integer index action into up/down/left/right\n",
    "\n",
    "    Args:\n",
    "      board: a triple of np arrays representing board state\n",
    "        pieces,       - batch_size x n_rows x n_cols\n",
    "        scores,       - batch_size\n",
    "        rounds_left   - batch_size\n",
    "      actions: a batch size ndarry of integer indexes for actions on each board\n",
    "\n",
    "    Returns:\n",
    "      offsets: a batch length list of strings 'up', 'down', 'left', 'right', 'still'\n",
    "    \"\"\"\n",
    "    offset_dict = {(0, 0, 0): 'still',\n",
    "                   (0, 0, 1): 'right',\n",
    "                   (0, 0,-1): 'left',\n",
    "                   (0, 1, 0): 'down',\n",
    "                   (0,-1, 0): 'up'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    critter_locs = np.where(board['pieces'] == critter)\n",
    "    moves = (np.arange(len(actions)),\n",
    "               np.floor_divide(actions, n_cols),\n",
    "               np.remainder(actions, n_cols))\n",
    "    # need to reverse this from above, moves is equiv to new_locs\n",
    "    # new_locs = np.array(critter_locs) + offsets_array\n",
    "    offsets_array = np.array(moves) - np.array(critter_locs)\n",
    "    offsets = [offset_dict[tuple(o_)] for o_ in offsets_array.T]\n",
    "    return offsets\n",
    "\n",
    "\n",
    "  def get_valid_directions(self, board, critter):\n",
    "    \"\"\"\n",
    "    Transforms output of get_valid_actions to a list of the valid directions\n",
    "    for each board in the batch for a given critter.\n",
    "    \"\"\"\n",
    "    offset_dict = {( 0, 1): 'right',\n",
    "                   ( 0,-1): 'left',\n",
    "                   ( 1, 0): 'down',\n",
    "                   (-1, 0): 'up',\n",
    "                   ( 0, 0): 'still'}\n",
    "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "    valid_actions = self.get_valid_actions(board, critter)\n",
    "    if batch_size != len(valid_actions):\n",
    "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
    "    critter_locs = np.column_stack(np.where(board['pieces'] == critter))\n",
    "    valid_directions = []\n",
    "    for g, batch_valid in enumerate(valid_actions):\n",
    "      valid_int_indices = np.where(batch_valid==1)[0]\n",
    "      critter_loc = critter_locs[critter_locs[:, 0] == g, 1:]\n",
    "      # critter_loc shape is (1, 2)\n",
    "      critter_loc = np.squeeze(critter_loc)\n",
    "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
    "      offsets = moves - critter_loc\n",
    "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
    "      valid_directions.append(batch_valid_directions)\n",
    "    return valid_directions\n",
    "\n",
    "\n",
    "  def get_game_ended(self, board):\n",
    "    \"\"\"\n",
    "    Helper function to signify if game has ended\n",
    "    Returns a batch size np.array of -1 if not ended, and scores for each game\n",
    "    in the batch if it is ended, note only returns scores if all games in the\n",
    "    batch have ended\n",
    "    \"\"\"\n",
    "    rounds_left = board['rounds_left']\n",
    "    scores = board['scores']\n",
    "    if np.any(rounds_left >= 1):\n",
    "      return np.ones(self.batch_size) * -1.0\n",
    "    else:\n",
    "      return scores\n",
    "\n",
    "\n",
    "  def critter_everywhere_state_expansion(self, board_state,\n",
    "                                         critter=1, to_expand=0):\n",
    "    \"\"\"\n",
    "    Expand a given board state by placing a critter at each non-food location.\n",
    "\n",
    "    The function takes a game state and returns an expanded version of it. For\n",
    "    each board in the state, it creates a new version of the board for every\n",
    "    non-food location, placing a critter at that location. The scores and\n",
    "    remaining rounds are copied for each new board. The result is a new game state\n",
    "    with a larger number of boards, each representing a possible configuration\n",
    "    with a critter at a different location.\n",
    "\n",
    "    Args:\n",
    "      board_state (dict): A dictionary containing the current game state.\n",
    "      It should have the following keys:\n",
    "        - 'pieces': a 3D numpy array (batch x n_col x n_row) representing the game\n",
    "          board. -1 -> food, 0 -> empty cell, and 1 -> critter.\n",
    "        - 'scores': 1D numpy array of the score for each board in the batch.\n",
    "        - 'rounds_left': a 1D numpy array of the rounds left for\n",
    "          each board in the batch.\n",
    "      critter: integer index to place on the expanded board state\n",
    "      to_expand (list (int)): list of batch indices to have state expanded\n",
    "\n",
    "    Returns:\n",
    "      dict: A dictionary containing the expanded game state with the same keys\n",
    "        as the input. The number of boards will be larger than the input state.\n",
    "    \"\"\"\n",
    "    pieces = board_state['pieces'].copy()\n",
    "    scores = board_state['scores'].copy()\n",
    "    rounds_left = board_state['rounds_left'].copy()\n",
    "    active_player = copy(board_state['active_player'])\n",
    "    # Determine non-food locations\n",
    "    non_food_locs = np.argwhere(pieces[to_expand] != -1)\n",
    "    #scrub all existing critter locations,\n",
    "    # maybe later only scrub specific critter type\n",
    "    pieces[pieces >= 1] = 0\n",
    "    # lists to store expanded states\n",
    "    expanded_pieces = []\n",
    "    expanded_scores = []\n",
    "    expanded_rounds_left = []\n",
    "    # Iterate over each non-food location\n",
    "    for i in range(non_food_locs.shape[0]):\n",
    "      # Create a copy of the board\n",
    "      expanded_board = np.copy(pieces[to_expand])\n",
    "      # Place the critter at the non-food location\n",
    "      # later consider only placing at non-food,\n",
    "      # non-other critter locs\n",
    "      expanded_board[tuple(non_food_locs[i])] = critter\n",
    "      # Add the expanded board to the list along score and rounds_left\n",
    "      expanded_pieces.append(expanded_board)\n",
    "      expanded_scores.append(scores[to_expand])\n",
    "      expanded_rounds_left.append(rounds_left[to_expand])\n",
    "    # Convert to arrays and create expanded board state\n",
    "    expanded_state = {'pieces': np.stack(expanded_pieces),\n",
    "                      'scores': np.array(expanded_scores),\n",
    "                      'rounds_left': np.array(expanded_rounds_left),\n",
    "                      'active_player': active_player}\n",
    "    return expanded_state\n",
    "\n",
    "\n",
    "  def play_game(self, players=[], collect_fov_data=False, fov_radius=2,\n",
    "                visualize = False):\n",
    "    \"\"\"This method takes a list of players the same length as num_critters,\n",
    "        and then plays a batch of games with them and returns the final board\n",
    "        state\"\"\"\n",
    "    if len(players) != self.num_critters:\n",
    "      raise ValueError(\"number of players different than expected\")\n",
    "\n",
    "    board = self.get_init_board()\n",
    "    if visualize == True:\n",
    "      self.display(board, 0)\n",
    "\n",
    "    if collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = board['pieces'].shape\n",
    "      adapted_board_params = self.board_params.copy()\n",
    "      adapted_board_params.update({'batch_size': batch_size,\n",
    "                                'init_board_state': board})\n",
    "      b = GridworldBoard(**adapted_board_params)\n",
    "    for p_idx, player_ in enumerate(players):\n",
    "      if player_.critter_index != p_idx+1:\n",
    "        print(player_.critter_index)\n",
    "        print(p_idx + 1)\n",
    "        raise ValueError(\"player order does not match assigned critter index\")\n",
    "\n",
    "    for ii in range(self.max_rounds_taken):\n",
    "      for player_ in players:\n",
    "        old_scores = board['scores']\n",
    "        if collect_fov_data is True:\n",
    "          b.set_state(board)\n",
    "          percepts = b.get_perceptions(fov_radius)\n",
    "\n",
    "        a_player, _, _ = player_.play(board)\n",
    "        board = self.get_next_state(board, player_.critter_index, a_player)\n",
    "        if visualize == True:\n",
    "          self.display(board, 0)\n",
    "      if self.end_prob > 0:\n",
    "        if np.random.rand() < self.end_prob:\n",
    "          print('game ended stochastically before max rounds taken')\n",
    "          break\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Interactive Gridworld Widget\n",
    "\n",
    "########################################\n",
    "# widgets refactor for multi-critter\n",
    "#########################################\n",
    "# Interactive Gridworld Game Widgets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RandomValidPlayer():\n",
    "  \"\"\"\n",
    "  Instantiate random player for GridWorld, could be prey or pred... or even food\n",
    "  It leans hard on the game's get valid method and then just samples from there\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def __init__(self, game, critter_index=1, speed=1):\n",
    "    self.game = game\n",
    "    self.critter_index = critter_index\n",
    "    self.speed = speed\n",
    "    assert (isinstance(critter_index, int) and\n",
    "        0 < critter_index <= game.num_critters), \"Value is not a positive integer or exceeds the upper limit.\"\n",
    "\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulates a batch of random game plays based on the given board state.\n",
    "\n",
    "    This function computes the probability of each valid move being played\n",
    "    (uniform for valid moves, 0 for others), then selects a move randomly for\n",
    "    each game in the batch based on these probabilities.\n",
    "\n",
    "    Args:\n",
    "      board (dict): A dictionary representing the state of the game. It\n",
    "          contains:\n",
    "          - 'pieces': A (batch_size, x_size, y_size) numpy array indicating\n",
    "                      the pieces on the board.\n",
    "          - 'scores' (not used directly in this function, but expected in dict)\n",
    "          - 'rounds_left' (not used directly in this function, but expected in dict)\n",
    "\n",
    "    Returns:\n",
    "      tuple:\n",
    "      - a (numpy array): An array of shape (batch_size,) containing randomly\n",
    "                         chosen actions for each game in the batch.\n",
    "      - a_1hots (numpy array): An array of shape (batch_size, action_size)\n",
    "                               with one-hot encoded actions.\n",
    "      - probs (numpy array): An array of the same shape as 'valids' containing\n",
    "                             the probability of each move being played.\n",
    "    \"\"\"\n",
    "    batch_size, x_size, y_size = board['pieces'].shape\n",
    "    valids = self.game.get_valid_actions(board, self.critter_index, self.speed)\n",
    "    action_size = self.game.get_action_size()\n",
    "\n",
    "    probs = valids / np.sum(valids, axis=1).reshape(batch_size,1)\n",
    "\n",
    "    a = [self.game.rng.choice(action_size, p=probs[ii])\n",
    "                                for ii in range(batch_size)]\n",
    "    a_1hots = np.zeros((batch_size, action_size))\n",
    "    a_1hots[(range(batch_size), a)] = 1.0\n",
    "    return np.array(a), a_1hots, probs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class InteractiveGridworld():\n",
    "  \"\"\"\n",
    "  A widget based object for interacting with a gridworld game\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, gridworld_game, init_board=None, has_fov=False,\n",
    "               radius=2, fov_opaque=False, collect_fov_data=False,\n",
    "               figsize=(6,5), critter_names=['Critter'], players=['human'],\n",
    "               final_score_type='raw'):\n",
    "    \"\"\"\n",
    "    Initializes a widget based object for interacting with a gridworld game\n",
    "\n",
    "    Args:\n",
    "      gridworld_game: an instance of GridworldGame object\n",
    "        InteractiveGridworld expects the GridworldGame to have batchsize 1\n",
    "      has_fov: bool, whether or not to display fog of war around the critter\n",
    "      radius: int, number of squares the critter can \"see\" around it\n",
    "      figsize: tuple (int, int), size of the figure\n",
    "      critter_names: a list of strings that determines what the critter is called\n",
    "        in the plot legend, order should align with players\n",
    "      player: a list of either 'human', None, or a player object with a play\n",
    "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
    "        default to making a RandomValidPlayer object, otherwise use the\n",
    "        player class provided to make the player objects and use a start button.\n",
    "        The list needs to be as long as the gridworld_game.num_critters\n",
    "        attribute. Order should align with critter_name.\n",
    "      score_type: magic string specifying whether score displayed should be\n",
    "        a 'raw' score or a per move 'normalized' score\n",
    "\n",
    "      Note: fov is going to look pretty janky with more than one player, maybe\n",
    "      we get fov to only turn on for the 'active' player?\n",
    "      Note: Specific initialization state is handled by the GridworldGame object\n",
    "    \"\"\"\n",
    "\n",
    "    # Set GridworldGame object and initialize the board state\n",
    "    self.gwg = gridworld_game\n",
    "    self.has_fov = has_fov\n",
    "    self.radius = radius\n",
    "    self.fov_opaque = fov_opaque\n",
    "    self.percept_len = 2*self.radius*(self.radius+1)\n",
    "    self.collect_fov_data = collect_fov_data\n",
    "    self.figsize = figsize\n",
    "    # initialize players and plotting specs together to ensure alignment\n",
    "    self.players = []\n",
    "    self.any_human_players = False\n",
    "    self.active_player_index = 0\n",
    "    self.crit_specs = []\n",
    "    markers = ['h', 'd']  # hexagon and diamond\n",
    "    colors = sns.color_palette(\"colorblind\")\n",
    "    for i in range(self.gwg.num_critters):\n",
    "      spec = {'marker': markers[i % len(markers)],\n",
    "              'color': colors[i // len(markers) % len(colors)],\n",
    "              'name': critter_names[i],\n",
    "              'int_id': i+1}\n",
    "      self.crit_specs.append(spec)\n",
    "      player = players[i] #implict check that players is at least long enough\n",
    "      if player is None:\n",
    "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
    "      elif player == 'human':\n",
    "        self.players.append('human')\n",
    "        # right now only ever have on human player with index 1\n",
    "        self.any_human_players = True\n",
    "      else:\n",
    "        # player objects expected to have a critter_index attribute\n",
    "        # we set it appropriately here so it aligns with the players list\n",
    "        # used to create the widget\n",
    "        player.critter_index = i+1\n",
    "        self.players.append(player)\n",
    "    self.final_scores = []\n",
    "    self.final_score_type = final_score_type # 'raw' or 'normalized'\n",
    "    self.board_state = self.gwg.get_init_board()\n",
    "    if self.collect_fov_data is True:\n",
    "      # keep raw records of percept and eating for manipulation later\n",
    "      self.percept_eat_records = []\n",
    "      # keep data in contingency table of how many food items were in\n",
    "      # the percept, and whether or not food was eaten\n",
    "      self.fov_eat_table_data = np.zeros((2, self.percept_len+1))\n",
    "    # Initialize widgets and buttons\n",
    "    self.output = widgets.Output(layout=widgets.Layout(\n",
    "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
    "      min_height='10.0em', overflow='auto'))\n",
    "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='12.5em', max_width='18.8em',\n",
    "      min_height='6.3em', overflow='auto'))\n",
    "    self.fov_eat_table_display = widgets.Output(layout=widgets.Layout(\n",
    "      min_width='25.0em', min_height='18.8em', overflow='auto'))\n",
    "    self.up_button = widgets.Button(description=\"Up\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.down_button = widgets.Button(description=\"Down\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.left_button = widgets.Button(description=\"Left\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.right_button = widgets.Button(description=\"Right\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "    self.start_button = widgets.Button(description=\"Start\",\n",
    "      layout=widgets.Layout(width='6.3em'))\n",
    "\n",
    "    # get plot canvas widgets and other plotting objects\n",
    "    plt.ioff()\n",
    "    if self.collect_fov_data and self.any_human_players:\n",
    "      self.legend_type = None # don't keep regenerating the legend\n",
    "      # do legend separately if showing observations and no human player\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          legend_type='separate', figsize=self.figsize, has_fov=self.has_fov,\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque)\n",
    "    elif len(self.players) > 1:\n",
    "      self.legend_type=None # don't keep regenerating the legend\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
    "       self.b_fig_legend, self.b_ax_legend) = self.gwg.plot_board(\n",
    "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
    "          has_fov=self.has_fov, legend_type='separate',\n",
    "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize)\n",
    "    else:\n",
    "      self.legend_type = 'included'\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "        ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                                critter_specs=self.crit_specs,\n",
    "                                has_fov=self.has_fov,\n",
    "                                fov_opaque=self.fov_opaque,\n",
    "                                radius=self.radius, figsize=self.figsize)\n",
    "    # lump buttons together\n",
    "    self.buttons = widgets.HBox([self.left_button,\n",
    "                               widgets.VBox([self.up_button, self.down_button]),\n",
    "                               self.right_button])\n",
    "    # automatically pick different layouts for different situations\n",
    "    if self.any_human_players:\n",
    "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
    "                                             self.buttons])\n",
    "      if len(self.players) == 1:\n",
    "        #one human player\n",
    "        self.output_and_score = widgets.HBox([self.scoreboard, self.output])\n",
    "        self.no_table_final_display = widgets.VBox([self.board_and_buttons,\n",
    "                                                  self.output_and_score])\n",
    "        if self.collect_fov_data == True:\n",
    "          # a single human player collecting data\n",
    "          self.final_display = widgets.HBox([self.no_table_final_display,\n",
    "                                           self.fov_eat_table_display])\n",
    "        else: # self.collect_fov_data == False:\n",
    "          # a single human player not collecting data\n",
    "          self.final_display = self.no_table_final_display\n",
    "      else:\n",
    "        # more than one player, one of them human\n",
    "        self.V_board_outbput = widgets.VBox([self.board_and_buttons,\n",
    "                                             self.output])\n",
    "        self.V_scoreboard_start_legend = widgets.VBox([\n",
    "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "        self.final_display = widgets.HBox([self.V_board_outbput,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "    else: # player is some kind of ai\n",
    "      if self.collect_fov_data == True:\n",
    "        # an ai player with recording\n",
    "        # in this case legend is separate\n",
    "        self.V_score_start_output_legend = widgets.VBox([self.scoreboard,\n",
    "          self.start_button,  self.output, self.b_fig_legend.canvas])\n",
    "        self.V_board_table = widgets.VBox([self.b_fig.canvas,\n",
    "                                           self.fov_eat_table_display])\n",
    "        self.final_display = widgets.HBox([self.V_board_table,\n",
    "                                           self.V_score_start_output_legend])\n",
    "      else:\n",
    "        if len(self.players) == 1:\n",
    "          # an ai player without recording\n",
    "          self.H_score_output_start = widgets.HBox([\n",
    "            self.scoreboard, self.output, self.start_button])\n",
    "          self.final_display = widgets.VBox([\n",
    "            self.b_fig.canvas, self.H_score_output_start])\n",
    "        else:\n",
    "          # more than one ai player\n",
    "          self.V_board_outbput = widgets.VBox([self.b_fig.canvas, self.output])\n",
    "          self.V_scoreboard_start_legend = widgets.VBox([\n",
    "              self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
    "          self.final_display = widgets.HBox([self.V_board_outbput,\n",
    "                                             self.V_scoreboard_start_legend])\n",
    "\n",
    "    # initialize text outputs\n",
    "    with self.scoreboard:\n",
    "      table = [['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "               ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "      if len(self.players) > 1:\n",
    "        headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "        print(tabulate(table, headers=headers))\n",
    "      else: # len(self.players) == 1\n",
    "        print(tabulate(table))\n",
    "    with self.output:\n",
    "      if self.any_human_players:\n",
    "        print('Click a button to start playing')\n",
    "        print('There are {} rounds in this game'.format(self.board_state['rounds_left'][0]))\n",
    "      else:\n",
    "        print('Click the start button to run the simulation')\n",
    "    with self.fov_eat_table_display:\n",
    "      printmd(\"**Observations**\")\n",
    "      table_data = [[str(ii),\n",
    "                     str(self.fov_eat_table_data[0,ii]),\n",
    "                     str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "      table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "               table_data)\n",
    "      print(tabulate(table))\n",
    "\n",
    "    # fussy off-by-one adjustement\n",
    "    self.board_state['rounds_left'] -= 1\n",
    "\n",
    "    # Connect the buttons to functions that do something\n",
    "    self.up_button.on_click(self.on_up_button_clicked)\n",
    "    self.down_button.on_click(self.on_down_button_clicked)\n",
    "    self.left_button.on_click(self.on_left_button_clicked)\n",
    "    self.right_button.on_click(self.on_right_button_clicked)\n",
    "    self.start_button.on_click(self.on_start_button_clicked)\n",
    "\n",
    "    if self.gwg.n_cols == 1:  # If the board only has one column\n",
    "      self.left_button.disabled = True\n",
    "      self.right_button.disabled = True\n",
    "\n",
    "    if self.gwg.n_rows == 1:  # If the board only has one row\n",
    "      self.up_button.disabled = True\n",
    "      self.down_button.disabled = True\n",
    "\n",
    "\n",
    "  def button_output_update(self, which_button):\n",
    "    old_board = self.board_state.copy()\n",
    "    # index of players is 0 through num_critter-1,\n",
    "    # same player represented by value of index + 1 in\n",
    "    old_scores = old_board['scores'][0]\n",
    "    if self.collect_fov_data is True:\n",
    "      batch_size, n_rows, n_cols = old_board['pieces'].shape\n",
    "      adapted_board_params = self.gwg.board_params.copy()\n",
    "      adapted_board_params['init_state'] = old_board\n",
    "      b = GridworldBoard(**adapted_board_params)\n",
    "      percept = b.get_perceptions(self.radius)[0]\n",
    "\n",
    "    if (isinstance(self.players[self.active_player_index], str) and\n",
    "        'human' in self.players[self.active_player_index]):\n",
    "      direction = which_button\n",
    "    else:\n",
    "      a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
    "      # print(a_player)\n",
    "      a_player = self.gwg.action_to_critter_direction(old_board,\n",
    "                                                      self.active_player_index+1,\n",
    "                                                      a_player)\n",
    "      # but we only want to apply their move to the appropriate board\n",
    "      direction = a_player[0]\n",
    "\n",
    "    self.board_state = self.gwg.critter_oriented_get_next_state(\n",
    "          self.board_state, self.active_player_index+1, [direction])\n",
    "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
    "    rounds_left = self.board_state['rounds_left'][0]\n",
    "    num_moves = np.floor(self.gwg.max_rounds_taken -\n",
    "                         rounds_left / self.gwg.num_critters)\n",
    "    if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
    "      #eating happened\n",
    "      eating_string = \"They ate the food/prey there!\"\n",
    "      did_eat = 1\n",
    "    else: #eating didn't happen\n",
    "      eating_string = \"There's no food/prey there.\"\n",
    "      did_eat = 0\n",
    "    row, col = self.gwg.get_critter_rc(self.board_state, 0,\n",
    "                                       self.active_player_index+1)\n",
    "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "     ) = self.gwg.plot_board(self.board_state, g=0,\n",
    "                             fig=self.b_fig, ax=self.b_ax,\n",
    "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
    "                             fov=self.b_fov, has_fov=self.has_fov,\n",
    "                             fov_opaque=self.fov_opaque,\n",
    "                             radius=self.radius, legend_type=self.legend_type)\n",
    "    if self.collect_fov_data is True:\n",
    "      p_e_data = {'perception': percept.copy(),\n",
    "                  'state': old_board,\n",
    "                  'did_eat': bool(did_eat)}\n",
    "      self.percept_eat_records.append(p_e_data)\n",
    "      percept_int = np.sum(percept==-1) # number of food items in FoV\n",
    "      self.fov_eat_table_data[did_eat, percept_int] += 1\n",
    "\n",
    "    with self.output:\n",
    "      clear_output()\n",
    "      if len(self.players) == 1:\n",
    "        print(\"The critter (tried) to move \" + direction +\n",
    "              \" and is now at ({}, {}).\".format(row,col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\\nFood Per Move: {:.2f}\".format(\n",
    "            rounds_left, new_scores[self.active_player_index],\n",
    "            new_scores[self.active_player_index] / num_moves))\n",
    "      else: # more than one players\n",
    "        print(\"Critter {} (tried) to move \".format(self.active_player_index+1) +\n",
    "              direction +\n",
    "              \" and is now at ({}, {}).\".format(row, col))\n",
    "        print(eating_string)\n",
    "        print(\"Rounds Left: {}\\nFood Eaten: {}\".format(\n",
    "            rounds_left, new_scores))\n",
    "    end_sample = self.gwg.rng.random()\n",
    "    if rounds_left == 0:\n",
    "      game_over_msg = 'Game Over.'\n",
    "    elif end_sample < self.gwg.end_prob:\n",
    "      game_over_msg = 'Game Over.\\nEnded stochastically before max moves taken'\n",
    "\n",
    "\n",
    "    if rounds_left == 0 or end_sample < self.gwg.end_prob: # game is over!\n",
    "      if self.final_score_type == 'raw':\n",
    "        self.final_scores.append(new_scores)\n",
    "      elif self.final_score_type == 'normalized':\n",
    "        self.final_scores.append(new_scores/num_moves)\n",
    "      else:\n",
    "        raise ValueError(f\"Invalid final_score_type: {self.final_score_type}. Expected 'raw' or 'normalized'.\")\n",
    "\n",
    "      game_over_msg = game_over_msg + '\\nResetting board for a new game.'\n",
    "      with self.output:\n",
    "        clear_output\n",
    "        print(game_over_msg)\n",
    "        self.board_state = self.gwg.get_init_board()\n",
    "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
    "       ) = self.gwg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
    "                               self.b_crit_specs, self.b_food, self.b_fov,\n",
    "                               has_fov=self.has_fov, radius=self.radius,\n",
    "                               fov_opaque=self.fov_opaque,\n",
    "                               legend_type=self.legend_type)\n",
    "      with self.scoreboard:\n",
    "        clear_output()\n",
    "        print('Games Played: ' + str(len(self.final_scores)))\n",
    "        if len(self.players) == 1:\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = [\n",
    "              ['High Score:', str(np.max(np.array(self.final_scores)))],\n",
    "              ['Last Score:', str(self.final_scores[-1][0])],\n",
    "              ['Average Score',\n",
    "              '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
    "          else:\n",
    "            table = [['High Score:', '--'],\n",
    "                     ['Last Score:', '--'],\n",
    "                     ['Average Score:', '--']]\n",
    "          print(tabulate(table))\n",
    "        else: # len(self.players) > 1\n",
    "          headers = [''] + [f'P{i+1}' for i in range(self.gwg.num_critters)]\n",
    "          if len(self.final_scores) > 0:\n",
    "            table = []\n",
    "            # Assuming the batch size is 1 for now\n",
    "            current_scores = self.final_scores[-1]\n",
    "            max_scores = np.max(np.array(self.final_scores), axis=0)\n",
    "            average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
    "            table.append(['High Scores:'] + [str(score) for score in max_scores])\n",
    "            table.append(['Last Scores:'] + [str(score) for score in current_scores])\n",
    "            table.append(['Average Scores:'] + ['{:.2f}'.format(score) for score in average_scores])\n",
    "          else:\n",
    "            table = [\n",
    "              ['High Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Last Score:'] + ['--'] * self.gwg.num_critters,\n",
    "              ['Average Score:'] + ['--'] * self.gwg.num_critters,]\n",
    "          print(tabulate(table, headers=headers))\n",
    "      if self.collect_fov_data is True:\n",
    "        with self.fov_eat_table_display:\n",
    "          clear_output()\n",
    "          printmd(\"**Observations**\")\n",
    "          table_data = [[str(ii),\n",
    "                         str(self.fov_eat_table_data[0,ii]),\n",
    "                         str(self.fov_eat_table_data[1,ii])] for ii in range(11)]\n",
    "          table = ([['Food in Percept', 'Food Not Eaten', 'Food Eaten']] +\n",
    "                   table_data)\n",
    "          print(tabulate(table))\n",
    "\n",
    "  def disable_direction_buttons(self):\n",
    "    self.up_button.disabled = True\n",
    "    self.down_button.disabled = True\n",
    "    self.left_button.disabled = True\n",
    "    self.right_button.disabled = True\n",
    "\n",
    "  def enable_direction_buttons(self):\n",
    "    self.up_button.disabled = False\n",
    "    self.down_button.disabled = False\n",
    "    self.left_button.disabled = False\n",
    "    self.right_button.disabled = False\n",
    "\n",
    "  def human_ai_player_loop(self, direction):\n",
    "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
    "    # Execute the move of the human who clicked the button\n",
    "    self.button_output_update(direction)\n",
    "    # Move to the next player\n",
    "    def update_player_and_rounds():\n",
    "      \"\"\"Update the player index and decrement rounds if a full loop is completed.\"\"\"\n",
    "      self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
    "      if self.active_player_index == 0:\n",
    "        self.board_state['rounds_left'] -= 1\n",
    "    update_player_and_rounds()\n",
    "    # Do AI moves if there are any\n",
    "    while self.players[self.active_player_index] != 'human':\n",
    "      self.button_output_update('tbd')\n",
    "      # Move to the next player\n",
    "      update_player_and_rounds()\n",
    "    # Next player is human turn buttons on for them\n",
    "    self.enable_direction_buttons()\n",
    "\n",
    "  def on_up_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('up')\n",
    "\n",
    "  def on_down_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('down')\n",
    "\n",
    "  def on_left_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('left')\n",
    "\n",
    "  def on_right_button_clicked(self, *args):\n",
    "    self.human_ai_player_loop('right')\n",
    "\n",
    "  def on_start_button_clicked(self, *args):\n",
    "    self.start_button.disabled = True\n",
    "    for ii in range(self.gwg.max_rounds_taken*self.gwg.num_critters):\n",
    "      self.button_output_update('tbd')\n",
    "      time.sleep(0.2)\n",
    "    self.start_button.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Graph Viz Helper Functions\n",
    "################################################################\n",
    "# @title Graphviz Helper Functions\n",
    "\n",
    "\n",
    "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
    "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
    "  fig, ax = plt.subplots(figsize=figsize)\n",
    "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
    "  ax.axis(\"off\")\n",
    "  #plt.tight_layout()\n",
    "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "def make_gw_png(gwg=None, state=None, file_path=None,\n",
    "                dpi=300, figsize=(8, 8), fontsize=12,\n",
    "                plot_score=False, plot_value=False,\n",
    "                state_value=None, state_num=None, state_pi=None):\n",
    "  \"\"\"Generate a PNG image for the given game world state.\"\"\"\n",
    "  # Validate required arguments or set defaults\n",
    "  if gwg is None or state is None or file_path is None:\n",
    "    raise ValueError(\"gwg, state, and file_path are required parameters.\")\n",
    "  title = \"\"\n",
    "  if plot_value:\n",
    "    title = f\"v*: {state_value}\\npi*: {state_pi}\"\n",
    "  elif plot_score:\n",
    "    if state_num is None:\n",
    "      title = f\"Score: {state['scores'][0][0]}\"\n",
    "    else:\n",
    "      title = f\"State: {state_num}\\nScore: {state['scores'][0][0]}\"\n",
    "  else:\n",
    "    title = \"\" if state_num is None else f\"State: {state_num}\"\n",
    "\n",
    "  fig, ax, critter_specs, food, fov = gwg.plot_board(state, legend_type=None, figsize=figsize)\n",
    "  ax.set_title(title, fontsize=fontsize)\n",
    "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
    "  plt.close()\n",
    "\n",
    "\n",
    "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
    "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
    "  for edge in edge_labels:\n",
    "    src, dest, latex_str = edge\n",
    "    if graph.has_edge(src, dest):\n",
    "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
    "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
    "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
    "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\",\n",
    "                     fixedsize=\"true\", width=\"1.5\", height=\"1\")\n",
    "      graph.delete_edge(src, dest)\n",
    "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
    "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
    "  return graph\n",
    "\n",
    "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
    "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
    "  for node in graph.nodes():\n",
    "    if not node.startswith(\"dummy\"):\n",
    "      node.attr['width'] = width\n",
    "      node.attr['height'] = height\n",
    "  return graph\n",
    "\n",
    "def collapse_intermediate_states(graph, intermediate_nodes):\n",
    "  for node in intermediate_nodes:\n",
    "    incoming_edges = graph.in_edges(node)\n",
    "    outgoing_edges = graph.out_edges(node)\n",
    "    for in_edge in incoming_edges:\n",
    "      for out_edge in outgoing_edges:\n",
    "        graph.add_edge(in_edge[0], out_edge[1])\n",
    "        # Remove the intermediate node\n",
    "    graph.remove_node(node)\n",
    "  return graph\n",
    "\n",
    "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
    "                            action_nodes = [],\n",
    "                            state_visualization = {},\n",
    "                            intermediate_nodes = [],\n",
    "                            state_nums = {},\n",
    "                            state_values = {},\n",
    "                            state_pi = {},\n",
    "                            gwg=None,\n",
    "                            node_colors = {},\n",
    "                            node_labels = {},\n",
    "                            output_path=\"graphviz_output.png\", dpi=600,\n",
    "                            latex_figsize=(1.0, 1.0), latex_fontsize=20,\n",
    "                            gwg_figsize=(1.0, 1.0), gwg_fontsize=20,\n",
    "                            plot_score=True,\n",
    "                            plot_value=True,\n",
    "                            rankdir='LR'):\n",
    "  \"\"\"\n",
    "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
    "\n",
    "  Parameters:\n",
    "    nodes_list (list): List of nodes in the graph.\n",
    "    edges_list (list): List of edges in the graph.\n",
    "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
    "    output_path (str): Path to save the rendered graph.\n",
    "    dpi (int): DPI for rendering the graph.\n",
    "    figsize (tuple): Figure size for the LaTeX labels.\n",
    "\n",
    "  Returns:\n",
    "    str: Path to the saved graph image.\n",
    "  \"\"\"\n",
    "  # Graph Creation and Configuration\n",
    "  G = pgv.AGraph(directed=True, strict=False, rankdir='UD', ranksep=0.5, nodesep=0.5)\n",
    "\n",
    "  # Add state and decision nodes\n",
    "  for node in nodes_list:\n",
    "    shape = \"box\" if node in action_nodes else \"ellipse\"  # Use 'box' for decision nodes\n",
    "    color = node_colors.get(node, \"black\")\n",
    "    label = node_labels.get(node, node)\n",
    "    if node in state_visualization.keys():\n",
    "      state = state_visualization[node]\n",
    "      state_num = state_nums.get(node)\n",
    "      state_value = state_values.get(node)\n",
    "      pi_value = state_pi.get(node)\n",
    "\n",
    "      img_path = f\"{node}.png\"\n",
    "      make_gw_png_args = {\n",
    "        \"gwg\": gwg, \"state\": state, \"file_path\": img_path,\n",
    "        \"dpi\": dpi, \"figsize\": gwg_figsize, \"fontsize\": gwg_fontsize,\n",
    "        \"plot_score\": plot_score, \"plot_value\": plot_value,\n",
    "        \"state_value\": state_value, \"state_num\": state_num, \"state_pi\": pi_value\n",
    "      }\n",
    "      make_gw_png(**make_gw_png_args)\n",
    "      label = \"\"\n",
    "    else:\n",
    "        img_path = \"\"\n",
    "    G.add_node(node, color=color, label=label, shape=shape, image=img_path,\n",
    "               fixedsize=\"true\", width=\"2\", height=\"2\")\n",
    "\n",
    "  for edge in edges_list:\n",
    "    G.add_edge(edge[0], edge[1])\n",
    "\n",
    "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
    "  # G = set_regular_node_sizes(G, width=1, height=1)\n",
    "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=latex_figsize, fontsize=latex_fontsize)\n",
    "\n",
    "  # Remove any itermediate nodes\n",
    "  G = collapse_intermediate_states(G, intermediate_nodes)\n",
    "\n",
    "  # Additional graph attributes\n",
    "  # G.graph_attr['size'] = \"8,8\"\n",
    "  G.graph_attr['dpi'] = str(dpi)\n",
    "\n",
    "  # Render and save the graph\n",
    "  G.layout(prog='dot')\n",
    "  G.draw(output_path)\n",
    "\n",
    "  return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.1 Perfection in a very simple Gridworld - Decision Tree Expansion\n",
    "\n",
    "The Gridworld problem we initially explored earlier in this book had some complexity, making the truly perfect optimal policy difficult to determine. Here we will look at a simplified version of the problem that is more readily tractable. Run the code cell below to try out this highly simplified version of the Gridworld MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld, but shorter, smaller, and food doesn't regenerate - 3 moves\n",
    "# @markdown Run this cell to try and eat as much food as possible in 3 moves\n",
    "init_state = {\n",
    "  'pieces': np.array([[[ 0, 0, 0, -1,],\n",
    "                       [-2, 1, 0, -3,]]], dtype=int),\n",
    "  'scores': np.array([[0]]),\n",
    "  'rounds_left': np.array([3]),\n",
    "  'is_over': np.array([0])\n",
    "}\n",
    "gwg1 = GridworldGame(batch_size=1,\n",
    "                    n_rows=2, n_cols=4,\n",
    "                    num_foragers=1,\n",
    "                    num_predators=0,\n",
    "                    max_rounds_taken=3,\n",
    "                    end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=init_state)\n",
    "\n",
    "igwg1 = InteractiveGridworld(\n",
    "    gwg1, players=['human'], critter_names=['Critter (You)'],\n",
    "    figsize=(5,4), has_fov=False)\n",
    "display(igwg1.b_fig.canvas)\n",
    "clear_output()\n",
    "display(igwg1.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully you can get a total return of two points in the Gridworld MDP above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld, but shorter, smaller, and food doesn't regenerate - 1 move\n",
    "# @markdown Run this cell to try and eat as much food as possible in 1 move\n",
    "init_state = {\n",
    "  'pieces': np.array([[[ 0, 0, 0, -1,],\n",
    "                       [-2, 1, 0, -3,]]], dtype=int),\n",
    "  'scores': np.array([[0]]),\n",
    "  'rounds_left': np.array([1]),\n",
    "  'is_over': np.array([0])\n",
    "}\n",
    "gwg2 = GridworldGame(batch_size=1,\n",
    "                    n_rows=2, n_cols=4,\n",
    "                    num_foragers=1,\n",
    "                    num_predators=0,\n",
    "                    max_rounds_taken=1,\n",
    "                    end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=init_state)\n",
    "\n",
    "igwg2 = InteractiveGridworld(\n",
    "    gwg2, players=['human'], critter_names=['Critter (You)'],\n",
    "    figsize=(5,4), has_fov=False)\n",
    "display(igwg2.b_fig.canvas)\n",
    "clear_output()\n",
    "display(igwg2.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully you can get a total return of one point in the Gridworld MDP above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld, but shorter, smaller, and food doesn't regenerate - 5 moves\n",
    "# @markdown Run this cell to try and eat as much food as possible in 5 moves\n",
    "init_state = {\n",
    "  'pieces': np.array([[[ 0, 0, 0, -1,],\n",
    "                       [-2, 1, 0, -3,]]], dtype=int),\n",
    "  'scores': np.array([[0]]),\n",
    "  'rounds_left': np.array([5]),\n",
    "  'is_over': np.array([0])\n",
    "}\n",
    "gwg3 = GridworldGame(batch_size=1,\n",
    "                    n_rows=2, n_cols=4,\n",
    "                    num_foragers=1,\n",
    "                    num_predators=0,\n",
    "                    max_rounds_taken=5,\n",
    "                    end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=init_state)\n",
    "\n",
    "igwg3 = InteractiveGridworld(\n",
    "    gwg3, players=['human'], critter_names=['Critter (You)'],\n",
    "    figsize=(5,4), has_fov=False)\n",
    "display(igwg3.b_fig.canvas)\n",
    "clear_output()\n",
    "display(igwg3.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hopefully you can get a total return of three points in the Gridworld MDP above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Thinking Exercises:**\n",
    "\n",
    "1. Is scoring 2 points the maximum achievable score in the 3-moves version of this simple Gridworld MDP?(Answer: Yes)\n",
    "\n",
    "2. How can you be sure this is the best outcome achievable? (Answer: Given there are only three moves and three pieces of food, the theoretical maximum score is 3. But, since the food pieces are not adjacent, achieving a score of 3 is impossible. The optimal score given these conditions must be less than or equal to 2. So we know we have attained an optimal score. Alternate reasoning that leads to this conclusion is also valid.)\n",
    "\n",
    "How did you figure out what the best policy was? Did you try different things and see what the result was? Did you carefully plan out moves in advance and calculate the result in your head? Or did you use some combination of these two approaches?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Regardless of your approach, the key to solving the 3-move problem lies in prioritizing future rewards over immediate ones. Think about how the 'eat when near' policy would do on the 3-move version of the problem, not well. 'Eat when near' fails in this scenario because it prioritizes immediate reward (which is often a good idea), but in this particular situation greedily optimizing for immediate reward means losing out on even greater future rewards.\n",
    "\n",
    "The total sum of rewards over an episode is called the *return*. A *return*, measures the overall success of a policy over an entire episode. Sometimes there is a divergence between the actions that maximize immediate reward and those that maximize return. This can be seen clearly by contrasting the 'eat when near' policy with the optimal policy for the 3-move version of the simple Gridworld MDP above.\n",
    "\n",
    "In general, the problem of learning to select actions that increase returns not just through immediate rewards but also by enabling future rewards is known as \"the credit assignment problem\". In general the rewards (positive or negative) resulting from actions may not be immediately apparent and can be delayed. This delay between actions and their resultant rewards creates a challenge: determining which actions are actually responsible for the rewards received, i.e. causal, especially when resultant rewards follow several steps after an action, amidst other intervening actions and rewards.\n",
    "\n",
    "The credit assignment problem becomes even more challenging in complex environments where combinations and sequences of actions contribute to a final outcome, and the effects of atomic actions are not immediately observable. In these situations successful learning must attribute the correct amount of credit (or blame) to each action taken, despite the complexity of the environment and the delay in outcomes. This requires a way of effectively linking actions with their long-term consequences on total reward received, i.e. *returns*, not just their impact on immediate *reward*. As we will see *value* holds the key to optimally balancing tensions between immediate reward and total returns.\n",
    "\n",
    "From an evolutionary perspective, we expect selection to generally favour policies that maximize returns, not immediate rewards. Broadly speaking, organisms will maximize their reproductive success by effectively accumulating resources and avoiding death over some series of reproductive events. Evolution selects organisms whose learning and developmental mechanisms produce policies that maximize returns in this sense, always within the physical and physiological constraints of policy learning and implementation. For instance a behavioural policy that greedily optimizes immediate rewards is often an effective and easy to implement approximation of a policy that optimizes returns, and so such a policy might be selected for given the tradeoff between near optimality and efficiency of implementation.\n",
    "\n",
    "In many scenarios though, adaptive balancing of risk and reward requires policies that are, at least seemingly, future oriented. Consider complex animal behaviours like food caching, nest/den building, migration and play. None of these behaviours have immediate benefit in terms of calories ingested, predators avoided, or viable offspring reared, yet for the animals that engage in these behaviours they are crucial to not starving or being eaten and raising offspring at some later time. How complex, future-oriented behaviours such as these emerge from the joint optimization processes of evolution and learning is still something of a mystery. We will review significant progress in this area in the book's final sections on Reinforcement Learning (Include box on Magnus' book and expand on related ideas here?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For now, we'll set aside these more complex challenges and focus on the simpler task of identifying the optimal policy in our simplified Gridworld MDP.\n",
    "This simplified version of Gridworld (focusing on the 3-move problem) is only slightly different from the original problem we investigated, but these slight differences make it relatively easy to figure out the very best policy. One obvious difference is that this problem is smaller in every way. The board is a 2x4 grid, not 7x7, there are 3 food items instead of 10, and the number of rounds is 3 instead of 30. Slighltly more subtle, is that the food does not regenerate, and the starting position is not randomized. This effectively removing any uncertainty and stochasticity from the MDP. Eliminating this stochasticity, both in the initial board position, and in terms of where food will respawn, makes it much easier to plan out an optimal trajectory of moves. Additionally, because this problem is so small, it is relatively easy to write out every possible course of action in a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the fully expanded decision tree for this simple scenario\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([3]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LU\": {\n",
    "        'pieces': np.array([[[1, 0, 0, -1], [0, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LUR\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [0, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LUD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [0, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LRL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LRU\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [0, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_LRR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [0, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_U\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UL\": {\n",
    "        'pieces': np.array([[[1, 0, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_ULR\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_ULD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UR\": {\n",
    "        'pieces': np.array([[[0, 0, 1, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_URR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, 1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_URL\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_URD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UDL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UDU\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_UDR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 1, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RLL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [1, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RLU\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RLR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RU\": {\n",
    "        'pieces': np.array([[[0, 0, 1, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RUL\": {\n",
    "        'pieces': np.array([[[0, 1, 0, -1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RUR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, 1], [-2, 0, 0, -3]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RUD\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, -3]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RR\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RRL\": {\n",
    "        'pieces': np.array([[[0, 0, 0, -1], [-2, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_RRU\": {\n",
    "        'pieces': np.array([[[0, 0, 0, 1], [-2, 0, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([False])\n",
    "    }\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      plot_value=False,\n",
    "                                      dpi=600)\n",
    "Image(output_path, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The decision tree visualized above starts with a single initial state at the top. Following from this initial state, we see that the actions (Left, Right, Up, Down) change the board state and that when the forager moves onto food, a reward is received and the score (accumulated reward) increases. The processes ends after three actions are taken. There are 21 different paths here. Examining the scores (returns) at the decision tree's final (leaf) nodes reveals that most trajectories yield a return of one or zero. However, there is a single trajectory, Right-Right-Up, that results in a return of two. This exhaustive method, detailing every possible outcome to identify the best trajectory, resembles a brute force search across a policy's parameter space. It is not very computationally efficient, but it will always work given enough computational resources. In this deterministic setting, where the decision tree is fully expanded, the concept of *value* is unnecessary for our analysis. It is enough to simply look at returns and choose a trajectory that maximizes final return.\n",
    "\n",
    "In this kind of problem, where everything is fully observable and determinisitic, the optimal policy and the optimal trajectory are basically the same thing. However, as soon as stochasticity is introduced, either in how the organism chooses their actions, or how those actions result in state transitions, there ceases to be certainty about what path through the decision tree will be followed, and a policy must be able to choose actions along any of the probable trajectories. In general a policy (together with the stochastic dynamics of the environment) can be thought of as inducing a distribution over all the possible trajectories through the decision tree. In this simple case because there is was no stochasticity at all, the optimal policy collapsed into simply following the optimal trajectory. Next, we will take a look at how this decision tree approach becomes both more interesting and more difficult when uncertainty is introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.2 Perfection in a very simple but uncertain Gridworld - Decision Tree Expansion with *Value*\n",
    "Previously, in the Gridworld MDPs discussed in chapter 1.1, food items regenerated in random empty locations after eating events, adding a level of unpredictability to the world. In the simple Gridworld MDP just discussed above, food does not regenerate and so there is no stochasticity. This makes the decision tree of the process much easier to expand. Now, let's look at an even smaller, shorter Gridworld MDP, but where food does regenerate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Gridworld, even smaller and shorter, but food does regenerate - 2 moves\n",
    "# @markdown Run this cell to try and eat as much food as possible in two moves.\n",
    "init_state = {\n",
    "  'pieces': np.array([[[ -1, 1, -2, 0,]]], dtype=int),\n",
    "  'scores': np.array([[0]]),\n",
    "  'rounds_left': np.array([2]),\n",
    "  'is_over': np.array([0])\n",
    "}\n",
    "gwg4 = GridworldGame(batch_size=1,\n",
    "                    n_rows=1, n_cols=4,\n",
    "                    num_foragers=1,\n",
    "                    num_predators=0,\n",
    "                    max_rounds_taken=2,\n",
    "                    end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=2/3,\n",
    "                    food_forager_regen=True,\n",
    "                    init_board_state=init_state)\n",
    "\n",
    "igwg4 = InteractiveGridworld(\n",
    "    gwg4, players=['human'], critter_names=['Critter (You)'],\n",
    "    figsize=(5,4), has_fov=False)\n",
    "display(igwg4.b_fig.canvas)\n",
    "clear_output()\n",
    "display(igwg4.final_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Are you able to always get a score of two, or do you sometimes get a score of one? Let's expand the decision tree for this simple game as well and think about the optimal trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the fully expanded decision tree for this simple scenario\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down',\n",
    "        '0': '0.5',\n",
    "        '1': '0.5',\n",
    "        '2': '0.5',\n",
    "        '3': '0.5'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[1, 0, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L3\": {\n",
    "        'pieces': np.array([[[1, 0, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L3R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1R\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R3L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R\": {\n",
    "        'pieces': np.array([[[-1, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      plot_value=False,\n",
    "                                      dpi=600)\n",
    "Image(output_path, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Naively, on the first move, it might seem like both going 'left' and going 'right' are equally good moves. They both result in eating food right away, and in both cases there is a chance of eating more food, depending on where the food regenerates. Looking at going 'left' first, we see that there is an even chance of food spawning in a place where the forager will be able to eat it on the next roung and spawning in a place where eating on the next round will not be possible. So in addition to the immediate reward from eating the future expected reward of moving 'left' is one half. In contrast when we look at going 'right' we see that no matter where the food spawns there is a move that will allow the forager to eat (also a move where the forager will not eat).  If we assume that the forager will eat the food adjacent to it on the second round, then the expected future reward when moving 'right' is one.\n",
    "\n",
    "In the decision tree above we have broken up the state transition into two parts, one where the forager moves, and another where the environment reacts and regenerates the food. From the perspective of the organism these two parts can be collapsed, since the organism doesn't choose actions at these 'intermediate' states. Removing these intermediate states our decision tree looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the decision tree without 'intermediate' states.\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down',\n",
    "        '0': '0.5',\n",
    "        '1': '0.5',\n",
    "        '2': '0.5',\n",
    "        '3': '0.5'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[1, 0, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R0\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R3\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L3\": {\n",
    "        'pieces': np.array([[[1, 0, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L3R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L2\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L3\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1R\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R3L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R\": {\n",
    "        'pieces': np.array([[[-1, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R2\": {\n",
    "        'pieces': np.array([[[-1, 0, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "state_nums = {\"init_state\": '0',\n",
    "              \"state_L1\": '1',\n",
    "              \"state_L3\": '2',\n",
    "              \"state_R1\": '3',\n",
    "              \"state_R3\": '4',\n",
    "              \"state_L1R0\": '5',\n",
    "              \"state_L1R3\": '6',\n",
    "              \"state_L3R\": '7',\n",
    "              \"state_R1L2\": '8',\n",
    "              \"state_R1L3\": '9',\n",
    "              \"state_R1R\": '10',\n",
    "              \"state_R3L\": '11',\n",
    "              \"state_R3R1\": '12',\n",
    "              \"state_R3R2\": '13',\n",
    "             }\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      intermediate_nodes=[\"state_L\", \"state_R\",\n",
    "                                                          \"state_L1R\",\n",
    "                                                          \"state_R1L\",\n",
    "                                                          \"state_R3R\"],\n",
    "                                      state_nums=state_nums,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      dpi=600,\n",
    "                                      plot_value=False,\n",
    "                                      gwg_fontsize=16,\n",
    "                                      gwg_figsize=(1.5,1.5))\n",
    "Image(output_path, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Squinting at these diagrams, we can see that taking a 'right' on the first move is the way to go. This is not because going 'right' gives food immediately, both moves do that, but because it gives the forager more flexibility in responding to the where new food regenerates. After moving 'right', regardless of where food regenerates the forager has an action that allows it to eat again. This illustrates a core principle in many Markov Decision Processes (MDPs): an action can be good both because it secures immediate reward, but also because it sets the organism up for future reward. Often there is a tradeoff to be made between immediate and future reward. The concept of *value* is used to encapsulates the potential for future rewards from any given state under a specific policy.\n",
    "\n",
    "Note when using using the term *value* it will typically be clear from context whether this is meant in the specific and technical meaning of MDPs and Reinforcement learning (RL) or in the more standard general usage, e.g. \"The value of the variable $x$ is three in this case.\" If there is some ambiguity we will use italics to indicate that the precise MDP meaning applies.\n",
    "\n",
    "To get a sense of what *value* is, let's compute the *value* of each of the states in this decision tree. Since the foraging session is over, the *value* of the leaf nodes (states 5 through 13) is zero; There are no more rewards coming! In symbols, $v_\\pi(s_i) = 0$ for $i \\in \\{5,6,7,8,9,10,11,12,13\\}$. Note that all terminal states of a process for any policy will always have a value of zero.\n",
    "\n",
    "The *value* of state $s_1$ is one, since the forager can only do one thing (move right), and the outcome always results in a reward being obtained, and ending up in either state $s_5$ or $s_6$, both of which have *value* zero, as terminal states.\n",
    "\n",
    "More formally, the *value* of a state is the expectation of the immediate reward that will be obtained combined with the expected *value* of the next state that is transitioned to, given the probability distribution over the actions that the organism takes in that state according to their policy, and the probability distribution over state transitions conditional on the organism's actions. That is\n",
    "$$\n",
    "v_\\pi(s)= \\sum_{a \\in \\mathcal{A}(s)}\\left[\\pi(a|s) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s, a) \\right]\n",
    "$$\n",
    "Here $\\pi(a|s)$ gives the probability of taking a given action, $a$, in a given state, $s$, when using the policy $\\pi$, and $p(s',r|s,a)$ gives the probability of transitioning from state $s$ to state $s'$ and recieving reward $r$, given that action $a$ is chosen. (Note that the function $p(s', r | s,a)$ gives a complete description of the environmental dynamics of an MDP, this function is sometimes called the transition kernel.)\n",
    "\n",
    "Working from this definition of value we have\n",
    "$$\\begin{align}\n",
    "v_\\pi(s_1) &= \\sum_{a \\in \\mathcal{A}(s_1)}\\left[\\pi(a|s_1) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s_1, a) \\right] \\\\\n",
    " &= \\pi(\\text{Right}|s_1) \\left[ (1 + v_\\pi(s_5))0.5 + (1+v_\\pi(s_6))0.5) \\right]\n",
    " \\end{align}$$\n",
    "\n",
    "The various transition probabilities can be looked up in the decision tree above. Regardless of what the policy is, 'Right' is the only possible move in state $s_1$ so $\\pi(\\text{Right}|s_1)=1$. Then $v_\\pi(s_1) = 1$\n",
    "\n",
    "The *value* of state 2 is zero since again the forager can only do one thing (move right) but from this state they will not obtain any reward, and the next state after this, $s_7$, has a value of zero. See if you can work from the definition of value, to obtain $v_\\pi(s_2)=0$.\n",
    "$$v_\\pi(s_2)= \\sum_{a \\in \\mathcal{A}(s_1)}\\left[\\pi(a|s_2) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s, a) \\right]$$\n",
    "\n",
    "The Value of state 3 is a little more complex, because it depends on what the forager does. In states $s_1$ and $s_2$ the forager could only move right, but here in state $s_3$ the forager can go both right or left, so the value of the state depends on what the forager will do. It depends on the foragers' policy. Working from the definition\n",
    "$$ \\begin{align}\n",
    "v_\\pi(s_3) = & \\ \\pi(\\text{Left} \\mid s_3) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s_3, \\text{Left}) \\ + \\\\\n",
    "& \\ \\pi(\\text{Right} \\mid s_3) \\sum_{s' \\in \\mathcal{S}, r \\in \\mathcal{R}} (r + v_{\\pi}(s')) p(s',r \\mid s_3, \\text{Right})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If this forager is using the 'Random Valid' policy, $\\pi_\\text{RV}$, i.e. it moves left or right with equal probability, then:\n",
    "$$\n",
    "v_{\\pi_\\text{RV}}(s_3) = 0.5 (0.5(1+0) + 0.5(1+0)) + 0.5 (0 + 0) = 0.5\n",
    "$$\n",
    "\n",
    "But, if the forager is playing the 'Eat Nearby Food' policy, $\\pi_\\text{EN}$, i.e. it moves to immediately adjacent food if possible, chooses between multiple adjacent food options with uniform probability, and chooses from multiple non-food options, when no food is immediately adjacent, with uniform probability. In this case the value is\n",
    "\n",
    "$$ v_{\\pi_\\text{EN}}(s_3) = 1 (0.5(1 + 0) + 0.5(1+0)) + 0 (0+0)) = 1$$\n",
    "\n",
    "The value of state 4, like that of state 3 is dependent on the policy, and can be calculated in much the same way with $ v_{\\pi_\\text{EN}}(s_4) = 1$, but $ v_{\\pi_\\text{RV}}(s_4) = 0.5$.\n",
    "\n",
    "This is a key aspect of value, it depends both on the **state**, $s$ and the **policy**, $\\pi$. This is because *value* is a measure of how well the policy is expected to exploit the opportunities for future reward afforded by that state going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We use $v_*(s)$ to denote the *value* of a state under an optimal policy, and $\\pi_*$ to denote this policy itself.\n",
    "\n",
    "The given goal is to maximize expected returns, that is total expected reward, and $v_*(s)$ is the expected reward going forward from a state. Consequently, the optimal policy $\\pi_*$ is defined as choosing the action in each state that maximizes the sum of the expected immediate reward and the expected future value, that is:\n",
    "$$\\pi_*(s) = \\underset{a \\in \\mathcal{A}(s)}{\\arg\\max} \\; \\mathbb{E} \\left[ R(s,a) \\mid s, a \\right] + \\mathbb{E} \\left[ v_*(s')  \\mid s, a\\right]$$\n",
    "\n",
    "With this definition of an optimal policy and considering our brief (2 rounds) foraging episode with fixed starting positions, we can work backwards through the decision tree to determine the optimal value and corresponding action for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown **Run this cell** to visualize the decision tree with the values we calculated.\n",
    "\n",
    "def parse_state_transitions(state_names):\n",
    "  # Initialize an empty list to store edges\n",
    "  edges = []\n",
    "  edge_labels = []\n",
    "  action_map = {\n",
    "        'L': 'Left',\n",
    "        'R': 'Right',\n",
    "        'U': 'Up',\n",
    "        'D': 'Down',\n",
    "        '0': '0.5',\n",
    "        '1': '0.5',\n",
    "        '2': '0.5',\n",
    "        '3': '0.5'\n",
    "    }\n",
    "  # Iterate over all state names except the initial state\n",
    "  for state in state_names:\n",
    "    if state == \"init_state\":\n",
    "      continue  # Skip the initial state\n",
    "    # Split the state name to isolate the sequence of actions\n",
    "    actions = state.split('_')[1:][0]  # This removes the \"state\" prefix\n",
    "    if len(actions) > 1:\n",
    "      # The direct predecessor is the state name minus the last action\n",
    "      predecessor_actions = actions[:-1]\n",
    "      predecessor_name = \"state_\" + predecessor_actions\n",
    "\n",
    "      # Add an edge from the predecessor to the current state\n",
    "      edges.append((predecessor_name, state))\n",
    "      edge_labels.append((predecessor_name, state, action_map[actions[-1]]))\n",
    "    else:\n",
    "      # If there's only one action, the predecessor is the initial state\n",
    "      edges.append((\"init_state\", state))\n",
    "      edge_labels.append((\"init_state\", state, action_map[actions]))\n",
    "  return edges, edge_labels\n",
    "\n",
    "state_nodes = {\n",
    "    \"init_state\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[0]]),\n",
    "        'rounds_left': np.array([2]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L\": {\n",
    "        'pieces': np.array([[[1, 0, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1\": {\n",
    "        'pieces': np.array([[[1, -1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L1R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R0\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L1R3\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_L3\": {\n",
    "        'pieces': np.array([[[1, 0, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_L3R\": {\n",
    "        'pieces': np.array([[[0, 1, -2, -1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 1, 0]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R1L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L2\": {\n",
    "        'pieces': np.array([[[-1, 1, -2, 0]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1L3\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R1R\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3\": {\n",
    "        'pieces': np.array([[[-1, 0, 1, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([1]),\n",
    "        'is_over': np.array([False])\n",
    "    },\n",
    "    \"state_R3L\": {\n",
    "        'pieces': np.array([[[-1, 1, 0, -2]]], dtype=int),\n",
    "        'scores': np.array([[1]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R\": {\n",
    "        'pieces': np.array([[[-1, 0, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R1\": {\n",
    "        'pieces': np.array([[[-1, -2, 0, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "    \"state_R3R2\": {\n",
    "        'pieces': np.array([[[-1, 0, -2, 1]]], dtype=int),\n",
    "        'scores': np.array([[2]]),\n",
    "        'rounds_left': np.array([0]),\n",
    "        'is_over': np.array([True])\n",
    "    },\n",
    "}\n",
    "\n",
    "# List of all state names (keys from the state_nodes dictionary)\n",
    "state_names = list(state_nodes.keys())\n",
    "state_nums = {\"init_state\": '0',\n",
    "              \"state_L1\": '1',\n",
    "              \"state_L3\": '2',\n",
    "              \"state_R1\": '3',\n",
    "              \"state_R3\": '4',\n",
    "              \"state_L1R0\": '5',\n",
    "              \"state_L1R3\": '6',\n",
    "              \"state_L3R\": '7',\n",
    "              \"state_R1L2\": '8',\n",
    "              \"state_R1L3\": '9',\n",
    "              \"state_R1R\": '10',\n",
    "              \"state_R3L\": '11',\n",
    "              \"state_R3R1\": '12',\n",
    "              \"state_R3R2\": '13',\n",
    "             }\n",
    "\n",
    "opt_state_values = {\n",
    "  \"init_state\": 2,\n",
    "  \"state_L1\": 1,\n",
    "  \"state_L3\": 0,\n",
    "  \"state_R1\": 1,\n",
    "  \"state_R3\": 1,\n",
    "  \"state_L1R0\": 0,\n",
    "  \"state_L1R3\": 0,\n",
    "  \"state_L3R\": 0,\n",
    "  \"state_R1L2\": 0,\n",
    "  \"state_R1L3\": 0,\n",
    "  \"state_R1R\": 0,\n",
    "  \"state_R3L\": 0,\n",
    "  \"state_R3R1\": 0,\n",
    "  \"state_R3R2\": 0,\n",
    "}\n",
    "\n",
    "opt_pi = {\n",
    "  \"init_state\": 'Right',\n",
    "  \"state_L1\": 'N/A',\n",
    "  \"state_L3\": 'N/A',\n",
    "  \"state_R1\": 'Left',\n",
    "  \"state_R3\": 'Right',\n",
    "  \"state_L1R0\": 'N/A',\n",
    "  \"state_L1R3\": 'N/A',\n",
    "  \"state_L3R\": 'N/A',\n",
    "  \"state_R1L2\": 'N/A',\n",
    "  \"state_R1L3\": 'N/A',\n",
    "  \"state_R1R\": 'N/A',\n",
    "  \"state_R3L\": 'N/A',\n",
    "  \"state_R3R1\": 'N/A',\n",
    "  \"state_R3R2\": 'N/A',\n",
    "}\n",
    "\n",
    "# Parse the state transitions to get the edges\n",
    "edges, edge_labels = parse_state_transitions(state_names)\n",
    "\n",
    "gwg = GridworldGame(batch_size=1, n_rows=2, n_cols=4, num_foragers=1,\n",
    "                    num_predators=0, max_rounds_taken=3, end_prob=0.0,\n",
    "                    food_num_deterministic=True,\n",
    "                    food_patch_prob=3/7,\n",
    "                    food_forager_regen=False,\n",
    "                    init_board_state=state_nodes['init_state'])\n",
    "\n",
    "output_path = create_and_render_graph(state_names, edges, edge_labels,\n",
    "                                      state_visualization=state_nodes,\n",
    "                                      intermediate_nodes=[\"state_L\", \"state_R\",\n",
    "                                                          \"state_L1R\",\n",
    "                                                          \"state_R1L\",\n",
    "                                                          \"state_R3R\"],\n",
    "                                      state_nums=state_nums,\n",
    "                                      state_values=opt_state_values,\n",
    "                                      state_pi=opt_pi,\n",
    "                                      plot_value=True,\n",
    "                                      plot_score=False,\n",
    "                                      gwg=gwg,\n",
    "                                      rankdir='DU',\n",
    "                                      dpi=600,\n",
    "                                      gwg_fontsize=16,\n",
    "                                      gwg_figsize=(1.5,1.5))\n",
    "Image(output_path, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The optimal policy can then be described as:\n",
    "\n",
    "\"Go right on the first move, then move to where the food is on the second move.\"\n",
    "\n",
    "Our earlier informal argument about going right offering more flexibility in terms of response to where the food might respawn is formally captured in the notion of *value*. Specifically, the expected *value* of going left and then following the optimal policy there after is only 0.5, since further eating is only possible in one of the two equally likely food respawn possibilities. However, the *value* of going right is 1, (assuming the optimal policy is follwed thereafter), because regardless of how food respawns further eating is always a possibility. We were able to intuit this in this short and simple scenario, and we see that the value concept lines up well with our intuitions. In more complex problems, e.g. if this decision tree were to be expanded out to a depth 30 within a 7x7 world with 10 food items, our intuitions might not be able to flash upon perfect solution. In such cases we will come to depend on the notion of value to optimally balance going after certain and imediate rewards against investing in less certain future reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# 1.4.1.3 How are MDPs a model for life?\n",
    "\n",
    "\n",
    "So how do we see the MDP as a model of life?\n",
    "\n",
    "At each time step the organism chooses some action. Taking a neuro-centric view of the organism, this can readily be thought of as the neural activation patterns sent out from the brain to the muscles (and other nerve driven actuators) of the organism.\n",
    "\n",
    "At each time step the organism experiences the state of the environment. Again taking a neuro-centric view of the organism, this can readily be thought of as the neural activation patterns of the various sensory neurons. Now the relationship between the raw sensory inputs of the organism and the \"true\" state of broader external environment, as relevant to the organism, is typically very complex. In general the environment is only partially observed. Leaving aside the (difficult!) problem of how to reconcile sensory experience with environmental dynamics, we think of the state as some \"relevant\" projection of the infinite richness of reality onto the sensory surface of the organism. Some of the key challenges here will be dealt with in the third part of the book on unsupervised learning.\n",
    "\n",
    "Sutton and Barto have this gem on how to think about actions and states:\n",
    "\"Some of what makes up a state could be based on memory of past sensations or even be entirely mental or subjective. For example, an agent could be in the state of not being sure where an object is, or of having just been surprised in some clearly defined sense. Similarly, some actions might be totally mental or computational. For example, some actions might control what an agent chooses to think about, or where it focuses its attention. In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them.\"\n",
    "\n",
    "The organism then lives its life according to a policy, a function that maps histories of sensory experience (encapsulated as states) to actions in the present moment. However, this policy is not fully formed at conception, nor permanently set at some fixed point in development. (Box about how even c. elegans has some plasticity despite near total genetic determination of its nervous system). Rather, the organism attempts to improve its policy over time, bootstrapping from intrinsic reward signals derived from experiences of the environment.\n",
    "\n",
    "Rewards, together with the learning process or neural plasticity rules that respond to rewards, are used to adapt the policy over the lifetime of the organism. This is neural plasticity, which allows for incredibly adaptive behavioural plasticity. Evolution acts by selecting for both intrinsic reward signals, and for plasticity rules, that will in turn result in adaptive behaviour emerging rapidly within the organism's lifetime. These intrinsic rewards often derive from hunger/satiety, thirst/satiety, sleep, comfort/discomfort. For some species intrinsic reward are also be connected to aspects of exploration, social interaction, mating, and parenting. How a learning process takes these intrinsic rewards and uses them to develop a policy with high returns will be the focus of the fourth part of this book on reinforcement learning. The important thing to keep in mind is that evolution is selecting for the learning process, but this does not mean the the perfect fitness optimizing behaviour will always be learned. Only to the extent that evolution has been an effective optimizer will the experiences that the organism finds intrinsically rewarding also do a good job of increasing the organism's fitness in the evolutionary sense.\n",
    "\n",
    "Note that we do not expect the policies of organisms to be optimal, since policies themselves are not directly slected for, but we do expect, applying a normative evolutionary perspective, that the intrinsic rewards and learning rules that animals use to develop their behavioural policies are under strong selection and hence near optimal subject to other constraints. Sutton and Barto have this wisdom to share on optimality:\n",
    "\n",
    "\"We have defined optimal value functions and optimal policies. Clearly, an agent that learns an optimal policy has done very well, but in practice this rarely happens. For the kinds of tasks in which we are interested, optimal policies can be generated only with extreme computational cost. A well-defined notion of optimality organizes the approach to learning we describe in this book and provides a way to understand the theoretical properties of various learning algorithms, but it is an ideal that agents can only approximate to varying degrees. As we discussed above, even if we have a complete and accurate model of the environment's dynamics, it is usually not possible to simply compute an optimal policy by solving the Bellman optimality equation. For example, board games such as chess are a tiny fraction of human experience, yet large, custom-designed computers still cannot compute the optimal moves. A critical aspect of the problem facing the agent is always the computational power available to it, in particular, the amount of computation it can perform in a single time step.\n",
    "\n",
    "The memory available is also an important constraint. A large amount of memory is often required to build up approximations of value functions, policies, and models. In tasks with small, finite state sets, it is possible to form these approximations using arrays or tables with one entry for each state (or state-action pair). This we call the tabular case, and the corresponding methods we call tabular methods. In many cases of practical interest, however, there are far more states than could possibly be entries in a table. In these cases the functions must be approximated, using some sort of more compact parameterized function representation.\n",
    "\n",
    "Our framing of the reinforcement learning problem forces us to settle for\n",
    "approximations. However, it also presents us with some unique opportunities\n",
    "for achieving useful approximations. For example, in approximating optimal behavior, there may be many states that the agent faces with such a low\n",
    "probability that selecting suboptimal actions for them has little impact on the\n",
    "amount of reward the agent receives. Tesauro's backgammon player, for exam-\n",
    "ple, plays with exceptional skill even though it might make very bad decisions\n",
    "on board configurations that never occur in games against experts. In fact, it\n",
    "is possible that TD-Gammon makes bad decisions for a large fraction of the\n",
    "game's state set. The on-line nature of reinforcement learning makes it possible to approximate optimal policies in ways that put more effort into learning to make good decisions for frequently encountered states, at the expense of less effort for infrequently encountered states. This is one key property that distinguishes reinforcement learning from other approaches to approximately solving MDPs\"\n",
    "\n",
    "The environment encapsulates that which is outside of the direct control organism. For us the brain is the learning agent, and the body is lumped together with the broader external environment. Sutton and Barto motivate this beautifully as follows:\n",
    "\n",
    "\"In particular, the boundary between agent and environment is not often the same as the physical boundary of a robot's or animal's body. Usually, the boundary is drawn closer to the agent than that. For example, the motors and mechanical linkages of a robot and its sensing hardware should usually be considered parts of the environment rather than parts of the agent. Similarly, if we apply the framework to a person or animal, the muscles, skeleton, and sensory organs should be considered part of the environment. Rewards, too, presumably are computed inside the physical bodies of natural and artificial learning systems, but are considered external to the agent. The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment. We do not assume that everything in the environment is unknown to the agent. For example, the agent often knows quite a bit about how its rewards are computed as a function of its actions and the states in which they are taken. But we always consider the reward computation to be external to the agent because it defines the task facing the agent and thus must be beyond its ability to change arbitrarily. In fact, in some cases the agent may know everything about how its environment works and still face a difficult reinforcement learning task, just as we may know exactly how a puzzle like Rubik's cube works, but still be unable to solve it. The agent-environment boundary represents the limit of the agent's absolute control, not of its knowledge.\"\n",
    "\n",
    "In this context the learner really is the brain of the organism, and it is this adaptive neural plasticity, and the resulting adaptive behavioural plasticity that is of primary interest to us.\n",
    "\n",
    "Again quoting Sutton and Barto: \"The reinforcement learning framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent's goal (the rewards).\"\n",
    "\n",
    "Utilizing this simplification has resulted in tremendous progress, and we cannot recommend Sutton and Barto's book enough to anyone interested in an overview of RL. However, our goal is different here. We are in fact interested in how this simplified RL framework can be productively extended into a holistic view of brain function and neural plasticity as a primary driver of adaptive behaviour. That is, we wish to ground the potentially very abstract notions of states, actions, and rewards, in concrete neural activation patterns, of sensory, motor, and internal/computational neurons. Recent advances in supervised and un/semi-supervised learning, as well as the integration of these advances into robust RL agents, allow us to set out a tantalizing proof of principle, for how the myriad details of sensory, memory, reward and control apparatus might be inegrated into a single, evolutionarily and physiologically plausible theoretical whole, with reinforcement learning at its core.  \n",
    "\n",
    "## MDP Notation, Glossary and Analogy to Animal Life\n",
    "\n",
    "1. An action $a$, from the set of possible actions $\\mathcal{A}$. Actions can be thought of pattern of motor neuron activation patterns which cause some physical behaviour. $A_t$ is the random variable for the action taken at time step $t$ of a particular MDP episode.\n",
    "\n",
    "2. A state $s$, from the set of possible states $\\mathcal{S}$. $S_t$ is the random variable for the state of the MDP at time step $t$ in a particular MDP episode. In simplified models, the organism experiences the state of the environment directly.\n",
    "\n",
    "3. An observation $o$, from the set of possible observations (or sensory experiences) \\mathcal{O}. $O_t$ is the random variable for the observation the organism experiences (of $S_t$) at time step $t$ of a particular MDP episode. In cases where the state of the environment is not fully observable to the organism, the aspect of the environmental state that is experienced by the organisms as an activation pattern of its sensory neurons, is called the observation. In simplified models the observation and the environmental state are treated as one and the same.\n",
    "\n",
    "4. A belief $b$, from the set of possible beliefs \\mathcal{B}. $B_t$ is the belief state of the organism at time step $t$ of a particular MDP episode. In more complicated models where the environmental state is not fully observed, the organism may also update some 'internal' state on the basis of observations. In animals this is likely in the form of activation levels of recurrent and modulatory neurons, which are not immediately connected to muscles, but serve to modulate motor output reactions to ongoing stimuli. In simpler models where the state is directly observable there is no need for beliefs, similarly for a very simple organism it may be appropriate to model motor outputs as purely reative to current observations, with history propagated through internal state updates. In more complex animals internal 'mental' states are clearly a critical part of adaptive behaviour.\n",
    "\n",
    "5. A reward $r$, from the set of possible rewards $\\mathcl{R}$. $R_t$ is the random variable for the reward received at time step $t$ of a particular MDP episode. In the typical MDP-RL framework all rewards share the same currency, and take a scalar value. This leaves open the possibility that eating cake while simultaneously stepping on a thumbtack yields a net reward of 0, the same net reward as nothing particularly interesting happening at all. More biologically oriented models may need to allow for different modalities of reward, e.g. physical pain, emotional distress, hunger, exhaustion, to affect different aspects of the animal's policy in different ways, avoiding the odd 'canceling out' described above.\n",
    "\n",
    "5. A policy $\\pi$, is the rule according to which the animal chooses its actions.\n",
    " * $\\pi(a|s)$: Maps from state-action pairs, $\\mathcal{S} \\times \\mathcal{A}(s)$, to probabilities, i.e. real values on the interval $[0, 1]$. This is a policy in the simplest case where the environment is fully observed.\n",
    " * $\\pi(a|o)$: Maps from observation-action pairs, $\\mathcal{S} \\times \\mathcal{A}(s)$, to probabilities, i.e. real values on the interval $[0, 1]$. This is a policy in the intermediate case where the environment is partially observed, but organisms do not update an maintain an internal belief state, and instead react purely to immediate observations.\n",
    " * $\\pi(a|b)$: Maps from belief-action pairs, $\\mathcal{B} \\times \\mathcal{A}(s)$, to probabilities, i.e. real values on the interval $[0, 1]$. This is a policy in the most complex case where the environment is only partially observed and the organism maintains and internal belief state.\n",
    "\n",
    "6. Transition dynamics $p(s', r | s, a)$. This is a probability function, mapping from state-action pairs together with the state transition and reward outcomes, $\\mathcal{S}^+ \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A}(s)$, to probabilities, real values on the interval $[0, 1]$. This gives the distribution of $R_t$ and $S_t$ conditional on $S_{t-1}$ and $A_{t-1}$.\n",
    " * In the partial observation case this is extended to $p(s', r, o | s, a)$, i.e. it gives the the distribution of $O_t$, $R_t$ and $S_t$ conditional on $S_{t-1}$ and $A_{t-1}$.\n",
    "\n",
    "7. Value of a state $\\pi$ is $v_\\pi(s)$, is the expected future reward given that the organism is in state $s$, and will use policy $\\pi$ going forward. Powerful learning rules capable of generating complex behaviour can emerge from relatively simple associative learning mechanisms that construct approximations of the value function.\n",
    "\n",
    "8. The q-value of a state-action $q_\\pi(s, a)$, is the expected future reward given that the organism is in state $s$, takes action $a$, and will use policy $\\pi$ going forward. Leveraging a learned approximation of a value function, a q-function, can be used to directly determine an organism's policy, simply choose the actions with the greatest q-value, or with probability increasing in q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_M3\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "P1C4_Sequence1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
