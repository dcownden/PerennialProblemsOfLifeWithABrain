{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dcownden/PerennialProblemsOfLifeWithABrain/blob/life-is-an-MDP/sequences/P1C4_LifeIsRL/P1C4_Sequence1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ob9RxGkDZpjk"
      },
      "source": [
        "The following is part of a test for an upcoming text book on computational neuroscience from an optimization and learning perspective. The book will start with evolution because ultimately, all aspects of the brain are shaped by evolution and, as we will see, evolution can also be seen as an optimization algorithm. We are sharing it now to get feedback on what works and what does not and the developments we should do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TosOaQjqZpjm"
      },
      "source": [
        "___\n",
        "# **1.4.1: Is Life Just One Big Markov Decision Process? Yeah, kind of.**\n",
        "## Objective:\n",
        "In this sequence we explore and \"solve\" a simplified version of our Gridworld within the Markov Decision Process (MDP) framework. MDPs offer a robust and flexible method for rigorously defining and identifying optimal policies across various scenarios. Although the formalism of MDPs may seem complex, its components have straightforward and intuitive meanings. We are already acquainted with the key elements of an MDP:\n",
        "\n",
        "* Policy: The behavioural rule that maps stimuli to an organism's actions.\n",
        "* Organism (Agent): The entity that reacts to stimuli and performs actions.\n",
        "* Actions: The specific responses an organism makes at any moment, guided by its policy.\n",
        "* Environment: The context in which an organism operates, including the source of stimuli, the rules for state changes in response to actions, and the nature of rewards based on actions and state transitions. Other organisms' policies can influence environmental conditions.\n",
        "* Reward: An evaluation of the outcomes of an organism's actions, typically considered in evolutionary terms such as survival, reproduction, and offspring survival.\n",
        "* Markov Process: A framework for understanding stochastic dynamics by dividing the world into possible states and defining transition probabilities between these states. Crucially, these probabilities depend solely on the current state, embodying the Markov property and simplifying analysis. Historical relevance to state dynamics is integrated into the state definition, ensuring a comprehensive state concept.\n",
        "\n",
        "In the sequence we will introduce the crucial notion of **Value**, which integrates these elements and allows for rigorous optimization. Value reflects the expected total future reward from a specific state under a particular policy. Our focus will be on utilizing Value to determine the optimal policy within an MDP. We'll examine Dynamic Programming, specifically Backward Induction, as a method to determine optimal behavior using Value. Although theoretically ideal, Backward Induction (and Dynamic Programming generally) is often impractical for complex problems due to scalability and computational limits. Despite its practical limitations, Backward Induction provides a strong theoretical foundation for more scalable and hence practical solutions. These practical solutions can then be understood as approximations to the ideal but intractable solution method.\n",
        "\n",
        "Previously we have also touched upon Partial Observability, i.e. situations where the full state of the environment is not known to the organism. Most organisms are not omniscient, so in some sense partial observability is always the case. For now though we are going to leave Partial Observability aside and focus on the simple case where the state of the world is perfectly known to the organism, to streamline the presentation of Value and Backward Induction.\n",
        "\n",
        "This all seems a bit much, but as we walk through our simplifed Gridworld example, these concepts will become more accessible and less daunting than they might initially appear. We promise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TLEhDnkDZpjm"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {},
        "id": "r95xBOAdZpjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5051a326-d1ee-477b-ae92-609788ca1816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n",
            "This notebook isn't using and doesn't need a GPU. Good.\n",
            "Running in colab\n"
          ]
        }
      ],
      "source": [
        "# @title Dependencies, Imports and Setup\n",
        "# @markdown You don't need to worry about how this code works â€“ but you do need to **run the cell**\n",
        "!apt install libgraphviz-dev > /dev/null 2> /dev/null #colab\n",
        "!pip install ipympl pygraphviz vibecheck datatops jupyterquiz > /dev/null 2> /dev/null #google.colab\n",
        "\n",
        "import requests\n",
        "from requests.exceptions import RequestException\n",
        "import numpy as np\n",
        "import itertools\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pygraphviz as pgv\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import warnings\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from io import BytesIO\n",
        "from enum import Enum\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tabulate import tabulate\n",
        "from IPython.display import display, clear_output, Markdown, HTML, Image, IFrame\n",
        "from jupyterquiz import display_quiz\n",
        "from vibecheck import DatatopsContentReviewContainer\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
        "# random seed settings and\n",
        "# getting torch to use gpu if it's there\n",
        "\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "  \"\"\"\n",
        "  DataLoader will reseed workers following randomness in\n",
        "  multi-process data loading algorithm.\n",
        "\n",
        "  Args:\n",
        "    worker_id: integer\n",
        "      ID of subprocess to seed. 0 means that\n",
        "      the data will be loaded in the main process\n",
        "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "\n",
        "def set_device():\n",
        "  \"\"\"\n",
        "  Set the device. CUDA if available, CPU otherwise\n",
        "\n",
        "  Args:\n",
        "    None\n",
        "\n",
        "  Returns:\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"This notebook isn't using and doesn't need a GPU. Good.\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook but not needed.\")\n",
        "    print(\"If possible, in the menu under `Runtime` -> \")\n",
        "    print(\"`Change runtime type.`  select `CPU`\")\n",
        "\n",
        "  return device\n",
        "\n",
        "\n",
        "SEED = 2021\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "  display(Markdown(string))\n",
        "\n",
        "\n",
        "# the different utility .py files used in this notebook\n",
        "filenames = ['gw_plotting.py', 'gw_board.py', 'gw_game.py',\n",
        "             'gw_widgets.py', 'gw_NN_RL.py']\n",
        "#filenames = []\n",
        "# just run the code straight out of the response, no local copies needed!\n",
        "for filename in filenames:\n",
        "  url = f'https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/utils/{filename}'\n",
        "  response = requests.get(url)\n",
        "  # Check that we got a valid response\n",
        "  if response.status_code == 200:\n",
        "    code = response.content.decode()\n",
        "    exec(code)\n",
        "  else:\n",
        "    print(f'Failed to download {url}')\n",
        "\n",
        "# environment contingent imports\n",
        "try:\n",
        "  print('Running in colab')\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "  from google.colab import data_table\n",
        "  data_table.disable_dataframe_formatter()\n",
        "  #from google.colab import output as colab_output\n",
        "  #colab_output.enable_custom_widget_manager()\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  print('Not running in colab')\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "%matplotlib widget\n",
        "plt.style.use(\"https://raw.githubusercontent.com/dcownden/PerennialProblemsOfLifeWithABrain/main/pplb.mplstyle\")\n",
        "plt.ioff() #need to use plt.show() or display explicitly\n",
        "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "def content_review(notebook_section: str):\n",
        "  return DatatopsContentReviewContainer(\n",
        "    \"\",  # No text prompt\n",
        "    notebook_section,\n",
        "    {\n",
        "      \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
        "      \"name\": \"neuro_book\",\n",
        "      \"user_key\": \"xuk960xj\",\n",
        "    },\n",
        "  ).render()\n",
        "feedback_prefix = \"P1C2_S3\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################################################\n",
        "# Graph Viz Helper Functions\n",
        "################################################################\n",
        "# @title Graphvis Helper Functions\n",
        "\n",
        "\n",
        "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
        "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
        "  ax.axis(\"off\")\n",
        "  #plt.tight_layout()\n",
        "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
        "  plt.close()\n",
        "\n",
        "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
        "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
        "  for edge in edge_labels:\n",
        "    src, dest, latex_str = edge\n",
        "    if graph.has_edge(src, dest):\n",
        "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
        "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
        "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
        "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\")\n",
        "      graph.delete_edge(src, dest)\n",
        "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
        "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
        "  return graph\n",
        "\n",
        "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
        "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
        "  for node in graph.nodes():\n",
        "    if not node.startswith(\"dummy\"):\n",
        "      node.attr['width'] = width\n",
        "      node.attr['height'] = height\n",
        "  return graph\n",
        "\n",
        "\n",
        "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
        "                            action_nodes = [],\n",
        "                            node_colors = {},\n",
        "                            node_labels = {},\n",
        "                            output_path=\"graphviz_output.png\", dpi=300,\n",
        "                            figsize=(0.6, 0.3), fontsize=16):\n",
        "  \"\"\"\n",
        "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
        "\n",
        "  Parameters:\n",
        "    nodes_list (list): List of nodes in the graph.\n",
        "    edges_list (list): List of edges in the graph.\n",
        "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
        "    output_path (str): Path to save the rendered graph.\n",
        "    dpi (int): DPI for rendering the graph.\n",
        "    figsize (tuple): Figure size for the LaTeX labels.\n",
        "\n",
        "  Returns:\n",
        "    str: Path to the saved graph image.\n",
        "  \"\"\"\n",
        "  # Graph Creation and Configuration\n",
        "  G = pgv.AGraph(directed=True, strict=False, rankdir='LR', ranksep=0.5, nodesep=0.5)\n",
        "\n",
        "  # Add state and decision nodes\n",
        "  for node in nodes_list:\n",
        "    shape = \"box\" if node in action_nodes else \"ellipse\"  # Use 'box' for decision nodes\n",
        "    color = node_colors.get(node, \"black\")\n",
        "    label = node_labels.get(node, node)\n",
        "    G.add_node(node, color=color, label=label, shape=shape)\n",
        "\n",
        "  for edge in edges_list:\n",
        "    G.add_edge(edge[0], edge[1])\n",
        "\n",
        "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
        "  G = set_regular_node_sizes(G, width=1, height=1)\n",
        "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=figsize, fontsize=fontsize)\n",
        "\n",
        "  # Additional graph attributes\n",
        "  G.graph_attr['size'] = \"8,8\"\n",
        "  G.graph_attr['dpi'] = str(dpi)\n",
        "\n",
        "  # Render and save the graph\n",
        "  G.layout(prog='dot')\n",
        "  G.draw(output_path)\n",
        "\n",
        "  return output_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# @title plotting functions\n",
        "#################################################\n",
        "# More plotting functions\n",
        "#################################################\n",
        "\n",
        "\n",
        "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
        "                    name=None):\n",
        "  \"\"\"\n",
        "  Plot vector field indicating critter direction probabilities.\n",
        "\n",
        "  Args:\n",
        "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
        "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
        "      and values as lists of direction probabilities corresponding to the\n",
        "      directions ['right', 'down', 'left', 'up'].\n",
        "    critter (int): Identifier for which critter directions are associated with.\n",
        "    deterministic (bool, optional): If True, the probabilities array is\n",
        "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
        "      and are larger. Defaults to False.\n",
        "  \"\"\"\n",
        "\n",
        "  #looks like direction ignores inverted axis\n",
        "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
        "                       'left': (-1, 0), 'up': (0, 1)}\n",
        "  # but offsets need to be aware of inverted\n",
        "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
        "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
        "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
        "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
        "  # same logic for colors\n",
        "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
        "  # Get the offset and color for this critter\n",
        "  critter_offset = critter_offsets[critter]\n",
        "  critter_color = critter_colors[critter]\n",
        "\n",
        "  # Add legend only if critter is not 0\n",
        "  custom_leg_handles = []\n",
        "  if critter != 0:\n",
        "    if name is None:\n",
        "      name = f'Critter {critter}'\n",
        "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
        "    # Add the legend for this critter\n",
        "    custom_leg_handles.append(legend_patch)\n",
        "\n",
        "  C, R, U, V, A = [], [], [], [], []\n",
        "\n",
        "  for loc in loc_prob_dict.keys():\n",
        "    row, col = loc\n",
        "    probs = loc_prob_dict[loc]\n",
        "    for dir_key, prob in probs.items():\n",
        "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
        "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
        "      U.append(direction_vectors[dir_key][0])\n",
        "      V.append(direction_vectors[dir_key][1])\n",
        "\n",
        "      if deterministic:\n",
        "        A.append(1 if prob == max(probs.values()) else 0)\n",
        "      else:\n",
        "        A.append(prob)\n",
        "\n",
        "  linewidth = 1.5 if deterministic else 0.5\n",
        "  scale = 15 if deterministic else 30\n",
        "\n",
        "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
        "            scale=scale, linewidth=linewidth)\n",
        "  return fig, ax, custom_leg_handles\n",
        "\n",
        "\n",
        "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
        "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
        "  returns fig and ax handles for further use\n",
        "  Args:\n",
        "    num_rows (int): number of rows in the grid (vertical dimension)\n",
        "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
        "\n",
        "  Returns:\n",
        "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
        "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
        "  \"\"\"\n",
        "  # Create a new figure and axes with given figsize\n",
        "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
        "  # Set width and height padding, remove horizontal and vertical spacing\n",
        "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
        "  # Show right and top borders (spines) of the plot\n",
        "  ax.spines[['right', 'top']].set_visible(True)\n",
        "  # Set major ticks (where grid lines will be) on x and y axes\n",
        "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
        "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
        "  # Set labels for major ticks with font size of 8\n",
        "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
        "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
        "  # Set minor ticks (no grid lines here) to be between major ticks\n",
        "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
        "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
        "  # Move x-axis ticks to the top of the plot\n",
        "  ax.xaxis.tick_top()\n",
        "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
        "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
        "  # Remove minor ticks (not the grid lines)\n",
        "  ax.tick_params(which='minor', bottom=False, left=False)\n",
        "  # Set limits of x and y axes\n",
        "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
        "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
        "  # Invert y axis direction\n",
        "  ax.invert_yaxis()\n",
        "  # If title is provided, set it as the figure title\n",
        "  if title is not None:\n",
        "    fig.suptitle(title)\n",
        "  # Hide header and footer, disable toolbar and resizing of the figure\n",
        "  fig.canvas.header_visible = False\n",
        "  fig.canvas.toolbar_visible = False\n",
        "  fig.canvas.resizable = False\n",
        "  fig.canvas.footer_visible = False\n",
        "  # Redraw the figure with these settings\n",
        "  fig.canvas.draw()\n",
        "  # Return figure and axes handles for further customization\n",
        "  return fig, ax\n",
        "\n",
        "\n",
        "def plot_food(fig, ax, rc_food_loc, food=None, size=None,\n",
        "              show_food=True):\n",
        "  \"\"\"\n",
        "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects\n",
        "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
        "    food: a handle for the existing food matplotlib PatchCollection object\n",
        "    if one exists\n",
        "  Returns:\n",
        "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
        "    new if no handle was passed or updated if it was\n",
        "  \"\"\"\n",
        "  # if no PathCollection handle passed in:\n",
        "  if size is None:\n",
        "    size=150\n",
        "  if food is None:\n",
        "    food = ax.scatter([], [], s=size, marker='o',\n",
        "                      color='red', label='Food')\n",
        "  if show_food:\n",
        "    rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
        "    #matrix indexing convention is is [row-vertical, col-horizontal]\n",
        "    #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
        "    food.set_offsets(np.fliplr(rc_food_loc))\n",
        "  return food\n",
        "\n",
        "\n",
        "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]],\n",
        "                  size=None) -> List[Dict[str, object]]:\n",
        "  \"\"\"\n",
        "  Plots multiple types of \"critters\" on a grid implied by the given\n",
        "  fig, ax arguments.\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects.\n",
        "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
        "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
        "    critter.\n",
        "\n",
        "  Returns:\n",
        "    Updated critter_specs with handles.\n",
        "  \"\"\"\n",
        "  if size is None:\n",
        "    size=250\n",
        "  for spec in critter_specs:\n",
        "    # Ensure required keys are present\n",
        "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
        "      if key not in spec:\n",
        "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
        "    handle_ = spec.get('handle')\n",
        "    if handle_ is None:\n",
        "      handle_ = ax.scatter([], [], s=size, marker=spec['marker'],\n",
        "                           color=spec['color'], label=spec['name'],\n",
        "                           edgecolors='white', linewidths=1)\n",
        "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
        "    spec.update({'handle': handle_})\n",
        "  return critter_specs\n",
        "\n",
        "\n",
        "def plot_critter(fig, ax, rc_critter_loc,\n",
        "                 critter=None, critter_name='Critter'):\n",
        "  \"\"\"\n",
        "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects\n",
        "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
        "    critter: a handle for the existing food matplotlib PatchCollection object\n",
        "    if one exists\n",
        "  Returns:\n",
        "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
        "    either new if no handle was passed in or updated if it was.\n",
        "  \"\"\"\n",
        "  if critter is None:\n",
        "    critter = ax.scatter([], [], s=250, marker='h',\n",
        "                         color='blue', label=critter_name)\n",
        "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
        "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
        "  critter.set_offsets(np.flip(rc_critter_loc))\n",
        "  return critter\n",
        "\n",
        "\n",
        "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov,\n",
        "             opaque=False, fov=None):\n",
        "  \"\"\"\n",
        "  Plots a mask on a grid implied by the given fig, ax arguments\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects\n",
        "    rc_critter: ndarry(int) (row,col) of the critter\n",
        "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
        "  Returns:\n",
        "    a handle for matplotlib Image object of mask, either new if no handle\n",
        "    was passed in or updated if it was.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
        "  mask_array = np.ones((n_rows, n_cols, 4))\n",
        "  mask_array[:, :, :3] = 0.5  # light grey color\n",
        "  if has_fov == True:\n",
        "    if opaque:\n",
        "      mask_array[:, :, 3] = 1.0  # 50% opacity\n",
        "    else:\n",
        "      mask_array[:, :, 3] = 0.5  # 50% opacity\n",
        "    # Create arrays representing the row and column indices\n",
        "    rows = np.arange(n_rows)[:, np.newaxis]\n",
        "    cols = np.arange(n_cols)[np.newaxis, :]\n",
        "    # Iterate over each critter location\n",
        "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
        "    # Set the region within the specified radius around the critter to transparent\n",
        "    mask_array[dist <= radius, 3] = 0\n",
        "  else:\n",
        "    mask_array[:, :, 3] = 0\n",
        "\n",
        "  if fov is None:\n",
        "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
        "  else:\n",
        "    fov.set_data(mask_array)\n",
        "\n",
        "  return fov\n",
        "\n",
        "\n",
        "def remove_ip_clutter(fig):\n",
        "  fig.canvas.header_visible = False\n",
        "  fig.canvas.toolbar_visible = False\n",
        "  fig.canvas.resizable = False\n",
        "  fig.canvas.footer_visible = False\n",
        "  fig.canvas.draw()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {},
        "id": "9ufNNgXqZpjs"
      },
      "outputs": [],
      "source": [
        "# @title Gridworld Board Class\n",
        "# Local definition to be put in utils later\n",
        "\n",
        "\n",
        "class GridworldBoard():\n",
        "  \"\"\"\n",
        "  A collection methods and parameters for our Gridworld game.\n",
        "\n",
        "  board state is represented by primarily by pieces, scores, moves_left and is_over\n",
        "\n",
        "  pieces is a batch x n_rows x n_cols numpy array positive integers are critter\n",
        "  locations 0's are empty space and negative integers are food. Each critter is\n",
        "  unique and executing it's own policy so they are non-fungible, whereas food\n",
        "  (of the same type) is always the same, so there can and typically will be\n",
        "  duplicates of negative integers in the pieces array, but never of positive\n",
        "  integers\n",
        "\n",
        "  For pieces first dim is batch, second dim row , third is col,\n",
        "  so pieces[0][1][7] is the square in row 2, in column 8 of the first board in\n",
        "  the batch of boards.\n",
        "\n",
        "  scores is a batchsize x num_critters numpy array giving the scores for each\n",
        "  critter on each board in the batch (note off by one indexing)\n",
        "\n",
        "  moves_left is how many moves are left in the game, for each critter.\n",
        "  This will be the same for every critter in every batch.\n",
        "\n",
        "  is_over just tracks whether each game in each batch has concluded, this allows\n",
        "  for probabalistic end times, not just deterministic end times based on moves left\n",
        "\n",
        "  Note:\n",
        "    In 2d np.array first dim is row (vertical), second dim is col (horizontal),\n",
        "    i.e. top left corner is (0,0), so take care when visualizing/plotting\n",
        "    as np.array visualization is aligned with typical tensor notation but at odds\n",
        "    with conventional plotting where (0,0) is bottom left, first dim, x, is\n",
        "    horizontal, second dim, y, is vertical, so we use invert y-axis when plotting\n",
        "    with matplotlib\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  class CritterFoodType(Enum):\n",
        "    FOOD = \"food\"\n",
        "    PREY = \"prey\"\n",
        "    PREDATOR = \"predator\"\n",
        "\n",
        "  ARRAY_PAD_VALUE = -200\n",
        "\n",
        "\n",
        "  def __init__(self, batch_size=2,\n",
        "               n_rows=7, n_cols=7,\n",
        "               num_foragers=1,\n",
        "               num_predators=0,\n",
        "               max_moves_taken=30,\n",
        "               end_prob=0.00,\n",
        "               food_num_deterministic = True,\n",
        "               food_patch_prob=10.0/49.0,\n",
        "               food_forager_regen = True,\n",
        "               rng=None,\n",
        "               state_elements = ['pieces', 'scores', 'is_over', 'moves_left'],\n",
        "               init_board_state = None\n",
        "               ):\n",
        "\n",
        "    \"\"\"Set the parameters of the game.\"\"\"\n",
        "    # size of the board/world\n",
        "    self.n_rows = n_rows\n",
        "    self.n_cols = n_cols\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    #number and type of critters on the board\n",
        "    self.num_foragers = num_foragers\n",
        "    self.num_predators = num_predators\n",
        "    # foragers will be indicated by lower valued positive integers, predators\n",
        "    # by higher valued intagers\n",
        "    self.forager_predator_threshold = self.num_foragers\n",
        "    self.num_critters = num_foragers + num_predators\n",
        "\n",
        "    # end conditions can be deterministic or stochastic\n",
        "    # one of moving, or eating or both might take time, e.g. eating might be\n",
        "    # automatic and free after moving, conversely, moving might be free, but\n",
        "    # eating count towards the session/episode ending, or both might\n",
        "    self.max_moves_taken = max_moves_taken\n",
        "    self.end_prob = end_prob\n",
        "\n",
        "    # what proportion of the (non-critter occupied) patches contain food.\n",
        "    self.food_patch_prob = food_patch_prob\n",
        "    self.food_num_deterministic = food_num_deterministic\n",
        "    if self.food_num_deterministic:\n",
        "      self.num_food = int((self.n_rows * self.n_cols - self.num_critters)\n",
        "                          * self.food_patch_prob)\n",
        "    self.food_forager_regen = food_forager_regen\n",
        "\n",
        "    # reproducible stochasticity\n",
        "    if rng is None:\n",
        "      self.rng = np.random.default_rng(seed=SEED)\n",
        "    else:\n",
        "      self.rng = rng\n",
        "\n",
        "    self.state_elements = state_elements\n",
        "\n",
        "    # initialize the board\n",
        "    if init_board_state is None:\n",
        "      init_board_state = self.get_init_board_state()\n",
        "\n",
        "    self.set_state(init_board_state)\n",
        "\n",
        "\n",
        "  def init_loc(self, n_rows, n_cols, num, rng=None):\n",
        "    \"\"\"\n",
        "    Samples random 2d grid locations without replacement, useful for placing\n",
        "    critters and food on the board.\n",
        "\n",
        "    Args:\n",
        "      n_rows: int, number of rows in the grid\n",
        "      n_cols: int, number of columns in the grid\n",
        "      num:    int, number of samples to generate. Should throw an error if num > n_rows x n_cols\n",
        "      rng:    instance of numpy.random's default rng. Used for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "      int_loc: ndarray(int) of shape (num,), flat indices for a 2D grid flattened into 1D\n",
        "      rc_index: tuple(ndarray(int), ndarray(int)), a pair of arrays with the first giving\n",
        "        the row indices and the second giving the col indices. Useful for indexing into\n",
        "        an n_rows by n_cols numpy array.\n",
        "      rc_plotting: ndarray(int) of shape (num, 2), 2D coordinates suitable for matplotlib plotting\n",
        "    \"\"\"\n",
        "\n",
        "    # Set up default random generator, use the boards default if none explicitly given\n",
        "    if rng is None:\n",
        "      rng = self.rng\n",
        "    # Choose 'num' unique random indices from a flat 1D array of size n_rows*n_cols\n",
        "    int_loc = rng.choice(n_rows * n_cols, num, replace=False)\n",
        "    # Convert the flat indices to 2D indices based on the original shape (n_rows, n_cols)\n",
        "    rc_index = np.unravel_index(int_loc, (n_rows, n_cols))\n",
        "    # Transpose indices to get num x 2 array for easy plotting with matplotlib\n",
        "    rc_plotting = np.array(rc_index).T\n",
        "    # Return 1D flat indices, 2D indices for numpy array indexing and 2D indices for plotting\n",
        "    return int_loc, rc_index, rc_plotting\n",
        "\n",
        "\n",
        "  def get_init_board_state(self):\n",
        "    \"\"\"\n",
        "    Set up starting board using game parameters\n",
        "    \"\"\"\n",
        "    state = {}\n",
        "    state['moves_left'] = (np.ones(self.batch_size) *\n",
        "                           self.max_moves_taken)\n",
        "    state['is_over'] = np.zeros(self.batch_size, dtype=bool)\n",
        "    state['scores'] = np.zeros((self.batch_size, self.num_critters))\n",
        "\n",
        "    # create an empty board array.\n",
        "    pieces = np.zeros((self.batch_size, self.n_rows, self.n_cols),\n",
        "                       dtype=int)\n",
        "    # Place critter and initial food items on the board randomly\n",
        "    if self.food_num_deterministic:\n",
        "      init_food_nums = [self.num_food] * self.batch_size\n",
        "    else:\n",
        "      init_food_nums = self.rng.binomial(self.n_rows * self.n_cols - self.num_critters,\n",
        "                                         self.food_patch_prob, size=self.batch_size)\n",
        "    # place food and critters randomly\n",
        "    for ii in np.arange(self.batch_size):\n",
        "      # num_food+num_critter because we want critter and food locations\n",
        "      int_loc, rc_idx, rc_plot = self.init_loc(\n",
        "        self.n_rows, self.n_cols, init_food_nums[ii]+self.num_critters)\n",
        "      # critter random start locations\n",
        "      for c_ in np.arange(self.num_critters):\n",
        "        pieces[(ii, rc_idx[0][c_], rc_idx[1][c_])] = c_ + 1\n",
        "      # food random start locations\n",
        "      for f_ in np.arange(init_food_nums[ii]):\n",
        "        pieces[(ii, rc_idx[0][self.num_critters + f_],\n",
        "                    rc_idx[1][self.num_critters + f_])] = -f_ - 1\n",
        "    state['pieces'] = pieces\n",
        "    return state\n",
        "\n",
        "\n",
        "  def set_state(self, board, check=False):\n",
        "    \"\"\" board is dictionary giving game state \"\"\"\n",
        "    if check:\n",
        "      if board['pieces'].shape != (self.batch_size, self.n_rows, self.n_cols):\n",
        "        raise ValueError(\"Invalid shape for 'pieces'\")\n",
        "      if board['scores'].shape != (self.batch_size, self.num_crititters):\n",
        "        raise ValueError(\"Invalid shape for 'scores'\")\n",
        "      if board['moves_left'].shape != (self.batch_size,):\n",
        "        raise ValueError(\"Invalid shape for 'moves_left'\")\n",
        "      if board['is_over'].shape != (self.batch_size,):\n",
        "        raise ValueError(\"Invalid shape for 'is_over'\")\n",
        "    for key in self.state_elements:\n",
        "      if key in board:\n",
        "        setattr(self, key, board[key].copy())\n",
        "      else:\n",
        "        raise ValueError(f\"Key '{key}' not found in the provided board state.\")\n",
        "\n",
        "\n",
        "  def get_state(self):\n",
        "    \"\"\" returns a board state dictionary\"\"\"\n",
        "    state = {key: getattr(self, key).copy() for key in self.state_elements}\n",
        "    return state\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.pieces[index]\n",
        "\n",
        "\n",
        "  def get_critter_food_type(self, critter_food):\n",
        "    if critter_food <= -1:\n",
        "        critter_food_type = self.CritterFoodType.FOOD\n",
        "    elif critter_food > self.forager_predator_threshold:\n",
        "        critter_food_type = self.CritterFoodType.PREDATOR\n",
        "    else:\n",
        "        critter_food_type = self.CritterFoodType.PREY\n",
        "    return critter_food_type\n",
        "\n",
        "\n",
        "  def get_type_masks(self):\n",
        "    \"\"\"\n",
        "    Returns masks indicating the position types on the board.\n",
        "    Returns:\n",
        "        tuple: Tuple containing masks for empty spaces, food, prey, and predator.\n",
        "    \"\"\"\n",
        "    empt_mask = self.pieces == 0\n",
        "    food_mask = self.pieces <= -1\n",
        "    prey_mask = (1 <= self.pieces) & (self.pieces <= self.forager_predator_threshold)\n",
        "    pred_mask = self.forager_predator_threshold < self.pieces\n",
        "    return empt_mask, food_mask, prey_mask, pred_mask\n",
        "\n",
        "\n",
        "  def get_collisions(self, moves, critter_food, critter_food_type):\n",
        "    \"\"\"\n",
        "    Determine the collision results and update scores accordingly.\n",
        "    Args:\n",
        "        moves (tuple): Tuple of arrays indicating the moves.\n",
        "        critter_food (int): Index to identify the critter or food.\n",
        "        critter_food_type (enum): Type of the critter or food\n",
        "    Returns:\n",
        "        tuple: Tuple containing move collision messages and separates out the\n",
        "        moves by where they land i.e., empty spaces, food, prey, and predator.\n",
        "    \"\"\"\n",
        "    batch_size, n_rows, n_cols = self.pieces.shape\n",
        "    move_mask = np.zeros(self.pieces.shape, dtype=bool)\n",
        "    move_mask[moves] = True\n",
        "    (empt_mask, food_mask,\n",
        "     prey_mask, pred_mask) = self.get_type_masks()\n",
        "\n",
        "    move_coll_msg = np.zeros(batch_size)\n",
        "    empt_moves = np.where(empt_mask & move_mask)\n",
        "    food_moves = np.where(food_mask & move_mask)\n",
        "    prey_moves = np.where(prey_mask & move_mask)\n",
        "    pred_moves = np.where(pred_mask & move_mask)\n",
        "    move_coll_msg[empt_moves[0]] = 1\n",
        "\n",
        "    if critter_food_type == self.CritterFoodType.PREY:\n",
        "      move_coll_msg[food_moves[0]] = 2\n",
        "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
        "      move_coll_msg[food_moves[0]] = 3\n",
        "      move_coll_msg[prey_moves[0]] = 4\n",
        "    # all collision types are blocking for food types\n",
        "\n",
        "    return (move_coll_msg, empt_moves, food_moves, prey_moves, pred_moves)\n",
        "\n",
        "\n",
        "  def update_scores(self, move_coll_msg, critter_food,\n",
        "                    critter_food_type, prey_moves):\n",
        "    if critter_food_type == self.CritterFoodType.PREY:\n",
        "      self.scores[:, critter_food-1] += (move_coll_msg == 2)\n",
        "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
        "      # predators that eat get a point\n",
        "      self.scores[:, critter_food-1] += (move_coll_msg == 4)\n",
        "      # prey that are eaten lose 10 points\n",
        "      who_eaten = self.pieces[prey_moves]\n",
        "      self.scores[prey_moves[0], who_eaten-1] -= 10\n",
        "    # food types don't get a score, it's a neuro book\n",
        "\n",
        "\n",
        "  def move_pieces(self, critter_food, move_coll_msg, moves):\n",
        "    \"\"\"\n",
        "    Move the pieces on the board based on the collision messages.\n",
        "\n",
        "    Args:\n",
        "        critter_food (int): Index to identify the critter or food.\n",
        "        move_coll_msg (np.array): Array of collision messages.\n",
        "        moves (tuple): Tuple of arrays indicating the moves.\n",
        "    \"\"\"\n",
        "    old_locs = np.where(self.pieces == critter_food)\n",
        "    vacated_old_locs = np.column_stack(old_locs)[np.where(move_coll_msg > 0)]\n",
        "    vacated_old_locs_idx = (vacated_old_locs[:,0],\n",
        "                            vacated_old_locs[:,1],\n",
        "                            vacated_old_locs[:,2])\n",
        "    self.pieces[vacated_old_locs_idx] = 0\n",
        "    new_locs = np.column_stack(moves)[np.where(move_coll_msg > 0)]\n",
        "    new_locs_idx = (new_locs[:,0], new_locs[:,1], new_locs[:,2])\n",
        "    self.pieces[new_locs_idx] = critter_food\n",
        "\n",
        "\n",
        "  def replace_destroyed(self, destroying_moves, old_pieces):\n",
        "    \"\"\"\n",
        "    Replace the destroyed pieces on the board.\n",
        "\n",
        "    Args:\n",
        "        destroying_moves (tuple): Tuple of arrays indicating the moves that\n",
        "        resulted in destruction.\n",
        "    \"\"\"\n",
        "    batch_size, n_rows, n_cols = old_pieces.shape\n",
        "    g_gone = np.zeros(batch_size)\n",
        "    g_gone[destroying_moves[0]] = 1\n",
        "    which_gone = old_pieces[destroying_moves]\n",
        "    if np.sum(g_gone) > 0:\n",
        "      num_empty_after = (n_rows*n_cols - self.num_food - self.num_critters + 1)\n",
        "      p_new_locs = np.where(np.logical_and(\n",
        "        self.pieces == 0, g_gone.reshape(batch_size, 1, 1)))\n",
        "      food_sample_ = self.rng.choice(num_empty_after, size=int(np.sum(g_gone)))\n",
        "      food_sample = food_sample_ + np.arange(int(np.sum(g_gone)))*num_empty_after\n",
        "      new_loc_vals = self.pieces[(p_new_locs[0][food_sample],\n",
        "                   p_new_locs[1][food_sample],\n",
        "                   p_new_locs[2][food_sample])]\n",
        "      # this requires that p_new_locs and destroying moves are both\n",
        "      # lexographically sorted... but they are not always\n",
        "      self.pieces[(p_new_locs[0][food_sample],\n",
        "                   p_new_locs[1][food_sample],\n",
        "                   p_new_locs[2][food_sample])] = which_gone\n",
        "\n",
        "\n",
        "  def execute_moves(self, moves, critter_food):\n",
        "    \"\"\"\n",
        "    Execute the moves on the board, handle collisions, update scores,\n",
        "    and replace destroyed/eaten pieces.\n",
        "\n",
        "    Args:\n",
        "      moves (tuple): Tuple of arrays indicating the moves.\n",
        "      critter_food (int): Index to identify the critter or food.\n",
        "    \"\"\"\n",
        "    # what type of critter is moving\n",
        "    critter_food_type = self.get_critter_food_type(critter_food)\n",
        "    # what do they land on when they move\n",
        "    (move_coll_msg, empt_moves, food_moves,\n",
        "     prey_moves, pred_moves) = self.get_collisions(\n",
        "        moves, critter_food, critter_food_type)\n",
        "    # based on what they move onto increment/decrement scores\n",
        "    self.update_scores(move_coll_msg, critter_food,\n",
        "                       critter_food_type, prey_moves)\n",
        "    # move the pieces\n",
        "    old_pieces = self.pieces.copy()\n",
        "    self.move_pieces(critter_food, move_coll_msg, moves)\n",
        "    # eaten/destroyed food and prey respawn in some variants\n",
        "    if critter_food_type == self.CritterFoodType.PREY:\n",
        "      if self.food_forager_regen:\n",
        "        self.replace_destroyed(food_moves, old_pieces)\n",
        "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
        "      if self.food_forager_regen:\n",
        "        self.replace_destroyed(food_moves, old_pieces)\n",
        "        self.replace_destroyed(prey_moves, old_pieces)\n",
        "\n",
        "    if self.food_forager_regen:\n",
        "      check_sum = np.sum(np.arange(start=-self.num_food,\n",
        "                                   stop=self.num_critters+1))\n",
        "      if np.any(np.sum(self.pieces, axis=(1,2)) != check_sum):\n",
        "        print('something went terribly wrong')\n",
        "        print(old_pieces)\n",
        "        print(critter_food)\n",
        "        print(moves)\n",
        "        print(self.pieces)\n",
        "\n",
        "\n",
        "  def get_neighbor_grc_indices(self, critter_food, radius, pad=False):\n",
        "    \"\"\"\n",
        "    Returns all grid positions within a certain cityblock distance radius from\n",
        "    the place corresponding to critter_food.\n",
        "\n",
        "    Args:\n",
        "        critter_food (int): The idex of the focal critter_food.\n",
        "        radius (int): The cityblock distance.\n",
        "        pad (bool): whether or not to pad the array, if padded all row, col\n",
        "          indexes are valid for the padded array, useful for getting percept\n",
        "          if not all indexes are correct for the original array, useful for\n",
        "          figuring out legal moves.\n",
        "\n",
        "    Returns:\n",
        "        an array of indices, each row is a g, r, c index for the neighborhoods\n",
        "        around the critters, can use the g value to know which board you are in.\n",
        "        if pad=True also returns the padded array (the indices in that case) are\n",
        "        for the padded array, so won't work on self.pieces, whereas if pad is\n",
        "        False the indices will be for the offsets in reference to the original\n",
        "        self.pieces, but note that some of these will be invalid, and will\n",
        "        need to be filtered out (as we do in get_legal)\n",
        "    \"\"\"\n",
        "    batch_size, n_rows, n_cols = self.pieces.shape\n",
        "    # Create meshgrid for offsets\n",
        "    if pad is True:\n",
        "      padded_arr = np.pad(self.pieces, ((0, 0), (radius, radius),\n",
        "        (radius, radius)), constant_values=self.ARRAY_PAD_VALUE)\n",
        "      batch, rows, cols = np.where(padded_arr == critter_food)\n",
        "    else:\n",
        "      batch, rows, cols = np.where(self.pieces == critter_food)\n",
        "    row_offsets, col_offsets = np.meshgrid(\n",
        "        np.arange(-radius, radius + 1),\n",
        "        np.arange(-radius, radius + 1),\n",
        "        indexing='ij')\n",
        "\n",
        "    # Filter for valid cityblock distances\n",
        "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
        "    valid_row_offsets = row_offsets[mask]\n",
        "    valid_col_offsets = col_offsets[mask]\n",
        "    # Extend rows and cols dimensions for broadcasting\n",
        "    extended_rows = rows[:, np.newaxis]\n",
        "    extended_cols = cols[:, np.newaxis]\n",
        "    # Compute all neighbors for each position in the batch\n",
        "    neighbors_rows = extended_rows + valid_row_offsets\n",
        "    neighbors_cols = extended_cols + valid_col_offsets\n",
        "\n",
        "    indices = np.column_stack((np.repeat(np.arange(batch_size),\n",
        "                                         neighbors_rows.shape[1]),\n",
        "                               neighbors_rows.ravel(),\n",
        "                               neighbors_cols.ravel()))\n",
        "    if pad is False:\n",
        "      return indices\n",
        "    elif pad is True:\n",
        "      return indices, padded_arr\n",
        "\n",
        "\n",
        "  def get_legal_moves(self, critter_food, radius=1):\n",
        "    \"\"\"\n",
        "    Identifies all legal moves for the critter, taking into acount which moves\n",
        "    are blocking based on type.\n",
        "\n",
        "    Returns:\n",
        "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
        "\n",
        "    Note:\n",
        "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
        "      fourth offset on the first board.\n",
        "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
        "      move on the second board and the second offset, actions are integers\n",
        "    \"\"\"\n",
        "\n",
        "    critter_locs = np.array(np.where(self.pieces == critter_food))\n",
        "    # turn those row, col offsets into a set of legal offsets\n",
        "    legal_offsets = self.get_neighbor_grc_indices(critter_food, radius)\n",
        "    legal_offsets = {tuple(m_) for m_ in legal_offsets}\n",
        "\n",
        "    # Apply logic of where a successful move can be made, by which\n",
        "    # type of critter, be they food, prey, predator or something else\n",
        "    empt_mask, food_mask, prey_mask, pred_mask = self.get_type_masks()\n",
        "    critter_food_type = self.get_critter_food_type(critter_food)\n",
        "    #print(critter_food_type)\n",
        "    if critter_food_type == self.CritterFoodType.FOOD:\n",
        "      #food only drifts into empty places\n",
        "      legal_destinations = np.where(empt_mask)\n",
        "    elif critter_food_type == self.CritterFoodType.PREY:\n",
        "      legal_destinations = np.where(empt_mask | food_mask)\n",
        "    elif critter_food_type == self.CritterFoodType.PREDATOR:\n",
        "      legal_destinations = np.where(empt_mask | food_mask | prey_mask)\n",
        "    else:\n",
        "      raise ValueError(\"Unexpected value for critter_food_type.\")\n",
        "    legal_destinations = {tuple(coords) for coords in zip(*legal_destinations)}\n",
        "    # Add the current locations of the critters to legal_destinations\n",
        "    current_locations = {tuple(loc) for loc in critter_locs.T}\n",
        "    legal_destinations = legal_destinations.union(current_locations)\n",
        "\n",
        "    # legal moves are both legal offsets and legal destinations\n",
        "    legal_moves = legal_offsets.intersection(legal_destinations)\n",
        "    return legal_moves\n",
        "\n",
        "\n",
        "  def get_legal_offsets(self, critter_food, radius):\n",
        "    \"\"\"\n",
        "    Identifies all legal offsets for a critter or food, so filter out moves\n",
        "    that are off the board, but does not filter out collisions that would be\n",
        "    blocking. For a random valid player likely better to use get_legal_moves,\n",
        "    but this is much quicker, because it doesn't check collision types, for\n",
        "    use by RL agents in training loops\n",
        "\n",
        "    Returns:\n",
        "      A numpy int array of size batch x 3(g,x,y) x 4(possible moves)\n",
        "\n",
        "    Note:\n",
        "      moves[0,1,3] is the x coordinate of the move corresponding to the\n",
        "      fourth offset on the first board.\n",
        "      moves[1,:,1] will give the g,x,y triple corresponding to the\n",
        "      move on the second board and the second offset, actions are integers\n",
        "    \"\"\"\n",
        "    batch_size, n_rows, n_cols = self.pieces.shape\n",
        "    batch, rows, cols = np.where(self.pieces == critter_food)\n",
        "    row_offsets, col_offsets = np.meshgrid(\n",
        "        np.arange(-radius, radius + 1),\n",
        "        np.arange(-radius, radius + 1),\n",
        "        indexing='ij')\n",
        "    # Filter for valid cityblock distances\n",
        "    mask = np.abs(row_offsets) + np.abs(col_offsets) <= radius\n",
        "    valid_row_offsets = row_offsets[mask]\n",
        "    valid_col_offsets = col_offsets[mask]\n",
        "    # Extend rows and cols dimensions for broadcasting\n",
        "    extended_rows = rows[:, np.newaxis]\n",
        "    extended_cols = cols[:, np.newaxis]\n",
        "    # Compute all neighbors for each position in the batch\n",
        "    potential_moves_rows = extended_rows + valid_row_offsets\n",
        "    potential_moves_cols = extended_cols + valid_col_offsets\n",
        "\n",
        "    # Filter offsets that would take the critter outside the board\n",
        "    c1 = potential_moves_rows >= 0\n",
        "    c2 = potential_moves_rows <= n_rows-1\n",
        "    c3 = potential_moves_cols >= 0\n",
        "    c4 = potential_moves_cols <= n_cols-1\n",
        "    valid_move_mask = np.logical_and.reduce([c1, c2, c3, c4])\n",
        "\n",
        "    legal_offsets_rows = potential_moves_rows[valid_move_mask]\n",
        "    legal_offsets_cols = potential_moves_cols[valid_move_mask]\n",
        "    batch_indexes = np.repeat(batch, valid_row_offsets.shape[0])\n",
        "    legal_offsets = np.column_stack((batch_indexes[valid_move_mask.ravel()],\n",
        "                                     legal_offsets_rows.ravel(),\n",
        "                                     legal_offsets_cols.ravel()))\n",
        "    return legal_offsets, valid_move_mask\n",
        "\n",
        "\n",
        "  def get_perceptions(self, critter_food, radius):\n",
        "    idx, pad_pieces = self.get_neighbor_grc_indices(critter_food,\n",
        "                                                    radius, pad=True)\n",
        "    #percept_mask = np.zeros(pad_pieces.shape, dtype=bool)\n",
        "    #percept_mask[idx[:,0], idx[:,1]], idx[:,2]] = True\n",
        "    percept = pad_pieces[idx[:,0], idx[:,1], idx[:,2]]\n",
        "    return(percept.reshape(self.batch_size, -1))\n",
        "\n",
        "\n",
        "  def execute_drift(self, offset_probs, wrapping=False):\n",
        "    \"\"\"\n",
        "    Drift the food on the board based on the given offsets probabilities.\n",
        "    Collisions handled by checking possible new locations in a random order and\n",
        "    cancelling moves that result in a collision.\n",
        "\n",
        "    Parameters:\n",
        "    - offset_probs: Probabilities corresponding to each offset, note implicit\n",
        "    order dependence here\n",
        "\n",
        "\n",
        "    Returns:\n",
        "    - nothing, just updates self.pieces\n",
        "    \"\"\"\n",
        "    # Check the length of offset_probs\n",
        "    #if len(offset_probs) != 5:\n",
        "    #    raise ValueError(\"offset_probs should be of length 5.\")\n",
        "    # Check if values are non-negative\n",
        "    #if any(p < 0 for p in offset_probs):\n",
        "    #    raise ValueError(\"All probabilities in offset_probs should be non-negative.\")\n",
        "    # Normalize the probabilities\n",
        "    #offset_probs = np.array(offset_probs) / np.sum(offset_probs)\n",
        "    # Convert offsets to a 2D numpy array\n",
        "    possible_offsets = np.array([[ 0, -1,  0], # up\n",
        "                                 [ 0,  1,  0], # down\n",
        "                                 [ 0,  0, -1], # left\n",
        "                                 [ 0,  0,  1], # right\n",
        "                                 [ 0,  0,  0]]) # still\n",
        "    batch_size, n_rows, n_cols = self.pieces.shape\n",
        "    # original food locations\n",
        "    food_locations = np.argwhere(self.pieces == -1)\n",
        "    # Sample offsets for each food location\n",
        "    num_food = food_locations.shape[0]\n",
        "    sampled_offsets = possible_offsets[self.rng.choice(\n",
        "        np.arange(possible_offsets.shape[0]),\n",
        "        size=num_food, replace=True, p=offset_probs)]\n",
        "    # Possible new food locations\n",
        "    possible_new_locations = food_locations + sampled_offsets\n",
        "    possible_wrap_row_indexes = self.rng.choice(np.arange(n_rows),\n",
        "                                                size=num_food)\n",
        "    possible_wrap_col_indexes = self.rng.choice(np.arange(n_cols),\n",
        "                                                size=num_food)\n",
        "\n",
        "    # Randomly iterate through the possible new locations\n",
        "    random_order = np.random.permutation(num_food)\n",
        "    for idx in random_order:\n",
        "      g, r, c = possible_new_locations[idx]\n",
        "      # Check if the new location is inside the boundaries of the board\n",
        "      if 0 <= r < self.pieces.shape[1] and 0 <= c < self.pieces.shape[2]:\n",
        "        # Check if the new location is empty or contains a critter\n",
        "        if self.pieces[g, r, c] == 0:\n",
        "          # Update the board\n",
        "          old_g, old_r, old_c = food_locations[idx]\n",
        "          self.pieces[g, r, c] = -1\n",
        "          self.pieces[old_g, old_r, old_c] = 0\n",
        "      elif wrapping == True:\n",
        "        # If wrapping is on then food can drift off the edge of the board and\n",
        "        # 'new' food will appear in a random loc on the opposite side\n",
        "        # Determine the opposite edge\n",
        "        if r < 0:  # Top edge\n",
        "          opposite_r = n_rows - 1\n",
        "          opposite_c = possible_wrap_col_indexes[idx]\n",
        "        elif r >= n_rows:  # Bottom edge\n",
        "          opposite_r = 0\n",
        "          opposite_c = possible_wrap_col_indexes[idx]\n",
        "        elif c < 0:  # Left edge\n",
        "          opposite_c = n_cols - 1\n",
        "          opposite_r = possible_wrap_row_indexes[idx]\n",
        "        elif c >= n_cols:  # Right edge\n",
        "          opposite_c = 0\n",
        "          opposite_r = possible_wrap_row_indexes[idx]\n",
        "\n",
        "        # Check if the opposite location is unoccupied\n",
        "        if self.pieces[g, opposite_r, opposite_c] == 0:\n",
        "          old_g, old_r, old_c = food_locations[idx]\n",
        "          self.pieces[g, opposite_r, opposite_c] = -1\n",
        "          self.pieces[old_g, old_r, old_c] = 0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = GridworldBoard()"
      ],
      "metadata": {
        "id": "zzR9wmGefV9g"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.get_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwWJH0fig9sz",
        "outputId": "36f793fd-9b58-49f4-a256-0f788fdb27b4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pieces': array([[[ 0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0],\n",
              "         [-4,  0,  0,  0,  0,  0, -7],\n",
              "         [ 0,  0,  0, -1,  0, -9,  0],\n",
              "         [ 0, -5, -8, -6,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0, -2,  0],\n",
              "         [ 0,  0,  0,  0,  0, -3,  1]],\n",
              " \n",
              "        [[ 0,  0, -2,  1,  0,  0, -8],\n",
              "         [ 0,  0, -3,  0,  0, -4, -1],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0, -9,  0],\n",
              "         [-6,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0, -7,  0,  0],\n",
              "         [ 0, -5,  0,  0,  0,  0,  0]]]),\n",
              " 'scores': array([[0.],\n",
              "        [0.]]),\n",
              " 'is_over': array([False, False]),\n",
              " 'moves_left': array([30., 30.])}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.get_legal_moves(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4EWfdrBhF9x",
        "outputId": "e4241006-ae61-4deb-a398-97ceb9fc99c4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(0, 5, 6), (0, 6, 5), (0, 6, 6), (1, 0, 2), (1, 0, 3), (1, 0, 4), (1, 1, 3)}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.execute_moves(([0,1], [5,0], [6,2]), 1)"
      ],
      "metadata": {
        "id": "lz9mptmuhe1W"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b.get_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kzn2YeKbpUts",
        "outputId": "ece43580-3a88-4af2-c589-73da169cc8f8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pieces': array([[[ 0,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0,  0,  0],\n",
              "         [-4,  0,  0,  0,  0,  0, -7],\n",
              "         [ 0,  0,  0, -1,  0, -9,  0],\n",
              "         [ 0, -5, -8, -6,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0, -2,  1],\n",
              "         [ 0,  0,  0,  0,  0, -3,  0]],\n",
              " \n",
              "        [[ 0,  0,  1,  0,  0,  0, -8],\n",
              "         [ 0,  0, -3,  0,  0, -4, -1],\n",
              "         [ 0, -2,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0,  0, -9,  0],\n",
              "         [-6,  0,  0,  0,  0,  0,  0],\n",
              "         [ 0,  0,  0,  0, -7,  0,  0],\n",
              "         [ 0, -5,  0,  0,  0,  0,  0]]]),\n",
              " 'scores': array([[0.],\n",
              "        [1.]]),\n",
              " 'is_over': array([False, False]),\n",
              " 'moves_left': array([30., 30.])}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PoB0wa_hsWcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "Vt_XX65WZpjt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "# make PatchyForageGame class locally before integrating in shared utils\n",
        "#######################################################################\n",
        "# @title PatchyForageGame class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PatchyForagingGame():\n",
        "  \"\"\"\n",
        "  A collection of methods and parameters of a patchy foraging game that allow\n",
        "  for interaction with and display of PatchyForageBoard objects.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, batch_size=1, n_rows=10, n_cols=5, num_foragers=1,\n",
        "               max_moves_taken=20,\n",
        "               end_prob = 0.05,\n",
        "               moves_cost=False,\n",
        "               food_patch_prob=0.3, food_forager_regen_prob=0.0,\n",
        "               forage_success_prob=0.6, food_extinct_prob=0.2, rng=None):\n",
        "    \"\"\"\n",
        "    Initializes an instance of the PatchyForagingGame with the specified parameters.\n",
        "    Args:\n",
        "    ... [same as in PatchyForageBoard]\n",
        "    \"\"\"\n",
        "    self.board_params = {\n",
        "      'batch_size': batch_size,\n",
        "      'n_rows': n_rows,\n",
        "      'n_cols': n_cols,\n",
        "      'num_foragers': num_foragers,\n",
        "      'max_moves_taken': max_moves_taken,\n",
        "      'end_prob': end_prob,\n",
        "      'moves_cost': moves_cost,\n",
        "      'food_patch_prob': food_patch_prob,\n",
        "      'forage_success_prob': forage_success_prob,\n",
        "      'food_extinct_prob': food_extinct_prob,\n",
        "      'food_forager_regen_prob': food_forager_regen_prob,\n",
        "      'rng': rng if rng is not None else np.random.default_rng(seed=SEED)\n",
        "    }\n",
        "\n",
        "  def get_init_board(self):\n",
        "    \"\"\"\n",
        "    Generates a starting board given the parameters of the game.\n",
        "    Returns the initial state of the game.\n",
        "    \"\"\"\n",
        "    board = PatchyForageBoard(**self.board_params)\n",
        "    return board.get_init_board_state()\n",
        "\n",
        "\n",
        "  def get_board_shape(self):\n",
        "    \"\"\"Shape of a single board, doesn't give batch size\"\"\"\n",
        "    return (self.board_params['n_rows'], self.board_params['n_cols'])\n",
        "\n",
        "  def get_action_size(self):\n",
        "    \"\"\"\n",
        "    Returns the number of all possible actions, even though only a subset\n",
        "    of these will ever be valid on a given turn.\n",
        "    Actions correspond to integer indexes of board locations,\n",
        "    moves to (batch,) row and column coordinate indexes of board locations.\n",
        "    \"\"\"\n",
        "    return self.board_params['n_rows'] * self.board_params['n_cols']\n",
        "\n",
        "  def get_batch_size(self):\n",
        "    return self.board_params['batch_size']\n",
        "\n",
        "  def get_scores(self, board):\n",
        "    return board['scores'].copy()\n",
        "\n",
        "  def get_moves_taken(self, board):\n",
        "    return board['moves_taken'].copy()\n",
        "\n",
        "  def get_square_symbol(self, piece, has_forager):\n",
        "    \"\"\"Returns the symbol representation of a board square.\"\"\"\n",
        "    if has_forager and piece < 0: return 'C'  # Critter on food patch\n",
        "    if has_forager: return 'P'  # Forager on an empty square\n",
        "    if piece == 0: return '.'  # Empty square\n",
        "    if piece < 0: return 'F'  # Food patch\n",
        "    return '?'  # Unknown piece type, for debugging\n",
        "\n",
        "  def display(self, board, g=0):\n",
        "    \"\"\"Displays the g-th game in the batch of boards.\"\"\"\n",
        "    print(\"   \", end=\"\")\n",
        "    for c_ in range(self.n_cols):\n",
        "      print(c_, end=\" \")\n",
        "    print(\"\")\n",
        "    print(\"-----------------------\")\n",
        "    for r_ in range(self.n_rows):\n",
        "      print(r_, \"|\", end=\"\")  # Print the row number\n",
        "      for c_ in range(self.n_cols):\n",
        "        piece = board['pieces'][g, r_, c_]  # Get the piece to print\n",
        "        # Check if the square is occupied by a forager\n",
        "        has_forager = False\n",
        "        for forager_num, locs in board['forager_locs'].items():\n",
        "          if g in locs[0] and r_ in locs[1] and c_ in locs[2]:\n",
        "            has_forager = True\n",
        "            break\n",
        "\n",
        "        print(self.get_square_symbol(piece, has_forager), end=\" \")\n",
        "      print(\"|\")\n",
        "    print(\"-----------------------\")\n",
        "    print(\"Foraging Attempts: \" + str(board['moves_taken'][g]))\n",
        "    print(\"Score: \" + str(board['scores'][g]))\n",
        "\n",
        "  def get_critter_rc(self, board, g, which_critter):\n",
        "    critter_locs = board['forager_locs'][which_critter]\n",
        "    return critter_locs[1][g], critter_locs[2][g]\n",
        "\n",
        "  def plot_board(self, board, g=0,\n",
        "                 fig=None, ax=None, critter_specs=None, food=None, fov=None,\n",
        "                 legend_type='included',\n",
        "                 has_fov=False, #fog_of_war field_of_view\n",
        "                 fov_opaque=False, #let human see through fog of war or not\n",
        "                 show_food=True,\n",
        "                 radius=2, figsize=(6,5), title=None,\n",
        "                 name='Critter',\n",
        "                 focal_critter_index = 0):\n",
        "    \"\"\"Uses plotting functions to make picture of the current board state\"\"\"\n",
        "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
        "    num_foragers = self.board_params['num_foragers']\n",
        "    plt.ioff()\n",
        "    if fig is None and ax is None:\n",
        "      fig, ax = make_grid(n_rows, n_cols, figsize=figsize, title=title)\n",
        "\n",
        "    # get food locs and plot them\n",
        "    rc_food_index = np.array(np.where(board['pieces'][g] <= -1))\n",
        "    rc_food_plotting = np.array(rc_food_index).T\n",
        "    if food is None:\n",
        "      food = plot_food(fig, ax, rc_food_plotting, size=550, show_food=show_food)\n",
        "    else:\n",
        "      food = plot_food(fig, ax, rc_food_plotting, food, show_food=show_food)\n",
        "\n",
        "    # generate critter plotting specs if we don't already have them\n",
        "    if critter_specs is None:\n",
        "      critter_specs = []\n",
        "      markers = ['h', 'd']  # hexagon and diamond\n",
        "      colors = sns.color_palette(\"colorblind\")\n",
        "      for i in range(num_foragers):\n",
        "        critter_name = name if num_foragers == 1 else f'{name} {i+1}'\n",
        "        spec = {'marker': markers[i % len(markers)],\n",
        "                'color': colors[i // len(markers) % len(colors)],\n",
        "                'name': critter_name,\n",
        "                'int_id': i+1}\n",
        "        critter_specs.append(spec)\n",
        "    # get critter locs and plot them\n",
        "    assert len(critter_specs) == num_foragers, \"More/fewer specs than critters\"\n",
        "    for spec in critter_specs:\n",
        "      rc_loc = np.array(self.get_critter_rc(board, g, spec['int_id'])).T\n",
        "      spec.update({'rc_loc': rc_loc})\n",
        "    critter_specs = plot_critters(fig, ax, critter_specs)\n",
        "\n",
        "    #plot field of view if doing that\n",
        "    if has_fov:\n",
        "      # plot field of view around the 'active player'\n",
        "      if fov is None:\n",
        "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
        "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque)\n",
        "      else:\n",
        "        fov = plot_fov(fig, ax, critter_specs[focal_critter_index]['rc_loc'][0],\n",
        "                       n_rows, n_cols, radius, has_fov, opaque=fov_opaque, fov=fov)\n",
        "    # make legend and draw and return figure\n",
        "    if legend_type == 'included':\n",
        "      fig.legend(loc = \"outside right upper\", markerscale=0.8)\n",
        "      fig.canvas.draw()\n",
        "      return fig, ax, critter_specs, food, fov\n",
        "    elif legend_type == 'separate':\n",
        "      fig_legend, ax_legend = plt.subplots(figsize=(1.5,1.5), layout='constrained')\n",
        "      fig_legend.get_layout_engine().set(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
        "      handles, labels = ax.get_legend_handles_labels()\n",
        "      ax_legend.legend(handles, labels, loc='center', markerscale=0.8)\n",
        "      ax_legend.axis('off')\n",
        "      fig_legend.canvas.header_visible = False\n",
        "      fig_legend.canvas.toolbar_visible = False\n",
        "      fig_legend.canvas.resizable = False\n",
        "      fig_legend.canvas.footer_visible = False\n",
        "      fig_legend.canvas.draw()\n",
        "      return fig, ax, critter_specs, food, fov, fig_legend, ax_legend\n",
        "    else: #no legend\n",
        "      fig.canvas.draw()\n",
        "      return fig, ax, critter_specs, food, fov\n",
        "\n",
        "\n",
        "  def get_legal_moves(self, board, which_critter=1, radius=1):\n",
        "    \"\"\"\n",
        "    A Helper function to get the legal moves, as a set of batch, row, col triples\n",
        "    for the given board. Does return moves that are technically legal\n",
        "    but that will result in a blocking move\n",
        "\n",
        "    Args:\n",
        "      board: a dictionary containing\n",
        "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
        "        - 'scores': The current scores of the critters.\n",
        "        - 'moves_taken': The number of foraging attempts each critter has made.\n",
        "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
        "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
        "\n",
        "      which_critter (int): value of critter we are getting the valid actions for\n",
        "      radius (int): how far, in city block distance the critter can move\n",
        "\n",
        "    Returns:\n",
        "      moves: set or tuples (g, r, c)\n",
        "    \"\"\"\n",
        "    b = PatchyForageBoard(**self.board_params)\n",
        "    b.set_state(board)\n",
        "    legal_moves =  b.get_legal_moves(which_critter, radius)\n",
        "    return legal_moves\n",
        "\n",
        "  def get_valid_actions(self, board, which_critter=1, radius=1):\n",
        "    \"\"\"\n",
        "    A Helper function to translate the g,x,y, tuples provided the\n",
        "    get_legal_moves method into valid actions, represented\n",
        "    as binary vectors of len num_actions.\n",
        "\n",
        "    Args:\n",
        "      board: a dictionary containing\n",
        "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
        "        - 'scores': The current scores of the critters.\n",
        "        - 'moves_taken': The number of foraging attempts each critter has made.\n",
        "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
        "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
        "      which_critter (int): value of critter we are getting the valid actions for\n",
        "      radius (int): how far, in city block distance the critter can move\n",
        "\n",
        "    Returns:\n",
        "      valids: np.ndarray(binary) batch_size x num_actions, 1's represent\n",
        "              valid moves\n",
        "    \"\"\"\n",
        "    legal_moves =  self.get_legal_moves(board, which_critter, radius)\n",
        "    g, r, c = zip(*legal_moves)\n",
        "    valids = np.zeros((self.batch_size, self.n_rows * self.n_cols))\n",
        "    valids[g, np.array(r) * self.n_cols + np.array(c)] = 1\n",
        "    return valids\n",
        "\n",
        "\n",
        "  def get_next_state(self, board, which_critter, actions):\n",
        "    \"\"\"\n",
        "    Helper function using GridworldBoard.execute_moves to update board state\n",
        "    given actions on a batch of boards, for a given critter\n",
        "\n",
        "    Args:\n",
        "      board: a dictionary containing\n",
        "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
        "        - 'scores': The current scores of the critters.\n",
        "        - 'moves_taken': The number of foraging attempts each critter has made.\n",
        "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
        "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
        "      which_critter: integer index of the critter type\n",
        "      actions: list of flat integer indexes of critter's new board positions\n",
        "\n",
        "    Returns:\n",
        "      a board triple signifying next state\n",
        "\n",
        "    Note:\n",
        "      if len(actions) > batch_size of board the returned board state will have\n",
        "      an expanded batch size, allowing multiple paths in the game tree to be\n",
        "      explored in parallel\n",
        "\n",
        "    \"\"\"\n",
        "    assert self.board_params['batch_size'] == len(actions)\n",
        "    b = PatchyForageBoard(**self.board_params)\n",
        "    b.set_state(board)\n",
        "    moves = self.actions_to_moves(actions)\n",
        "    b.execute_moves(moves, which_critter)\n",
        "    return b.get_state()\n",
        "\n",
        "  def actions_to_moves(self, actions):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      actions: a batch length list of integer indexes for the flattened boards,\n",
        "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
        "      as training targets for NN based RL agents.\n",
        "    Returns\n",
        "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
        "        the first array gives the specific board within the batch,\n",
        "        the second array in the tuple gives the new row coord for each critter\n",
        "        on each board and the third gives the new col coord. Note this is\n",
        "        exactly the format expected by GridworldBoard.execute_moves, and\n",
        "        is a canonical way of indexing arrays for quick numpy operations.\n",
        "    \"\"\"\n",
        "    moves = (np.arange(len(actions)),\n",
        "             np.floor_divide(actions, self.n_cols),\n",
        "             np.remainder(actions, self.n_cols))\n",
        "    return moves\n",
        "\n",
        "  def moves_to_actions(self, moves):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
        "        the first array gives the specific board within the batch,\n",
        "        the second array in the tuple gives the new row coord for each critter\n",
        "        on each board and the third gives the new col coord. Note this is\n",
        "        exactly the format expected by GridworldBoard.execute_moves, and\n",
        "        is a canonical way of indexing arrays for quick numpy operations.\n",
        "    Returns:\n",
        "      actions: a batch length list of integer indexes for the flattened boards,\n",
        "      i.e. in the range(n_cols * n_rows) actions are often much easier to use\n",
        "      as training targets for NN based RL agents.\n",
        "    \"\"\"\n",
        "    _, rows, cols = moves\n",
        "    actions = rows * self.n_cols + cols\n",
        "    return actions\n",
        "\n",
        "  def critter_oriented_get_next_state(self, board, which_critter, offsets):\n",
        "    \"\"\"\n",
        "    Translates directions in reference to the critter's location into\n",
        "    moves on the board in absolute terms, while checking for\n",
        "    bouncing/reflecting then get's the next state.\n",
        "\n",
        "    Args:\n",
        "      board: a dictionary containing\n",
        "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
        "        - 'scores': The current scores of the critters.\n",
        "        - 'moves_taken': The number of foraging attempts each critter has made.\n",
        "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
        "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
        "      which_critter: integer index of the critter type\n",
        "      offsets: batch length list of strings one 'up', 'down', 'left', 'right' 'still'\n",
        "\n",
        "    Returns:\n",
        "      a board triple signifying next state\n",
        "\n",
        "    Note:\n",
        "      Unlike get_next_state, this method does not allow for expansion\n",
        "      of the game tree, i.e. len(offsets)==batch_size required\n",
        "    \"\"\"\n",
        "    assert len(offsets) == self.board_params['batch_size']\n",
        "    b = PatchyForageBoard(**self.board_params)\n",
        "    b.set_state(board)\n",
        "    moves = self.critter_direction_to_move(board, offsets, which_critter)\n",
        "    b.execute_moves(moves, which_critter)\n",
        "    return(b.get_state())\n",
        "\n",
        "  def critter_direction_to_move(self, board, offsets, critter):\n",
        "    \"\"\"\n",
        "    Translates directions in reference to the critter's location into\n",
        "    moves on the board in absolute terms, while checking for\n",
        "    bouncing/reflecting then returns moves. Doesn't check for collisions with\n",
        "    other critters though. In general player's move methods should be checking\n",
        "    valid moves and only making legal ones.\n",
        "\n",
        "    Args:\n",
        "      board: a dictionary containing\n",
        "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
        "        - 'scores': The current scores of the critters.\n",
        "        - 'moves_taken': The number of foraging attempts each critter has made.\n",
        "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
        "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
        "      which_critter: integer index of the critter type\n",
        "      offsets: batch length list of strings,\n",
        "        one of 'up', 'down', 'left', 'right', 'still'\n",
        "\n",
        "    Returns:\n",
        "      moves: a 3-tuple of 1-d arrays each of length batch_size,\n",
        "        the first array gives the specific board within the batch,\n",
        "        the second array in the tuple gives the new row coord for each critter\n",
        "        on each board and the third gives the new col coord. Note this is\n",
        "        exactly the format expected by GridworldBoard.execute_moves, and\n",
        "        is a canonical way of indexing arrays for numpy.\n",
        "    \"\"\"\n",
        "    assert len(offsets) == board['pieces'].shape[0]\n",
        "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
        "    offset_dict = {'up': (0, -1, 0),\n",
        "                   'down': (0, 1, 0),\n",
        "                   'left': (0, 0, -1),\n",
        "                   'right': (0, 0, 1),\n",
        "                   'still': (0, 0, 0)}\n",
        "    this_critter_locs = board['forager_locs'][critter]\n",
        "    all_critter_locs = np.where(board['pieces'] >= 1)\n",
        "    offsets_array = np.hstack([np.array(offset_dict[ost_]).reshape((3,1))\n",
        "                           for ost_ in offsets])\n",
        "    new_locs = np.array(this_critter_locs) + offsets_array\n",
        "    #check bounces at boundaries\n",
        "    new_locs[1,:] = np.where(new_locs[1] >=\n",
        "                               n_rows, n_rows-2, new_locs[1])\n",
        "    new_locs[2,:] = np.where(new_locs[2,:] >=\n",
        "                               n_cols, n_cols-2, new_locs[2,:])\n",
        "    new_locs[1,:] = np.where(new_locs[1,:] < 0, 1, new_locs[1,:])\n",
        "    new_locs[2,:] = np.where(new_locs[2,:] < 0, 1, new_locs[2,:])\n",
        "    moves = tuple(new_locs)\n",
        "    return moves\n",
        "\n",
        "  def critter_directions_to_actions(self, board, directions, critter):\n",
        "    \"\"\"\n",
        "    Converts a list of direction strings to a list of action indices for the given board state and critter.\n",
        "\n",
        "    Args:\n",
        "      board (dict): The current state of the game.\n",
        "      directions (list of str): List of directions, where each direction is one of 'up', 'down', 'left', 'right', 'still'.\n",
        "      critter (int): The critter index.\n",
        "\n",
        "    Returns:\n",
        "      list of int: List of action indices corresponding to the directions.\n",
        "    \"\"\"\n",
        "    # Ensure the length of directions matches the batch size\n",
        "    assert len(directions) == board['pieces'].shape[0], \"Mismatch between directions length and batch size\"\n",
        "\n",
        "    # Convert directions to moves\n",
        "    moves = self.critter_direction_to_move(board, directions, critter)\n",
        "\n",
        "    # Convert moves to actions\n",
        "    actions = self.moves_to_actions(moves)\n",
        "\n",
        "    return actions\n",
        "\n",
        "\n",
        "  def get_valid_directions(self, board, which_critter):\n",
        "    \"\"\"\n",
        "    Transforms output of get_valid_actions to a list of the valid directions\n",
        "    for each board in the batch for a given critter.\n",
        "    \"\"\"\n",
        "    offset_dict = {( 0, 1): 'right',\n",
        "                   ( 0,-1): 'left',\n",
        "                   ( 1, 0): 'down',\n",
        "                   (-1, 0): 'up',\n",
        "                   ( 0, 0): 'still'}\n",
        "    batch_size, n_rows, n_cols = board['pieces'].shape\n",
        "    valid_actions = self.get_valid_actions(board, which_critter)\n",
        "    if batch_size != len(valid_actions):\n",
        "      raise ValueError(\"Need Exactly one set of valid actions per board in batch\")\n",
        "    critter_locs = board['forager_locs'][which_critter]\n",
        "    valid_directions = []\n",
        "    for g, batch_valid in enumerate(valid_actions):\n",
        "      valid_int_indices = np.where(batch_valid==1)[0]\n",
        "      critter_loc = np.array([[critter_locs[1][g],critter_locs[2][g]]])\n",
        "      # critter_loc shape is (1, 2)\n",
        "      moves = np.column_stack([valid_int_indices // n_cols, valid_int_indices % n_cols])\n",
        "      offsets = moves - critter_loc\n",
        "      batch_valid_directions = [offset_dict[tuple(offset)] for offset in offsets]\n",
        "      valid_directions.append(batch_valid_directions)\n",
        "    return valid_directions\n",
        "\n",
        "\n",
        "  def get_perceptions(self, board, radius, which_critter):\n",
        "    b = PatchyForageBoard(**self.board_params)\n",
        "    b.set_state(board)\n",
        "    return(b.get_perceptions(radius, which_critter))\n",
        "\n",
        "\n",
        "  def play_game(self, players=[], visualize = False):\n",
        "    \"\"\"This method takes a list of players the same length as num_foragers,\n",
        "        and then plays a batch of games with them and returns the final board\n",
        "        states of each game\"\"\"\n",
        "    if len(players) != self.num_foragers:\n",
        "      raise ValueError(\"number of players different than expected\")\n",
        "\n",
        "    board = self.get_init_board()\n",
        "    if visualize == True:\n",
        "      self.display(board, 0)\n",
        "\n",
        "    for p_idx, player_ in enumerate(players):\n",
        "      if player_.critter_index != p_idx+1:\n",
        "        print(player_.critter_index)\n",
        "        print(p_idx + 1)\n",
        "        raise ValueError(\"player order does not match assigned critter index\")\n",
        "\n",
        "    while np.any(board['is_over'] == False):\n",
        "      for player_ in players:\n",
        "        old_scores = board['scores']\n",
        "        if player_.return_direction:\n",
        "          directions = player_.play(board)\n",
        "          a_player = self.critter_directions_to_actions(board, directions, player_.critter_index)\n",
        "        else: # player returns actions directly\n",
        "          a_player, _, _ = player_.play(board)\n",
        "        board = self.get_next_state(board, player_.critter_index, a_player)\n",
        "        if visualize == True:\n",
        "          self.display(board, 0)\n",
        "    return board\n",
        "\n",
        "\n",
        "  def plot_visualizations(board):\n",
        "    # Extracting scores and moves_taken for all batches\n",
        "    scores = board['scores']\n",
        "    moves_taken = board['moves_taken']\n",
        "\n",
        "    # Calculating average scores per round for each batch\n",
        "    avg_scores_per_round = scores / moves_taken\n",
        "\n",
        "    # Histogram of Average Score Per Round\n",
        "    plt.figure()\n",
        "    plt.hist(avg_scores_per_round, bins=30, edgecolor='black')\n",
        "    plt.xlabel('Average Score Per Round')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of Average Score Per Round')\n",
        "    plt.show()\n",
        "\n",
        "    # Scatter Plot of Averages vs. Foraging Attempts\n",
        "    plt.figure()\n",
        "    plt.scatter(moves_taken, avg_scores_per_round, c='blue', alpha=0.5)\n",
        "    plt.xlabel('Foraging Attempts')\n",
        "    plt.ylabel('Average Score Per Round')\n",
        "    plt.title('Scatter Plot of Average Scores vs. Foraging Attempts')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "# make InteractivePatchyForage class locally before integrating in shared utils\n",
        "#######################################################################\n",
        "# @title Interactive Patchy Foraging\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InteractivePatchyForage():\n",
        "  \"\"\"\n",
        "  A widget based object for interacting with a gridworld game\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, PatchyForage_game, init_board=None, has_fov=False,\n",
        "               radius=2, fov_opaque=False, show_food=True, show_misses=False,\n",
        "               figsize=(6,5), critter_names=['Critter'], players=['human']):\n",
        "    \"\"\"\n",
        "    Initializes a widget based object for interacting with a patchy foraging game\n",
        "\n",
        "    Args:\n",
        "      gridworld_game: an instance of PatchyForageGame object\n",
        "        expects this to have batchsize 1\n",
        "      init_board: (optional) a dictionary containing\n",
        "        - 'pieces': Current food patch locations as a batch x row x col numpy array.\n",
        "        - 'scores': The current scores of the critters.\n",
        "        - 'moves_taken': The number of foraging attempts each critter has made.\n",
        "        - 'is_over': Flags indicating if the game is over for each board in the batch.\n",
        "        - 'forager_locs': dictionary of current locations of the foragers on the board.\n",
        "      has_fov: bool, whether or not to display fog of war around the critter\n",
        "      radius: int, number of squares the critter can \"see\" around it\n",
        "      figsize: tuple (int, int), size of the figure\n",
        "      critter_names: a list of strings that determines what the critter is called\n",
        "        in the plot legend, order should align with players\n",
        "      player: a list of either 'human', None, or a player object with a play\n",
        "        method and a critter_index attribute. If 'human' use buttons,  if None\n",
        "        default to making a RandomValidPlayer object, otherwise use the\n",
        "        player class provided to make the player objects and use a start button.\n",
        "        The list needs to be as long as the PatchyForage_game.num_foragers\n",
        "        attribute. Order should align with critter_name.\n",
        "\n",
        "      Note: fov only turns on for the 'active' player.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set GridworldGame object and initialize the board state\n",
        "    self.pfg = PatchyForage_game\n",
        "    self.num_foragers = self.pfg.board_params['num_foragers']\n",
        "    self.moves_cost = self.pfg.board_params['moves_cost']\n",
        "    self.has_fov = has_fov\n",
        "    self.radius = radius\n",
        "    self.fov_opaque = fov_opaque\n",
        "    self.show_food = show_food\n",
        "    self.percept_len = 2*self.radius*(self.radius+1)\n",
        "    self.figsize = figsize\n",
        "    # initialize players and plotting specs together to ensure alignment\n",
        "    self.players = []\n",
        "    self.any_human_players = False\n",
        "    self.active_player_index = 0\n",
        "    self.crit_specs = []\n",
        "    markers = ['h', 'd']  # hexagon and diamond\n",
        "    colors = sns.color_palette(\"colorblind\")\n",
        "    for i in range(self.num_foragers):\n",
        "      spec = {'marker': markers[i % len(markers)],\n",
        "              'color': colors[i // len(markers) % len(colors)],\n",
        "              'name': critter_names[i],\n",
        "              'int_id': i+1}\n",
        "      self.crit_specs.append(spec)\n",
        "      player = players[i] #implicit check that players is at least long enough\n",
        "      if player is None:\n",
        "        self.players.append(RandomValidPlayer(self.gwg, critter_index=i+1))\n",
        "      elif player == 'human':\n",
        "        self.players.append('human')\n",
        "        # right now only ever have on human player with index 1\n",
        "        self.any_human_players = True\n",
        "      else:\n",
        "        # player objects expected to have a critter_index attribute\n",
        "        # we set it appropriately here so it aligns with the players list\n",
        "        # used to create the widget\n",
        "        player.critter_index = i+1\n",
        "        self.players.append(player)\n",
        "    self.final_scores = []\n",
        "    # Initialize the sidebar for displaying misses if needed\n",
        "    self.show_misses = show_misses\n",
        "    if self.show_misses:\n",
        "      self.misses_sidebar = widgets.Output(layout=widgets.Layout(\n",
        "          min_width='12.5em', max_width='18.8em',\n",
        "          min_height='6.3em', overflow='auto'))\n",
        "      self.misses_new_patch = [0] * self.num_foragers\n",
        "      self.misses_known_patch = ['--'] * self.num_foragers\n",
        "      self.at_new_patch = [True] * self.num_foragers\n",
        "\n",
        "    if init_board is None:\n",
        "      self.board_state = self.pfg.get_init_board()\n",
        "    else:\n",
        "      self.board_state = init_board\n",
        "    # Initialize widgets and buttons\n",
        "    self.output = widgets.Output(layout=widgets.Layout(\n",
        "      width = '20.0em', min_width='20.0em', max_width='21.0em',\n",
        "      min_height='10.0em', overflow='auto'))\n",
        "    self.scoreboard = widgets.Output(layout=widgets.Layout(\n",
        "      min_width='12.5em', max_width='18.8em',\n",
        "      min_height='6.3em', overflow='auto'))\n",
        "    self.up_button = widgets.Button(description=\"Up\",\n",
        "      layout=widgets.Layout(width='6.3em'))\n",
        "    self.down_button = widgets.Button(description=\"Down\",\n",
        "      layout=widgets.Layout(width='6.3em'))\n",
        "    self.left_button = widgets.Button(description=\"Left\",\n",
        "      layout=widgets.Layout(width='6.3em'))\n",
        "    self.right_button = widgets.Button(description=\"Right\",\n",
        "      layout=widgets.Layout(width='6.3em'))\n",
        "    self.forage_button = widgets.Button(description=\"Forage\",\n",
        "      layout=widgets.Layout(width='6.3em'))\n",
        "    self.start_button = widgets.Button(description=\"Start\",\n",
        "      layout=widgets.Layout(width='6.3em'))\n",
        "    self.empty_space = widgets.Box(layout=widgets.Layout(height='2.5em'))\n",
        "\n",
        "    # get plot canvas widgets and other plotting objects\n",
        "    plt.ioff()\n",
        "    if len(self.players) > 1:\n",
        "      self.legend_type=None # don't keep regenerating the legend\n",
        "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov,\n",
        "       self.b_fig_legend, self.b_ax_legend) = self.pfg.plot_board(\n",
        "          self.board_state, g=0, critter_specs=self.crit_specs,\n",
        "          has_fov=self.has_fov, legend_type='separate',\n",
        "          radius=self.radius, fov_opaque=self.fov_opaque, figsize=self.figsize,\n",
        "          show_food=self.show_food)\n",
        "    else:\n",
        "      self.legend_type = 'included'\n",
        "      (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
        "        ) = self.pfg.plot_board(self.board_state, g=0,\n",
        "                                critter_specs=self.crit_specs,\n",
        "                                has_fov=self.has_fov,\n",
        "                                fov_opaque=self.fov_opaque,\n",
        "                                show_food=self.show_food,\n",
        "                                radius=self.radius, figsize=self.figsize)\n",
        "    # lump buttons together\n",
        "    self.buttons = widgets.HBox([widgets.VBox([self.forage_button, self.left_button]),\n",
        "                                 widgets.VBox([self.up_button, self.down_button]),\n",
        "                                 widgets.VBox([self.empty_space, self.right_button])])\n",
        "    # automatically pick different layouts for different situations\n",
        "    if self.any_human_players:\n",
        "      self.board_and_buttons = widgets.VBox([self.b_fig.canvas,\n",
        "                                             self.buttons])\n",
        "      if len(self.players) == 1:\n",
        "        #one human player\n",
        "        self.output_and_score = widgets.VBox([self.scoreboard, self.output])\n",
        "        if self.show_misses:\n",
        "            self.final_display = widgets.HBox([self.board_and_buttons,\n",
        "                widgets.VBox([self.misses_sidebar, self.output_and_score])])\n",
        "        else:\n",
        "            self.final_display = widgets.VBox([self.board_and_buttons,\n",
        "                                               self.output_and_score])\n",
        "      else:\n",
        "        # more than one player, one of them human\n",
        "        self.V_board_output= widgets.VBox([self.board_and_buttons,\n",
        "                                             self.output])\n",
        "        self.V_scoreboard_start_legend = widgets.VBox([\n",
        "        self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
        "        if self.show_misses:\n",
        "          self.final_display = widgets.HBox([self.V_board_output,\n",
        "                                             self.V_scoreboard_start_legend,\n",
        "                                             self.misses_sidebar])\n",
        "        else:\n",
        "          self.final_display = widgets.HBox([self.V_board_output,\n",
        "                                             self.V_scoreboard_start_legend])\n",
        "    else: # all players are ai\n",
        "      if len(self.players) == 1:\n",
        "        # one ai player\n",
        "        if self.show_misses:\n",
        "          self.final_display = widgets.HBox(\n",
        "              [widgets.VBox([self.b_fig.canvas, self.scoreboard]),\n",
        "               widgets.VBox([self.misses_sidebar, self.output,\n",
        "                             self.start_button])])\n",
        "        else:\n",
        "          self.H_score_output_start = widgets.HBox([\n",
        "            self.scoreboard, self.output, self.start_button])\n",
        "          self.final_display = self.HBox(\n",
        "              [widgets.VBox([self.b_fig.canvas, self.H_score_output_start])])\n",
        "      else:\n",
        "        # more than one ai player\n",
        "        self.V_board_output = widgets.VBox([self.b_fig.canvas, self.output])\n",
        "        self.V_scoreboard_start_legend = widgets.VBox([\n",
        "          self.scoreboard, self.start_button, self.b_fig_legend.canvas])\n",
        "        if self.show_misses:\n",
        "          self.final_display = widgets.HBox([self.V_board_output,\n",
        "                                             self.V_scoreboard_start_legend,\n",
        "                                             self.misses_sidebar])\n",
        "        else:\n",
        "          self.final_display = widgets.HBox([self.V_board_output,\n",
        "                                             self.V_scoreboard_start_legend])\n",
        "    # initialize text outputs\n",
        "    with self.scoreboard:\n",
        "      table = [['Best Eating Rate:'] + ['--'] * self.num_foragers,\n",
        "               ['Last Eating Rate:'] + ['--'] * self.num_foragers,\n",
        "               ['Average Eating Rate:'] + ['--'] * self.num_foragers,]\n",
        "      if len(self.players) > 1:\n",
        "        headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
        "        print(tabulate(table, headers=headers))\n",
        "      else: # len(self.players) == 1\n",
        "        print(tabulate(table))\n",
        "    with self.output:\n",
        "      if self.any_human_players:\n",
        "        print('Click a button to start playing')\n",
        "      else:\n",
        "        print('Click the start button to run the simulation')\n",
        "    # If show_misses is enabled, initialize the misses_sidebar content\n",
        "    if self.show_misses:\n",
        "      with self.misses_sidebar:\n",
        "        table = [['Misses (New Patch):'] + ['0'] * self.num_foragers,\n",
        "                 ['Misses (Known Patch):'] + ['--'] * self.num_foragers]\n",
        "        if len(self.players) > 1:\n",
        "          headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
        "          print(tabulate(table, headers=headers))\n",
        "        else: # len(self.players) == 1\n",
        "            print(tabulate(table))\n",
        "\n",
        "    # Connect the buttons to functions that do something\n",
        "    self.up_button.on_click(self.on_up_button_clicked)\n",
        "    self.down_button.on_click(self.on_down_button_clicked)\n",
        "    self.left_button.on_click(self.on_left_button_clicked)\n",
        "    self.right_button.on_click(self.on_right_button_clicked)\n",
        "    self.forage_button.on_click(self.on_forage_button_clicked)\n",
        "    self.start_button.on_click(self.on_start_button_clicked)\n",
        "\n",
        "\n",
        "  def update_state_based_on_move(self, direction):\n",
        "    old_board = self.board_state.copy()\n",
        "    # index of players is 0 through num_critter-1,\n",
        "    # same player represented by value of index + 1 in\n",
        "    if (isinstance(self.players[self.active_player_index], str) and\n",
        "        'human' in self.players[self.active_player_index]):\n",
        "      direction = direction\n",
        "    else:\n",
        "      if self.players[self.active_player_index].return_direction:\n",
        "        directions = self.players[self.active_player_index].play(old_board)\n",
        "      else:\n",
        "        a_player, _, _ = self.players[self.active_player_index].play(old_board)\n",
        "        # print(a_player)\n",
        "        directions = self.pfg.action_to_critter_direction(old_board,\n",
        "                                                        self.active_player_index+1,\n",
        "                                                        a_player)\n",
        "      # but we only want to apply their move to the appropriate board\n",
        "      direction = directions[0]\n",
        "    self.board_state = self.pfg.critter_oriented_get_next_state(\n",
        "          self.board_state, self.active_player_index+1, [direction])\n",
        "    return direction\n",
        "\n",
        "\n",
        "  def update_output_and_scores(self, direction, old_board):\n",
        "    old_scores = old_board['scores'][0]\n",
        "    old_row, old_col = self.pfg.get_critter_rc(old_board, 0,\n",
        "                                               self.active_player_index+1)\n",
        "    new_scores = self.board_state['scores'][0] #first batch first critter type\n",
        "    moves_taken = self.board_state['moves_taken'][0]\n",
        "    row, col = self.pfg.get_critter_rc(self.board_state, 0,\n",
        "                                       self.active_player_index+1)\n",
        "\n",
        "    did_eat = False\n",
        "    # Check if the forager moved or tried to forage and what happened\n",
        "    if (row, col) != (old_row, old_col):\n",
        "      # Moved to a new patch\n",
        "      self.misses_new_patch[self.active_player_index] = 0\n",
        "      self.misses_known_patch[self.active_player_index] = '--'\n",
        "      self.at_new_patch[self.active_player_index] = True\n",
        "      action_string = \"tried to move \" + direction + \" to ({}, {})\".format(row, col)\n",
        "      eating_string = \"They were too busy moving to look for food.\"\n",
        "    elif (row, col) == (old_row, old_col):\n",
        "      # they didn't move, tried to forage\n",
        "      action_string = \"tried to forage.\"\n",
        "      if new_scores[self.active_player_index] > old_scores[self.active_player_index]:\n",
        "        # They found food\n",
        "        eating_string = \"They found some food at the patch!\"\n",
        "        did_eat = True\n",
        "        if self.at_new_patch[self.active_player_index]:\n",
        "          # They found food at a new patch\n",
        "          self.misses_new_patch[self.active_player_index] = '--'\n",
        "          self.misses_known_patch[self.active_player_index] = 0\n",
        "          self.at_new_patch[self.active_player_index] = False\n",
        "        else:\n",
        "          # They found food at a known patch\n",
        "          # Reset count as they found food\n",
        "          self.misses_known_patch[self.active_player_index] = 0\n",
        "      else:\n",
        "        # They didn't find food\n",
        "        eating_string = \"They didn't find any food at the patch.\"\n",
        "        if self.at_new_patch[self.active_player_index]:\n",
        "          # They are at a new patch\n",
        "          self.misses_new_patch[self.active_player_index] += 1\n",
        "        else:\n",
        "          # They are at a known patch\n",
        "          self.misses_known_patch[self.active_player_index] += 1\n",
        "\n",
        "    #make the picture of the new board position\n",
        "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
        "     ) = self.pfg.plot_board(self.board_state, g=0,\n",
        "                             fig=self.b_fig, ax=self.b_ax,\n",
        "                             critter_specs=self.b_crit_specs, food=self.b_food,\n",
        "                             fov=self.b_fov, has_fov=self.has_fov,\n",
        "                             fov_opaque=self.fov_opaque,\n",
        "                             show_food=self.show_food,\n",
        "                             radius=self.radius, legend_type=self.legend_type)\n",
        "    with self.output:\n",
        "      clear_output()\n",
        "      if len(self.players) == 1:\n",
        "        print(\"The critter {}\".format(action_string))\n",
        "        print(eating_string)\n",
        "        moves_taken_count = moves_taken[self.active_player_index]\n",
        "        new_score = new_scores[self.active_player_index]\n",
        "        food_per_attempt = \"-\" if moves_taken_count == 0 else \"{:.2f}\".format(new_score / moves_taken_count)\n",
        "        if self.moves_cost:\n",
        "          leading_string1 = \"Moves Taken\"\n",
        "          mid_string1 = \"Move\"\n",
        "          leading_string2 = \"Moves Left\"\n",
        "        else:\n",
        "          leading_string1 = \"Foraging Attempts\"\n",
        "          mid_string1 = \"Attempt\"\n",
        "          leading_string2 = \"Foraging Attempts Left\"\n",
        "\n",
        "        print(f\"{leading_string1}: {moves_taken_count}\\n\"\n",
        "              f\"Food Eaten: {new_score}\\n\"\n",
        "              f\"Food Per {mid_string1}: {food_per_attempt}\")\n",
        "\n",
        "        print(f\"{leading_string2}: \"\n",
        "              f\"{self.pfg.board_params['max_moves_taken'] - moves_taken_count}\")\n",
        "      else:  # more than one player\n",
        "        print(f\"Critter {self.active_player_index + 1} {action_string}\")\n",
        "        print(eating_string)\n",
        "        # Assuming moves_taken and new_scores are aggregated lists; adjust as necessary.\n",
        "        print(f\"{leading_string1}: {moves_taken}\\nFood Eaten: {new_scores}\")\n",
        "\n",
        "\n",
        "    if self.show_misses:\n",
        "      with self.misses_sidebar:\n",
        "        clear_output()\n",
        "        table = [['Misses (New Patch):'] + [str(miss) for miss in self.misses_new_patch],\n",
        "                 ['Misses (Known Patch):'] + [str(miss) for miss in self.misses_known_patch]]\n",
        "        if len(self.players) > 1:\n",
        "          headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
        "          print(tabulate(table, headers=headers))\n",
        "        else: # len(self.players) == 1\n",
        "            print(tabulate(table))\n",
        "\n",
        "\n",
        "  def handle_game_end(self):\n",
        "    \"\"\"Handle the logic when the game is over.\"\"\"\n",
        "    self.final_scores.append(self.board_state['scores'][0] / self.board_state['moves_taken'][0])\n",
        "    self.board_state = self.pfg.get_init_board()\n",
        "    for player in self.players:\n",
        "      if hasattr(player, 'last_direction'):\n",
        "        player.last_direction = ['right'] * self.pfg.board_params['batch_size']\n",
        "    if self.show_misses:\n",
        "      self.misses_new_patch = [0] * self.num_foragers\n",
        "      self.misses_known_patch = ['--'] * self.num_foragers\n",
        "      self.at_new_patch = [True] * self.num_foragers\n",
        "    with self.output:\n",
        "      clear_output()\n",
        "      print('Game Over. Final Food per Attempt {}'.format(self.final_scores[-1]))\n",
        "      print('Resetting the board for another game')\n",
        "    (self.b_fig, self.b_ax, self.b_crit_specs, self.b_food, self.b_fov\n",
        "     ) = self.pfg.plot_board(self.board_state, 0, self.b_fig, self.b_ax,\n",
        "                             self.b_crit_specs, self.b_food, self.b_fov,\n",
        "                             has_fov=self.has_fov, radius=self.radius,\n",
        "                             fov_opaque=self.fov_opaque,\n",
        "                             show_food=self.show_food,\n",
        "                             legend_type=self.legend_type)\n",
        "    with self.scoreboard:\n",
        "      clear_output()\n",
        "      print('Games Played: ' + str(len(self.final_scores)))\n",
        "      if len(self.players) == 1:\n",
        "        if len(self.final_scores) > 0:\n",
        "          table = [\n",
        "            ['Best Eating Rate: ', '{:.2f}'.format(np.max(np.array(self.final_scores)))],\n",
        "            ['Last Eating Rate: ', '{:.2f}'.format(self.final_scores[-1][0])],\n",
        "            ['Average Eating Rate', '{:.2f}'.format(np.mean(np.array(self.final_scores)))]]\n",
        "        else:\n",
        "          table = [['Best Eating Rate:', '--'],\n",
        "                   ['Last Eating Rate:', '--'],\n",
        "                   ['Average Eating Rate:', '--']]\n",
        "        print(tabulate(table))\n",
        "      else: # len(self.players) > 1\n",
        "        headers = [''] + [f'P{i+1}' for i in range(self.num_foragers)]\n",
        "        if len(self.final_scores) > 0:\n",
        "          table = []\n",
        "          # Assuming the batch size is 1 for now\n",
        "          current_scores = self.final_scores[-1]\n",
        "          max_scores = np.max(np.array(self.final_scores), axis=0)\n",
        "          average_scores = np.mean(np.array(self.final_scores), axis=0)\n",
        "          table.append(['Besat Rates:'] +\n",
        "          [str(score) for score in max_scores])\n",
        "          table.append(['Last Rates:'] +\n",
        "            [str(score) for score in current_scores])\n",
        "          table.append(['Average Rates:'] +\n",
        "              ['{:.2f}'.format(score) for score in average_scores])\n",
        "        else:\n",
        "          table = [\n",
        "            ['High Score:'] + ['--'] * self.num_foragers,\n",
        "            ['Last Score:'] + ['--'] * self.num_foragers,\n",
        "            ['Average Score:'] + ['--'] * self.num_foragers,]\n",
        "        print(tabulate(table, headers=headers))\n",
        "\n",
        "  def disable_direction_buttons(self):\n",
        "    self.up_button.disabled = True\n",
        "    self.down_button.disabled = True\n",
        "    self.left_button.disabled = True\n",
        "    self.right_button.disabled = True\n",
        "    self.forage_button.disabled = True\n",
        "\n",
        "  def enable_direction_buttons(self):\n",
        "    self.up_button.disabled = False\n",
        "    self.down_button.disabled = False\n",
        "    self.left_button.disabled = False\n",
        "    self.right_button.disabled = False\n",
        "    self.forage_button.disabled = False\n",
        "\n",
        "  def on_up_button_clicked(self, *args):\n",
        "    self.on_direction_button_click('up')\n",
        "\n",
        "  def on_down_button_clicked(self, *args):\n",
        "    self.on_direction_button_click('down')\n",
        "\n",
        "  def on_left_button_clicked(self, *args):\n",
        "    self.on_direction_button_click('left')\n",
        "\n",
        "  def on_right_button_clicked(self, *args):\n",
        "    self.on_direction_button_click('right')\n",
        "\n",
        "  def on_forage_button_clicked(self, *args):\n",
        "    self.on_direction_button_click('still')\n",
        "\n",
        "  def execute_moves(self, human_direction=None):\n",
        "    ai_direction = None\n",
        "    while not self.board_state['is_over'][0]:\n",
        "      old_board = self.board_state.copy()\n",
        "      # Check if the current player is human\n",
        "      if self.players[self.active_player_index] == 'human':\n",
        "        if human_direction is None:\n",
        "          # If the human direction is not provided,\n",
        "          # it means the human has not yet made a move\n",
        "          # Break out and wait for one\n",
        "          break\n",
        "        else:\n",
        "          # The human made a move, so execute it and reset the human_direction\n",
        "          self.update_state_based_on_move(human_direction)\n",
        "          self.update_output_and_scores(human_direction, old_board)\n",
        "          human_direction = None  # Reset for next loop iteration\n",
        "      else:\n",
        "        # AI player\n",
        "        ai_direction = self.update_state_based_on_move('tbd')\n",
        "        self.update_output_and_scores(ai_direction, old_board)\n",
        "\n",
        "      # Move to the next player\n",
        "      self.active_player_index = (self.active_player_index + 1) % len(self.players)\n",
        "\n",
        "  def on_direction_button_click(self, direction):\n",
        "    self.disable_direction_buttons()  # Disable buttons, no double clicks\n",
        "    self.execute_moves(human_direction=direction)\n",
        "    if self.board_state['is_over'][0]:\n",
        "        self.handle_game_end()\n",
        "    self.enable_direction_buttons()  # Re-enable buttons\n",
        "\n",
        "  def on_start_button_clicked(self, *args):\n",
        "    self.start_button.disabled = True\n",
        "    self.execute_moves()\n",
        "    if self.board_state['is_over'][0]:\n",
        "        self.handle_game_end()\n",
        "    self.start_button.disabled = False\n",
        "\n",
        "\n",
        "\n",
        "# @title plotting functions\n",
        "#################################################\n",
        "# More plotting functions\n",
        "#################################################\n",
        "\n",
        "\n",
        "def plot_directions(fig, ax, loc_prob_dict, critter, deterministic=False,\n",
        "                    name=None):\n",
        "  \"\"\"\n",
        "  Plot vector field indicating critter direction probabilities.\n",
        "\n",
        "  Args:\n",
        "    fig, ax (matplotlib objects): Figure and axes objects for plotting.\n",
        "    loc_prob_dict (dict): Dictionary with keys as (row, col) location tuples\n",
        "      and values as lists of direction probabilities corresponding to the\n",
        "      directions ['right', 'down', 'left', 'up'].\n",
        "    critter (int): Identifier for which critter directions are associated with.\n",
        "    deterministic (bool, optional): If True, the probabilities array is\n",
        "      converted to 1-hot, and the arrows are plotted at the center of the cell\n",
        "      and are larger. Defaults to False.\n",
        "  \"\"\"\n",
        "\n",
        "  #looks like direction ignores inverted axis\n",
        "  direction_vectors = {'right': (1, 0), 'down': (0, -1),\n",
        "                       'left': (-1, 0), 'up': (0, 1)}\n",
        "  # but offsets need to be aware of inverted\n",
        "  direction_offsets = {'right': (0.1, 0), 'down': (0, 0.1),\n",
        "                       'left': (-0.1, 0), 'up': (0, -0.1)}\n",
        "  # Offsets for each critter type 1 and 2 to be used together, 0 by itself\n",
        "  critter_offsets = {0: (0, 0), 1: (-0.05, -0.05), 2: (0.05, 0.05)}\n",
        "  # same logic for colors\n",
        "  critter_colors = {0: 'black', 1: 'red', 2: 'blue'}\n",
        "  # Get the offset and color for this critter\n",
        "  critter_offset = critter_offsets[critter]\n",
        "  critter_color = critter_colors[critter]\n",
        "\n",
        "  # Add legend only if critter is not 0\n",
        "  custom_leg_handles = []\n",
        "  if critter != 0:\n",
        "    if name is None:\n",
        "      name = f'Critter {critter}'\n",
        "    legend_patch = mpatches.Patch(color=critter_color, label=name)\n",
        "    # Add the legend for this critter\n",
        "    custom_leg_handles.append(legend_patch)\n",
        "\n",
        "  C, R, U, V, A = [], [], [], [], []\n",
        "\n",
        "  for loc in loc_prob_dict.keys():\n",
        "    row, col = loc\n",
        "    probs = loc_prob_dict[loc]\n",
        "    for dir_key, prob in probs.items():\n",
        "      C.append(col + critter_offset[0] + direction_offsets[dir_key][0])\n",
        "      R.append(row + critter_offset[1] + direction_offsets[dir_key][1])\n",
        "      U.append(direction_vectors[dir_key][0])\n",
        "      V.append(direction_vectors[dir_key][1])\n",
        "\n",
        "      if deterministic:\n",
        "        A.append(1 if prob == max(probs.values()) else 0)\n",
        "      else:\n",
        "        A.append(prob)\n",
        "\n",
        "  linewidth = 1.5 if deterministic else 0.5\n",
        "  scale = 15 if deterministic else 30\n",
        "\n",
        "  ax.quiver(C, R, U, V, alpha=A, color=critter_color,\n",
        "            scale=scale, linewidth=linewidth)\n",
        "  return fig, ax, custom_leg_handles\n",
        "\n",
        "\n",
        "def make_grid(num_rows, num_cols, figsize=(7,6), title=None):\n",
        "  \"\"\"Plots an n_rows by n_cols grid with cells centered on integer indices and\n",
        "  returns fig and ax handles for further use\n",
        "  Args:\n",
        "    num_rows (int): number of rows in the grid (vertical dimension)\n",
        "    num_cols (int): number of cols in the grid (horizontal dimension)\n",
        "\n",
        "  Returns:\n",
        "    fig (matplotlib.figure.Figure): figure handle for the grid\n",
        "    ax: (matplotlib.axes._axes.Axes): axes handle for the grid\n",
        "  \"\"\"\n",
        "  # Create a new figure and axes with given figsize\n",
        "  fig, ax = plt.subplots(figsize=figsize, layout='constrained')\n",
        "  # Set width and height padding, remove horizontal and vertical spacing\n",
        "  fig.get_layout_engine().set(w_pad=4 / 72, h_pad=4 / 72, hspace=0, wspace=0)\n",
        "  # Show right and top borders (spines) of the plot\n",
        "  ax.spines[['right', 'top']].set_visible(True)\n",
        "  # Set major ticks (where grid lines will be) on x and y axes\n",
        "  ax.set_xticks(np.arange(0, num_cols, 1))\n",
        "  ax.set_yticks(np.arange(0, num_rows, 1))\n",
        "  # Set labels for major ticks with font size of 8\n",
        "  ax.set_xticklabels(np.arange(0, num_cols, 1),fontsize=8)\n",
        "  ax.set_yticklabels(np.arange(0, num_rows, 1),fontsize=8)\n",
        "  # Set minor ticks (no grid lines here) to be between major ticks\n",
        "  ax.set_xticks(np.arange(0.5, num_cols-0.5, 1), minor=True)\n",
        "  ax.set_yticks(np.arange(0.5, num_rows-0.5, 1), minor=True)\n",
        "  # Move x-axis ticks to the top of the plot\n",
        "  ax.xaxis.tick_top()\n",
        "  # Set grid lines based on minor ticks, make them grey, dashed, and half transparent\n",
        "  ax.grid(which='minor', color='grey', linestyle='-', linewidth=2, alpha=0.5)\n",
        "  # Remove minor ticks (not the grid lines)\n",
        "  ax.tick_params(which='minor', bottom=False, left=False)\n",
        "  # Set limits of x and y axes\n",
        "  ax.set_xlim(( -0.5, num_cols-0.5))\n",
        "  ax.set_ylim(( -0.5, num_rows-0.5))\n",
        "  # Invert y axis direction\n",
        "  ax.invert_yaxis()\n",
        "  # If title is provided, set it as the figure title\n",
        "  if title is not None:\n",
        "    fig.suptitle(title)\n",
        "  # Hide header and footer, disable toolbar and resizing of the figure\n",
        "  fig.canvas.header_visible = False\n",
        "  fig.canvas.toolbar_visible = False\n",
        "  fig.canvas.resizable = False\n",
        "  fig.canvas.footer_visible = False\n",
        "  # Redraw the figure with these settings\n",
        "  fig.canvas.draw()\n",
        "  # Return figure and axes handles for further customization\n",
        "  return fig, ax\n",
        "\n",
        "\n",
        "def plot_food(fig, ax, rc_food_loc, food=None, size=None,\n",
        "              show_food=True):\n",
        "  \"\"\"\n",
        "  Plots \"food\" on a grid implied by the given fig, ax arguments\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects\n",
        "    rc_food_loc: ndarry(int) of shape (N:num_food x 2:row,col)\n",
        "    food: a handle for the existing food matplotlib PatchCollection object\n",
        "    if one exists\n",
        "  Returns:\n",
        "    a handle for matplotlib PathCollection object of food scatter plot, either\n",
        "    new if no handle was passed or updated if it was\n",
        "  \"\"\"\n",
        "  # if no PathCollection handle passed in:\n",
        "  if size is None:\n",
        "    size=150\n",
        "  if food is None:\n",
        "    food = ax.scatter([], [], s=size, marker='o',\n",
        "                      color='red', label='Food')\n",
        "  if show_food:\n",
        "    rc_food_loc = np.array(rc_food_loc, dtype=int)\n",
        "    #matrix indexing convention is is [row-vertical, col-horizontal]\n",
        "    #plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
        "    food.set_offsets(np.fliplr(rc_food_loc))\n",
        "  return food\n",
        "\n",
        "\n",
        "def plot_critters(fig, ax, critter_specs: List[Dict[str, object]],\n",
        "                  size=None) -> List[Dict[str, object]]:\n",
        "  \"\"\"\n",
        "  Plots multiple types of \"critters\" on a grid implied by the given\n",
        "  fig, ax arguments.\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects.\n",
        "    critter_specs: List of dictionaries with keys 'location', 'name', 'color',\n",
        "    'marker', 'int_id', 'rc_critter_loc' and optionally 'handle' for each\n",
        "    critter.\n",
        "\n",
        "  Returns:\n",
        "    Updated critter_specs with handles.\n",
        "  \"\"\"\n",
        "  if size is None:\n",
        "    size=250\n",
        "  for spec in critter_specs:\n",
        "    # Ensure required keys are present\n",
        "    for key in ['marker', 'color', 'name', 'rc_loc']:\n",
        "      if key not in spec:\n",
        "        raise ValueError(f\"Key '{key}' missing in critter spec.\")\n",
        "    handle_ = spec.get('handle')\n",
        "    if handle_ is None:\n",
        "      handle_ = ax.scatter([], [], s=size, marker=spec['marker'],\n",
        "                           color=spec['color'], label=spec['name'],\n",
        "                           edgecolors='white', linewidths=1)\n",
        "    handle_.set_offsets(np.flip(spec['rc_loc']))\n",
        "    spec.update({'handle': handle_})\n",
        "  return critter_specs\n",
        "\n",
        "\n",
        "def plot_critter(fig, ax, rc_critter_loc,\n",
        "                 critter=None, critter_name='Critter'):\n",
        "  \"\"\"\n",
        "  Plots \"critter\" on a grid implied by the given fig, ax arguments\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects\n",
        "    rc_critter_loc: ndarry(int) of shape (N:num_critters x 2:row,col)\n",
        "    critter: a handle for the existing food matplotlib PatchCollection object\n",
        "    if one exists\n",
        "  Returns:\n",
        "    a handle for matplotlib PathCollection object of critter scatter plot,\n",
        "    either new if no handle was passed in or updated if it was.\n",
        "  \"\"\"\n",
        "  if critter is None:\n",
        "    critter = ax.scatter([], [], s=250, marker='h',\n",
        "                         color='blue', label=critter_name)\n",
        "  # matrix indexing convention is is [row-vertical, col-horizontal]\n",
        "  # plotting indexing convention is (x-horizontal,y-vertical), hence flip\n",
        "  critter.set_offsets(np.flip(rc_critter_loc))\n",
        "  return critter\n",
        "\n",
        "\n",
        "def plot_fov(fig, ax, rc_critter, n_rows, n_cols, radius, has_fov,\n",
        "             opaque=False, fov=None):\n",
        "  \"\"\"\n",
        "  Plots a mask on a grid implied by the given fig, ax arguments\n",
        "\n",
        "  Args:\n",
        "    fig, ax: matplotlib figure and axes objects\n",
        "    rc_critter: ndarry(int) (row,col) of the critter\n",
        "    mask: a handle for the existing mask matplotlib Image object if one exists\n",
        "  Returns:\n",
        "    a handle for matplotlib Image object of mask, either new if no handle\n",
        "    was passed in or updated if it was.\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize mask as a semi-transparent overlay for the entire grid\n",
        "  mask_array = np.ones((n_rows, n_cols, 4))\n",
        "  mask_array[:, :, :3] = 0.5  # light grey color\n",
        "  if has_fov == True:\n",
        "    if opaque:\n",
        "      mask_array[:, :, 3] = 1.0  # 50% opacity\n",
        "    else:\n",
        "      mask_array[:, :, 3] = 0.5  # 50% opacity\n",
        "    # Create arrays representing the row and column indices\n",
        "    rows = np.arange(n_rows)[:, np.newaxis]\n",
        "    cols = np.arange(n_cols)[np.newaxis, :]\n",
        "    # Iterate over each critter location\n",
        "    dist = np.abs(rows - rc_critter[0]) + np.abs(cols - rc_critter[1])\n",
        "    # Set the region within the specified radius around the critter to transparent\n",
        "    mask_array[dist <= radius, 3] = 0\n",
        "  else:\n",
        "    mask_array[:, :, 3] = 0\n",
        "\n",
        "  if fov is None:\n",
        "    fov = ax.imshow(mask_array, origin='lower', zorder=2)\n",
        "  else:\n",
        "    fov.set_data(mask_array)\n",
        "\n",
        "  return fov\n",
        "\n",
        "\n",
        "def remove_ip_clutter(fig):\n",
        "  fig.canvas.header_visible = False\n",
        "  fig.canvas.toolbar_visible = False\n",
        "  fig.canvas.resizable = False\n",
        "  fig.canvas.footer_visible = False\n",
        "  fig.canvas.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPynoewPlnqz"
      },
      "source": [
        "# 1.4.1.1 Foraging in a patchy environment, two flavours.\n",
        "\n",
        "In Seqeunce 1.2.3 on Normative thinking we introduced a patchy foraging game. The game from that sequence (vanilla here) is playable below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "MCCqjDSOZpjv"
      },
      "outputs": [],
      "source": [
        "# @title Omniscient Patchy Foraging - Vanilla\n",
        "# @markdown **Run this cell** to play the patchy foraging game.\n",
        "rng = np.random.default_rng(1)\n",
        "pfg = PatchyForagingGame(max_moves_taken=20, food_patch_prob=0.3,\n",
        "                         forage_success_prob=0.6, food_extinct_prob=0.2,\n",
        "                         moves_cost=False, end_prob=0, rng=rng)\n",
        "omni_ipfg = InteractivePatchyForage(pfg, show_food=True, show_misses=True,\n",
        "                                    figsize=(4,5))\n",
        "display(omni_ipfg.b_fig.canvas)\n",
        "clear_output()\n",
        "display(omni_ipfg.final_display)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "zLfvxRqHZpjv"
      },
      "outputs": [],
      "source": [
        "# @title Omniscient Patchy Foraging - Chocolate\n",
        "# @markdown **Run this cell** to play the a slight variation on the previous patchy foraging game.\n",
        "rng = np.random.default_rng(1)\n",
        "pfg = PatchyForagingGame(max_moves_taken=20, food_patch_prob=0.3,\n",
        "                         forage_success_prob=0.6, food_extinct_prob=0.2,\n",
        "                         moves_cost=True, end_prob=0, rng=rng)\n",
        "omni_ipfg = InteractivePatchyForage(pfg, show_food=True, show_misses=True,\n",
        "                                    figsize=(4,5))\n",
        "display(omni_ipfg.b_fig.canvas)\n",
        "clear_output()\n",
        "display(omni_ipfg.final_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9wQn5SipZpjw"
      },
      "source": [
        "Can you spot the difference between the two variants? Hint: Look at what happens to the number of rounds left when you make a move (up, down, left, right) in each variant.\n",
        "\n",
        "When we can see the food patches and tell when they have been exhausted, the optimal policy is similar in both scenarios. Move to a patch with food forager there until food is exhausted, then move on to the next patch with food. In the variant where moves are costly, some care needs to be taken so that patches with food are navagated to efficiently, much like in our earliest Gridworld foraging problems. But other than this requirement on efficient movement between patches, the decision about when to move on is identical when the state of food patches is known. However, in sequence 1.2.3 we focused on a more complex situation, where the presence or absence of food at a location was not immediately detectable, but rather could only be inferred from the recent history of foraging successes and failures at a given patch.\n",
        "\n",
        "You can try out that variant of the game below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "8O7ypmOiZpjw"
      },
      "outputs": [],
      "source": [
        "# @title Cryptic Patchy Foraging - Moves Cost\n",
        "# @markdown **Run this cell** to play the patchy foraging game with cryptic patches and movement has an opportunity cost.\n",
        "rng = np.random.default_rng(3)\n",
        "pfg = PatchyForagingGame(max_moves_taken=20, food_patch_prob=0.3,\n",
        "                         forage_success_prob=0.6, food_extinct_prob=0.2,\n",
        "                         end_prob=0, moves_cost=True, rng=rng)\n",
        "cryptic_ipfg = InteractivePatchyForage(pfg, show_food=False,\n",
        "                                       show_misses=True,\n",
        "                                       figsize=(4,5))\n",
        "display(cryptic_ipfg.b_fig.canvas)\n",
        "clear_output()\n",
        "display(cryptic_ipfg.final_display)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "3-fDTlkdZpjw"
      },
      "source": [
        "While the variant where moves are free and the variant where moves have an opportunity cost are very similar in many ways, this slight difference, means that the approach we used to determine an optimal policy (behaviour rule) for one variant will not work for the other.\n",
        "\n",
        "Consider, when movement between patches has no opportunity cost, all an optimal organism needs to worry about is foraging at a patch that has the highest possible probability of having food present. If that happens to be the patch the organism currently occupies, great, forage there, but if not, no problem, movement is free in some sense, so just move on to a fresh patch if the foraging odds are better there. In contrast, more nuance is required when time spent moving between patches uses up time that could have been spent foraging, i.e. when movement has an opportunity cost. To see this think about what happens when there is a single round left in the foraging episode. When movement is costly is there any situation where movement is preferable to foraging at the current patch on this last round? No, there is always some chance of success (from the forager's perspective) at the current patch, but there is zero chance of foraging success when moving, so an optimal forager would never move on the last round.\n",
        "\n",
        "Things are certainly more complicated things, but the kind of thinking we applied in sequence 1.2.3 can be extended to find an optimal policy for this new, trickier problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "V7anPtSuZpjw"
      },
      "source": [
        "# 1.4.1.2 Reasoning About The Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-I5zBeafZpjx"
      },
      "source": [
        "## **Defining the Problem**\n",
        "First we need define our model problem precisely.\n",
        "\n",
        "* **Patchy Environment**: The foraging environment consists of discrete patches (represented as grid cells). At the start of the simulation each patch has a probability $p_e \\in (0,1)$ of containing food. The forager starts at a fresh patch.\n",
        "\n",
        "* **Possible Actions**: In each turn, the organism has two options:\n",
        "  - Try to forage at its current patch.\n",
        "  - Move to a new patch.\n",
        "\n",
        "* **Foraging Success**: When a patch contains food, foraging is often successful but not always guaranteed. In this model, foraging at a patch with food is successful with probability $p_s \\in (0,1)$. Conversely, foraging on a food-less patch is certain to be unsuccessful.\n",
        "\n",
        "* **Patch Exhaustion**: After each foraging success, there is a probability $p_x \\in (0,1)$ that the patch becomes exhausted. In such cases, the patch won't provide any more food.\n",
        "\n",
        "* **Session Limit**: The foragers can take a fixed number of actions, $T$, before the session end. Both move actions and foraging attemp actions count towards this limit.\n",
        "\n",
        "* **Rewards**: Every successful foraging attempt gives the organism a reward of 1 point. If the foraging attempt is unsuccessful, no points are awarded for that round. Similarly, if the organism moves, no points are awarded. We denote the reward received on round $t$ as $R_t$.\n",
        "\n",
        "* **Goal**: The overarching objective for the organism is to maximize its *expected cumulative reward* over the entire session. Formally, the forager aims to maximize:\n",
        "$$\n",
        "\\mathbb{E}\\left[ \\sum_{t=1}^{T} R_t \\right] = \\sum_{t=1}^{T} \\mathbb{E}\\left[ R_t \\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "HILMu54_Zpjx"
      },
      "source": [
        "Previously we kind of glossed over where exactly the actions of the forager were applied in the decision making process. We made simplifying assumptions (implicitly! Yikes!) that of course the forager would forage at a newly arrived patch (why else would it have moved these), but for a moment we're going to leave that aside and be as totally verbose and explicit as possible about all the different things that can happen. Just as a result of the foragers very first action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "3W9us168Zpjx"
      },
      "outputs": [],
      "source": [
        "def latex_to_png(latex_str, file_path, dpi, fontsize, figsize):\n",
        "  \"\"\"Convert a LaTeX string to a PNG image.\"\"\"\n",
        "  fig, ax = plt.subplots(figsize=figsize)\n",
        "  ax.text(0.5, 0.5, f\"${latex_str}$\", size=fontsize, ha='center', va='center')\n",
        "  ax.axis(\"off\")\n",
        "  #plt.tight_layout()\n",
        "  plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "  plt.savefig(file_path, dpi=dpi, bbox_inches='tight', transparent=True, pad_inches=0.02)\n",
        "  plt.close()\n",
        "\n",
        "def add_latex_edge_labels(graph, edge_labels, dpi=150, fontsize=16, figsize=(0.4,0.2)):\n",
        "  \"\"\"Add LaTeX-rendered images as edge labels using the dummy node approach.\"\"\"\n",
        "  for edge in edge_labels:\n",
        "    src, dest, latex_str = edge\n",
        "    if graph.has_edge(src, dest):\n",
        "      img_path = f\"{src}_to_{dest}_{latex_str}.png\"\n",
        "      latex_to_png(latex_str, img_path, dpi=dpi, fontsize=fontsize, figsize=figsize)\n",
        "      dummy_node_name = f\"dummy_{src}_{dest}_{latex_str}\"\n",
        "      graph.add_node(dummy_node_name, shape=\"box\", image=img_path, label=\"\", color=\"green\")\n",
        "      graph.delete_edge(src, dest)\n",
        "      graph.add_edge(src, dummy_node_name, dir=\"none\", weight=10)\n",
        "      graph.add_edge(dummy_node_name, dest, dir=\"forward\", weight=10)\n",
        "  return graph\n",
        "\n",
        "def set_regular_node_sizes(graph, width=1.0, height=1.0):\n",
        "  \"\"\"Set the size of regular nodes (excluding dummy label nodes).\"\"\"\n",
        "  for node in graph.nodes():\n",
        "    if not node.startswith(\"dummy\"):\n",
        "      node.attr['width'] = width\n",
        "      node.attr['height'] = height\n",
        "  return graph\n",
        "\n",
        "\n",
        "def create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
        "                            action_nodes = [],\n",
        "                            node_colors = {},\n",
        "                            node_labels = {},\n",
        "                            output_path=\"graphviz_output.png\", dpi=300,\n",
        "                            figsize=(0.6, 0.3), fontsize=16):\n",
        "  \"\"\"\n",
        "  Create a graph with given nodes, edges, and LaTeX edge labels, then render and save it.\n",
        "\n",
        "  Parameters:\n",
        "    nodes_list (list): List of nodes in the graph.\n",
        "    edges_list (list): List of edges in the graph.\n",
        "    latex_edge_labels (list): List of tuples containing edge and its LaTeX label.\n",
        "    output_path (str): Path to save the rendered graph.\n",
        "    dpi (int): DPI for rendering the graph.\n",
        "    figsize (tuple): Figure size for the LaTeX labels.\n",
        "\n",
        "  Returns:\n",
        "    str: Path to the saved graph image.\n",
        "  \"\"\"\n",
        "  # Graph Creation and Configuration\n",
        "  G = pgv.AGraph(directed=True, strict=False, rankdir='LR', ranksep=0.5, nodesep=0.5)\n",
        "\n",
        "  # Add state and decision nodes\n",
        "  for node in nodes_list:\n",
        "    shape = \"box\" if node in action_nodes else \"ellipse\"  # Use 'box' for decision nodes\n",
        "    if node in action_nodes:\n",
        "      # action nodes are square and default to blue colour\n",
        "      color = node_colors.get(node, \"blue\")\n",
        "      shape = \"box\"\n",
        "    else:\n",
        "      # state nodes are round and default to black colour\n",
        "      shape = \"ellipse\"\n",
        "      color = node_colors.get(node, \"black\")\n",
        "    label = node_labels.get(node, node)\n",
        "    G.add_node(node, color=color, label=label, shape=shape)\n",
        "\n",
        "  for edge in edges_list:\n",
        "    G.add_edge(edge[0], edge[1])\n",
        "\n",
        "  # Set size for regular nodes and add LaTeX-rendered image labels to the edges\n",
        "  G = set_regular_node_sizes(G, width=1, height=1)\n",
        "  G = add_latex_edge_labels(G, latex_edge_labels, dpi=dpi, figsize=figsize, fontsize=fontsize)\n",
        "\n",
        "  # Additional graph attributes\n",
        "  G.graph_attr['size'] = \"8,8\"\n",
        "  G.graph_attr['dpi'] = str(dpi)\n",
        "\n",
        "  # Render and save the graph\n",
        "  G.layout(prog='dot')\n",
        "  G.draw(output_path)\n",
        "\n",
        "  return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "ABVVO6olZpjx"
      },
      "outputs": [],
      "source": [
        "# @markdown **Run This Cell** to visualize the decision tree\n",
        "\n",
        "\n",
        "nodes_list = [\n",
        "    \"New Patch\", \"Has Food0\", \"No Food0\", \"Didn't Find Food0\", \"Found the Food0\",\n",
        "    \"No Food to Find0\", \"Found Impossible Food0\",\n",
        "    \"Search (has food)0\", \"Leave (has food)0\",\n",
        "    \"Search (no food)0\", \"Leave (no food)0\",\n",
        "    \"Has Food1\", \"No Food1\", \"Has Food2\", \"No Food2\"\n",
        "]\n",
        "\n",
        "edges_list = [\n",
        "    (\"New Patch\", \"Has Food0\"), (\"New Patch\", \"No Food0\"),\n",
        "    (\"Has Food0\", \"Search (has food)0\"), (\"Has Food0\", \"Leave (has food)0\"),\n",
        "    (\"No Food0\", \"Search (no food)0\"), (\"No Food0\", \"Leave (no food)0\"),\n",
        "    (\"Search (has food)0\", \"Found the Food0\"), (\"Search (has food)0\", \"Didn't Find Food0\"),\n",
        "    (\"Search (no food)0\", \"Found Impossible Food0\"), (\"Search (no food)0\", \"No Food to Find0\"),\n",
        "    (\"Leave (has food)0\", \"Has Food1\"), (\"Leave (has food)0\", \"No Food1\"),\n",
        "    (\"Leave (no food)0\", \"Has Food2\"), (\"Leave (no food)0\", \"No Food2\"),\n",
        "]\n",
        "\n",
        "latex_edge_labels = [\n",
        "    (\"New Patch\", \"Has Food0\", \"p_e\"),\n",
        "    (\"New Patch\", \"No Food0\", \"1-p_e\"),\n",
        "    (\"Search (has food)0\", \"Didn't Find Food0\", \"1-p_s\"),\n",
        "    (\"Search (has food)0\", \"Found the Food0\", \"p_s\"),\n",
        "    (\"Search (no food)0\", \"No Food to Find0\", \"1\"),\n",
        "    (\"Search (no food)0\", \"Found Impossible Food0\", \"0\"),\n",
        "    (\"Leave (has food)0\", \"No Food1\", \"p_e\"),\n",
        "    (\"Leave (has food)0\", \"Has Food1\", \"1-p_e\"),\n",
        "    (\"Leave (no food)0\", \"No Food2\", \"p_e\"),\n",
        "    (\"Leave (no food)0\", \"Has Food2\", \"1-p_e\")\n",
        "]\n",
        "\n",
        "action_nodes = [\n",
        "    \"Search (has food)0\", \"Search (no food)0\", \"Leave (has food)0\", \"Leave (no food)0\"\n",
        "]\n",
        "\n",
        "node_colors = {\n",
        "    \"New Patch\": \"red\",\n",
        "    \"Has Food0\": \"red\",\n",
        "    \"No Food0\": \"red\",\n",
        "}\n",
        "\n",
        "node_labels = {\n",
        "    \"Has Food0\": \"New Patch\\nHas Food\",\n",
        "    \"No Food0\": \"New Patch Has\\nNo Food\",\n",
        "    \"Has Food1\": \"New Patch\\nHas Food\",\n",
        "    \"No Food1\": \"New Patch Has\\nNo Food\",\n",
        "    \"Has Food2\": \"New Patch\\nHas Food\",\n",
        "    \"No Food2\": \"New Patch Has\\nNo Food\",\n",
        "    \"Search (has food)0\": \"Search\",\n",
        "    \"Leave (has food)0\": \"Leave\",\n",
        "    \"Search (no food)0\": \"Search\",\n",
        "    \"Leave (no food)0\": \"Leave\",\n",
        "    \"Didn't Find Food0\": \"No Food Found\",\n",
        "    \"No Food to Find0\": \"No Food Found\",\n",
        "    \"Found the Food0\": \"Food Found\\n+1 Reward Point\",\n",
        "    \"Found Impossible Food0\": \"Food Found\\n+1 Reward Point\"\n",
        "}\n",
        "\n",
        "\n",
        "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
        "                                      action_nodes=action_nodes,\n",
        "                                      node_colors=node_colors,\n",
        "                                      node_labels=node_labels)\n",
        "Image(output_path, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "SQEC9fPRZpjx"
      },
      "source": [
        "In this diagram rounded nodes represent states of this process, i.e. the situation the organism is in with respect to the environment. Blue squares represent actions taken by the organism, and yellow squares give the probability of transitioning from the previous state, to the next state, given the action the organism took. These transitions can also be thought of as actions taken by the environment. This the full expansion, but looking at this we can see that if the organism leaves a patch, it doesn't matter whether or not there was food there, the state of the new patch is unaffected by this so we can already simplify this slightly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "fHzapqleZpjx"
      },
      "outputs": [],
      "source": [
        "# @markdown **Run This Cell** to visualize the decision tree\n",
        "\n",
        "\n",
        "nodes_list = [\n",
        "  \"New Patch\", \"Has Food0\", \"No Food0\", \"Didn't Find Food0\", \"Found the Food0\",\n",
        "  \"No Food to Find0\", \"Found Impossible Food0\",\n",
        "  \"Search (has food)0\", \"Leave0\",\n",
        "  \"Search (no food)0\",\n",
        "  \"Has Food1\", \"No Food1\"\n",
        "]\n",
        "\n",
        "edges_list = [\n",
        "  (\"New Patch\", \"Has Food0\"), (\"New Patch\", \"No Food0\"),\n",
        "  (\"Has Food0\", \"Search (has food)0\"), (\"Has Food0\", \"Leave0\"),\n",
        "  (\"No Food0\", \"Search (no food)0\"), (\"No Food0\", \"Leave0\"),\n",
        "  (\"Search (has food)0\", \"Found the Food0\"), (\"Search (has food)0\", \"Didn't Find Food0\"),\n",
        "  (\"Search (no food)0\", \"Found Impossible Food0\"), (\"Search (no food)0\", \"No Food to Find0\"),\n",
        "  (\"Leave0\", \"Has Food1\"), (\"Leave0\", \"No Food1\"),\n",
        "]\n",
        "\n",
        "latex_edge_labels = [\n",
        "  (\"New Patch\", \"Has Food0\", \"p_e\"),\n",
        "  (\"New Patch\", \"No Food0\", \"1-p_e\"),\n",
        "  (\"Search (has food)0\", \"Didn't Find Food0\", \"1-p_s\"),\n",
        "  (\"Search (has food)0\", \"Found the Food0\", \"p_s\"),\n",
        "  (\"Search (no food)0\", \"No Food to Find0\", \"1\"),\n",
        "  (\"Search (no food)0\", \"Found Impossible Food0\", \"0\"),\n",
        "  (\"Leave0\", \"No Food1\", \"p_e\"),\n",
        "  (\"Leave0\", \"Has Food1\", \"1-p_e\"),\n",
        "]\n",
        "\n",
        "action_nodes = [\n",
        "  \"Search (has food)0\", \"Search (no food)0\", \"Leave0\"\n",
        "]\n",
        "\n",
        "node_colors = {\n",
        "    \"New Patch\": \"red\",\n",
        "    \"Has Food0\": \"red\",\n",
        "    \"No Food0\": \"red\",\n",
        "}\n",
        "\n",
        "node_labels = {\n",
        "    \"Has Food0\": \"New Patch\\nHas Food\",\n",
        "    \"No Food0\": \"New Patch Has\\nNo Food\",\n",
        "    \"Has Food1\": \"New Patch\\nHas Food\",\n",
        "    \"No Food1\": \"New Patch Has\\nNo Food\",\n",
        "    \"Search (has food)0\": \"Search\",\n",
        "    \"Leave0\": \"Leave\",\n",
        "    \"Search (no food)0\": \"Search\",\n",
        "    \"Didn't Find Food0\": \"No Food Found\",\n",
        "    \"No Food to Find0\": \"No Food Found\",\n",
        "    \"Found the Food0\": \"Food Found\\n+1 Reward Point\",\n",
        "    \"Found Impossible Food0\": \"Food Found\\n+1 Reward Point\"\n",
        "}\n",
        "\n",
        "\n",
        "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
        "                                      action_nodes=action_nodes,\n",
        "                                      node_colors=node_colors,\n",
        "                                      node_labels=node_labels)\n",
        "Image(output_path, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "LgCbXrC9Zpjy"
      },
      "source": [
        "Similarly we can remove the zero probability event\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "y9nFQk2SZpjy"
      },
      "outputs": [],
      "source": [
        "nodes_list = [\n",
        "  \"New Patch\", \"Has Food0\", \"No Food0\", \"Didn't Find Food0\", \"Found the Food0\",\n",
        "  \"No Food to Find0\",\n",
        "  \"Search (has food)0\",\n",
        "  \"Search (has food)1\"\n",
        "  \"Leave0\",\n",
        "  \"Leave1\"\n",
        "  \"Search (no food)0\",\n",
        "  \"Search (no food)1\",\n",
        "  \"Has Food1\", \"No Food1\",\n",
        "  \"Search (didn't find food)0\",\n",
        "  \"Search (found the food)0\",\n",
        "  \"Search (no food to find)0\",\n",
        "  \"Leave (didn't find food)0\",\n",
        "  \"Leave (found the food)0\",\n",
        "  \"Leave (no food to find)0\",\n",
        "]\n",
        "\n",
        "edges_list = [\n",
        "  (\"New Patch\", \"Has Food0\"), (\"New Patch\", \"No Food0\"),\n",
        "  (\"Has Food0\", \"Search (has food)0\"), (\"Has Food0\", \"Leave0\"),\n",
        "  (\"No Food0\", \"Search (no food)0\"), (\"No Food0\", \"Leave0\"),\n",
        "  (\"Search (has food)0\", \"Found the Food0\"), (\"Search (has food)0\", \"Didn't Find Food0\"),\n",
        "  (\"Search (no food)0\", \"No Food to Find0\"),\n",
        "  (\"Leave0\", \"Has Food1\"), (\"Leave0\", \"No Food1\"),\n",
        "]\n",
        "\n",
        "latex_edge_labels = [\n",
        "  (\"New Patch\", \"Has Food0\", \"p_e\"),\n",
        "  (\"New Patch\", \"No Food0\", \"1-p_e\"),\n",
        "  (\"Search (has food)0\", \"Didn't Find Food0\", \"1-p_s\"),\n",
        "  (\"Search (has food)0\", \"Found the Food0\", \"p_s\"),\n",
        "  (\"Search (no food)0\", \"No Food to Find0\", \"1\"),\n",
        "  (\"Leave0\", \"No Food1\", \"p_e\"),\n",
        "  (\"Leave0\", \"Has Food1\", \"1-p_e\"),\n",
        "]\n",
        "\n",
        "action_nodes = [\n",
        "  \"Search (has food)0\", \"Search (no food)0\", \"Leave0\"\n",
        "]\n",
        "\n",
        "node_colors = {}\n",
        "\n",
        "node_labels = {\n",
        "    \"Has Food0\": \"New Patch\\nHas Food\",\n",
        "    \"No Food0\": \"New Patch Has\\nNo Food\",\n",
        "    \"Has Food1\": \"New Patch\\nHas Food\",\n",
        "    \"No Food1\": \"New Patch Has\\nNo Food\",\n",
        "    \"Search (has food)0\": \"Search\",\n",
        "    \"Leave0\": \"Leave\",\n",
        "    \"Search (no food)0\": \"Search\",\n",
        "    \"Didn't Find Food0\": \"No Food Found\",\n",
        "    \"No Food to Find0\": \"No Food Found\",\n",
        "    \"Found the Food0\": \"Food Found\\n+1 Reward Point\",\n",
        "}\n",
        "\n",
        "\n",
        "output_path = create_and_render_graph(nodes_list, edges_list, latex_edge_labels,\n",
        "                                      action_nodes=action_nodes,\n",
        "                                      node_colors=node_colors,\n",
        "                                      node_labels=node_labels)\n",
        "Image(output_path, height=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "kOH6cO5-Zpjy"
      },
      "source": [
        "# MDP Notation\n",
        "To get precise about what we are trying to optimize we first need to introduce some important notation, and formalize many of the general concepts introduced earlier in the book through our Gridworld example. If you are already farmiliar with these ideas feel free to skip this bit. Similarly if you find mathematical notation a bit overwhelming, you can also skim this section, (don't worry about understanding it all right away) and then use this as glossary as needed. A shortened version of these definitions also appear in the glossary/notation reference section found at the end of each notebook.\n",
        "\n",
        "* $\\pi_{\\theta}(a|s)$: **Policy Function** - A policy is the behavioural blueprint for the organism. It's a function that takes (some representation or filtered down aspect of) the environmental state $s$ as input, and guided by its parameters $\\theta$, gives the probability of taking action $a$, where $a$ is in the set $\\mathcal{A}(s)$ of possible actions given state $s$. The organism can then sample an actio from this set according to these probabilities. Sometimes the explicit reference to $\\theta$ is dropped when it is clear from context or does not need to be emphasized as in $\\pi(a|s)$, other times the the reference to the parameters is made more explicit by writing $\\pi(a | s, \\theta)$. In our Gridworld example each of the organisms we defined, 'Random Valid', 'Parameterized Weights', 'Eat When Near' all had a policy function at their core.\n",
        "\n",
        "* $s$: **A State** - The state represents a complete snapshot of what the environment looks like at a given moment. In our Gridworld example this is primarily the positions of food pieces and the organism, but also the number of rounds left in the simulation. The set of all possible states is denoted $\\mathcal{S}$.\n",
        "\n",
        "* $a$: **An Action** - The action an organism takes. Depending on how things are set up in our Gridworld example this might be represented as a direction or as a (row, columns) coordinate of the organism's new position, or as a flattened boolean index of the organism's new position. The set of all possible actions is denoted $\\mathcal{A}$, and the set of possible actions in a given state as $\\mathcal{A}(s)$.\n",
        "\n",
        "* $r$ : **A Reward** - The immediate reward (feedback, score, points etc.) an organism recieves after taking an action $a$ in state $s$ and transitioning to new state $s'$. In our Gridworld example $r = 1$ if the organism eats a food piece as a result of its move and $r = 0$ otherwise.\n",
        "\n",
        "* $\\theta$: **Parameters** - The aspects of an organism's policy function that can be represented by numbers. Note that these do not describe the overall structure of the policy function, but rather determine a particular instance of the policy functions possible *given* the structure (archietecture) of a policy function. In our 'Parameterized Weights' policy from our Gridworld example, the connective weight strengths $W$ are the paramweters, i.e. $\\theta = W$ in for this particular policy. For a more complicated policy with many layers of connective weights we might write $\\theta = \\{W_1, W_2,\\dots, W_N \\}$. We use $\\theta$ as a generic term so that we can make general statements about parameterized policies without having to worry about the particular archiectecure or functional form of the policy.   \n",
        "\n",
        "Given the stochastic nature of the environment (and often the policy as well), at any given time $t$ over the course of a simulation run, each of states, actions and rewards can be thought of as random variables specifically:\n",
        "\n",
        "* $S_t$: **State at Time $t$** - A random variable that denotes the state of the environment at a specific time $t$. For example, $S_t = s$ means that at time $t$, in a particular simulation run, the environment was in state $s$, or in other words that $s$ is the realization of the random variable $S_t$.\n",
        "\n",
        "* $A_t$: **Action at Time $t$** - A random variable denoting the action taken by the organism at time $t$. $A_t = a$ indicates that the action $a$ is taken at time $t$, or that $a$ is the realization of the random variable $A_t$ in a particular simulation run.\n",
        "\n",
        "* $R_t$: **Reward at Time $t$** - A random variable indicating the immediate reward received by the organism at time $t$. $R_t = r$ indicates that the reward $r$ is obtained at time $t$, or in that $r$ is the realization of the random variable $R_t$ in a particular simulation run.\n",
        "\n",
        "* $T$: **Total Simulation Time** - The total number of time steps in a given simulation. There are cases where having an infinite time horizon, $T=\\infty$, is a mathematical convenience, but since our focus is on evolved, living and learning systems, and few things live forever, we will typically work with a finite time horizons.\n",
        "\n",
        "* $t$: **Time-Step Index** - We typically subscript with $t$ to denote the value of a state, reward, action, etc. at a given specific time $t$.\n",
        "\n",
        "We can then think of simulation run as sequence of random variables:\n",
        "$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, \\dots, S_{T-1}, A_{T-1}, R_{T}, {S_T}$$\n",
        "\n",
        "The dynamics, or equations of motion, that generate this sequence of random variables are primarily encapsulated in a *transition function*, together with an *initial state distribution*, both defined as follows.\n",
        "\n",
        "* $p(s', r | s, a)$: **Transition Function** - Sometimes called the *State Transition Function*, or the *Transition Kernel* (kernel is more common when dealing with continuous state spaces) this function give the probability of transitioning from state $s$ to $s'$ and recieving reward $r$ from time-step $t$ to $t+1$, given that action $a$ is taken at time $t$. In terms of our previous notation this is defined as:\n",
        "$$ p(s', r | s, a) := \\Pr \\{S_{t+1} = s' , R_{t+1} = r | S_t = s, A_t = a \\}$$  \n",
        "\n",
        "* $p_0(s)$: **Initial State Distribution** - This is the probability distribution (density function) over the set of possible states, \\mathcal{S}, so $p_0(s) := \\Pr \\{S_0 = s\\}$. Sometimes we write $S_0 \\sim p_0$, which is read as 'The random variable $S_0$ is distributed according to the probability density function $p_0$'.\n",
        "\n",
        "This random variable notation also allows us to make uur definition of a policy function more precise: $$\\pi_{\\theta}(a | s) := \\Pr \\{A_{t} = a | S_t = s\\}.$$\n",
        "\n",
        "Then, if a policy is fixed it can simply be folded into the dynamics of the environment, creating what is refered to as the *policy-induced dynamics*.\n",
        "\n",
        "* $p_\\pi(s', r | s)$: **Policy-Induced Dynamics** - This is also called the 'dynamics under policy $\\pi$' and is defined as:\n",
        "$$p_\\pi(s', r | s):= \\Pr \\{S_{t+1} = s' , R_{t+1} = r | S_t = s, \\pi \\} = \\sum_{a\\in\\mathcal{A}(s)} \\pi_\\theta (a | s) \\ p(s', r | s, a).$$\n",
        "Sometimes the depedence on a specific policy, $\\pi$, is taken as implicit and we simply write $p(s', r | s)$.\n",
        "\n",
        "The takeaway here is that for a fixed $\\pi$ and a given transition function $p$ (and initial state distribution $p_0$) the stochastic dynamics of the system are completely determined.\n",
        "\n",
        "With all that defined we can start to formally describe how rewards should be added up over time to define our goals. We just need to introduce the idea of a *Return* and a *Value* function.\n",
        "\n",
        "* $G_t$: **Return following time $t$** - Sometimes called the reward to go, or simply the return, this a random variable that indicates the total reward yet to be realized after time $t$, i.e. $G_t := \\sum_{k=t+1}^T R_k$.\n",
        "\n",
        "* $v_{\\pi}(s,t)$: **Value Function** - A function giving the *expected* return conditional on being in state $s$ at time $t$ and following a given policy $\\pi$, specifically:\n",
        "$$v_{\\pi}(s,t) := \\mathbb{E}_\\pi \\left[G_t | S_t = s \\right].$$\n",
        "In a slight stretch of notation $t$ can be treated as part of $s$ and we can write $v_{\\pi}(s)$. The dependence on a specific policy is sometimes treated as implicit and we write $v(s)$ or $v(s,t)$.\n",
        "\n",
        "In this context then our goal is to maximize the *Expectation* of a simulation run, or equivalently the average value from playing through many simulations (in the limit as many --> $\\infty$). We call this formalization of our goal objective function and define our particular objective function in this context as\n",
        "\n",
        "* $J(\\theta)$: **Objective Function** - The function that we are trying to maximize, emphasizing the dependence on the parameters, $\\theta$.\n",
        "\n",
        "The objective function is in some ways the most subjective thing in this whole set up. It's what defines the \"problem to be solved\". In our particular case we we are going to use the following as our objective.\n",
        "\n",
        "$$J(\\theta):= \\mathbb{E}\\left[ v_{\\pi_\\theta}(S_0) \\right] = \\sum_{s \\in \\mathcal{S}} p_0(s) \\cdot v_{\\pi_{\\theta}}(s)$$\n",
        "\n",
        "Then the formalization of our problem is choosing parameters $\\theta$ such that $J(\\theta)$ is as high as possible. In general this goal is written as:\n",
        "$$ \\max_{\\theta} J(\\theta),$$\n",
        "\n",
        "and in our particular case of maximizing the expected value, given a finite and discrete state space, our goal is written as:\n",
        "\n",
        "$$ \\max_\\theta\\sum_{s \\in \\mathcal{S}} p_0(s) \\cdot v_{\\pi_{\\theta}}(s)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "sDhxEIc3Zpjz"
      },
      "outputs": [],
      "source": [
        "# @markdown Submit your feedback\n",
        "content_review(f\"{feedback_prefix}_M3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "1cwpPN34Zpjz"
      },
      "source": [
        "# Quiz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "vv_eGTSUZpjz"
      },
      "outputs": [],
      "source": [
        "# @markdown **Run this cell** to take the quiz\n",
        "# @markdown **Run this cell** to take the quiz\n",
        "comprehension_quiz = [\n",
        "  {\n",
        "    \"question\": \"What is the impact of mutation on the evolutionary process?\",\n",
        "    \"type\": \"multiple_choice\",\n",
        "    \"answers\": [\n",
        "      {\n",
        "        \"answer\": \"It always increases the fitness of individuals in a population.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Mutation does not always increase fitness; it typically introduces neutral or deleterious variations. Beneficial mutations are rare.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"It introduces necessary variation to explore new genotypes.\",\n",
        "        \"correct\": True,\n",
        "        \"feedback\": \"Correct! Mutation is essential for introducing genetic variation, which allows populations to explore new genotypes and adapt over time.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"It decreases genetic diversity within a population.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Mutation actually increases genetic diversity by introducing new genetic variations.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"It reduces the population size over time.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Mutation itself does not necessarily reduce population size; it's the selection process that might influence population numbers based on the fitness effects of mutations.\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"How does evolution compare to a typical optimization algorithm?\",\n",
        "    \"type\": \"multiple_choice\",\n",
        "    \"answers\": [\n",
        "      {\n",
        "        \"answer\": \"Evolution has a clear termination condition when a global maximum is reached.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Evolution lacks a termination condition and does not stop even when high-fitness solutions are found.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"Evolution is a process that continually explores and exploits, without awareness of the global fitness landscape.\",\n",
        "        \"correct\": True,\n",
        "        \"feedback\": \"Exactly! Evolution continuously explores new genotypes and exploits current adaptations without a concept of the overall fitness landscape. In this way it is like a 'black-box' optimization algorithm\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"Evolutionary processes always find the globally optimal solution.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Evolution does not necessarily find global optima; it often settles on local maxima due to its hill-climbing nature.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"Evolution stops mutations once a sufficiently good solution is found.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Evolution does not stop mutating genotypes even after finding high-fitness solutions, which can lead to further, fitness reducing variation.\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"In the context of evolutionary simulations, what does a low mutation rate typically lead to?\",\n",
        "    \"type\": \"multiple_choice\",\n",
        "    \"answers\": [\n",
        "      {\n",
        "        \"answer\": \"A diverse population with many different genotypes coexisting.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"A low mutation rate usually results in less genetic diversity, not more.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"Rapid convergence to the global fitness peak.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Low mutation rates can lead to rapid convergence, but not necessarily to global peaksâ€”often to local ones.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"The population is generally dominated by a single variant.\",\n",
        "        \"correct\": True,\n",
        "        \"feedback\": \"Correct! Low mutation rates can lead to populations being dominated by a single, high-fitness variant.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"An increase in the number of harmful mutations.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"A low mutation rate means fewer mutations overall, not an increase in harmful ones specifically.\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What role do 'sticky attractors' play in evolutionary processes?\",\n",
        "    \"type\": \"multiple_choice\",\n",
        "    \"answers\": [\n",
        "      {\n",
        "        \"answer\": \"They prevent the population from reaching any kind of fitness peak.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Sticky attractors do not prevent the attainment of fitness peaks; they are the peaks where populations tend to stabilize.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"They represent states of low fitness that populations tend to avoid.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Sticky attractors are not low-fitness states; they are high-fitness states that populations are drawn to.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"They are high-fitness states in the genotype space that populations are likely to remain in for long periods.\",\n",
        "        \"correct\": True,\n",
        "        \"feedback\": \"Exactly! Sticky attractors are robust high-fitness states where populations tend to remain stable over time.\"\n",
        "      },\n",
        "      {\n",
        "        \"answer\": \"They are synonymous with global maxima in the fitness function.\",\n",
        "        \"correct\": False,\n",
        "        \"feedback\": \"Sticky attractors correspond to local maxima, not necessarily global maxima, in the fitness landscape.\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "\n",
        "display_quiz(comprehension_quiz)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "P1C4_Sequence1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}